---
layout: post
title: 'RL | 速读 IJCAI 2025 的强化学习论文'
date: "2025-10-09T00:39:14Z"
---
RL | 速读 IJCAI 2025 的强化学习论文
==========================

速读一下 IJCAI 2025 的 RL 相关论文。

  

论文列表
----

*   359 Multi-granularity Knowledge Transfer for Continual Reinforcement Learning - 为持续 RL 而设计的多粒度知识迁移
*   769 BILE: An Effective Behavior-based Latent Exploration Scheme for Deep Reinforcement Learning - BILE：一种有效的 Behavior-based 的 DRL latent 探索方案
*   908 Imagination-Limited Q-Learning for Offline Reinforcement Learning - 用于 offline RL 的想象力限制的 Q-Learning
*   2430 Self-Consistent Model-based Adaptation for Visual Reinforcement Learning - 为视觉 RL 而设计的自我一致的 model-based 的自适应
*   3591 Two-Stage Feature Generation with Transformer and Reinforcement Learning - 使用 Transformer 和强化学习进行两阶段特征生成
*   3621 PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning - PNAct：在 safe RL 中制作后门攻击
*   3768 Efficient Diversity-based Experience Replay for Deep Reinforcement Learning - 为 DRL 设计的 基于 diversity 的高效 experience replay
*   4744 Deduction with Induction: Combining Knowledge Discovery with Reasoning for Interpretable Deep Reinforcement Learning - 演论与归纳法：将知识发现与推理相结合，实现可解释的深度强化学习
*   4997 From End-to-end to Step-by-step: Learning to Abstract via Abductive Reinforcement Learning - 从 end-to-end 到 step-by-step：通过归纳（Abductive）强化学习 学习抽象
*   5103 Efficient Multi-view Clustering via Reinforcement Contrastive Learning - 通过强化对比学习，进行高效的多视图聚类

* * *

目录

*   [论文列表](#论文列表)
*   [359 Multi-granularity Knowledge Transfer for Continual Reinforcement Learning - 为持续 RL 而设计的多粒度知识迁移](#359-multi-granularity-knowledge-transfer-for-continual-reinforcement-learning---为持续-rl-而设计的多粒度知识迁移)
    *   [一、 研究背景与核心痛点（The Gap）](#一-研究背景与核心痛点the-gap)
    *   [二、 动机与故事线构建（Motivation & Narrative）](#二-动机与故事线构建motivation--narrative)
    *   [三、 审稿策略分析（Positioning Strategy）](#三-审稿策略分析positioning-strategy)
    *   [四、 方法合理性与技术细节（Method Justification）](#四-方法合理性与技术细节method-justification)
        *   [1\. 架构：分层协作（HRL Structure）](#1-架构分层协作hrl-structure)
        *   [2\. 知识迁移机制：策略库与符号化食谱](#2-知识迁移机制策略库与符号化食谱)
        *   [3\. 鲁棒性保障：闭环反馈（Closed-Loop Feedback）](#3-鲁棒性保障闭环反馈closed-loop-feedback)
*   [769 BILE: An Effective Behavior-based Latent Exploration Scheme for Deep Reinforcement Learning - BILE：一种有效的 Behavior-based 的 DRL latent 探索方案](#769-bile-an-effective-behavior-based-latent-exploration-scheme-for-deep-reinforcement-learning---bile一种有效的-behavior-based-的-drl-latent-探索方案)
    *   [零、介绍 \\(\\pi\\)\-Bisimulation metric](#零介绍--bisimulation-metric)
    *   [一、背景与挑战：高维稀疏环境下的探索困境](#一背景与挑战高维稀疏环境下的探索困境)
    *   [二、核心机制：隐向量 \\(\\mathbf{z}\\) 的作用与采样](#二核心机制隐向量--的作用与采样)
    *   [三、BILE 的关键技术创新：鲁棒的行为度量学习](#三bile-的关键技术创新鲁棒的行为度量学习)
        *   [3.1 度量的目标：价值多样性](#31--度量的目标价值多样性)
        *   [3.2 鲁棒性机制：引入预测误差](#32--鲁棒性机制引入预测误差)
    *   [四、BILE 与 METRA / ETD 的对比分析](#四bile-与-metra--etd-的对比分析)
        *   [关键差异总结：](#关键差异总结)
*   [908 Imagination-Limited Q-Learning for Offline Reinforcement Learning - 用于 offline RL 的想象力限制的 Q-Learning](#908-imagination-limited-q-learning-for-offline-reinforcement-learning---用于-offline-rl-的想象力限制的-q-learning)
    *   [一、引言：offline RL 的挑战与现有困境](#一引言offline-rl-的挑战与现有困境)
    *   [二、ILQ 的叙事核心：寻找“合理的乐观”](#二ilq-的叙事核心寻找合理的乐观)
    *   [三、核心方法：想象力受限 Bellman 算子 (ILB)](#三核心方法想象力受限-bellman-算子-ilb)
        *   [3.1 想象值 (\\(y^Q\_{img}\\))：提供合理的乐观基准](#31-想象值-提供合理的乐观基准)
        *   [3.2 限制值 (\\(y^Q\_{lmt}\\))：确保保守性与安全性](#32-限制值-确保保守性与安全性)
    *   [四、ILQ 与 Model-Based RL的关系：Model-Assisted 的 Model-Free RL](#四ilq-与-model-based-rl的关系model-assisted-的-model-free-rl)
    *   [五、结论与理论保障](#五结论与理论保障)
*   [2430 Self-Consistent Model-based Adaptation for Visual Reinforcement Learning - 为视觉 RL 而设计的自我一致的 model-based 的自适应](#2430-self-consistent-model-based-adaptation-for-visual-reinforcement-learning---为视觉-rl-而设计的自我一致的-model-based-的自适应)
*   [3591 Two-Stage Feature Generation with Transformer and Reinforcement Learning - 使用 Transformer 和强化学习进行两阶段特征生成](#3591-two-stage-feature-generation-with-transformer-and-reinforcement-learning---使用-transformer-和强化学习进行两阶段特征生成)
*   [3621 PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning - PNAct：在 safe RL 中制作后门攻击](#3621-pnact-crafting-backdoor-attacks-in-safe-reinforcement-learning---pnact在-safe-rl-中制作后门攻击)
*   [3768 Efficient Diversity-based Experience Replay for Deep Reinforcement Learning - 为 DRL 设计的 基于 diversity 的高效 experience replay](#3768-efficient-diversity-based-experience-replay-for-deep-reinforcement-learning---为-drl-设计的-基于-diversity-的高效-experience-replay)
*   [4744 Deduction with Induction: Combining Knowledge Discovery with Reasoning for Interpretable Deep Reinforcement Learning - 演论与归纳法：将知识发现与推理相结合，实现可解释的深度强化学习](#4744-deduction-with-induction-combining-knowledge-discovery-with-reasoning-for-interpretable-deep-reinforcement-learning---演论与归纳法将知识发现与推理相结合实现可解释的深度强化学习)
*   [4997 From End-to-end to Step-by-step: Learning to Abstract via Abductive Reinforcement Learning - 从 end-to-end 到 step-by-step：通过归纳（Abductive）强化学习 学习抽象](#4997-from-end-to-end-to-step-by-step-learning-to-abstract-via-abductive-reinforcement-learning---从-end-to-end-到-step-by-step通过归纳abductive强化学习-学习抽象)
*   [5103 Efficient Multi-view Clustering via Reinforcement Contrastive Learning - 通过强化对比学习，进行高效的多视图聚类](#5103-efficient-multi-view-clustering-via-reinforcement-contrastive-learning---通过强化对比学习进行高效的多视图聚类)

* * *

359 Multi-granularity Knowledge Transfer for Continual Reinforcement Learning - 为持续 RL 而设计的多粒度知识迁移
--------------------------------------------------------------------------------------------------

Continual reinforcement learning (CRL) empowers RL agents with the ability to learn a sequence of tasks, accumulating knowledge learned in the past and using the knowledge for problemsolving or future task learning. However, existing methods often focus on transferring fine-grained knowledge across similar tasks, which neglects the multi-granularity structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance coarse-grained knowledge transfer, we propose a novel framework called MT-Core (as shorthand for Multi-granularity knowledge Transfer for Continual reinforcement learning). MT-Core has a key characteristic of multi-granularity policy learning: 1) a coarsegrained policy formulation for utilizing the powerful reasoning ability of the large language model (LLM) to set goals, and 2) a fine-grained policy learning through RL which is oriented by the goals. We also construct a new policy library (knowledge base) to store policies that can be retrieved for multi-granularity knowledge transfer. Experimental results demonstrate the superiority of the proposed MT-Core in handling diverse CRL tasks versus popular baselines.

*   background & gap：
    *   持续强化学习（CRL）使 RL 智能体能够学习一系列任务，积累过去学到的知识，并将这些知识用于解决问题或未来的任务学习。
    *   然而，现有方法往往侧重于在相似任务之间传递细粒度知识，而忽视了人类认知控制的多粒度结构，导致不同任务之间的知识传递不足。
*   method：
    *   为了增强粗粒度的知识迁移，我们提出了一种称为 MT-Core 的新框架，作为持续强化学习的多粒度知识转移的简写。
    *   MT-Core 具有多粒度策略学习的一个关键特征：1）利用大语言模型（LLM）强大的推理能力设定目标的粗粒度策略制定，2）以目标为导向的 RL 细粒度策略学习。
    *   我们还构建了一个新的策略库（知识库）来存储可以检索的策略，以进行多粒度的知识转移。
*   exp：实验结果表明，与流行的基线相比，所提出的 MT-Core 在处理各种 CRL 任务方面具有优越性。

### 一、 研究背景与核心痛点（The Gap）

传统的强化学习（RL）在解决复杂、长序列任务时，面临两大核心挑战：

1.  **样本效率低下（Low Sample Efficiency）：** Agent 必须通过大量的试错（Trial-and-Error）从零开始学习每个新任务，这在现实世界应用中成本极高。
2.  **知识泛化困难（Poor Generalization）：** 现有的知识迁移方法（如策略权重迁移）通常是低级（Sub-Symbolic）的，难以将经验鲁棒地应用于结构差异较大的新任务。

**目标设定：** 我们的目标是构建一个框架，能够像人类一样，通过**知识检索和高层规划**快速掌握新任务，实现从纯粹的试错学习，向基于知识的规划执行的范式转变。

### 二、 动机与故事线构建（Motivation & Narrative）

该论文的动机和故事线构建巧妙，将工作定位为解决上述 RL 核心痛点的新颖解决方案：

动机元素

核心论点（Gap）

解决方案的直觉（Intuition）

**效率**

传统 RL 必须从头开始学习。

**人类学习是基于知识和规划的。** 引入 LLM 作为知识库和 planner，可以将学习转化为高效的“检索历史信息 → 推理 high-level goal → 做出 low-level action”。

**泛化**

传统知识迁移是 low-level 的、脆弱的。

**高层知识是通用的。** 迁移**符号化、结构化**的 planning 知识，如一系列 high-level goal，而非底层动作，可以实现鲁棒的跨任务复用。

**规划**

复杂任务规划和稀疏奖励难以处理。

**LLM 是卓越的 planner。** 利用 LLM 的常识推理能力，将复杂任务分解为一系列可管理的子目标序列。

**总结：** 论文的叙事核心是：不声称自己是“HRL 的 LLM 变体”，而是**“利用 LLM 实现高效、符号化知识迁移”**的新范式。

### 三、 审稿策略分析（Positioning Strategy）

该论文 **将自己定位为“知识迁移”工作，同时利用 HRL（hierarchical RL）/ GCRL（goal-conditioned RL）作为底层工具。**

*   **核心定位：知识迁移（Knowledge Transfer）。** 这一主题具有广泛的吸引力，直接针对 RL 的样本效率痛点。通过强调**符号化、高层规划**的迁移机制，论文突出了范式的新颖性。
*   **规避竞争：** 通过强调知识迁移和 LLM 的实用性，论文有效地弱化了其在纯粹的 HRL、GCRL 领域可能面临的方法新颖性 concern 和严格理论要求（如收敛性证明）。
*   **HRL / GCRL 审稿人视角：** 论文将 HRL / GCRL 视为实现其知识迁移目标的**工具**。它向 HRL 审稿人展示了如何利用 LLM 解决传统 HRL 的规划和奖励设计难题；向 GCRL 审稿人展示了如何利用 LLM 自动生成复杂、逻辑化的目标序列。

### 四、 方法合理性与技术细节（Method Justification）

该方法的核心直觉是**“分工协作，优势互补”**，将高层规划交给 LLM，将底层执行交给 RL Agent。

#### 1\. 架构：分层协作（HRL Structure）

系统采用分层结构：

*   **高层（LLM Planner）：** 负责将用户给定的复杂任务（如“制作咖啡”）分解为一系列符号化的子目标序列 \\(G = \\{g\_1, g\_2, ...\\}\\)（如：\[找到杯子\] \\(\\to\\) \[加水\] \\(\\to\\) \[按启动键\]）。
*   **低层（RL Agent）：** 负责执行具体的子目标 \\(g\_i\\)，使用一个目标条件策略 \\(\\pi(a|s, g\_i)\\) 来实现精细的连续控制。

#### 2\. 知识迁移机制：策略库与符号化食谱

为了实现高效的知识复用，系统引入了**策略库（Strategy Library）**：

*   **知识存储：** 成功的任务经验被编码成**符号化 Recipe**，即高层子目标的序列，并存储在库中。
*   **知识检索与适应：** 当遇到新任务时，LLM 通过语义检索找到最相关的 Recipe，并利用其强大的上下文学习（In-Context Learning）能力对食谱进行微调和适应，生成新任务的规划。

#### 3\. 鲁棒性保障：闭环反馈（Closed-Loop Feedback）

为了防止 LLM 生成“幻想”的、不可执行的规划，系统设计了闭环机制：

*   **执行与验证：** 低层 Agent 在执行子目标 \\(g\_i\\) 时，会根据环境观测和预设的成功条件进行验证。
*   **在线修正：** 如果低层执行失败，失败的观测信息会被反馈给 LLM，LLM 会通过 Prompt 进行**在线推理和重新规划**，调整后续的子目标序列。
    *   例子： LLM 规划 \\(g\_1\\): \[找到杯子\]。Agent 执行失败。LLM 接收反馈后，可能修正规划为 \\(g\_1'\\): \[搜索柜子\]，然后 \\(g\_2\\): \[找到杯子\]。

**总结：** 这种方法通过 HRL 实现了规划与执行的解耦，通过策略库实现了知识的高效迁移，并通过闭环反馈确保了 LLM 规划的物理合理性和鲁棒性。

* * *

769 BILE: An Effective Behavior-based Latent Exploration Scheme for Deep Reinforcement Learning - BILE：一种有效的 Behavior-based 的 DRL latent 探索方案
---------------------------------------------------------------------------------------------------------------------------------------------

Efficient exploration of state spaces is critical for the success of deep reinforcement learning (RL). While many methods leverage exploration bonuses to encourage exploration instead of relying solely on extrinsic rewards, these bonus-based approaches often face challenges with learning efficiency and scalability, especially in environments with highdimensional state spaces. To address these issues, we propose BehavIoral metric-based Latent Exploration (BILE). The core idea is to learn a compact representation within the behavioral metric space that preserves value differences between states. By introducing additional rewards to encourage exploration in this latent space, BILE drives the agent to visit states with higher value diversity and exhibit more behaviorally distinct actions, leading to more effective exploration of the state space. Additionally, we present a novel behavioral metric for efficient and robust training of the state encoder, backed by theoretical guarantees. Extensive experiments on high-dimensional environments, including realistic indoor scenarios in Habitat, robotic tasks in Robosuite, and challenging discrete Minigrid benchmarks, demonstrate the superiority and scalability of our method over other approaches.

*   background & gap：对状态空间的有效探索对于深度强化学习 （RL） 的成功至关重要。虽然许多方法利用探索奖励来鼓励探索，而不是仅仅依赖外在奖励，但这些基于奖励的方法往往面临学习效率和可扩展性的挑战，尤其是在具有高维状态空间的环境中。
*   method：为了解决这些问题，我们提出了基于 behavioral metric 的 latent 探索（BILE）。核心思想是在行为度量空间内学习一个紧凑的 representation，以保留状态之间的值差异。通过引入额外的奖励来鼓励在这个 latent space 中探索，BILE 驱使智能体访问具有更高价值多样性的状态，并表现出更多行为上不同的行为，从而更有效地探索状态空间。
*   此外，我们还提出了一种新的行为指标，用于高效、稳健地训练状态编码器，并得到理论保证的支持。
*   exp：对高维环境的广泛实验，包括 Habitat 中的真实室内场景、Robosuite 中的机器人任务以及具有挑战性的离散 Minigrid 基准测试，证明了我们的方法相对于其他方法的优越性和可扩展性。

### 零、介绍 \\(\\pi\\)\-Bisimulation metric

\\(\\pi\\)\-Bisimulation 度量（\\(d^\\pi\\)）是一种衡量两个状态 \\(s\_i\\) 和 \\(s\_j\\) **行为相似度**的数学工具。它的 motivation 是：如果两个状态在行为上是等价的，那么它们应该具有相同的价值（Value）。

定义：这个度量值 \\(d^\\pi(s\_i, s\_j)\\) 是通过一个递归定义，或者说是一个不动点方程来确定的，它包含两个核心部分：

\\\[d^\\pi(s\_i, s\_j) = \\underbrace{|r^\\pi(s\_i) - r^\\pi(s\_j)|}\_{\\text{奖励差异项 (Reward Difference)}} + \\underbrace{\\gamma W\_1(d^\\pi)(P^\\pi(\\cdot|s\_i), P^\\pi(\\cdot|s\_j))}\_{\\text{分布差异项 (Distribution Divergence)}} \\\]

1.  **奖励差异项**：衡量两个状态 \\(s\_i\\) 和 \\(s\_j\\) 在执行当前策略 \\(\\pi\\) 后，所获得的**即时奖励**的差异。如果奖励差异很大，则说明这两个状态的行为后果不同。
2.  **分布差异项**：衡量从 \\(s\_i\\) 和 \\(s\_j\\) 出发，执行策略 \\(\\pi\\) 后，**下一个状态的分布** \\(P^\\pi(\\cdot|s\_i)\\) 和 \\(P^\\pi(\\cdot|s\_j)\\) 有多大的不同。这里使用 \\(W\_1\\)（1-Wasserstein 距离）来量化这种分布差异，并用 \\(\\gamma\\)（折扣因子）进行加权。

理论保证：\\(\\pi\\)\-Bisimulation 度量具有非常重要的理论保证。核心保证在于：**状态之间的 \\(\\pi\\)\-Bisimulation 距离，上界了它们的状态价值函数 \\(V^\\pi\\) 的差异。**

这意味着：\\(|V^\\pi(s\_i) - V^\\pi(s\_j)| \\leq \\text{常数} \\cdot d^\\pi(s\_i, s\_j)\\) 。

如果两个状态在 \\(\\pi\\)\-Bisimulation 度量空间中距离很近，那么它们的长期 value（\\(V^\\pi\\)）也一定很接近。反之，如果距离很远，则表明它们在行为后果、未来价值上有显著差异。

### 一、背景与挑战：高维稀疏环境下的探索困境

对于深度强化学习（Deep RL）而言，如何在状态空间巨大、奖励信号稀疏的环境中进行高效探索，始终是一个核心挑战。传统的基于奖励（Bonus-based）的探索方法，如 RND 或 ICM，在高维状态空间（如图像输入）中存在两大挑战：

1.  **可扩展性限制：** 高维 state space 里的状态差异过小，导致奖励信号不稳定，难以有效区分状态的新颖性。
    
2.  **表示崩溃（Representation Collapse）：** 特别是基于 \\(\\pi\\)\-Bisimulation 的度量方法，在稀疏奖励（绝大多数状态回报为零）环境下，状态编码器倾向于将所有状态映射到相近的点，失去区分度（BILE 论文中的 **Theorem 1** 阐述了这一问题）。
    
3.  **无意义探索：** 智能体可能为了最大化内在奖励而采取重复行为，偏离外部任务目标（如 Figure 1 所示）。
    

BILE 旨在通过学习一个**鲁棒的、行为驱动的潜藏空间**，并结合**隐向量条件化策略（Latent-Conditioned Policy, LCP）**来解决这些问题，从而实现高效且具有目的性的探索。

### 二、核心机制：隐向量 \\(\\mathbf{z}\\) 的作用与采样

BILE 的核心思想与技能发现（Skill Discovery）领域高度相似，即利用一个随机采样的隐向量 \\(\\mathbf{z}\\) 来条件化策略，以生成多样化的行为。（skill discovery 领域 sota 方法 metra 的 [本站博客解读](https://www.cnblogs.com/moonout/p/18980763)）。

\\(\\mathbf{z}\\) 的采样方式与作用：

*   **采样方式：** 潜藏向量 \\(\\mathbf{z}\\) 在每个回合（Episode）开始时，从一个预定义的分布 \\(P(\\mathbf{Z})\\) 中随机采样一次，并在整个回合中保持不变。论文实验表明，BILE 对 \\(\\mathbf{z}\\) 的具体分布（如均匀分布、正态分布等）具有鲁棒性。
*   **条件化策略：** 策略被定义为 \\(\\pi(\\mathbf{s}, \\mathbf{z})\\)。不同的 \\(\\mathbf{z}\\) 向量代表了智能体应追求的不同“意图”或“行为模式”。例如，在导航任务中，改变 \\(\\mathbf{z}\\) 可以使智能体探索房间的不同区域（如 Figure 3 所示）。
*   **内在奖励因子化：** \\(\\mathbf{z}\\) 用于构建内在探索奖励 \\(b(\\mathbf{s}, \\mathbf{z})\\)：\\(b(\\mathbf{s}, \\mathbf{z}) = f(\\mathbf{s}, \\mathbf{s}') \\cdot \\mathbf{z}\\)，其中 \\(f(\\mathbf{s}, \\mathbf{s}')\\) 是状态 \\(\\mathbf{s}\\) 到 \\(\\mathbf{s}'\\) 在 BILE 潜藏空间中的距离度量。

策略的目标是最大化这个 intrinsic reward。由于 \\(\\mathbf{z}\\) 是随机且多样化的，这迫使策略学习如何在潜藏空间中沿着**所有可能的方向**实现最大的“移动”，从而确保了行为的多样性（跟 metra 非常像）。

### 三、BILE 的关键技术创新：鲁棒的行为度量学习

BILE 的核心优势在于其构建潜藏空间所依赖的**行为度量（Behavioral Metric）**，它解决了稀疏奖励下的表示崩溃问题。

#### 3.1 度量的目标：价值多样性

BILE 的潜藏空间 \\(\\phi(\\mathbf{s})\\) 旨在学习一个基于 \\(\\pi\\)\-Bisimulation 的度量 \\(d\_{BILE}^{\\phi}\\)。根据 **Theorem 3**，这个距离度量是状态之间的价值差异的**上界（Upper-bound）**：

\\\[|V^\\pi(s\_i) - V^\\pi(s\_j)| \\leq d\_{BILE}^{\\phi}(s\_i, s\_j) \\\]

这意味着在 BILE 潜藏空间中距离较远的状态，其未来期望回报（价值）必然存在显著差异。因此，BILE 鼓励智能体探索具有**高价值多样性**的状态。（有点抽象，还没完全想清楚，为什么探索 value 沿某一方向变化大的 state，就可以鼓励探索）

#### 3.2 鲁棒性机制：引入预测误差

为了避免在稀疏奖励环境中出现表示崩溃，即所有状态的 value 差异趋近于零，BILE 在其度量学习目标（Equation 4/7）中引入了**动态模型预测误差** \\(r\_{PE}^\\pi(s)\\) ，作为额外的奖励信号：

\\\[F(d\_{BILE}^{\\phi}, \\pi) = |\\dots| + \\alpha \\sum |r\_{PE}^\\pi(s)| + \\gamma E\[\\dots\] \\\]

通过将状态转移模型的预测误差整合到编码器训练中，即使外部奖励为零，编码器仍然有足够的信息来区分状态。这确保了潜藏表示的鲁棒性，使其能够有效应用于高维、稀疏奖励场景。

### 四、BILE 与 METRA / ETD 的对比分析

*   metra 是目前无监督 RL 中 skill discovery 领域的 sota 方法，ICLR 2024 oral。论文标题：METRA: Scalable Unsupervised RL with Metric-Aware Abstraction。metra [本站博客](https://www.cnblogs.com/moonout/p/18980763)。
*   ETD 是 ICLR 2025 的文章，做的 setting 跟 BILE 一样，也关注无监督 RL 的 exploration。论文标题：Episodic Novelty Through Temporal Distance。ETD [本站博客](https://www.cnblogs.com/moonout/p/18812958)。

BILE 的机制与 METRA (Metric-Aware Abstraction) 和 ETD (Episodic Temporal Distance) 具有高度相似性，尤其是在利用潜藏空间距离和 \\(\\mathbf{z}\\) 进行探索激励方面。它们的核心区别在于**隐空间度量的基础定义**和**所关注的探索特性**。

特征

BILE

METRA

ETD

**度量基础**

\\(\\pi\\)\-Bisimulation 行为度量

时间距离 (Temporal Distance, TD)

时间距离 (Temporal Distance, TD)

**度量目标**

**价值多样性**：上界状态价值差异。

**时序结构**：保持状态间的时序可达性。

**时序结构**：直接奖励时序距离。

**探索激励**

沿着 \\(\\mathbf{z}\\) 方向最大化**价值差异**驱动的移动。

沿着 \\(\\mathbf{z}\\) 方向最大化**时序可达性**驱动的移动。

直接奖励访问时间距离大的状态。

**稀疏奖励鲁棒性**

**高**：通过引入动态模型预测误差，明确避免了表示崩溃。

**中到高**：TD 度量本身对奖励依赖性较低。

**高**：直接基于可达性，与奖励无关。

#### 关键差异总结：

1.  **度量属性不同：**
    
    *   **BILE** 关注的是**行为等价性**和**价值差异**。它学习的潜藏空间是价值驱动的，确保探索是朝着最大化未来回报差异的方向进行的。
    *   **METRA/ETD** 关注的是**时序可达性**。它们学习的潜藏空间反映了状态在时间轴上的邻近性或可达性。
2.  **鲁棒性机制不同：**
    
    *   BILE 明确针对基于 Bisimulation 的度量在稀疏奖励下易发生的**表示崩溃**问题，通过引入预测误差项进行修正，这是其区别于 LIBERTY 等早期 Bisimulation 方法的关键创新。
3.  **探索目的性：**
    
    *   BILE 的探索信号（基于价值差异）与最终任务目标（最大化期望回报）具有更强的关联性，这使得 BILE 的探索更具“目的性”（Goal-directed），而非仅仅是最大化状态空间中的覆盖或时序差异。

综上所述，BILE 成功地将 LCP 带来的行为多样性优势，与一个经过理论保证且对稀疏奖励鲁棒的**价值驱动的行为度量**相结合，实现了在高维、稀疏奖励环境下的高效探索。

* * *

908 Imagination-Limited Q-Learning for Offline Reinforcement Learning - 用于 offline RL 的想象力限制的 Q-Learning
--------------------------------------------------------------------------------------------------------

Offline reinforcement learning seeks to derive improved policies entirely from historical data but often struggles with over-optimistic value estimates for out-of-distribution (OOD) actions. This issue is typically mitigated via policy constraint or conservative value regularization methods. However, these approaches may impose overly constraints or biased value estimates, potentially limiting performance improvements. To balance exploitation and restriction, we propose an Imagination-Limited Q-learning (ILQ) method, which aims to maintain the optimism that OOD actions deserve within appropriate limits. Specifically, we utilize the dynamics model to imagine OOD action-values, and then clip the imagined values with the maximum behavior values. Such design maintains reasonable evaluation of OOD actions to the furthest extent, while avoiding its over-optimism. Theoretically, we prove the convergence of the proposed ILQ under tabular Markov decision processes. Particularly, we demonstrate that the error bound between estimated values and optimality values of OOD state-actions possesses the same magnitude as that of in-distribution ones, thereby indicating that the bias in value estimates is effectively mitigated. Empirically, our method achieves state-of-the-art performance on a wide range of tasks in the D4RL benchmark.

*   background & gap：离线强化学习试图完全从历史数据中得出改进的策略，但经常难以应对分布外 （OOD）作的过于乐观的价值估计。此问题通常可以通过策略约束或保守的值正则化方法来缓解。然而，这些方法可能会施加过度的限制或有偏见的价值估计，从而可能限制性能改进。
*   method：为了平衡剥削和限制，我们提出了一种想象力有限的 Q 学习 （ILQ） 方法，旨在在适当的范围内保持 OOD 行动应得的乐观情绪。具体来说，我们利用动力学模型来想象 OOD 动作值，然后用最大行为值（maximum behavior values）裁剪想象值。这样的设计在最大程度上保持了对 OOD 动作的合理评估，同时避免了其过度乐观。
*   理论：从理论上讲，我们证明了所提出的 ILQ 在表格马尔可夫决策过程中的收敛性。特别是，我们证明了 OOD 状态动作的估计值和最优值之间的误差范围与分布内状态动作的误差范围相同，从而表明价值估计的偏差得到了有效缓解。
*   exp：根据经验，我们的方法在 D4RL 基准测试中的各种任务上实现了最先进的性能。

### 一、引言：offline RL 的挑战与现有困境

**离线强化学习 (Offline RL)** 的核心挑战在于**分布偏移 (Distributional Shift)**。由于完全依赖于一个固定的数据集 \\(D\\)，当 agent 尝试执行数据集中未出现过的 **OOD 动作 (Out-of-Distribution)** 时，Q 函数往往会给出**过度乐观 (Over-Optimistic)** 的价值估计，导致策略崩溃。

现有的解决方案陷入了一个二元困境：

1.  **策略约束 (Policy Constraint)：** 强制新策略 \\(\\pi\\) 贴近行为策略 \\(\\beta\\) (如 BCQ)。
    
    缺陷： **过度保守**。如果原始数据质量不高，策略性能将受限于数据，无法实现超越。
    
2.  **价值正则化 (Value Regularization)：** 通过惩罚 OOD 动作的 Q 值来抑制乐观估计 (如 CQL)。
    
    缺陷： **不可控的悲观偏差**。为了安全，它将所有 OOD 动作的价值都压低了。例如，在 MuJoCo 任务中，CQL 的 Q 值估计显著低于数据集中的最大回报（论文图 1(c)），限制了策略改进的空间。
    

### 二、ILQ 的叙事核心：寻找“合理的乐观”

**想象力受限 Q-Learning (ILQ)** 的核心动机是打破上述困境。它的“故事”在于：我们不应盲目地惩罚 OOD 动作的价值，而应该在“合理的乐观”和“必要的限制”之间找到平衡点。

ILQ 的核心洞察是：

1.  **想象 (Imagination)：** 首先，为 OOD 动作提供一个**合理的价值 baseline**，即“如果这个 OOD 动作是可信的，它的价值应该是什么？”
2.  **限制 (Limitation)：** 然后，设定一个**安全上限**，确保这个想象值不会超过数据集中**已知最好的动作**的价值。

通过这种方式，ILQ 旨在**最大限度地保持 OOD 动作应有的乐观性，同时避免过度乐观导致的风险。**

### 三、核心方法：想象力受限 Bellman 算子 (ILB)

ILQ 将上述直觉转化为数学操作，提出了 **想象力受限 Bellman (ILB) 算子** \\(T\_{ILB}\\)。

对于一个状态 \\(s\\) 和动作 \\(a\\)：

\\\[T\_{ILB}Q(s, a) = \\begin{cases} \\text{Standard Bellman Backup} & \\text{if } a \\text{ is In-Sample} \\\\ \\min(y^Q\_{img}, y^Q\_{lmt}) + \\delta & \\text{if } a \\text{ is OOD} \\end{cases} \\\]

其中，OOD 动作的价值估计由两个关键组件构成：

#### 3.1 想象值 (\\(y^Q\_{img}\\))：提供合理的乐观基准

\\(y^Q\_{img}\\) 的目标是为 OOD 动作 \\((s, a\_{oos})\\) 估计一个接近真实的 Q 值。

*   **技术细节：** ILQ 首先预训练一个 **env dynamic model** \\((\\hat{P}, \\hat{r})\\)，用于预测 OOD 动作的即时奖励 \\(r\\) 和下一状态 \\(s'\\)。
    
*   **计算方式：** 使用动态模型进行**单步 Bellman backup**：
    
    \\\[y^Q\_{img} = \\hat{r}(s, a\_{oos}) + \\gamma E\_{\\hat{s}' \\sim \\hat{P}} \\left\[ \\max\_{a'} Q(\\hat{s}', a') \\right\] \\\]
    
*   **合理性：** 这为 OOD 动作提供了一个**“有依据的”**乐观估计，解决了传统价值正则化方法（如 CQL）对 OOD 动作的**盲目悲观**问题。
    

#### 3.2 限制值 (\\(y^Q\_{lmt}\\))：确保保守性与安全性

\\(y^Q\_{lmt}\\) 的目标是设定一个安全的上限，防止 env dynamic model 的误差 导致过度乐观。

*   **技术细节：** \\(y^Q\_{lmt}\\) 被定义为在当前状态 \\(s\\) 下，**行为策略 \\(\\beta\\) 的 support set 内最大的 Q 值**。
    
    \\\[y^Q\_{lmt} = \\max\_{a \\in \\text{Supp}(\\beta(\\cdot|s))} Q(s, a) \\\]
    
*   **实现机制：** 为了准确识别行为策略 \\(\\beta\\) 的复杂支持集，ILQ 采用了强大的**条件扩散模型 (Conditional Diffusion Model)** 来建模 \\(\\beta\\)，即，去确定哪些 action 是数据集中实际出现过的。
    
*   **为什么使用 diffusion**： 在连续动作空间（如 MuJoCo 机器人任务）中，行为策略 \\(\\beta(a|s)\\) 的分布可能非常复杂，甚至是**多模态**的，即在某个状态下，数据集中可能存在多组不同的、有效的动作。使用简单的模型（如高斯分布）无法捕捉这种多模态性，而 diffusion 可以。（与此相关的文章：[DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning](https://neurips.cc/virtual/2025/poster/116688)）
    
*   **合理性：** 限制值保证了，OOD 动作的 value 永远不会超过 in-distribution 动作中的最大 value。这提供了必要的**保守性**，防止了模型误差带来的灾难性后果。
    

### 四、ILQ 与 Model-Based RL的关系：Model-Assisted 的 Model-Free RL

虽然 ILQ 使用了 **env dynamics model**来计算 \\(y^Q\_{img}\\)，但它本质上仍然是一个 **Model-Free (无模型)** 的 Q-Learning 框架，可以称之为 **“Model-Assisted Model-Free”** 方法。

特性

传统 Model-Based RL

ILQ 的处理方式

**模型用途**

用于**多步规划**，或生成长轨迹数据。

仅用于计算 OOD 动作的**单步 Bellman backup 目标**。

**误差风险**

**误差积累**：多步预测导致误差呈指数级增长。

**规避误差积累**：只进行单步预测，并立即使用 Q 函数进行 bootstrap，限制了误差传播。

**核心机制**

依赖模型预测来驱动策略。

依赖 Model-Free 的 Q 函数学习，模型仅作为 OOD 动作价值的**辅助估计工具**。

因此，ILQ 借鉴了 Model-Based 的“想象”能力，但通过 Model-Free 的 Q 函数 bootstrap 和 \\(y^Q\_{lmt}\\) 的保守限制机制，确保了其在离线 RL 设定下的稳定性和可靠性。

### 五、结论与理论保障

ILQ 的理论分析表明，通过 ILB 算子学习到的 OOD 动作的价值估计误差界限，与 In-Sample 动作的误差界限处于**相同的数量级** \\(O(r\_{max}/(1-\\gamma)^2)\\)。这一关键的理论结果，证明了 ILQ 成功缓解了 OOD action 的偏差，使其价值估计的可靠性达到了 in-distribution action 相当的水平。实验结果也证实，ILQ 在 D4RL 基准测试的广泛任务上实现了最先进的性能。

* * *

2430 Self-Consistent Model-based Adaptation for Visual Reinforcement Learning - 为视觉 RL 而设计的自我一致的 model-based 的自适应
-----------------------------------------------------------------------------------------------------------------

Visual reinforcement learning agents typically face serious performance declines in real-world applications caused by visual distractions. Existing methods rely on fine-tuning the policy’s representations with hand-crafted augmentations. In this work, we propose Self-Consistent Model-based Adaptation (SCMA), a novel method that fosters robust adaptation without modifying the policy. By transferring cluttered observations to clean ones with a denoising model, SCMA can mitigate distractions for various policies as a plug-and-play enhancement. To optimize the denoising model in an unsupervised manner, we derive an unsupervised distribution matching objective with a theoretical analysis of its optimality. We further present a practical algorithm to optimize the objective by estimating the distribution of clean observations with a pre-trained world model. Extensive experiments on multiple visual generalization benchmarks and real robot data demonstrate that SCMA effectively boosts performance across various distractions and exhibits better sample efficiency.

*   background & gap：视觉强化学习代理在实际应用中通常会因视觉干扰而面临严重的性能下降。现有方法依赖于通过手工制作的增强来微调策略的表示。
*   method：
    *   在这项工作中，我们提出了基于自洽模型的自适应（SCMA），这是一种在不修改策略的情况下促进鲁棒自适应的新方法。通过使用去噪模型（denoising model）将杂乱的观察结果转移（transfer）到干净的观察结果上，SCMA 可以作为各种 policy 的即插即用的增强功能，以减轻视觉上的干扰。
    *   为了以无监督的方式优化去噪模型，我们推导了一个无监督分布匹配目标，并对其最优性进行了理论分析。我们进一步提出了一种实用的算法，通过使用预训练的世界模型估计清洁观测值的分布来优化目标。
*   exp：在多个视觉泛化基准和真实机器人数据上的广泛实验表明，SCMA 可以有效地提高各种干扰的性能，并表现出更好的样本效率。

3591 Two-Stage Feature Generation with Transformer and Reinforcement Learning - 使用 Transformer 和强化学习进行两阶段特征生成
-------------------------------------------------------------------------------------------------------------

Feature generation is a critical step in machine learning, aiming to enhance model performance by capturing complex relationships within the data and generating meaningful new features. Traditional feature generation methods heavily rely on domain expertise and manual intervention, making the process labor-intensive and challenging to adapt to different scenarios. Although automated feature generation techniques address these issues to some extent, they often face challenges such as feature redundancy, inefficiency in feature space exploration, and limited adaptability to diverse datasets and tasks. To address these problems, we propose a Two-Stage Feature Generation (TSFG) framework, which integrates a Transformer-based encoder-decoder architecture with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG leverages the Transformer’s self-attention mechanism to efficiently represent and transform features, capturing complex dependencies within the data. PPO further enhances TSFG by dynamically adjusting the feature generation strategy based on task-specific feedback, optimizing the process for improved performance and adaptability. TSFG dynamically generates high-quality feature sets, significantly improving the predictive performance of machine learning models. Experimental results demonstrate that TSFG outperforms existing state-of-the-art methods in terms of feature quality and adaptability.

*   background & gap：特征生成是机器学习的关键步骤，旨在通过捕获数据中的复杂关系 并生成有意义的新特征，来增强模型性能。传统的特征生成方法严重依赖领域专业知识和人工干预，使得该过程具有劳动密集型和适应不同场景的挑战性。尽管自动化特征生成技术在一定程度上解决了这些问题，但它们经常面临特征冗余、特征空间探索效率低下、以及对不同数据集和任务的适应性有限等挑战。
*   method：
    *   为了解决这些问题，我们提出了一种两阶段特征生成（TSFG）框架，该框架将基于 Transformer 的 encoder-decoder 架构，与近端策略优化（PPO）集成在一起。
    *   TSFG 中的 encoder-decoder 模型利用 Transformer 的自注意力机制来有效地表示和转换特征，捕获数据中的复杂依赖关系。
    *   然后，PPO 通过根据特定任务的反馈动态调整特征生成策略，进一步增强 TSFG，优化流程以提高性能和适应性。
    *   TSFG 动态生成高质量的特征集，显着提高机器学习模型的预测性能。
*   exp：实验结果表明，TSFG 在特征质量和适应性方面优于现有的先进方法。
*   【看起来做的很初步，看看】

3621 PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning - PNAct：在 safe RL 中制作后门攻击
----------------------------------------------------------------------------------------------

Reinforcement Learning (RL) is widely used in tasks where agents interact with an environment to maximize rewards. Building on this foundation, Safe Reinforcement Learning (Safe RL) incorporates a cost metric alongside the reward metric, ensuring that agents adhere to safety constraints during decision-making. In this paper, we identify that Safe RL is vulnerable to backdoor attacks, which can manipulate agents into performing unsafe actions. First, we introduce the relevant concepts and evaluation metrics for backdoor attacks in Safe RL. It is the first attack framework in the Safe RL field that involves both Positive and Negative Action sample (PNAct) is to implant backdoors, where positive action samples provide reference actions and negative action samples indicate actions to be avoided. We theoretically point out the properties of PNAct and design an attack algorithm. Finally, we conduct experiments to evaluate the effectiveness of our proposed backdoor attack framework, evaluating it with the established metrics. This paper highlights the potential risks associated with Safe RL and underscores the feasibility of such attacks. Our code and supplementary material are available at [https://github.com/azure-123/PNAct](https://github.com/azure-123/PNAct).

*   background：RL 广泛应用于代理与环境交互以最大化奖励的任务。在此基础上，Safe RL 将成本指标与奖励指标相结合，确保代理在决策过程中遵守安全约束。
*   提出的新概念（？）：在本文中，我们发现 Safe RL 容易受到后门攻击，后门攻击可以操纵 agent 执行不安全的 action。首先，我们介绍 Safe RL 中后门攻击的相关概念和评估指标。PNAct 是 safe RL 领域中，第一个同时涉及 positive 和 negative action 样本的植入后门的攻击框，其中 positive action 样本提供参考 action，negative action 示例指示要避免的 action。
*   理论分析 + 算法（？）：我们从理论上指出了 PNAct 的属性，并设计了一种攻击算法。
*   exp：最后，我们进行实验来评估我们提出的后门攻击框架的有效性，并使用既定的指标对其进行评估。
*   总结 上价值：本文强调了与 Safe RL 相关的潜在风险，并强调了此类攻击的可行性。
*   我们的代码和补充材料可在 [https://github.com/azure-123/PNAct](https://github.com/azure-123/PNAct) 获得。

3768 Efficient Diversity-based Experience Replay for Deep Reinforcement Learning - 为 DRL 设计的 基于 diversity 的高效 experience replay
-------------------------------------------------------------------------------------------------------------------------------

Experience replay is widely used to improve learning efficiency in reinforcement learning by leveraging past experiences. However, existing experience replay methods, whether based on uniform or prioritized sampling, often suffer from low efficiency, particularly in real-world scenarios with high-dimensional state spaces. To address this limitation, we propose a novel approach, Efficient Diversity-based Experience Replay (EDER). EDER employs a determinantal point process to model the diversity between samples and prioritizes replay based on the diversity between samples. To further enhance learning efficiency, we incorporate Cholesky decomposition for handling large state spaces in realistic environments. Additionally, rejection sampling is applied to select samples with higher diversity, thereby improving overall learning efficacy. Extensive experiments are conducted on robotic manipulation tasks in MuJoCo, Atari games, and realistic indoor environments in Habitat. The results demonstrate that our approach not only significantly improves learning efficiency but also achieves superior performance in high-dimensional, realistic environments.

*   background & gap：经验回放（experience replay）被广泛用于利用过去的经验来提高强化学习的学习效率。然而，现有的经验回放方法，无论是基于统一采样还是优先采样，往往效率低下，特别是在具有高维状态空间的现实场景中。
*   method：为了解决这一限制，我们提出了一种新方法，即基于 diversity 的高效经验回放 （EDER）。EDER 采用行列点过程（determinantal point process）对样本之间的多样性进行建模，并根据样本之间的多样性，确定回放的优先级。为了进一步提高学习效率，我们结合了 Cholesky 分解来处理现实环境中的大状态空间。此外，拒绝抽样用于选择具有更高多样性的样本，从而提高整体学习效率。
*   exp：对 MuJoCo、Atari 游戏中的机器人纵任务以及 Habitat 中的真实室内环境进行了广泛的实验。结果表明，我们的方法不仅显著提高了学习效率，而且在高维、现实的环境中也取得了卓越的表现。

4744 Deduction with Induction: Combining Knowledge Discovery with Reasoning for Interpretable Deep Reinforcement Learning - 演论与归纳法：将知识发现与推理相结合，实现可解释的深度强化学习
-----------------------------------------------------------------------------------------------------------------------------------------------------------

Deep reinforcement learning (DRL) has achieved remarkable success in dynamic decision-making tasks. However, its inherent opacity and cold start problem hinder transparency and training efficiency. To address these challenges, we propose HRL-ID, a neural-symbolic framework that combines automated rule discovery with logical reasoning within a hierarchical DRL structure. HRL-ID dynamically extracts first-order logic rules from environmental interactions, iteratively refines them through success-based updates, and leverages these rules to guide action execution during training. Extensive experiments on Atari benchmarks demonstrate that HRL-ID outperforms state-of-the-art methods in training efficiency and interpretability, achieving higher reward rates and successful knowledge transfer between domains.

*   background & gap：深度强化学习（DRL）在动态决策任务中取得了显著的成功。然而，其固有的不透明性和冷启动问题阻碍了透明度和训练效率。
*   method：为了应对这些挑战，我们提出了 HRL-ID，这是一种神经符号框架，它将自动规则发现与分层 DRL 结构中的逻辑推理相结合。HRL-ID 从环境交互中动态提取一阶逻辑规则，通过基于成功的更新 迭代细化它们，并利用这些规则在训练期间指导动作执行。
*   exp：Atari 基准测试的广泛实验表明，HRL-ID 在训练效率和可解释性方面优于最先进的方法，实现了更高的奖励率和领域之间的成功知识转移。
*   听不懂

4997 From End-to-end to Step-by-step: Learning to Abstract via Abductive Reinforcement Learning - 从 end-to-end 到 step-by-step：通过归纳（Abductive）强化学习 学习抽象
------------------------------------------------------------------------------------------------------------------------------------------------------

Abstraction is a critical technique in general problem-solving, allowing complex tasks to be decomposed into smaller, manageable sub-tasks. While traditional symbolic planning relies on predefined primitive symbols to construct structured abstractions, its reliance on formal representations limits applicability to real-world tasks. On the other hand, reinforcement learning excels at learning end-to-end policies directly from sensory inputs in unstructured environments but struggles with compositional generalization in complex tasks with delayed rewards. In this paper, we propose Abductive Abstract Reinforcement Learning (A2RL), a novel neuro-symbolic RL framework bridging the two paradigms based on Abductive Learning (ABL), enabling RL agents to learn abstractions directly from raw sensory inputs without predefined symbols. A2RL induces a finite state machine to represent high-level, step-by-step procedures, where each abstract state corresponds to a sub-algebra of the original Markov Decision Process (MDP). This approach not only bridges the gap between symbolic abstraction and sub-symbolic learning but also provides a natural mechanism for the emergence of new symbols. Experiments show that A2RL can mitigate the delayed reward problem and improve the generalization capability compared to traditional end-to-end RL methods.

*   background：抽象是一般问题解决中的一项关键技术，允许将复杂的任务分解为更小的、可管理的子任务。
*   literature：虽然传统的符号规划依赖于预定义的原始符号来构建结构化抽象，但其对形式表示的依赖限制了对现实世界任务的适用性。另一方面，强化学习擅长直接从非结构化环境中的感官输入中学习端到端策略，但在奖励延迟的复杂任务中难以进行组合泛化。
*   method：在本文中，我们提出了归纳抽象强化学习（A2RL），这是一种新型的神经符号强化学习框架，它桥接了基于归纳学习（ABL）的两种范式，使 RL 智能体能够直接从原始感官输入中学习抽象，而无需预定义的符号。A2RL 诱导有限状态机来表示高级、分步的过程，其中每个抽象状态对应于原始马尔可夫决策过程 （MDP） 的子代数。这种方法不仅弥合了符号抽象和亚符号学习之间的差距，而且为新符号的出现提供了自然的机制。
*   exp：实验表明，与传统的端到端 RL 方法相比，A2RL 可以缓解延迟奖励问题，提高泛化能力。
*   合作者说，感觉 method 很初级，可能三年前就有类似工作了，不知道这种文章怎么中的。所以来学习讲故事和 writing，分析一下。

5103 Efficient Multi-view Clustering via Reinforcement Contrastive Learning - 通过强化对比学习，进行高效的多视图聚类
-------------------------------------------------------------------------------------------------

Contrastive multi-view clustering has demonstrated remarkable potential in complex data analysis, yet existing approaches face two critical challenges: difficulty in constructing high-quality positive and negative pairs and high computational overhead due to static optimization strategies. To address these challenges, we propose an innovative Efficient Multi-View Clustering framework with Reinforcement Contrastive Learning (EMVCRCL). Our key innovation is developing a reinforcement contrastive learning paradigm for dynamic clustering optimization. First, we leverage multi-view contrastive learning to obtain latent features, which are then sent to the reinforcement learning module to refine low-quality features. Specifically, it selects high-confident features to guide the positive/negative pair construction of contrastive learning. For the low-confident features, it utilizes the prior balanced distribution to adjust their assignment. Extensive experimental results showcase the effectiveness and superiority of our proposed method on multiple benchmark datasets.

*   background & gap：对比多视图聚类在复杂数据分析中显示出巨大的潜力，但现有方法面临两个关键挑战：难以构建高质量的正负对以及由于静态优化策略导致的高计算开销。
*   method：为了应对这些挑战，我们提出了一种创新的高效多视图聚类框架，采用强化对比学习（EMVCRCL）。我们的关键创新是，开发一种用于动态聚类优化的强化对比学习范式。首先，我们利用多视图对比学习获得潜在特征，然后将其发送到强化学习模块中，以细化低质量特征。具体来说，它选择高置信度特征来指导对比学习的正/负对构建。对于低置信度特征，它利用先验平衡分布（prior balance distribution）来调整其分配。
*   exp：大量的实验结果证明了我们所提方法在多个基准数据集上的有效性和优越性。
*   【看看对比学习和 RL 是怎么结合的】