---
layout: post
title: 'å¼ é«˜å…´çš„å¤§æ¨¡å‹å¼€å‘å®æˆ˜ï¼šï¼ˆå››ï¼‰ä½¿ç”¨ LangGraph å®ç°å¤šæ™ºèƒ½ä½“åº”ç”¨'
date: "2025-04-07T00:40:35Z"
---
å¼ é«˜å…´çš„å¤§æ¨¡å‹å¼€å‘å®æˆ˜ï¼šï¼ˆå››ï¼‰ä½¿ç”¨ LangGraph å®ç°å¤šæ™ºèƒ½ä½“åº”ç”¨
====================================

ç›®å½•

*   [ç¯å¢ƒæ­å»ºä¸é…ç½®](#ç¯å¢ƒæ­å»ºä¸é…ç½®)
*   [å®šä¹‰æ™ºèƒ½ä½“](#å®šä¹‰æ™ºèƒ½ä½“)
    *   [åŠ è½½æ¨¡å‹](#åŠ è½½æ¨¡å‹)
    *   [æå–å…³é”®è¯](#æå–å…³é”®è¯)
    *   [ç”Ÿæˆå›ç­”](#ç”Ÿæˆå›ç­”)
*   [è¿æ¥æ™ºèƒ½ä½“](#è¿æ¥æ™ºèƒ½ä½“)
    *   [å®šä¹‰å›¾çš„çŠ¶æ€](#å®šä¹‰å›¾çš„çŠ¶æ€)
    *   [å®šä¹‰èŠ‚ç‚¹æ–¹æ³•](#å®šä¹‰èŠ‚ç‚¹æ–¹æ³•)
        *   [æ ¹æ®æŒ‡ä»¤è·¯ç”±](#æ ¹æ®æŒ‡ä»¤è·¯ç”±)
        *   [ç”Ÿæˆå›ç­”](#ç”Ÿæˆå›ç­”-1)
        *   [æ–‡ä»¶å¤„ç†](#æ–‡ä»¶å¤„ç†)
        *   [æå–å…³é”®è¯](#æå–å…³é”®è¯-1)
        *   [ç½‘ç»œæœç´¢](#ç½‘ç»œæœç´¢)
    *   [å®šä¹‰å›¾çš„ç»“æ„](#å®šä¹‰å›¾çš„ç»“æ„)
    *   [è¿è¡Œå›¾](#è¿è¡Œå›¾)
*   [è¿è¡ŒæŒ‡å—](#è¿è¡ŒæŒ‡å—)
    *   [åœ¨æ§åˆ¶å°ä¸­æµ‹è¯•ç¨‹åº](#åœ¨æ§åˆ¶å°ä¸­æµ‹è¯•ç¨‹åº)
    *   [ä½¿ç”¨ Streamlit æ„å»ºå‰ç«¯é¡µé¢](#ä½¿ç”¨-streamlit-æ„å»ºå‰ç«¯é¡µé¢)

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬æœŸæœ›åˆ©ç”¨ LLM è§£å†³å„ç§å¤æ‚é—®é¢˜ï¼Œåœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæ„å»ºæ™ºèƒ½ä½“ï¼ˆAgentï¼‰åº”ç”¨å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç”¨æˆ·ä¸ LLM çš„äº¤äº’å¯ä»¥è¢«è§†ä¸ºä¸€ç§ **å•æ™ºèƒ½ä½“ï¼ˆSingle-Agentï¼‰** è¡Œä¸ºï¼šç”¨æˆ·é€šè¿‡æç¤ºè¯ï¼ˆpromptï¼‰ä¸é€šç”¨ LLM è¿›è¡Œå¯¹è¯ï¼ŒLLM ç†è§£é—®é¢˜å¹¶æä¾›åé¦ˆã€‚ç„¶è€Œï¼Œå•ä¸€æ™ºèƒ½ä½“åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å­˜åœ¨æ˜æ˜¾å±€é™æ€§ï¼Œä¾‹å¦‚éœ€è¦ç”¨æˆ·å¤šæ¬¡å¼•å¯¼ã€ç¼ºä¹å¯¹å¤–éƒ¨ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€å¯¹è¯å†å²è®°å¿†æœ‰é™ç­‰ã€‚

è¯•æƒ³ä»¥ä¸‹åœºæ™¯ï¼šåœ¨ä¸åŒå¤„ç†é˜¶æ®µè°ƒç”¨ä¸åŒçš„æ¨¡å‹ï¼›å½“ LLM æ— æ³•å®Œæˆä»»åŠ¡æ—¶ï¼Œè‡ªåŠ¨æŸ¥è¯¢å¤–éƒ¨çŸ¥è¯†åº“ï¼›æˆ–è€…ç”± LLM è‡ªä¸»çº æ­£ç”Ÿæˆå†…å®¹ä¸­çš„å¹»è§‰å’Œé”™è¯¯ã€‚è¿™äº›éœ€æ±‚å¦‚ä½•å®ç°ï¼Ÿ **å¤šæ™ºèƒ½ä½“ï¼ˆMulti-Agentï¼‰** ç³»ç»Ÿæ­£æ˜¯è§£å†³è¿™ç±»é—®é¢˜çš„æœ‰æ•ˆå·¥å…·ã€‚é€šè¿‡æç¤ºè¯æ¨¡æ¿ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†é…è§’è‰²å¹¶è§„èŒƒå…¶è¡Œä¸ºï¼Œå¤šä¸ªæ™ºèƒ½ä½“ç›¸äº’åä½œï¼Œä»è€Œå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚

ç„¶è€Œï¼Œæ„å»ºå¤šæ™ºèƒ½ä½“åº”ç”¨å¹¶éæ˜“äº‹ï¼Œå¼€å‘è€…éœ€è¦é¢å¯¹æ™ºèƒ½ä½“è®¾è®¡ã€é€šä¿¡åè®®ã€åè°ƒç­–ç•¥ç­‰å¤šæ–¹é¢çš„é—®é¢˜ã€‚LangGraph æä¾›äº†ä¸€ç§ä»¥å›¾ï¼ˆgraphï¼‰ä¸ºæ ¸å¿ƒçš„è§£å†³æ–¹æ¡ˆï¼Œæ¸…æ™°å®šä¹‰äº†æ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»ä¸äº¤äº’è§„åˆ™ï¼Œå¹¶é€šè¿‡å†…ç½®çš„é€šä¿¡æ¥å£å’Œåè°ƒç­–ç•¥ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿæ„å»ºé«˜æ•ˆä¸”å¯æ‰©å±•çš„åˆ†å¸ƒå¼æ™ºèƒ½ç³»ç»Ÿã€‚

![](https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155907600-36102623.png)

**æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå®ä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ `LangGraph` æ„å»ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå¹¶ç»“åˆ `Streamlit` å®ç°ç”¨æˆ·å‹å¥½çš„å‰ç«¯ç•Œé¢ã€‚** è¯¥åº”ç”¨å…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼š

1.  æ ¹æ®å¯¹è¯ç±»å‹å°†è¯·æ±‚è·¯ç”±åˆ°é€‚å½“çš„å¤„ç†èŠ‚ç‚¹ã€‚
2.  æ”¯æŒè”ç½‘æœç´¢ï¼Œè·å–å®æ—¶ä¿¡æ¯ã€‚
3.  æ ¹æ®é—®é¢˜å’Œå¯¹è¯å†å²ç”Ÿæˆä¼˜åŒ–çš„æœç´¢æç¤ºè¯ã€‚
4.  æ”¯æŒæ–‡ä»¶ä¸Šä¼ ä¸å¤„ç†ã€‚
5.  åˆ©ç”¨ç¼–ç¨‹ä¸“ç”¨çš„ LLM è§£å†³ä»£ç ç›¸å…³é—®é¢˜ã€‚
6.  åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæ€»ç»“ç”Ÿæˆç­”æ¡ˆã€‚

![](https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155913943-35085412.png)

ç¯å¢ƒæ­å»ºä¸é…ç½®
-------

é¡¹ç›®ç»“æ„å¦‚ä¸‹ï¼š

    .
    â”œâ”€â”€ .streamlit  # Streamlit é…ç½®
    â”‚Â Â  â””â”€â”€ config.toml
    â”œâ”€â”€ chains  # æ™ºèƒ½ä½“
    â”‚Â Â  â”œâ”€â”€ generate.py
    â”‚Â Â  â”œâ”€â”€ models.py
    â”‚Â Â  â””â”€â”€ summary.py
    â”œâ”€â”€ graph   # å›¾ç»“æ„
    â”‚Â Â  â”œâ”€â”€ graph.py
    â”‚Â Â  â””â”€â”€ graph_state.py
    â”œâ”€â”€ upload_files    # ä¸Šä¼ çš„æ–‡ä»¶
    â”‚Â Â  â””â”€â”€ .keep
    â”œâ”€â”€ .env   # ç¯å¢ƒå˜é‡é…ç½®
    â”œâ”€â”€ app.py  # Streamlit åº”ç”¨
    â”œâ”€â”€ main.py # å‘½ä»¤è¡Œç¨‹åº
    â””â”€â”€ requirements.txt    # ä¾èµ–
    

`requirements.txt` ä¸­åˆ—å‡ºäº†ç¨‹åºå¿…è¦çš„åŒ…ï¼Œä½¿ç”¨å‘½ä»¤ `pip install -r requirements.txt` å®‰è£…ä¾èµ–ã€‚

    # LangChain ç›¸å…³åŒ…
    langchain
    langchain-ollama
    langchain-chroma
    langchain-community
    langgraph
    chromadb
    tavily-python
    python-dotenv
    
    # æ–‡æ¡£å¤„ç†ç›¸å…³åŒ…
    marker-pdf
    weasyprint
    mammoth
    openpyxl
    unstructured[all-docs]
    libmagic
    
    # Streamlit ç›¸å…³åŒ…
    streamlit
    streamlit-chat
    streamlit-extras
    
    # æ–‡æ¡£ä½¿ç”¨GPUå¤„ç†æ—¶ï¼Œå®‰è£…GPUç‰ˆPyTorch
    # use 'pip install -r requirements.txt --proxy=127.0.0.1:23474' to accelerate download speed
    # --extra-index-url https://download.pytorch.org/whl/cu124
    # torch==2.6.0+cu124
    # torchvision==0.21.0+cu124
    

ç›¸å…³çš„ç¯å¢ƒå˜é‡é…ç½®åœ¨ `.env` æ–‡ä»¶ä¸­ï¼Œåœ¨ç¨‹åºä¸­é€šè¿‡ `dotenv` åŒ…è¯»å–ã€‚

    TAVILY_API_KEY=tvly-dev-xxxxxx  # Tavily API å¯†é’¥
    OMP_NUM_THREADS=8   # è®¾ç½®çº¿ç¨‹æ•°
    

å…¶ä¸­ `TAVILY_API_KEY` æ˜¯ Tavily çš„ API å¯†é’¥ï¼Œç”¨äºç½‘ç»œæœç´¢æœåŠ¡ï¼Œéœ€è¦åœ¨ [https://app.tavily.com/home](https://app.tavily.com/home) æ³¨å†Œå¹¶è·å–ï¼Œæ¯æœˆæœ‰ 1000 æ¬¡çš„å…è´¹é¢åº¦ã€‚

![](https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155948515-749727306.png)

å®šä¹‰æ™ºèƒ½ä½“
-----

åœ¨ LangChain ä¸­ï¼Œä½¿ç”¨ **é“¾ï¼ˆchainï¼‰** æ¥å®šä¹‰ç”¨æˆ·ä¸ LLM äº¤äº’çš„è¡Œä¸ºï¼Œå³æ™ºèƒ½ä½“ã€‚é“¾æ˜¯ä¸€ä¸ªå¯è°ƒç”¨çš„å¯¹è±¡ï¼Œæ¥æ”¶è¾“å…¥å¹¶è¿”å›è¾“å‡ºã€‚åœ¨ `chains` ç›®å½•ä¸‹ï¼Œå®šä¹‰äº†ä¸¤ä¸ªé“¾ï¼š`summary.py` å’Œ `generate.py`ï¼Œåˆ†åˆ«ç”¨äºæå–å…³é”®è¯å’Œç”Ÿæˆå›ç­”ã€‚

    .
    â”œâ”€â”€ chains  # æ™ºèƒ½ä½“
    â”‚Â Â  â”œâ”€â”€ generate.py
    â”‚Â Â  â”œâ”€â”€ models.py
    â”‚Â Â  â””â”€â”€ summary.py
    

### åŠ è½½æ¨¡å‹

åœ¨å®šä¹‰æ™ºèƒ½ä½“ä¹‹å‰ï¼Œéœ€è¦å…ˆå®šä¹‰å¥½åŠ è½½æ¨¡å‹çš„æ–¹æ³•ã€‚`models.py` æ–‡ä»¶è´Ÿè´£æ ¹æ®æä¾›çš„æ¨¡å‹åç§°åŠ è½½ç›¸åº”çš„æ¨¡å‹ã€‚

    from langchain_ollama import ChatOllama, OllamaEmbeddings
    from langchain_core.vectorstores import InMemoryVectorStore
    
    def load_model(model_name: str) -> ChatOllama:
        """
        åŠ è½½è¯­è¨€æ¨¡å‹
    
        å‚æ•°:
            model_name (str): æ¨¡å‹åç§°
    
        è¿”å›:
            ChatOllamaå®ä¾‹ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å’Œå›ç­”é—®é¢˜
        """
        return ChatOllama(model=model_name)
        
    def load_embeddings(model_name: str) -> OllamaEmbeddings:
        """
        åŠ è½½åµŒå…¥æ¨¡å‹
    
        å‚æ•°:
            model_name (str): æ¨¡å‹åç§°
    
        è¿”å›:
            OllamaEmbeddingså®ä¾‹ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
        """
        return OllamaEmbeddings(model=model_name)
    
    def load_vector_store(model_name: str) -> InMemoryVectorStore:
        """
        åˆ›å»ºå†…å­˜å‘é‡å­˜å‚¨
    
        å‚æ•°:
            model_name (str): ç”¨äºç”ŸæˆåµŒå…¥çš„æ¨¡å‹åç§°
    
        è¿”å›:
            InMemoryVectorStoreå®ä¾‹ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡åŒ–çš„æ–‡æœ¬
        """
        embeddings = load_embeddings(model_name)
        return InMemoryVectorStore(embeddings)
    

### æå–å…³é”®è¯

åœ¨ `summary.py` æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº† `SummaryChain` ç±»ï¼Œç”¨äºä»ç”¨æˆ·é—®é¢˜å’ŒèŠå¤©è®°å½•ä¸­æå–å…³é”®è¯ï¼Œå¹¶ç”Ÿæˆé«˜æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚

    from langchain.prompts import ChatPromptTemplate
    from chains.models import load_model
    
    class SummaryChain:
        """
        ä¸€ä¸ªç”¨äºç”Ÿæˆæœç´¢æŸ¥è¯¢çš„ç±»ã€‚
        å®ƒä»ç”¨æˆ·é—®é¢˜å’ŒèŠå¤©è®°å½•ä¸­æå–å…³é”®è¯ï¼Œå¹¶ç”Ÿæˆé«˜æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚
        """
        def __init__(self, model_name):
            """
            åˆå§‹åŒ– SummaryChain ç±»ï¼Œå¹¶åŠ è½½æŒ‡å®šçš„è¯­è¨€æ¨¡å‹ã€‚
    
            å‚æ•°:
                model_name (str): è¦åŠ è½½çš„è¯­è¨€æ¨¡å‹çš„åç§°ã€‚
            """
            self.llm = load_model(model_name)
            self.prompt = ChatPromptTemplate.from_template(
                "You are a professional assistant specializing in extracting keywords from user questions and chat histories. Extract keywords and connect them with spaces to output a efficient and precise search query. Be careful not answer the question directly, just output the search query.\n\nHistories: {history}\n\nQuestion: {question}"
            )
            self.chain = self.prompt | self.llm
    
        def invoke(self, input_data):
            """
            ä½¿ç”¨æä¾›çš„è¾“å…¥æ•°æ®è°ƒç”¨é“¾ä»¥ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€‚
    
            å‚æ•°:
                input_data (dict): åŒ…å« 'history' å’Œ 'question' é”®çš„å­—å…¸ã€‚
    
            è¿”å›:
                str: é“¾ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢ã€‚
            """
            return self.chain.invoke(input_data)
    

### ç”Ÿæˆå›ç­”

åœ¨ `generate.py` æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº† `GenerateChain` ç±»ï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜ã€èŠå¤©è®°å½•å’Œæ–‡æ¡£å†…å®¹ç”Ÿæˆå›ç­”ã€‚

    from langchain.prompts import ChatPromptTemplate
    from chains.models import load_model
    
    class GenerateChain:
        """
        ä¸€ä¸ªç”¨äºç”Ÿæˆé—®ç­”ä»»åŠ¡å“åº”çš„ç±»ã€‚
        å®ƒä½¿ç”¨è¯­è¨€æ¨¡å‹å’Œæç¤ºæ¨¡æ¿æ¥å¤„ç†è¾“å…¥æ•°æ®ã€‚
        """
        def __init__(self, model_name):
            """
            åˆå§‹åŒ– GenerateChain ç±»ï¼Œå¹¶åŠ è½½æŒ‡å®šçš„è¯­è¨€æ¨¡å‹ã€‚
    
            å‚æ•°:
                model_name (str): è¦åŠ è½½çš„è¯­è¨€æ¨¡å‹çš„åç§°ã€‚
            """
            self.llm = load_model(model_name)
            self.prompt = ChatPromptTemplate.from_template("You are an assistant for question-answering tasks. Use the following documents or chat histories to answer the question. If the documents or chat histories is empty, answer the question based on your own knowledge. If you don't know the answer, just say that you don't know.\n\nDocuments: {documents}\n\nHistories: {history}\n\nQuestion: {question}")
    
            self.chain = self.prompt | self.llm
    
        def invoke(self, input_data):
            """
            ä½¿ç”¨æä¾›çš„è¾“å…¥æ•°æ®è°ƒç”¨é“¾ä»¥ç”Ÿæˆå“åº”ã€‚
    
            å‚æ•°:
                input_data (dict): åŒ…å« 'documents'ã€'history' å’Œ 'question' é”®çš„å­—å…¸ã€‚
    
            è¿”å›:
                str: é“¾ç”Ÿæˆçš„å“åº”ã€‚
            """
            return self.chain.invoke(input_data)
    

è¿æ¥æ™ºèƒ½ä½“
-----

åœ¨ LangGraph ä¸­ï¼Œæ™ºèƒ½ä½“ä¹‹é—´çš„è¿æ¥é€šè¿‡ **çŠ¶æ€å›¾ï¼ˆgraphï¼‰** æ¥å®ç°ï¼Œä½¿ç”¨ **çŠ¶æ€ï¼ˆstateï¼‰** å­˜å‚¨äº¤äº’çš„ä¿¡æ¯ã€‚å›¾ç”±èŠ‚ç‚¹ï¼ˆnodeï¼‰å’Œè¾¹ï¼ˆedgeï¼‰ç»„æˆï¼ŒèŠ‚ç‚¹è¡¨ç¤ºæ™ºèƒ½ä½“ï¼Œè¾¹è¡¨ç¤ºæ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»ã€‚åœ¨ `graph` ç›®å½•ä¸‹å®šä¹‰äº†ä¸¤ä¸ªæ–‡ä»¶ï¼š`graph.py` å’Œ `graph_state.py`ã€‚

    .
    â”œâ”€â”€ graph   # å›¾ç»“æ„
    â”‚Â Â  â”œâ”€â”€ graph.py
    â”‚Â Â  â””â”€â”€ graph_state.py
    

### å®šä¹‰å›¾çš„çŠ¶æ€

åœ¨ `graph_state.py` æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº† `GraphState` ç±»ï¼Œç”¨äºå­˜å‚¨å›¾çš„çŠ¶æ€ä¿¡æ¯ã€‚

    from typing import Literal, Annotated, Optional
    from typing_extensions import TypedDict
    from langgraph.graph.message import add_messages
    
    class GraphState(TypedDict):
        """
        å®šä¹‰å›¾çŠ¶æ€çš„ç±»å‹å­—å…¸ã€‚
        ç”¨äºè¡¨ç¤ºå›¾ä¸­çš„çŠ¶æ€ä¿¡æ¯ã€‚
        """
        model_name: str  # ä½¿ç”¨çš„æ¨¡å‹åç§°
        type: Literal["websearch", "file", "chat"]  # æ“ä½œç±»å‹ï¼ŒåŒ…æ‹¬è”ç½‘æœç´¢ã€ä¸Šä¼ æ–‡ä»¶å’ŒèŠå¤©
        messages: Annotated[list, add_messages]  # æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿ç”¨add_messagesæ³¨è§£å¤„ç†æ¶ˆæ¯è¿½åŠ 
        documents: Optional[list] = []  # æ–‡æ¡£åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©ºåˆ—è¡¨
    

### å®šä¹‰èŠ‚ç‚¹æ–¹æ³•

åœ¨ `graph.py` æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº†å¤šä¸ªæ–¹æ³•ï¼Œè¡¨ç¤ºå›¾çš„ç»“æ„å’Œè¡Œä¸ºï¼Œç”¨äºå¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚ã€‚å…ˆå¼•å…¥æ‰€éœ€è¦çš„åŒ…ã€‚

    import os
    from langchain.schema import Document
    from langchain_core.runnables import RunnableConfig
    from langchain_community.document_loaders import TextLoader
    from langchain_community.tools.tavily_search import TavilySearchResults
    from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
    from langgraph.graph.state import StateGraph, CompiledStateGraph, END
    from langgraph.checkpoint.memory import MemorySaver
    from marker.converters.pdf import PdfConverter
    from marker.models import create_model_dict
    from marker.output import text_from_rendered
    from graph.graph_state import GraphState
    from chains.summary import SummaryChain
    from chains.generate import GenerateChain
    

#### æ ¹æ®æŒ‡ä»¤è·¯ç”±

`route_question()` æ–¹æ³•æ ¹æ® `GraphState` ç±»ä¸­çš„æ“ä½œç±»å‹å°†è¯·æ±‚è·¯ç”±åˆ°ç›¸åº”çš„å¤„ç†èŠ‚ç‚¹ã€‚

    def route_question(state: GraphState) -> str:
        """
        æ ¹æ®æ“ä½œç±»å‹è·¯ç”±åˆ°ç›¸åº”çš„å¤„ç†èŠ‚ç‚¹ã€‚
    
        å‚æ•°:
            state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
    
        è¿”å›:
            str: ä¸‹ä¸€ä¸ªè¦è°ƒç”¨çš„èŠ‚ç‚¹åç§°
        """
    
        print("--- ROUTE QUESTION ---")
        if state['type'] == 'websearch':
            print("--- ROUTE QUESTION TO EXTRACT KEYWORDS ---")
            return "extract_keywords"
        if state['type'] == 'file':
            print("--- ROUTE QUESTION TO FILE PROCESS ---")
            return "file_process"
        elif state['type'] == 'chat':
            print("--- ROUTE QUESTION TO GENERATE ---")
            return "generate"
    

å½“ç„¶ï¼Œä¹Ÿå¯ä»¥å°†è·¯ç”±äº¤ç»™ LLM å†³å®šï¼Œåªéœ€è¦å†™å¥½ç›¸åº”çš„æç¤ºè¯å³å¯ï¼Œä¾‹å¦‚ä¸‹é¢çš„æç¤ºè¯å°†ç”± LLM å†³å®šæ˜¯è¿›è¡ŒçŸ¥è¯†åº“æŸ¥è¯¢è¿˜æ˜¯ç½‘ç»œæœç´¢ã€‚

    from langchain_core.output_parsers import JsonOutputParser
    
    prompt = ChatPromptTemplate.from_template("You are an expert at routing a user question to a vectorstore or web search. Use the vectorstore for questions on LangChain and LangGraph. You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and no premable or explaination. Question to route: {question}")
    router = prompt | llm | JsonOutputParser()
    
    source = router.invoke({"question": question})
    if source['datasource'] == 'web_search':
        # TODO: route to web search
    elif source['datasource'] == 'vectorstore':
        # TODO: route to vectorstore
    

#### ç”Ÿæˆå›ç­”

`generate()` æ–¹æ³•æ ¹æ®ç”¨æˆ·é—®é¢˜ã€èŠå¤©è®°å½•å’Œæ–‡æ¡£å†…å®¹ç”Ÿæˆå›ç­”ã€‚

    def generate(state: GraphState) -> GraphState:
        """
        æ ¹æ®æ–‡æ¡£å’Œå¯¹è¯å†å²ç”Ÿæˆç­”æ¡ˆã€‚
    
        å‚æ•°:
            state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
    
        è¿”å›:
            state (GraphState): è¿”å›æ·»åŠ äº†LLMç”Ÿæˆå†…å®¹çš„æ–°çŠ¶æ€
        """
    
        print("--- GENERATE ---")
        chain = GenerateChain(state["model_name"])
        messages = state["messages"]
        state["messages"] = chain.invoke({"question": messages[-1].content, "history": messages[:-1], "documents": state["documents"]})
        return state
    

#### æ–‡ä»¶å¤„ç†

`file_process()` æ–¹æ³•å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæå–æ–‡æœ¬å†…å®¹å¹¶è¿›è¡Œè¯åµŒå…¥ï¼ˆembeddingï¼‰ï¼Œç„¶åå°†å‘é‡å­˜å‚¨è‡³å†…å­˜æ•°æ®åº“ä¸­ã€‚`config` æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå­˜å‚¨ LLM è¿è¡Œæ—¶çš„é…ç½®å‚æ•°ï¼Œä¼šåœ¨è°ƒç”¨ LLM æ—¶ä¼ å…¥ã€‚

    def file_process(state: GraphState, config: RunnableConfig) -> GraphState:
        """
        å¤„ç†æ–‡ä»¶ã€‚
    
        å‚æ•°:
            state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
            config (RunnableConfig): å¯è¿è¡Œé…ç½®
    
        è¿”å›:
            state (GraphState): è¿”å›å›¾çŠ¶æ€ï¼Œå°†æ–‡æ¡£æ·»åŠ  config ä¸­çš„å‘é‡å­˜å‚¨
        """
    
        print("--- FILE PROCESS ---")
        vector_store = config["configurable"]["vectorstore"]
    
        for doc in state["documents"]:
            file_path: str = doc.page_content
            if os.path.exists(file_path):
                split_docs: list[Document] = None
                if file_path.endswith(".txt") or file_path.endswith(".md"):
                    # å¤„ç†æ–‡æœ¬æˆ–Markdownæ–‡ä»¶
                    docs = TextLoader(file_path, autodetect_encoding=True).load()
                    # æ–‡æœ¬åˆ†å‰²
                    splitter = RecursiveCharacterTextSplitter(separators=["\n\n", "\n", " ", ".", ",", "\u200B", "\uff0c", "\u3001", "\uff0e", "\u3002", ""], chunk_size=1000, chunk_overlap=100, add_start_index=True)
                    split_docs = splitter.split_documents(docs)
                else: 
                    # ä½¿ç”¨ marker-pdf å¤„ç†å…¶ä»–æ–‡ä»¶
                    converter = PdfConverter(artifact_dict=create_model_dict())
                    rendered = converter(file_path)
                    docs, _, _ = text_from_rendered(rendered)
                    splitter = MarkdownHeaderTextSplitter([("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")], strip_headers = False)
                    split_docs = splitter.split_text(docs)
                # å°†å¤„ç†åçš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
                vector_store.add_documents(split_docs)
        return state
    

#### æå–å…³é”®è¯

`extract_keywords()` æ–¹æ³•ä»ç”¨æˆ·é—®é¢˜å’ŒèŠå¤©è®°å½•ä¸­æå–å…³é”®è¯ï¼Œå¹¶ç”Ÿæˆé«˜æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚

    def extract_keywords(state: GraphState, config: RunnableConfig) -> GraphState:
        """
        ä»é—®é¢˜ä¸­æå–å…³é”®è¯ã€‚
    
        å‚æ•°:
            state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
            config (RunnableConfig): å¯è¿è¡Œé…ç½®
    
        è¿”å›:
            state (GraphState): è¿”å›æ·»åŠ äº†æå–å…³é”®è¯çš„æ–°çŠ¶æ€
        """
    
        print("--- EXTRACT KEYWORDS ---")
        chain = SummaryChain(state["model_name"])
        messages = state["messages"]
        query = chain.invoke({"question": messages[-1].content, "history": messages[:-1]})
        print(query.content)
    
        if state["type"] == "websearch":
            # å°†ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­ï¼Œä¸‹ä¸€ä¸ªèŠ‚ç‚¹å°†ä¼šä½¿ç”¨
            state["messages"] = query
        elif state["type"] == "file":
            # ä½¿ç”¨ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
            docs = config["configurable"]["vectorstore"].max_marginal_relevance_search(query.content)
            state["documents"] = docs
        return state
    

å¯¹äºâ€œä¸Šä¼ æ–‡ä»¶â€ï¼Œâ€œæå–å…³é”®è¯â€æ—¶å·²ç»è¿›è¡Œäº†æŸ¥è¯¢å¤„ç†ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œâ€œç”Ÿæˆå›ç­”â€ï¼›å¯¹äºâ€œç½‘ç»œæœç´¢â€ï¼Œâ€œæå–å…³é”®è¯â€è¿›è¡Œæœç´¢åï¼Œæ‰èƒ½è¿›è¡Œâ€œç”Ÿæˆå›ç­”â€ã€‚æ‰§è¡Œè·¯å¾„ä¸åŒï¼Œè¿˜éœ€è¦è¿›è¡Œåˆ¤æ–­ã€‚

    def decide_to_generate(state: GraphState) -> str:
        """
        å†³å®šæ˜¯è¿›è¡Œç½‘ç»œæœç´¢è¿˜æ˜¯ç›´æ¥ç”Ÿæˆå›ç­”ã€‚
    
        å‚æ•°:
            state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
    
        è¿”å›:
            str: ä¸‹ä¸€ä¸ªè¦è°ƒç”¨çš„èŠ‚ç‚¹åç§°
        """
    
        if state["type"] == "websearch":
            print("--- DECIDE TO WEB SEARCH ---")
            return "websearch"
        elif state["type"] == "file":
            print("--- DECIDE TO GENERATE ---")
            return "generate"
    

#### ç½‘ç»œæœç´¢

`web_search()` æ–¹æ³•ä½¿ç”¨ Tavily API è¿›è¡Œç½‘ç»œæœç´¢ï¼Œè·å–å®æ—¶ä¿¡æ¯ã€‚

    def web_search(state: GraphState) -> GraphState:
        """
        åŸºäºé—®é¢˜è¿›è¡Œç½‘ç»œæœç´¢ã€‚
    
        å‚æ•°:
            state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
    
        è¿”å›:
            state (GraphState): è¿”å›æ·»åŠ äº†ç½‘ç»œæœç´¢ç»“æœçš„æ–°çŠ¶æ€
        """
    
        print("--- WEB SEARCH ---")
        web_search_tool = TavilySearchResults(k=3)
        documents = state["documents"]
        try:
            docs = web_search_tool.invoke({"query": state["messages"][-1].content})
            web_results = "\n".join([d["content"] for d in docs])
            web_results = Document(page_content=web_results)
            documents.append(web_results)
            state["documents"] = documents
        except:
            pass
        return state
    

### å®šä¹‰å›¾çš„ç»“æ„

åœ¨ LangGraph ä¸­ï¼Œå›¾çš„è¾¹åˆ†ä¸ºæ™®é€šè¾¹å’Œæ¡ä»¶è¾¹ã€‚æ™®é€šè¾¹è¡¨ç¤ºä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„ç›´æ¥è¿æ¥ï¼Œè€Œæ¡ä»¶è¾¹åˆ™æ ¹æ®ç‰¹å®šæ¡ä»¶å†³å®šæ˜¯å¦è¿æ¥ä¸¤ä¸ªèŠ‚ç‚¹ã€‚åœ¨ `create_graph()` æ–¹æ³•ä¸­å®šä¹‰äº†å›¾çš„ç»“æ„ï¼š`add_node()` æ–¹æ³•å°†å®šä¹‰çš„èŠ‚ç‚¹æ–¹æ³•æ·»åŠ åˆ°å›¾ä¸­ï¼›`add_edge()` æ–¹æ³•å®šä¹‰äº†èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å…³ç³»ï¼Œä¹Ÿå°±æ˜¯æ™®é€šè¾¹ï¼›`add_conditional_edges()` æ–¹æ³•å®šä¹‰äº†æ¡ä»¶è¾¹çš„è¿æ¥å…³ç³»ï¼›`set_conditional_entry_point()` æ–¹æ³•å®šä¹‰äº†å›¾çš„æ¡ä»¶å…¥å£èŠ‚ç‚¹ã€‚

![](https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155939600-1605236821.png)

    def create_graph() -> CompiledStateGraph:
        """
        åˆ›å»ºå¹¶é…ç½®çŠ¶æ€å›¾å·¥ä½œæµã€‚
    
        è¿”å›:
            CompiledStateGraph: ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾
        """
    
        workflow = StateGraph(GraphState)
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("websearch", web_search)
        workflow.add_node("extract_keywords", extract_keywords)
        workflow.add_node("file_process", file_process)
        workflow.add_node("generate", generate)
        # æ·»åŠ è¾¹
        workflow.set_conditional_entry_point(
            route_question,
            {
                "extract_keywords": "extract_keywords",
                "generate": "generate",
                "file_process": "file_process",
            },
        )
        workflow.add_edge("file_process", "extract_keywords")
        workflow.add_conditional_edges(
            "extract_keywords",
            decide_to_generate,
            {
                "websearch": "websearch",
                "generate": "generate",
            },
        )
        workflow.add_edge("websearch", "generate")
        workflow.add_edge("generate", END)
    
        # åˆ›å»ºå›¾ï¼Œå¹¶ä½¿ç”¨ `MemorySaver()` åœ¨å†…å­˜ä¸­ä¿å­˜çŠ¶æ€
        return workflow.compile(checkpointer=MemorySaver())
    

### è¿è¡Œå›¾

æœ€åé€šè¿‡ `stream_graph_updates()` æ–¹æ³•è¿è¡Œå›¾ï¼Œå¹¶æµå¼è¿”å›ç»“æœå†…å®¹ã€‚

    def stream_graph_updates(graph: CompiledStateGraph, user_input: GraphState, config: dict):
        """
        æµå¼å¤„ç†å›¾æ›´æ–°å¹¶è¿”å›æœ€ç»ˆç»“æœã€‚
    
        å‚æ•°:
            graph (CompiledStateGraph): ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾
            user_input (GraphState): ç”¨æˆ·è¾“å…¥çš„çŠ¶æ€
            config (dict): é…ç½®å­—å…¸
    
        è¿”å›:
            generator: ç”Ÿæˆå™¨å¯¹è±¡ï¼Œé€æ­¥è¿”å›å›¾æ›´æ–°çš„å†…å®¹
        """
    
        for chunk, _ in graph.stream(user_input, config, stream_mode="messages"):
            yield chunk.content
    

è¿è¡ŒæŒ‡å—
----

### åœ¨æ§åˆ¶å°ä¸­æµ‹è¯•ç¨‹åº

åœ¨ `main.py` æ–‡ä»¶ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ªå‘½ä»¤è¡Œç¨‹åºï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¾“å…¥é—®é¢˜ä¸æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ã€‚

    import uuid
    from dotenv import load_dotenv
    from langchain.schema import Document
    from langchain_core.messages import AIMessage, HumanMessage
    from chains.models import load_vector_store
    from graph.graph import create_graph, stream_graph_updates, GraphState
    
    def main():
        # langchain.debug = True  # å¯ç”¨langchainè°ƒè¯•æ¨¡å¼ï¼Œå¯ä»¥è·å¾—å¦‚å®Œæ•´æç¤ºè¯ç­‰ä¿¡æ¯
        load_dotenv(verbose=True)  # åŠ è½½ç¯å¢ƒå˜é‡é…ç½®
    
        # åˆ›å»ºçŠ¶æ€å›¾ä»¥åŠå¯¹è¯ç›¸å…³çš„è®¾ç½®
        config = {"configurable": {"thread_id": uuid.uuid4().hex, "vectorstore": load_vector_store("nomic-embed-text")}}  
        state = GraphState(
            model_name="qwen2.5:7b",
            type="chat",
            documents=[Document(page_content="upload_files/test.pdf")],
        )
        graph = create_graph()
    
        # å¯¹è¯
        while True:
            user_input = input("User: ")
            if user_input.lower() in ["exit", "quit"]:
                break
            state["messages"] = HumanMessage(user_input)
            # æµå¼è·å–AIçš„å›å¤
            for answer in stream_graph_updates(graph, state, config):
                print(answer, end="")
            print()
    
        # æ‰“å°å¯¹è¯å†å²
        print("\nHistory: ")
        for message in graph.get_state(config).values["messages"]:
            if isinstance(message, AIMessage):
                prefix = "AI"
            else:
                prefix = "User"
            print(f"{prefix}: {message.content}")
    
    if __name__ == "__main__":
        main()
    

### ä½¿ç”¨ Streamlit æ„å»ºå‰ç«¯é¡µé¢

åœ¨ `app.py` æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨ `Streamlit` æ„å»ºäº†ä¸€ä¸ªç®€å•çš„å‰ç«¯ç•Œé¢ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è¾“å…¥æ¡†ä¸æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ã€‚å®Œæ•´çš„ç¨‹åºä»£ç å¦‚ä¸‹ï¼š

![](https://img2024.cnblogs.com/blog/1085877/202504/1085877-20250406155913943-35085412.png)

    import uuid
    import datetime
    from dotenv import load_dotenv
    from langchain.schema import Document
    import streamlit as st
    from streamlit_extras.bottom_container import bottom
    from chains.models import load_vector_store
    from graph.graph import create_graph, stream_graph_updates, GraphState
    
    # è®¾ç½®ä¸Šä¼ æ–‡ä»¶çš„å­˜å‚¨è·¯å¾„
    file_path = "upload_files/"
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv(verbose=True)
    
    def upload_pdf(file):
        """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
        with open(file_path + file.name, "wb") as f:
            f.write(file.getbuffer())
            return file_path + file.name
    
    # è®¾ç½®é¡µé¢é…ç½®ä¿¡æ¯
    st.set_page_config(
        page_title="AI-Powerwd Assistant",
        page_icon="ğŸŒ",
        layout="wide"
    )
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡ï¼Œåˆ›å»ºå›¾
    if "graph" not in st.session_state:
        st.session_state.graph = create_graph()
    # åˆå§‹åŒ–ä¼šè¯IDå’Œå‘é‡å­˜å‚¨
    if "config" not in st.session_state:
        st.session_state.config = {"configurable": {"thread_id": uuid.uuid4().hex, "vectorstore": load_vector_store("nomic-embed-text")}}
    # åˆå§‹åŒ–å¯¹è¯å†å²è®°å½•
    if "history" not in st.session_state:
        st.session_state.history = []
    # åˆå§‹åŒ–ä¸Šä¼ çŠ¶æ€ã€æ¨¡å‹åç§°å’Œå¯¹è¯ç±»å‹
    if "settings" not in st.session_state:
        st.session_state.settings = {"uploaded": False, "model_name": "qwen2.5:7b", "type": "chat"}
    
    # æ˜¾ç¤ºåº”ç”¨æ ‡é¢˜
    st.header("ğŸ‘½ AI-Powerwd Assistant")
    
    # å®šä¹‰å¯é€‰çš„æ¨¡å‹
    model_options = {"é€šä¹‰åƒé—® 2.5 7B": "qwen2.5:7b", "DeepSeek R1 7B": "deepseek-r1:7b"}
    with st.sidebar:
        # ä¾§è¾¹æ è®¾ç½®éƒ¨åˆ†
        st.header("è®¾ç½®")
        # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
        st.session_state.settings["model_name"] = model_options[st.selectbox("é€‰æ‹©æ¨¡å‹", model_options, index=list(model_options.values()).index(st.session_state.settings["model_name"]))]
    
        st.divider()
    
        # æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯
        st.text(f"{datetime.datetime.now().strftime('%Y.%m.%d')} - ZHANG GAOXING")
    
    # å®šä¹‰å¯¹è¯ç±»å‹é€‰é¡¹
    type_options = {"ğŸ¤– å¯¹è¯": "chat", "ğŸ” è”ç½‘æœç´¢": "websearch", "ğŸ‘¾ ä»£ç æ¨¡å¼": "code"}
    question = None
    with bottom():
        # åº•éƒ¨å®¹å™¨ï¼ŒåŒ…å«å·¥å…·é€‰æ‹©ã€æ–‡ä»¶ä¸Šä¼ å’Œè¾“å…¥æ¡†
        st.session_state.settings["type"] = type_options[st.radio("å·¥å…·é€‰æ‹©", type_options.keys(), horizontal=True, label_visibility="collapsed", index=list(type_options.values()).index(st.session_state.settings["type"]))]
        # æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
        uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["pdf", "docx", "xlsx", "txt", "md"], accept_multiple_files=False, label_visibility="collapsed")
        # èŠå¤©è¾“å…¥æ¡†
        question = st.chat_input('è¾“å…¥ä½ è¦è¯¢é—®çš„å†…å®¹')
    
    # æ˜¾ç¤ºå†å²å¯¹è¯å†…å®¹
    for message in st.session_state.history:
        with st.chat_message(message["role"]):
          st.markdown(message["content"])  
    
    # å¤„ç†ç”¨æˆ·æé—®
    if question: 
        # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        with st.chat_message("user"):
            st.markdown(question)
    
        # å‡†å¤‡è¯·æ±‚çŠ¶æ€
        state = []
        if st.session_state.settings["type"] == "code":
            # ä»£ç æ¨¡å¼ä½¿ç”¨ä¸“é—¨çš„ä»£ç æ¨¡å‹
            state = {"model_name": "qwen2.5-coder:7b", "messages": [{"role": "user", "content": question}], "type": "chat", "documents": []}
        else:
            # å…¶ä»–æ¨¡å¼ä½¿ç”¨é€‰æ‹©çš„æ¨¡å‹
            state = {"model_name": st.session_state.settings["model_name"], "messages": [{"role": "user", "content": question}], "type": st.session_state.settings["type"], "documents": []}
    
        # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
        if uploaded_file:
            state["type"] = "file"
            if not st.session_state.settings["uploaded"]:
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                file_path = upload_pdf(uploaded_file)
                # æ·»åŠ æ–‡æ¡£åˆ°è¯·æ±‚
                state["documents"].append(Document(page_content=file_path))
                st.session_state.settings["uploaded"] = True
    
        # è·å–AIå›ç­”å¹¶ä»¥æµå¼æ–¹å¼æ˜¾ç¤º
        answer = st.chat_message("assistant").write_stream(stream_graph_updates(st.session_state.graph, state, st.session_state.config))
    
        # å°†å¯¹è¯ä¿å­˜åˆ°å†å²è®°å½•
        st.session_state.history.append({"role": "user", "content": question})
        st.session_state.history.append({"role": "assistant", "content": answer})