---
layout: post
title: 'CV中常用Backbone-2：ConvNeXt模型详解及其代码'
date: "2025-05-02T00:40:30Z"
---
CV中常用Backbone-2：ConvNeXt模型详解及其代码
================================

这里介绍新的一个Backbone：ConvNeXt，主要来自两篇比较老的来自Meta论文： 1、《\*\*A ConvNet for the 2020s\*\*》 > arXiv:2201.03545 2、《\*\*ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\*\*》 > arXiv:2301.00808 两篇论文讲的都是一个模型：\*ConvNeXt\*。这也是证明一点：Vit效果好并不是attention本身而是因为transform的超大感受野和各种trick。因此作者也是不断借鉴Vit的操作（用斜体表示）

之前介绍了CV常用Backbon：  
[CV中常用Backbone-1：Resnet/Unet/Vit系列/多模态系列等)以及代码](https://www.big-yellow-j.top/posts/2025/01/18/CV-Backbone.html)  
这里介绍新的一个Backbone：ConvNeXt，主要来自两篇比较老的来自Meta论文：  
1、《**A ConvNet for the 2020s**》

> arXiv:2201.03545

2、《**ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders**》

> arXiv:2301.00808  
> 两篇论文讲的都是一个模型：_ConvNeXt_。这也是证明一点：Vit效果好并不是attention本身而是因为transform的超大感受野和各种trick。因此作者也是不断借鉴Vit的操作（用斜体表示）

ConvNeXt v1
-----------

> A ConvNet for the 2020s  
> ⚙-官方代码：[https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py](https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py)  
> ⚙-自己修改：[https://www.big-yellow-j.top/code/ConvNeXt.py](https://www.big-yellow-j.top/code/ConvNeXt.py.txt)

![](https://img2024.cnblogs.com/blog/3395559/202505/3395559-20250501144947732-2059857809.png)  
值得注意的一点是在 _ConvNeXt_ 其实就是一个大型的模型调参（不断调节网络参数取得不错效果，于此同时作者对于模型为什么这么做也都是：对比其他模型做法而后而后借鉴到自己做法中），首先作者在论文中做了如下的一些对比（和采用 swin-transformer的resnet进行对比）：  
![](https://img2024.cnblogs.com/blog/3395559/202505/3395559-20250501144947745-581585027.png)

模型在改进上主要是如下几点：1、macro design；2、 ResNeXt；3、inverted bottleneck；4、large kernel size；5、various layer-wise micro designs。

*   **1、Macro design**

> 这一点主要是对模型的参数结构做了调整在准确率的提升上起到的效果还是比较有限的

在这里作者主要是做了如下几点修改：1、**修改堆叠数量**。将ResNet-50中的block堆叠数量从：\\((3,4,6,3)\\) 改为：\\((3,3,9,3)\\)。之所以这样设计作者对比 _Swin Transformers中主要的比率_ 为：\\((1,1,9,1)\\) 通过这样调整对于准确率提升还是比较有限的（78.8%-->79.4%），resnet中堆叠数量  
![](https://img2024.cnblogs.com/blog/3395559/202505/3395559-20250501144947275-479317548.png)  
2、**修改卷积核**。这点没有过多解释直接使用：步长为4，大小也为4的卷积操作（这里是因为：在 _Vit网络架构中通常使用一个步长为4，大小也为4的卷积_ ），准确率有79.4%-->79.5%  
除此之外作者还有一点修改就是将最初的 _通道数由64调整成96和Swin Transformer保持一致_ ，准确率：80.5%

*   **2、Inverted bottleneck**

> 上面第1点是做模型宏观参数（卷积核大小等）做修改，而在这里作者做得主要修改网络结构顺序

![](https://img2024.cnblogs.com/blog/3395559/202505/3395559-20250501144947088-652150266.png)

> a：resnet；b：MobileNetV2；c：ConvNeXt

这里作者给出的解释是：_在Vit中的MLP做的处理和上图中的（b）操作很相像_（代码：[⚙](https://www.big-yellow-j.top/code/CVBackbone/Vit.py.txt)）

    ...
    self.linear1 = nn.Linear(embed_dim, dim_feedforward)
    self.dropout = nn.Dropout(dropout)
    self.linear2 = nn.Linear(dim_feedforward, embed_dim)
    ...
    src2 = self.linear2(self.dropout(F.relu(self.linear1(src2))))
    

因此作者给出的做法是：先降低后提高。在较小的模型上准确率由80.5%提升到了80.6%，在较大的模型上准确率由81.9%提升到82.6%。

*   **3、Large Kernel Sizes**

> 换成更加大的卷积核操作

这里就比较简单直接将最开始的3x3卷积核改为7x7卷积核，它将模型的准确率提升至80.6%

*   **4、Micro Design**

**激活函数替换**：将Relu改为GELU（对结果影响不是很大）；  
**减少激活函数**：之前网络结构可能对每一个卷积处理之后都会使用一个激活函数处理，这里的话只在 两个 \\(1\\times1\\) 卷积后面添加一个激活函数进行处理；  
**减少归一化层**：因此在ConvNeXt中也使用了更少的归一化操作，它仅在第一个\\(1\\times1\\)卷积之前添加了一个BN  
**替换归一化层**：像之前的描述最开始在卷积网络中都是用BN作为归一化层，这里作者使用LN也取得不错效果；  
**拆分采样层**：在残差网络中，它通常使用的是步长为 2的3x3卷积或者1x1卷积来进行降采样，这使得降采样层和其它层保持了基本相同的计算策略。但是 _Swin Transformer将降采样层从其它运算中剥离开来_ ，即使用一个步长为2的2x2卷积插入到不同的Stage之间。ConvNeXt也是采用了这个策略，而且在降采样前后各加入了一个LN，而且在全局均值池化之后也加入了一个LN，这些归一化用来保持模型的稳定性。这个策略将模型的准确率提升至82.0%  
![](https://img2024.cnblogs.com/blog/3395559/202505/3395559-20250501144947081-282144703.png)

总的来说这篇论文主要还是集中在玄学调参：1、对于ResNet去修改他的堆叠数量（1,1,9,1）；2、换更加大的卷积核（4x4，4）；3、更加少的激活函数核归一化处理（都只在1x1卷积之后进行操作）。提供如下几种模型  
![](https://img2024.cnblogs.com/blog/3395559/202505/3395559-20250501144947357-103429991.png)  
其中C代表4个stage中输入的通道数，B代表每个stage重复堆叠block的次数

ConvNeXt v2
-----------

> ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders

这篇文章主要就是两个内容：1、Masked Autoencoder（MAE）；2、Global Response Normalization（GRN）。将这两个内容用到 **ConvNeXt** 中来。依次介绍这两个内容。

*   **1、Masked Autoencoder**

这点其实并不是很新早在 何凯明提出模型（MAE）以及提到过（详细描述：[🔗](https://www.big-yellow-j.top/posts/2025/01/18/CV-Backbone.html#:~:text=768-,MAE%20%E4%B8%BB%E8%A6%81%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B,-1%E3%80%81patch)）在本文中也是：**原始图片随机移除60%的32x32的patches**。不过需要注意一点是：Vit的MAE和FCMAE（全卷积的MAE）有区别的前者是直接通过decoder将图像中被maske进行还原，而后者是“全局还原”，比如下图（上：ConvNeXt，下：Vit MAE）：  
![](https://img2024.cnblogs.com/blog/3395559/202505/3395559-20250501144947997-83641047.png)

*   **2、Global Response Normalization**

理解这个概念是作者在做FCMAE发现一个问题：有许多死亡或饱和的特征图，并且激活在各个通道之间变成了冗余。这种行为主要是在Convnext块中的Dimensive expantasion MLP层中观察到的

> there are many dead or saturated feature maps and the activation becomes redundant across channels.This behavior was mainly observed in the dimensionexpansion MLP layers in a ConvNeXt block

还是用上面图像在ConvNeXt v1中很多处理后的图像“失真”（可以理解为decoder构建不出较好的图像全局特征）在使用 **GRN**时候就可以解决这个问题。计算公式为：  
![](https://img2024.cnblogs.com/blog/3395559/202505/3395559-20250501144947384-116756144.png)

不过值得注意的是其和[instance norm](https://www.big-yellow-j.top/posts/2025/02/23/dl-norm.html#:~:text=%F0%9D%91%81-,Instance%2Dnorm,-%EF%BC%9A)区别（虽然都是对channel来计算）用一个例子描述：  
假设输入是一个 H×W×C 的特征图（比如 56×56×64）：  
GRN：计算**所有64个通道**在 56×56 空间上的全局平方和，作为归一化分母。每个通道的每个像素都被这个全局值标准化，通道间相互影响。  
InstanceNorm：对**每个通道单独计算** 56×56 的均值和方差。每个通道的像素只根据自己的统计量标准化，64个通道互不干扰

总结
--

提到的论文中可能在学术上可以提供的参考意义不大，毕竟都是拿来主义，先不管他为什么这样只要能够起到好的作用那他就是好的模型（🤪🤪🤪🤪🤪），另外一点值得注意的是在convNeXt论文出发点是：通过实验来证明Vit效果好的原因不是因为attention本身，而是因为transform的超大感受野和各种trick（我在卷积上使用Vit的操作，结果也可以实现这个效果，卷积不输你Vit！）。