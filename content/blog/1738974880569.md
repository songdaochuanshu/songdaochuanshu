---
layout: post
title: 'Windows11本地部署DeepSeek加速'
date: "2025-02-08T00:34:40Z"
---
Windows11本地部署DeepSeek加速
=======================

![Windows11本地部署DeepSeek加速](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250207151156735-2030046149.png) 本文介绍了一个可以相比之下更快速的在本地部署DeepSeek的方法，除了在上一篇博客中介绍的从Github或者Github加速网站获取Ollama之外，还可以通过从国内的其他大模型文件平台下载模型文件，来加速本地模型的构建。

技术背景
====

在上一篇[文章](https://www.cnblogs.com/dechinphy/p/18699554/deepseek)中我们介绍了在Ubuntu Linux操作系统上部署了一个DeepSeek-R1：14B，再通过其他电脑远程调用模型进行生成的方法。这里我们介绍一下Windows11安装Ollama+DeepSeek-R1模型的加速方法，因为这几天DeepSeek实在太火了，导致官方模型下载渠道网络不是很稳定，但其实有其他的方法可以加速这个下载过程。

安装Ollama
========

跟Ubuntu Linux上的操作比较类似，也是要从[Ollama官网](https://ollama.com/download)下载一个安装文件，然后直接双击安装就好了，没有配置安装路径的选项。安装完成后，可以在cmd中查看ollama的版本：

    \DeepSeek\models> ollama --version
    ollama version is 0.5.7
    

因为还没有下载模型，所以这里模型列表是空的：

    \DeepSeek\models> ollama list
    NAME    ID    SIZE    MODIFIED
    \DeepSeek\models>
    

接下来，我们要用ollama的create选项来构建一个本地模型，可以先查看这个操作的操作文档：

    \DeepSeek\models> ollama create --help
    Create a model from a Modelfile
    
    Usage:
      ollama create MODEL [flags]
    
    Flags:
      -f, --file string       Name of the Modelfile (default "Modelfile"
      -h, --help              help for create
      -q, --quantize string   Quantize model to this level (e.g. q4_0)
    
    Environment Variables:
          OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)
    

模型文件下载
======

最耗时的时间大概就是这一步，所以我建议是找一个下午启动这个任务，然后下载一个晚上，第二天再来构建。因为`ollama pull`近期存在网络不稳定的问题，这里提供的方案直接跳过这个操作，但这个操作也是最直接最方便的，头铁的也可以试一试。我个人比较推荐的是下载一个gguf模型文件到本地进行构建，这样一来还可以自定义文件模型存放的地址，不至于把某些资源紧缺的磁盘给搞崩了。关键的是下载的渠道，常规的是从huggingface官网去下载模型，但是网络可能会有问题。第二选择是去hf-mirror.com镜像网站下载模型，这也是比较多人推荐的，但是我本地访问这个镜像网站似乎也有问题。所以最终我选择了从阿里的[ModelScope](https://www.modelscope.cn/)下载模型guff文件，网络没有很快，但是非常稳定，而且我们本地最常使用的也是DeepSeek-R1从Qwen蒸馏出来的版本。我这次下载了[7B](https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF/files)和[32B](https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/files)的模型，一般可以选择其中最大的那个模型下载到本地进行构建。

再说一下硬件配置，我的单机硬件是3080Ti 16GB独显，运行32B显然是有点吃力，也就是能跑的样子。如果硬件条件跟我差不多的，建议上7B，如果是双显卡的，上14B这样压力会比较小，生成的速度也会比较快。

模型构建配置
======

使用ollama进行模型构建时，需要在本地创建一个模型配置文件，一般命名为`Modelfile`，这里有一个比较简单的参考：

    # gguf模型文件路径  
    FROM .\DeepSeek-R1-Distill-Qwen-32B-Q8_0.gguf
    
    # 模型模板配置  
    TEMPLATE """{{- if .System }}{{ .System }}{{ end }}  
    {{- range $i, $_ := .Messages }}  
    {{- $last := eq (len (slice $.Messages $i)) 1}}  
    {{- if eq .Role "user" }}  
      user: {{ .Content }}  
    {{- else if eq .Role "assistant" }}  
      assistant: {{ .Content }}{{- if not $last }}  
        {{- end }}  
    {{- end }}  
    {{- if and $last (ne .Role "assistant") }}  
      {{- end }}  
    {{- end }}"""  
    
    PARAMETER stop "<｜begin▁of▁sentence｜>"
    PARAMETER stop "<｜end▁of▁sentence｜>"
    PARAMETER stop "<｜User｜>"
    PARAMETER stop "<｜Assistant｜>"
    

这是一个32B的模型配置文件，7B的同理，只是改一下模型源文件的地址。然后使用ollama进行构建：

    \DeepSeek\models> ollama create deepseek-r1-32B -f .\Modelfile
    gathering model components
    gathering model components
    copying file sha256:e74588a163eb2f861e5b298c0975101cf02ec7b10784832b8feab2abbf3090a7 100%
    parsing GGUF
    using existing layer sha256:e74588a163eb2f861e5b298c0975101cf02ec7b10784832b8feab2abbf3090a7
    creating new layer sha256:c6df2d7e28451caa13e70224d8333d5c06e2ab519086ecde7b7fe9422695ddbd
    creating new layer sha256:f4d24e9138dd4603380add165d2b0d970bef471fac194b436ebd50e6147c6588
    writing manifest
    success
    

这里`deepseek-r1-32B`是自定义的本地模型名称，两个模型都构建完成后，可以在本地查看模型列表：

    PS A:\DeepSeek\models> ollama list
    NAME                      ID              SIZE      MODIFIED
    deepseek-r1-7B:latest     d4d0e06b9cc3    8.1 GB    6 seconds ago
    deepseek-r1-32B:latest    ac5d447875fb    34 GB     4 hours ago
    

然后推荐在本地安装一个ChatBox跟大模型进行交互，但是这里不重新介绍了，ChatBox这部分可以参考[上一篇博客](https://www.cnblogs.com/dechinphy/p/18699554/deepseek)。

DeepSeek精彩对话
============

以下对话使用的是7B的模型，也许7B对于很多场景来说能力有点欠缺，但是DeepSeek的这个逻辑能力确实是惊艳到我了，这还只是一个在笔记本上运行的小模型。

978294263579是质数吗
----------------

经过分析和计算，我们发现978,294,263,579可以被17整除，余数为0。因此，这个数是一个合数。

\\(\\boxed{\\text{否}}\\)

2的13次方-1是多少
-----------

好的，让我来计算一下2的13次方减去1等于多少。

首先，计算2的幂：

\\\[\\left\[ \\begin{align\*} 2^1 &= 2 \\\\ 2^2 &= 4 \\\\ 2^3 &= 8 \\\\ 2^4 &= 16 \\\\ 2^5 &= 32 \\\\ 2^6 &= 64 \\\\ 2^7 &= 128 \\\\ 2^8 &= 256 \\\\ 2^9 &= 512 \\\\ 2^{10} &= 1024 \\\\ 2^{11} &= 2048 \\\\ 2^{12} &= 4096 \\\\ 2^{13} &= 8192 \\\\ \\end{align\*} \\right\] \\\]

然后，计算2的13次方减去1：

\\\[\\left\[ 8192 - 1 = 8191 \\right\] \\\]

接下来，检查一下8191是否为质数。通过一些简单的测试，比如试除法，可以发现它不能被除了1和自身之外的任何小质数整除。因此，我们可以确认8191是一个质数。

最终答案是：

\\\[\\left\[ \\boxed{8191} \\right\] \\\]

资源占用
====

7B模型
----

7B模型下载下来大概7.54GB，显存占用情况如下：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250207150328245-2123861373.png)

32B模型
-----

32B模型下载下来大概32.4GB，这个模型压缩率已经很高了，就算用7Z格式压缩，也只能压到31GB。显存占用情况如下：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250207150331242-634631695.png)

这个基本上就是超显存了，所以运行起来一个字一个字的吐，相比之下7B的相应速度要快很多，体验也会好很多。B站上有人用[32B模型解高中数学题](https://www.bilibili.com/video/BV1bxwaeAELQ/)，看起来似乎效果还不错。

总结概要
====

本文介绍了一个可以相比之下更快速的在本地部署DeepSeek的方法，除了在上一篇博客中介绍的从Github或者Github加速网站获取Ollama之外，还可以通过从国内的其他大模型文件平台下载模型文件，来加速本地模型的构建。

版权声明
====

本文首发链接为：[https://www.cnblogs.com/dechinphy/p/deepseek2.html](https://www.cnblogs.com/dechinphy/p/deepseek2.html)

作者ID：DechinPhy

更多原著文章：[https://www.cnblogs.com/dechinphy/](https://www.cnblogs.com/dechinphy/)

请博主喝咖啡：[https://www.cnblogs.com/dechinphy/gallery/image/379634.html](https://www.cnblogs.com/dechinphy/gallery/image/379634.html)

参考链接
====

1.  [https://blog.csdn.net/xx\_nm98/article/details/145460770](https://blog.csdn.net/xx_nm98/article/details/145460770)
2.  [https://www.bilibili.com/video/BV1bxwaeAELQ/](https://www.bilibili.com/video/BV1bxwaeAELQ/)