---
layout: post
title: '你还在用Claude Code?我早已开始使用 Codex + Claude Code MCP 进行 AI Coding'
date: "2025-12-30T00:44:44Z"
---
你还在用Claude Code?我早已开始使用 Codex + Claude Code MCP 进行 AI Coding

> 写在圣诞节之后的年末：把“AI 会写代码”升级为“AI 能在终端里把活干完”。

这篇文章是一次偏工程落地的分享：用 **Codex CLI** 作为主控（模型用 `gpt-5.2`，推理开到 `xhigh`），再把 **Claude Code** 通过 `claude mcp serve` 以 **MCP Server** 的方式接入，让 Codex 的工具链更完整、更顺手；同时用一个兼容 OpenAI API 的第三方服务（`https://api.routin.ai/v1`）作为模型提供商入口，达到“体验不变、成本更友好”的目的。

文中会给出一份“核心配置”（你提供的那份），并解释每个字段为什么重要、怎么验证、常见坑怎么避。

* * *

1\. 这套组合解决什么问题？
---------------

很多人用 AI Coding 的卡点不在“模型不会写”，而在：

*   工具链割裂：模型写了，但不会/不方便自己查资料、跑命令、读写文件、串起多步工作流。
*   推理不够深：复杂重构/排障需要更强的规划与验证能力。
*   成本不可控：高强度推理模型一开，账单飞涨。

这套方案的思路是：

1.  **Codex CLI 做主控代理**：负责理解任务、拆解步骤、在本地执行/修改。
2.  **Claude Code 以 MCP Server 方式挂载**：通过 `claude mcp serve` 把 Claude Code 的能力“工具化”提供给 Codex（Codex 侧只需要配置一个 MCP 服务器即可）。
3.  **自定义模型提供商**：把 `model_provider` 指向 `meteor-ai`，并把 `base_url` 配到 `https://api.routin.ai/v1`，让 Codex 仍按 OpenAI 风格调用，但走更适合自己的计费/网络入口。

* * *

2\. MCP 是什么？为什么要接 Claude Code MCP？
----------------------------------

**MCP（Model Context Protocol）** 可以理解为“AI 的通用扩展口”：Codex 作为 MCP Client，通过 MCP Server 获取外部工具、资源、服务的能力。

Codex 官方配置文档明确支持通过 `mcp_servers` 配置两类 MCP：

*   **STDIO**：由 Codex 启动本地进程，通过标准输入输出通信（本机工具最常用）。
*   **Streamable HTTP**：通过 HTTP URL 连接远程 MCP Server（适合云服务/内网服务）。

而 **Claude Code** 自带 `claude mcp serve`，可以直接启动一个 MCP Server。因此，把 Claude Code 当作一个“工具供应商”，接到 Codex 上，是一种非常自然的组合方式：你可以用 Codex 统一驱动工作流，同时复用 Claude Code 已经打磨好的 MCP 服务能力。

* * *

3\. 环境准备（一次装好，后面全自动）
--------------------

### 3.1 安装 Codex CLI

常见方式：

    npm install -g @openai/codex
    

或在 macOS 用 Homebrew：

    brew install --cask codex
    

安装后验证：

    codex --version
    

### 3.2 安装 Claude Code（确保有 `claude` 命令）

Claude Code 官方仓库提供了多种安装方式（Windows 也支持）。

安装后验证：

    claude --version
    

以及确认 `claude mcp serve` 存在：

    claude mcp --help
    claude mcp serve --help
    

### 3.3 配置环境变量：`OPENAI_API_KEY`

你希望 Codex 走 `meteor-ai`（`https://api.routin.ai/v1`）时读取 `OPENAI_API_KEY`，所以需要在系统环境变量里设置：

*   变量名：`OPENAI_API_KEY`
*   变量值：你在对应平台获取到的实际 Key（请当作密钥管理，不要写进仓库/截图/日志）

Windows（PowerShell）临时设置（仅当前终端生效）：

    $env:OPENAI_API_KEY = "你的Key"
    

Windows（持久化到用户环境变量，需重开终端）：

    setx OPENAI_API_KEY "你的Key"
    

macOS/Linux（当前 shell 生效）：

    export OPENAI_API_KEY="你的Key"
    

* * *

4\. 核心配置（原样保留 + 逐项解释）
---------------------

Codex 的配置文件默认为：

*   macOS/Linux：`~/.codex/config.toml`
*   Windows：`%USERPROFILE%\.codex\config.toml`

把下面这份内容写进去即可（你给的“最核心配置”，原样保留）：

    model = "gpt-5.2"
    model_provider = "meteor-ai"
    disable_response_storage = true
    approval_policy = "never"                  # 可选值: "untrusted" | "on-failure" | "on-request" | "never"
    sandbox_mode = "danger-full-access"
    
    rmcp_client = true
    model_reasoning_effort = "xhigh"
    
    # Reasoning summary: auto | concise | detailed | none (default: auto)
    model_reasoning_summary = "detailed"
    
    # Text verbosity for GPT-5 family (Responses API): low | medium | high (default: medium)
    model_verbosity = "high"
    
    # Force-enable reasoning summaries for current model (default: false)
    model_supports_reasoning_summaries = true
    
    [mcp_servers.claude]
    command = "claude"
    # Optional
    args = ["mcp", "serve"]
    
    [model_providers.meteor-ai]
    name = "meteor-ai"
    base_url = "https://api.routin.ai/v1"
    env_key = "OPENAI_API_KEY"
    wire_api = "responses"
    

下面解释每个关键点（只讲“为什么重要”）：

### 4.1 `model = "gpt-5.2"`：主模型选择

Codex 的核心价值是“在本地跑多步任务”。复杂任务（重构、定位 bug、跨多文件一致性修改）更依赖模型推理与规划能力，因此选 `gpt-5.2` 这种更强的通用模型是合理的。

### 4.2 `model_provider = "meteor-ai"` + `[model_providers.meteor-ai]`

这表示你不走默认的 OpenAI Provider，而是注册一个新的 Provider：

*   `base_url = "https://api.routin.ai/v1"`：把 OpenAI 风格的 `/v1/...` 请求转到该地址
*   `env_key = "OPENAI_API_KEY"`：从系统环境变量取 Key
*   `wire_api = "responses"`：强制走 Responses API 形态（对 `reasoning_effort / verbosity` 这类参数更关键）

你也可以把 `env_key` 改成更语义化的名字（比如 `ROUTIN_API_KEY`），但本文按你的设定使用 `OPENAI_API_KEY`，这样对 Codex 的默认生态也更兼容。

### 4.3 `model_reasoning_effort = "xhigh"`：把“想清楚再动手”拉满

`xhigh` 通常意味着：

*   更强的规划与多步推理能力
*   更高的延迟与成本

建议你把它当作“复杂任务档”，简单任务（改文案/小函数/读文档）可以切到低一点以省钱省时。

### 4.4 `model_reasoning_summary = "detailed"` + `model_supports_reasoning_summaries = true`

这会让 Codex 更倾向输出**推理摘要**（不是完整思维链，而是可读的总结），适合做技术分享/团队协作：你能看见它为什么这么改、考虑了哪些边界。

### 4.5 `model_verbosity = "high"`：输出更详细

适合教程/分享场景，尤其是你希望它把操作步骤、验证方式讲清楚的时候。

### 4.6 `approval_policy = "never"` + `sandbox_mode = "danger-full-access"`：全自动，但风险最高

这套组合的含义是：

*   Codex 不会向你请求批准（所有命令/改动默认都能执行）
*   文件系统与网络基本不设限

它非常适合：

*   容器/虚拟机/隔离环境里的自动化原型开发
*   你明确知道自己在做什么，并且能接受“AI 可能误删/误改”的风险

但不建议在重要生产机、核心仓库、带密钥/账单权限的环境中直接长期使用。更推荐的做法是：用 Profile 分出“安全档”和“全自动档”（见后文“可选增强”）。

### 4.7 `rmcp_client = true`：关于 RMCP 客户端的兼容性提示

Codex 官方文档里提到 Streamable HTTP 连接会使用 Rust MCP client（rmcp）。不同版本对开关字段可能存在差异：

*   如果你发现 `rmcp_client` 报“未知字段”，优先以 `docs/config.md` 为准，删除该项通常不影响 STDIO MCP。
*   如果你在用较老版本并且需要显式启用 Streamable HTTP，可能会见到类似 `experimental_use_rmcp_client` 的开关（以你使用版本的文档/发行说明为准）。

本文按你提供的配置保留 `rmcp_client = true`，便于复现你的“核心配置”。

### 4.8 `disable_response_storage = true`：关于“关闭存储”的两层含义

`disable_response_storage` 不是 Codex 官方 `docs/config.md` 里的标准字段（至少在本文写作时）。如果你的 Codex 版本支持它，它通常表达的是“不要把对话/响应持久化”。这里建议你区分两层：

1.  **本地历史**：Codex 默认会把消息写入 `$CODEX_HOME/history.jsonl`。如果你希望彻底不落盘，可以用官方的 `[history]` 配置关闭持久化：

    [history]
    persistence = "none"
    

2.  **远端留存**：不同 API 提供商对日志与留存策略不同。即便本地不写盘，也不代表服务端不记录。做团队推广时，建议明确告知读者：以提供商的隐私/合规条款为准。

如果你的 Codex 启动时报 “unknown field disable\_response\_storage”，直接删除这一行，并改用上面的 `[history] persistence = "none"` 达到“本地不存”的效果。

* * *

5\. 把 Claude Code 作为 Codex 的 MCP Server
---------------------------------------

你的配置核心就在这里：

    [mcp_servers.claude]
    command = "claude"
    args = ["mcp", "serve"]
    

这表示：Codex 需要该 MCP Server 时，会启动一个本地进程执行：

    claude mcp serve
    

如果你想单独排障（例如看看 server 有没有启动、有没有报错），可以手动跑：

    claude mcp serve --debug
    

然后在另一个终端启动 Codex，观察 Codex 的 MCP 列表：

    codex mcp list
    

* * *

6\. 推荐 `https://api.routin.ai/v1`（meteor-ai）与 Key 配置说明
------------------------------------------------------

你提供的配置中，`meteor-ai` 的 `base_url` 指向：

*   `https://api.routin.ai/v1` 推荐RoutinAI 价格更优惠Codex低至0.2￥=1 美元的超低汇率。

最关键的落地动作有两步：

1.  把 `base_url` 配到 `https://api.routin.ai/v1`
2.  在系统里设置 `OPENAI_API_KEY=你的实际Key`

正常情况下会返回类似“需要 API Key”的报错，说明地址可达、路由正常；设置 `OPENAI_API_KEY` 后再由 Codex 调用即可。

* * *

7\. 实战工作流建议（把 AI Coding 变成可复制的流程）
---------------------------------

给一个我更推荐的“稳定工作姿势”：

1.  让 Codex 先做“计划 + 探索”（读项目、定位入口、列修改点）
2.  再让它执行修改，并要求：
    *   改动要小步提交（一次只做一件事）
    *   每一步都能运行验证（lint/test/build）
3.  需要外部能力时，再通过 MCP 接更多服务器（GitHub、Playwright、文档检索等）

示例提示词（你可以直接复制改项目名）：

    请在不改变对外行为的前提下重构 XXX 模块：
    1) 先解释现状与风险点
    2) 给出 3~5 个小步骤计划
    3) 每一步都要可验证（说明运行哪些命令）
    4) 只在我确认后再执行危险操作
    

在 `approval_policy = "never"` 的配置下，第 4 条更多是“自律要求”，但它能显著减少误操作概率。

AI Coding 技术交流群：  
wk28u9123456789