---
layout: post
title: '手把手教你更优雅的享受 DeepSeek'
date: "2025-02-16T00:39:28Z"
---
手把手教你更优雅的享受 DeepSeek
--------------------

2025-02-15 09:02  [AlfredZhao](https://www.cnblogs.com/jyzhao)  阅读(336)  评论(0)  [编辑](https://i.cnblogs.com/EditPosts.aspx?postid=18716495)  [收藏](javascript:void\(0\))  [举报](javascript:void\(0\))

开始之前，首先要确定你已经配置好Ollama软件并正常运行DeepSeek本地模型。如果这一步还不清楚，请翻看之前的手把手教程《[手把手教你部署 DeepSeek 本地模型](https://mp.weixin.qq.com/s/AgQkyzV7cr-gUsfTqP7mFg?token=334328943&lang=zh_CN)》。

![logo1](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208263-1269773954.png)

本文是手把手教程系列第3篇，包含内容如下：

*   1.如何使用浏览器调用DeepSeek
*   2.为何使用最小DeepSeek模型演示
*   3.使用API方式免费体验满血版DeepSeek

1.如何使用浏览器调用DeepSeek
===================

先前教程中，在UI界面实现这部分，给大家推荐的是 `Chatbox` 桌面软件，这是为了更多的小白能够没有任何门槛的直接上手实践。  
但实际上，对于有梯子的读者，还有一种更加简单、轻量且优雅的Web调用方案：

只需要在Chrome浏览器中添加扩展程序 `Page Assist`，就可以实现在Web端更优雅的调用本地 AI 模型。

*   Page Assist - 本地 AI 模型的 Web UI

注：

*   1.Page Assist插件安装好之后，也可实现在无网环境下，通过Web UI随时和本地部署的各种大模型畅快聊天。
*   2.如果你的机器是全周期无法上网的，还可以下载对应Page Assist的离线安装包，手动安装即可。

下面演示下具体步骤。  
打开Chrome浏览器，在地址栏输入扩展程序的网址：

*   chrome://extensions/

![1](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208264-264999530.png)

这里点击`Chrome 应用商店`并搜索扩展程序 `Page Assist`：

![2](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208609-1124629548.png)

选择`添加至Chrome`。

![3](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208264-2146421027.png)

选择`添加扩展程序`。

![4](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208263-518738724.png)

如此就成功将 `Page Assist` 添加至Chrome。

然后，为了今后更方便的调用，我们可以把它固定下。

![5](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208264-510483068.png)

这样，当我们再次打开浏览器，就可以直接点击红框中的这个按钮启动扩展程序 `Page Assist`：

![6](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208636-1580873481.png)

启动后，按下图步骤，选择本地部署的DeepSeek模型，确认Ollama运行状态正常，就可以直接输入文字聊天了：

![7](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208665-1961440063.png)

是不是这种方式更加简单，且很优雅！

2.为何使用最小DeepSeek模型演示
====================

这里特别解答下，之前有很多读者私信好心建议我不要使用1.5b的模型测试，至少也要7b以上版本，效果会更好。

但实际上由于笔者目前测试硬件比较低，虽然也能勉强运行起下一个级别的7b模型，但反应慢的完全不可接受，我这里实测7b的模型，问简单问题，都需要思考20s：

![8](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208447-1027124779.png)

而换回1.5b的模型，基本秒出，即便思考也就2s的样子：

![9](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208484-68871168.png)

而且，即便7b要比1.5b的模型能力是有所提升，但并没有到质的差异。

所以在硬件没升级之前，后续演示主要还是会选择这个1.5b的最小模型，这样测试反馈的效率高，心情也能舒畅。

通过从DeepSeek官方给出的测试图来看：

![benchmark](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208866-885503213.jpg)

也可以发现，在这张benchmark的对比图上，人家都没考虑比32b更低的模型，这其实也算暗示了32b以下的模型基本表现都拿不出手，可是如果想要本地部署32b或更大的模型，首先你的硬件得跟得上。

3.使用API方式免费体验满血版DeepSeek
========================

既然本地部署情况，在不具备好的硬件条件之前，实际使用效果一定欠佳。

那如果想提前体验下更大模型的具体效果，该怎么做呢？

答案就是使用API的方式，这种方式下连Ollama都不需要了，只需要配置好对应的API key，就可以轻松体验到满血版的DeepSeek。

1）使用DeepSeek官方API

官方API网址：

*   [platform.deepseek.com](https://platform.deepseek.com/api_keys)

这个网址曾经处于维护，但如今已经可以打开，但由于目前服务器资源还是紧张，官方已暂停API服务充值，之前余额依然可用。

![3-1-ds](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208430-1884230046.png)

需要注意这个key创建时就需要复制保留好，以后不提供再复制。

如果忘记保存，只能这样重建新的key：

![3-2-ds](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208278-631830554.png)

测试下接入官方API的效果，使用API的方式还是要用到之前的Chatbox：

![3-3-ds-show](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208587-435717257.png)

这里测试，想到笔者在学生时期，老师曾经说通常古诗最难翻译好。  
让它帮我翻译《画》这首古诗为英文，选择普通的`deepseek-chat`模型，这个模型不会推理，直接会给出答案，我觉得效果也非常好。

附：官方的API在Chatbox软件中的设置方式如下图：

![3-4-ds-set](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208502-709409978.png)

2）使用第三方服务商API

因为官方的资源目前还是紧张，所以除了官方的途径之外，还有一些第三方服务商的选择，比如这里以流行度较高的siliconflow为例，点击下面的邀请链接（邀请码：aYaHaxLo），注册登录后即送免费的14元配额，可以够玩上一阵子了：

*   [siliconflow.cn](https://cloud.siliconflow.cn/i/aYaHaxLo)

![3-5-siliconflow](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090209090-1680233823.png)

注意，siliconflow这个API的key如果没保存，是支持随时再次复制保存的，这点安全策略上和官方的设计有所不同。

复制key之后，在Chatbox下设置silicon flow的API粘贴进去即可：

![3-6-siliconflow](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208598-1450840235.png)

配置好之后，同样测试下，帮我翻译《画》这首古诗为英文，这回选择`deepseek-ai/DeepSeek-R1`模型，效果如下：

![3-7-siliconflow-1](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208766-611123764.png)

![3-7-siliconflow-2](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208807-1926346647.png)

![3-7-siliconflow-3](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208707-237501273.png)

超过了3页篇幅的推理思考，然后给出了答案：

![3-7-siliconflow-4](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208725-1558993276.png)

可以直观的看到，这满血版的deepseek-r1思考推理能力，确实要比本地部署的小号模型强大许多。

![logo1](https://img2023.cnblogs.com/blog/635610/202502/635610-20250215090208263-1269773954.png)

至此，基础篇就差不多了，后面计划持续研究分享进阶篇，后续教程大家还想了解哪些方面，欢迎在评论区留言。

AlfredZhao©版权所有「从Oracle起航，领略精彩的IT技术。」