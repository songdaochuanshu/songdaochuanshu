---
layout: post
title: '纯离线部署本地知识库LLM大模型'
date: "2025-02-27T00:37:03Z"
---
纯离线部署本地知识库LLM大模型
================

纯离线部署本地知识库LLM大模型
----------------

### 一、下载离线大模型

> 下载的网址：[https://hf-mirror.com/](https://hf-mirror.com/)

    deepseek qwen 相关的模型，只建议使用1.5B的，GGUF后缀的模型
    推荐下载llama相关模型，同样是GGUF后缀的，自己笔记本电脑推荐下载8B的	
    

![image-20250208091357388](https://gitee.com/xiaohuya1/image_test/raw/master/png/image-20250208091357388.png)

### 二、下载大模型管理平台 LM Studio

> 下载网址：[https://lmstudio.ai/](https://lmstudio.ai/)
> 
> 安装过程只需要修改一个安装路径，后面一直下一步安装成功。

### 三、将离线大模型导入到 LM Studio 中

> 注意：默认情况下，LM Studio 所识别的大模型的目录在C盘
> 
> 默认路径：C:\\Users\\用户名\\.lmstudio\\models

![image-20250208092315983](https://gitee.com/xiaohuya1/image_test/raw/master/png/image-20250208092315983.png)

> 修改大模型的加载目录
> 
> *   先创建一个根目录 ，例如：F:\\LMStudioModels
> *   再创建一个二级目录，例如：F:\\LMStudioModels\\shujia\_models 【必须要有一个二级目录】
> *   将模型除.gguf意外的名字拷贝出来，当作一个文件夹的名字
> *   将该模型放在这个文件夹里面

**上面操作做完后，LM Studio就可以读取到我们的大模型。**

### 四、通过LM Studio加载我们的大模型【重要，涉及GPU的能力】

*   点击对话正上方的`select a model to load` ,选择该对象要使用的大模型
*   参数解释：
    *   Context Length: 该模型一次最大可以加载多少个token
        *   若是简单的问答，推荐4096
        *   若是小红书文案，推荐10000以上
        *   若是写作文，小说，推荐100000左右
    *   GPU Offload: 运行时，所占用的GPU显存，建议先给一半
    *   CPU Thread Pool Size: 拉满
    *   Evaluation Batch Size: 512
    *   后面不动，都以推荐为准

### 五、调整参数，进行对话

> 点击右上角实验室器皿图标，show settings，设置Preset，例如添加一个：数学家
> 
> 理解为：将当前对话的大模型设置成一个固定的角色
> 
> 设置System Prompt，形容一下这个数学家

### 六、使用AnythingLLM工具使用LM Studio中的模型，加载知识库

*   将LM Studio作为一个服务对外提供，让AnythingLLM连接上LM Studio
    *   打开LM Studio点击左边的Developer
    *   打开 Start Server按钮
*   再AnythingLLM中，点击聊天设置，配置LM Studio，选择，模型

### 七、提供API服务

*   在AnythingLLM中左下角点击open settings
*   点击工具
*   点击API密钥，生成密钥