---
layout: post
title: '智能眼镜论文笔记'
date: "2025-11-14T00:42:57Z"
---
智能眼镜论文笔记
========

智能眼镜论文笔记
========

目录

*   [智能眼镜论文笔记](#智能眼镜论文笔记)
    *   [0x00 概要](#0x00-概要)
    *   [0x01 论文内容总结](#0x01-论文内容总结)
        *   [1.1 AI for Service](#11-ai-for-service)
            *   [1.1.1 研究背景与核心范式](#111-研究背景与核心范式)
            *   [1.1.2 核心技术挑战与解决方案](#112-核心技术挑战与解决方案)
            *   [1.1.3 Alpha-Service 框架设计](#113-alpha-service-框架设计)
            *   [1.1.4 思考](#114-思考)
        *   [1.2 EgoLife](#12-egolife)
            *   [1.2.1 背景](#121-背景)
            *   [1.2.2 数据采集与数据集构建](#122-数据采集与数据集构建)
                *   [1\. 采集设计](#1-采集设计)
                *   [2\. 数据规模与处理](#2-数据规模与处理)
            *   [1.2.3 EgoLifeQA](#123-egolifeqa)
            *   [1.2.4 EgoButler 系统](#124-egobutler-系统)
                *   [1\. EgoGPT](#1-egogpt)
                *   [2\. EgoRAG](#2-egorag)
            *   [1.2.5 现存挑战](#125-现存挑战)
            *   [1.2.6 关键问题](#126-关键问题)
    *   [0x02 自己的需求](#0x02-自己的需求)
    *   [0x03 业界调研](#0x03-业界调研)
        *   [3.1 分类](#31-分类)
        *   [3.2 交互](#32-交互)
        *   [3.2 功能](#32-功能)
        *   [3.3 痛点](#33-痛点)
        *   [3.4 方案](#34-方案)
        *   [3.5 和手机的关系](#35-和手机的关系)
        *   [3.6 眼镜公司](#36-眼镜公司)
    *   [0xFF 参考](#0xff-参考)

0x00 概要
-------

刷到两篇论文

*   ”AI for Service: Proactive Assistance with AI Glasses“，[https://arxiv.org/pdf/2510.14359。](https://arxiv.org/pdf/2510.14359%E3%80%82)
*   “EgoLife: Towards Egocentric Life Assistant”，[https://arxiv.org/pdf/2503.03803](https://arxiv.org/pdf/2503.03803)

因为我身边也有几个朋友在做智能眼镜。于是就研读了这两篇论文，也从用户角度做了下调研，整理出来了这篇笔记。

注：事实上，本文后半部分早在4月份就写好了，因为我总想继续完善，想把更好的作品呈现出来，所以它一直躺在我的草稿箱中（类似的草稿有几十篇）。最近看到一个说法："行业发展比写PPT的速度还快"。于是决定，类似这种半成品，还是早早发出来吧，不再继续吃灰了。

0x01 论文内容总结
-----------

### 1.1 AI for Service

#### 1.1.1 研究背景与核心范式

当前 AI 服务多以被动响应为主，需用户发出明确指令才能提供服务，难以深度融入日常生活。为此，论文提出 “AI for Service（AI4Service）” 这一全新范式，旨在通过 AI 技术实现主动、实时且个性化的服务，推动服务模式从 “人找服务” 向 “AI 代理找服务” 转变。该范式的核心特征包括通用性（应对多样化生活场景）、主动性（主动发现服务需求）和定制化（适配用户个体差异）。

#### 1.1.2 核心技术挑战与解决方案

1.  “何时服务”（Know When）：需精准识别环境中的服务触发时机并分类事件类型，通过高精度时序模式识别与上下文感知技术，平衡服务及时性与非侵入性。
2.  “如何服务”（Know How）：提供通用服务与个性化服务两层解决方案，通用服务基于短期场景提供标准化内容，个性化服务则融合用户长期行为模式实现定制化输出。

#### 1.1.3 Alpha-Service 框架设计

受冯・诺依曼计算机架构启发，论文提出 Alpha-Service 统一框架，通过五大核心组件协同实现主动服务，具体如下：

1.  输入单元：采用双模型架构，轻量级在线模型持续监测第一视角视频流以触发服务时机，重量级离线模型则对场景进行精细化分析，平衡实时性与分析深度。
2.  中央处理单元：作为系统 “中枢”，基于大语言模型实现任务分解、调度与结果合成，将复杂任务拆解为子任务并分配至对应组件，最终整合多源信息生成统一响应。
3.  算术逻辑单元：负责工具集成与调用，当系统内部知识不足时，通过网页搜索等外部工具获取实时信息，支撑复杂决策。
4.  记忆单元：以结构化 JSON 文件存储用户长期交互历史与偏好，通过检索增强提示策略，为个性化服务提供数据支撑。
5.  输出单元：将系统分析结果提炼为简洁指令，通过语音合成技术实现多模态输出，适配免提等实际使用场景。

#### 1.1.4 思考

几点思考如下：

1.  模型轻量化与效率提升：针对边缘设备的资源约束，引入模型压缩、量化或知识蒸馏技术，优化轻量级与重量级模型的协同机制，减少能耗同时降低推理延迟；探索异构计算架构，合理分配 CPU、GPU 资源，提升实时处理能力。
2.  工具生态与服务多样性拓展：当前算术逻辑单元主要依赖网页搜索工具，可丰富工具库，集成地图导航、设备控制、专业数据库查询等多样化工具；建立工具调用的智能决策机制，基于任务类型与场景特征自适应选择最优工具组合。
3.  记忆单元智能化升级：引入向量数据库与语义检索技术，提升用户偏好挖掘的精准度；结合联邦学习等隐私计算方法，在保障数据本地化与匿名化的前提下，优化个性化服务的冷启动问题。
4.  服务干预策略精细化：针对主动服务的非侵入性需求，可设计用户意图置信度评估机制，根据置信度动态调整服务触发阈值；提供可定制化的服务频率与干预方式设置，满足不同用户的隐私偏好与使用习惯。
5.  多场景适配能力强化：拓展极端环境（如强光、噪音）、多用户交互、跨场景切换等复杂场景的测试与优化；增强系统对特殊用户群体（如视障、老年用户）的适配性，提升服务的包容性。

### 1.2 EgoLife

EgoLife 把“第一视角生活流”升级为“可预测、可干预的个人服务”，让模型从「看见」走向「预见」。

#### 1.2.1 背景

南洋理工大学刘子纬助理教授领导的联合团队，针对当前 AI 助手 “被动响应、难以理解人类长期行为与社交互动” 的局限，发起**EgoLife 项目**，旨在开发基于 AI 眼镜的 “自传式记忆” 智能助手 —— 通过第一人称视角捕捉日常数据，让 AI 真正 “读懂生活”，实现如 “推荐符合口味的餐厅”“提醒会议”“预测遗漏日用品” 等主动辅助功能。项目最终形成三大核心成果：**EgoLife 数据集**、**EgoLifeQA 长情境问答基准**、**EgoButler 智能助手系统**。

#### 1.2.2 数据采集与数据集构建

##### 1\. 采集设计

为获取丰富、真实的第一人称数据，团队设计了严谨的采集方案：

*   **参与者**：6 人（通过小红书招募，因男性参与者临时缺席，最终含项目负责人；MBTI 多为直觉型（N）+ 感知型（P），适配开放式探索）
*   **采集环境**：定制 “EgoHouse”，除满足日常居住外，布置 15 个 GoPro 摄像头（覆盖公共区域，提供第三人称视角）、2 个毫米波雷达（获取空间与运动数据）
*   **核心任务**：让 6 人共同生活 7 天，以 “筹备地球日庆祝活动” 为目标，自然产生讨论、购物、烹饪、排练等社交与协作场景
*   **采集设备**：每人佩戴 Meta Aria 智能眼镜，该设备集成高清摄像头、空间音频麦克风与 IMU（惯性测量单元），可全方位捕捉视觉、听觉、运动信息，要求每人每天至少记录 6 小时清醒活动

##### 2\. 数据规模与处理

*   **原始数据规模**：共收集**300 小时自我中心视频**，同步获取第三人称视角视频、空间 / 运动数据，确保多模态覆盖
*   数据处理流程（4 大核心模块）：
    1.  **EgoSync**：将 6 人的眼镜数据、外部摄像头 / 雷达数据进行时间同步，解决多源数据错位问题
    2.  **EgoBlur**：对敏感信息（如人脸、私人文件）进行模糊处理，保障参与者隐私
    3.  **EgoCaption**：将视频按 5 分钟分段，以 0.8 倍速播放，由注释员口述标注，生成**36.1 万条 “旁白” 片段**（平均每条 2.65 秒），再通过 GPT-4o-mini 合并为 2.5 万条 “合并字幕”，最终结合抽样画面与转录文本生成 “视听字幕”，并经人类验证
    4.  **EgoTranscript**：应用语音识别技术生成初步转录文本，通过开源算法区分 6 位说话人，将音轨拆分为 6 条独立轨道，确保每条转录准确反映对应参与者的听觉内容

#### 1.2.3 EgoLifeQA

现有基准（如 EgoSchema、EgoPlan-Bench）多处理短时间上下文，而 EgoLifeQA 聚焦 “长情境、生活导向” 问答，要求 AI 处理**远超 2 小时甚至数天前**的信息，更贴近真实生活中 AI 助手的使用场景（如 “回忆 3 天前的早餐”）。EgoLifeQA 通过 5 类任务，全面评估 AI 对生活场景的理解能力，具体如下表：

任务类型

核心能力

示例问题

回答要求

**EntityLog**

物品细节长期记忆

“我们付的酸奶价格最接近哪个选项？A.2 元 B.3 元 C.4 元 D.5 元”

回忆购物场景价格，需精准到具体数值

**EventRecall**

过去事件回忆

“在计划跳舞后第一首被提到的歌是什么？A.Why Not Dance B.Mushroom...”

定位特定会话，提取关键信息

**HabitInsight**

个人习惯洞察

“我喝咖啡时通常同时做什么活动？A. 刷 TikTok B. 发短信 C. 整理房间 D. 做手工”

从多天数据中归纳行为规律

**RelationMap**

人际互动模式映射

“Shure 正在弹吉他，还有谁通常和我们一起弹吉他？A.Choizst B.Jake C.Nicrous”

识别人物身份，关联社交历史

**TaskMaster**

任务管理与意图追踪

“我的购物车里已经有很多东西了，我们之前讨论过但我还没买的是什么？”

记忆清单，区分已购 / 未购物品

#### 1.2.4 EgoButler 系统

EgoButler（融合视听理解与长期记忆的 AI 助手） 是 EgoLife 项目的核心落地成果，由**EgoGPT（全模态理解模块）** 与**EgoRAG（分层检索模块）** 组成，二者协同实现对超长上下文的理解与问答。

##### 1\. EgoGPT

*   基础架构：基于 LLaVA-OneVision 模型（底层为 Qwen2 架构），为增强音频处理能力，参考 Ola 模型设计新增音频分支 —— 使用 Whisper Large v3 编码音频，在 LibriSpeech 数据集上训练音频投影模块，最终通过 EgoIT-99K 数据集微调整合
    *   **EgoIT-99K 数据集**：涵盖 9 个经典自我中心视频数据集（如 Ego4D、Charades-Ego），精选 1529 个视频（686 个带音频），总时长 43.16 小时，生成 9.948 万条问答对（含视频描述、音视频描述等类型）
*   核心功能：
    1.  持续视频描述：处理每个 30 秒的自我中心视频片段，结合视觉 + 音频输入生成详细场景描述
    2.  辅助问答：接收 EgoRAG 检索到的相关线索，整合信息生成精准回答
*   个性化优化：使用 EgoLife 项目第一天的视频数据进行微调，让模型具备身份识别能力，为理解人际互动（如 RelationMap 任务）奠定基础

##### 2\. EgoRAG

EgoRAG 模拟人类 “分层回忆” 逻辑（如 “回忆 3 天前早餐：先定位日期→再定位时段→最后记细节”），解决超长上下文检索效率问题，分为 “记忆库构建” 与 “检索回答” 两阶段：

阶段

核心操作

目的

记忆库构建

1\. 收集 EgoGPT 生成的 30 秒片段描述（细粒度记忆）2. 定期生成小时级摘要（汇总每小时主要事件）3. 生成天级摘要（捕捉每日关键点）

建立多层索引，实现高效信息管理

检索回答

1\. 问题分析：提取关键词（如 “昨天”“超市”“酸奶”）2. 粗检索：在天级摘要中缩小范围到目标时段3. 精检索：在小时级 / 细粒度记忆中定位相关片段4. 结果整合：将片段送入 EgoGPT 生成回答

避免逐帧搜索，秒级完成 300 小时数据检索

#### 1.2.5 现存挑战

*   EgoGPT 的局限：
    1.  语音情感理解不足：依赖 ASR 训练数据，难以识别笑声、情绪语调等非语义信息
    2.  身份识别过拟合：仅用第一天数据微调，易将 “穿相似衣服的不同人” 误判为同一人
*   EgoRAG 的局限：
    1.  缺乏多步推理能力：仅支持单次检索，无法迭代优化搜索策略
    2.  无容错性：若检索不到直接支持证据，无法通过推理补充缺失信息，导致无法回答

#### 1.2.6 关键问题

问题 1：EgoLife 项目的 EgoButler 系统如何解决 “AI 难以处理超长（如数天前）上下文” 的问题？其核心技术逻辑与传统问答系统有何不同？

**答案**：EgoButler 通过 “EgoGPT 全模态理解 + EgoRAG 分层检索” 的协同架构解决超长上下文问题，核心技术逻辑与传统系统的差异如下：

1.  **分层记忆管理**：EgoRAG 不存储原始超长视频，而是将 EgoGPT 生成的 30 秒片段描述（细粒度记忆）定期汇总为 “小时级摘要→天级摘要”，建立多层索引 —— 传统系统多存储原始数据或单一维度摘要，检索时需逐帧 / 逐片段遍历，效率极低；
2.  **模拟人类回忆逻辑**：检索时先通过天级摘要缩小到目标日期 / 时段（如 “昨天”），再通过小时级摘要定位大致场景（如 “下午购物”），最后通过细粒度记忆找到具体信息（如 “酸奶价格”），实现 “粗→精” 的高效检索 —— 传统系统多直接基于关键词全局搜索，无法利用时间维度的层级关系；
3.  **全模态理解支撑**：EgoGPT 通过视觉 + 音频融合理解生成精准片段描述，为检索提供高质量 “记忆单元”，且能整合检索到的多片段信息生成连贯回答 —— 传统系统常单模态处理（如仅文本），难以应对生活场景中的多模态信息（如 “看到吉他 + 听到弹唱” 的关联）。

问题 2：EgoLifeQA 基准包含哪几类任务？这些任务分别对应真实生活中 AI 助手的哪些核心能力？请结合具体任务示例说明其与传统基准的核心差异。

**答案**：EgoLifeQA 包含 5 类任务，对应 AI 助手的 5 项核心生活辅助能力，与传统基准（如 EgoSchema）的核心差异在于 “处理超 2 小时甚至数天前的长情境”，具体如下：

1.  **EntityLog（物品细节长期记忆）**：对应 “记住生活物品关键信息” 的能力，如 “我们付的酸奶价格最接近哪个选项？”—— 传统基准多处理短期物品信息（如 “当前画面中的物品”），而该任务需回忆数天前购物场景的具体价格；
2.  **EventRecall（过去事件回忆）**：对应 “追溯特定过往事件” 的能力，如 “在计划跳舞后第一首被提到的歌是什么？”—— 传统基准多聚焦 “当前 / 近期事件”，该任务需从海量历史对话中定位特定时间点的信息；
3.  **HabitInsight（个人习惯洞察）**：对应 “归纳用户长期习惯” 的能力，如 “我喝咖啡时通常同时做什么活动？”—— 传统基准无习惯归纳需求，该任务需从多天数据中提炼行为规律（如 “3 次喝咖啡时做手工”）；
4.  **RelationMap（人际互动映射）**：对应 “理解用户社交关系” 的能力，如 “Shure 正在弹吉他，还有谁通常和我们一起弹吉他？”—— 传统基准少涉及人际互动，该任务需关联多场景下的人物行为历史；
5.  **TaskMaster（任务管理）**：对应 “追踪任务进度与未完成事项” 的能力，如 “我的购物车里已经有很多东西了，我们之前讨论过但我还没买的是什么？”—— 传统基准多处理单一任务指令，该任务需记忆历史讨论内容并对比当前进度。

0x02 自己的需求
----------

以下是我自己的需求。

我对AI眼镜的期望是“附带某些功能的常规眼镜”。首先是眼镜，要无感佩戴，自然融入日常生活；然后在此之上再添加一些生活中的便利AI功能。而不是把手机顶在头上。具体有如下述求：

*   足够轻便，可以长时间舒适地佩戴。
*   电池续航足够支撑全天佩戴。希望有快充。因为对于近视人来说要长期佩戴，频繁充电意味需要摘下眼镜。
*   支持眼睛盒充电。眼镜盒要支持5次以上满额充电；对眼睛盒充电时长不要超过3小时。最好能支持手机给眼镜充电，边充边用。
*   可以方便的操控，比如通过指环。
*   能客串蓝牙耳机，具备隔音、降噪能力；
*   不要有声音外溢，保证聆听私密性。
*   外形足够吸引人或者说符合用户的自我标签（因为眼镜外观是用户自我标签的对外表达，生活中手机“撞脸”的概率极高，但眼镜撞脸的概率却极低，AI眼镜这个品类先天就需要更丰富多样的产品）。
*   可以随手拍摄，不要有快门延迟，而且当头部有频繁动作时，要保证拍摄稳定性。暗光环境要保证拍摄效果。
*   眼镜录制的内容可以无缝传到社交媒体、手机或者电脑。
*   可以应对整机下水清洗。
*   希望眼镜可以接收到手机上的通知，比如微信消息等（决定了眼镜在日常生活中的被使用的频次到底有多高）。

总结之后，我不确定在当前的行业现状下，AI眼镜可以全部满足这些要求。因为这涉及了眼镜行业的通用痛点。

0x03 业界调研
---------

### 3.1 分类

智能眼镜的分类有不同说法。

*   一种说法是：不带屏幕的眼镜称作AI眼镜，带显示模块光学组件的眼镜称作AR眼镜。
*   也有再加上摄像头进行分类，即：
    *   (1)无摄像头、无显示屏幕；
    *   (2)无摄像头、带显示屏幕；
    *   (3)带摄像头、无显示屏幕；
    *   (4)带摄像头、带显示屏幕。

长期方向是AI+AR融合发展：AI提升AR的交互智能（如手势识别、眼动追踪等），AR为AI提供虚实融合的显示载体。

### 3.2 交互

AI 眼镜的交互形态正突破传统设备的边界，当前已形成触摸、语音、显示、手势识别、眼动追踪等多元方式。从人体工学角度看，其佩戴位置天然贴近三大感官枢纽 —— 负责言语输出的嘴巴、接收声音的耳朵、获取视觉信息的眼睛，这使得它能够无缝集成语音交互、音频输出、高清影像捕捉等功能，甚至兼容智能耳机、相机等硬件，进化为复合型智能设备，灵活适配不同场景。

与手机相比，AI 眼镜的交互优势体现在信息维度的拓展与使用自由度的提升。视觉信息的密度与带宽远胜声音，为输入输出提供了更丰富的载体；而 “免提机位” 的特性，让第一视角的生活记录不再局限于碎片化瞬间，更能完整留存连续体验。这种特性催生了新的交互逻辑：以手部微手势为基础模态，用户只需将手自然放置于身体一侧、裤兜或大腿等放松位置即可操作，再结合语音指令、镜腿轻触等辅助方式，构建全局交互体系。

目前，指环作为过渡形态展现出独特价值 —— 低成本、兼具时尚感，且能实现基础交互需求。

对行业来说，交互设计的关键是别再一门心思盯着屏幕界面了。不是说屏幕界面没用，而是要追求 “少操作、步骤简单、反应快” 的界面逻辑，把场景放在第一位。得先搞清楚用户在什么时间、什么地方，当下最需要啥，然后琢磨怎么让服务自然地出现，能不能做到不用动手操作。要是实在得设计一次操作，那也得精准戳中用户最核心的需求点。

### 3.2 功能

AI 眼镜的功能潜力，根植于多模态 AI 基础模型的能力边界 —— 搜索、私人助理、实时字幕、场景识别、动作分析等基础能力，为其拓展出丰富的应用可能：从实时视觉识别、语音交互、场景理解，到 AR 导航、记忆辅助、健康监测，乃至专业领域的手术辅助、设备巡检等。

但现实使用数据揭示了功能设计的深层规律：据不完全统计，70% 的用户要么很少用 AI 功能，要么尝个鲜就不用了；剩下 30% 经常用的用户里，超过一半是拿它当搜索引擎，大概 30% 用来翻译，其他的就是查导航、看天气这些基础操作。还有份统计显示，大家常用的功能集中在拍照录像、处理音频、沉浸式办公；骑行导航、同声传译这些用得不算多；而帮听障人士辅助、采集动作数据这些，只适合特定人群。

这些数据指向一个结论：功能堆得再多也不算厉害，关键是在成本、戴着舒服不舒服、好不好看之间找到平衡，把场景价值做透。行业别再执着于打造 “啥功能都有的 AI+AR 眼镜” 了，得从用户的实际需求出发，该减的功能果断减，专心做细分市场。设计思路得围绕 “场景优先”：先明确用户在具体场景里最需要啥，再选合适的方式（比如用看的、用听的）呈现数据和服务，最后把操作方式优化好，让每一个功能都能真正帮到用户。

### 3.3 痛点

行业痛点也就是技术难点，也有不同说法：

*   不可能三角：即续航、重量、算力这三者不可能同时满足。
*   不可能六边形：续航、功能、重量、体积、美观、方便 这六方面不可能同时满足。
*   也有人认为痛点是内容、价格、舒适度、性能和成本。

其本质就是：功能越多，能耗和需要的算力就越高。如果扩大电池的容量，又会影响到眼镜的重量。这可能需要用低功耗芯片和高能量密度电池来解决。

### 3.4 方案

在智能硬件领域，如何把产品”做小“是个业界难题。需要产业链各个环节的紧密配合、工程化探索以及大量行业Know-How的积累。比如：

*   如何从系统层面降低整体功耗；
*   如何处理/配置端侧AI算力？多核异构系统？分布式架构？
*   如何优化优化双芯片、双系统能力？有些方案不是“开箱即用”，相当考验技术实力。
*   如何确定边缘计算与云协同的平衡点；
*   有意思的是，眼镜续航与AI的第一性原理是相悖的，AI的第一性原理是“数据量”，指的是数据体量和数据质量，而眼镜在有限的电池容量下，在满足基本交互的情况下，能给AI提供多少体量的数据，POV视觉数据和远近声场数据的数据质量从何体现？这些都需要更多行业从业者来摸索。

### 3.5 和手机的关系

关于 AI 眼镜和手机到底啥关系，行业一直有两种说法：一种说 AI 眼镜早晚取代手机，另一种说它本质就是手机的延伸。从现在的情况来看，后一种更靠谱。

手机是生活和数字世界的核心，攒下了庞大的开发者资源和服务体系，这种核心地位短时间内没法被替代。

作为手机的配件，AI 眼镜的价值在于接手那些更适合在眼前完成的操作 —— 手机上那些简单、不用复杂处理的界面和操作，挪到眼镜上后，靠着第一视角的便捷性，体验会更好。但这不是说 AI 眼镜只能靠手机活着，它得有自己的特色：借着佩戴位置靠近感官的优势，在不用手操作、实时识别场景这些方面打造别人替代不了的能力，和手机形成互补，而不是互相竞争。

### 3.6 眼镜公司

AI 眼镜的产业链已经很成熟了，像光学零件、芯片、显示模块、传感器这些核心部件，都能靠现有的消费电子产业链实现批量生产。但想在行业里站稳脚跟，企业得具备多种能力：既要懂手机及相关生态的技术，又得会让端侧的小模型和云端的大模型配合工作，还得有整合 IoT 生态的经验，以及研发适合眼镜的操作系统的实力。

模型策略是 AI 眼镜做出差异化的关键。谷歌眼镜的例子早就证明了，模型能力是产品价值的核心。对企业来说，别想着和大公司在通用模型上硬碰硬，不如走特色路线：比如用端侧的小模型处理语音识别和简单的语义理解，再联动云端大模型给出更深入的回应；把设备打造成有情景智能的助手，专门弥补通用模型的不足，在行业理解、懂用户需求、自己积累的专有技术上建立优势。更重要的是要预判模型的发展节奏 —— 提前五个月预估技术能达到啥水平，同时研发原型产品和应对方案，这样等模型更新迭代时，才能快速适配场景需求。

数据怎么流转也是个没解决的核心问题。哪些场景的数据需要收集、什么时候收集、收集多少、数据是由手机系统接收还是应用接收、处理时端侧和云端怎么分工、返回的内容怎么呈现、数据怎么存储和销毁，这些问题的答案直接影响产品体验和用户信任，也是行业需要一起探索的基础问题。

0xFF 参考
-------

[8000字深度思考：AI眼镜的格局、困局、破局](https://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652782250&idx=2&sn=7774accac6b4c14e5344bd2e00ea0f90&chksm=8512ac0a1ea8d7860940a9dcaca0ea5fa578c322ba29eded0d636c46692e2ee7b700f1542b08&mpshare=1&scene=1&srcid=05088VMI2qR7OLovkIcyECGS&sharer_shareinfo=21b9086c280e476fe19d54aa61db4050&sharer_shareinfo_first=21b9086c280e476fe19d54aa61db4050#rd) ZeR0 \[智东西\](javascript:void(0)😉

[万字解剖：百镜大战，如何胜出？回到AI眼镜产品本身](https://zhuanlan.zhihu.com/p/21356410025) \[Ian在AR行业\]