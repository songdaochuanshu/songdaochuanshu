---
layout: post
title: '连接语言大模型（LLM）服务进行对话'
date: "2025-07-21T00:49:00Z"
---
连接语言大模型（LLM）服务进行对话
==================

本文展示了如何使用阿里云百炼平台的API通过openai模块和LangChain框架与大模型deepseek-r1进行对话及批量文本分类。

1\. 引言
======

最近开始接触AI大模型方向的工作，第一个实例就尝试一下连接大模型进行对话的实现。

2\. 实现
======

2.1 openai模块
------------

要实现这个功能很简单，直接翻各大模型平台的给的API案例一般都可以实现，例如笔者这里使用的阿里云的百炼平台给出的API：

    from openai import OpenAI
    
    client = OpenAI(
        # 使用大模型对应的Key
        api_key = "sk-xxx",
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    
    completion = client.chat.completions.create(
        model="deepseek-r1",  # 此处以 deepseek-r1 为例，可按需更换模型名称。
        messages=[
            {'role': 'user', 'content': '想快速入门AI大模型，给我推荐一下具体的学习方案。'}
        ]
    )
    
    # 通过reasoning_content字段打印思考过程
    print("思考过程：")
    print(completion.choices[0].message.reasoning_content)
    
    # 通过content字段打印最终答案
    print("最终答案：")
    print(completion.choices[0].message.content)
    

我这里使用的大模型是deepseek，但是使用的是`openai`模块。这是因为现在的大模型服务基本都兼容OpenAI API标准的接口，因此可以通过设置不同的base\_url和api\_key来使用相同的openai Python客户端库进行访问。这也是为啥现在大模型平台都可以选择接入不同的大模型来实现AI应用。

除了Python接口，阿里云百炼平台还提供了Node.js和HTTP的接入方式，理论上可以前端、后端、移动端以及桌面端都可以连入大模型来实现自己的AI应用。`messages=[{'role': 'user', 'content': '想快速入门AI大模型，给我推荐一下具体的学习方案。'}]`就是大模型的提示词，通过更改提示词，可以与大模型对话来得到自己想要的结果。

2.2 LangChain
-------------

除了使用openai模块，使用LangChain是个更好的选择。LangChain是一个构建于大型语言模型（LLMs）之上的框架，提供了一系列的工具和接口来简化与这些模型交互的过程。如下所示：

    # 初始化模型
    chat = ChatOpenAI(
        model_name="deepseek-r1",
        temperature=0,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", #服务地址
        api_key="sk-xxx"  #API密钥
    )
    
    # 发送请求
    response = chat.invoke([HumanMessage(content="请用中文介绍你自己。")])
    
    # 输出结果
    print(response.content)
    

为什么说LangChain更好用一点呢，比如说你要执行批量任务，对一些文本进行多标签分类，那么可能需要进行批量提问以提升效率。在这方面LangChain提供了batch接口：

    from langchain_openai import ChatOpenAI
    
    llm_client = ChatOpenAI(
        temperature=0.0,
        model_name="deepseek-r1",
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        api_key="sk-852da921b11545c99de697e584210fc7"
    )
    
    # 假设你的多标签集合如下：
    total_class = {"正面", "负面", "价格问题", "物流问题", "推荐", "外观设计", "用户体验"}
    
    # 系统提示词
    system_prompt = """你是一个多标签分类助手，请从以下标签中选出适用于文本的所有标签（可以多选）：
    正面, 负面, 价格问题, 物流问题, 推荐, 外观设计, 用户体验
    
    只输出标签，多个标签之间用英文逗号分隔。如果无法判断，则返回空字符串。
    """
    
    def llm_labeling(texts: list[str]) -> list[list[str]]:
        """
        对输入文本列表进行多标签打标，返回每条文本对应的标签列表。
        """
        results = []
        batch_inputs = [system_prompt + '\n' + t for t in texts] 
        res = llm_client.batch(batch_inputs)
        for item in res:
            content = item.content.strip()
            if not content:
                results.append([])
                continue
            # 解析逗号分隔标签，清洗一下
            tags = [t.strip() for t in content.split(',')]
            # 只保留在 total_class 中的合法标签
            tags = [t for t in tags if t in total_class]
            results.append(tags)
        return results
    
    texts = [
        "这个产品非常好用，值得推荐",
        "物流速度太慢了，体验不好",
        "外观漂亮，使用方便，就是价格稍贵"
    ]
    
    result = llm_labeling(texts)
    for i, tags in enumerate(result):
        print(f"第{i+1}条: 标签 = {tags}")
    

运行结果如下：

    第1条: 标签 = ['正面', '推荐', '用户体验']
    第2条: 标签 = ['负面', '物流问题', '用户体验']
    第3条: 标签 = ['正面', '外观设计', '用户体验', '价格问题']
    

其实deepseek不一定真的支持批量提问的接口，即使真的不支持，LangChain为我们提供了抽象层，在内部进行并发处理。当然，如果有的大模型提供batch接口，LangChain就会直接调用它。

2.3 其他
------

在阿里的百炼平台上还提供了“多轮对话”和“流式输出”的使用方式。这两种方式是构建AI Chat应用必须的。“多轮对话”就是需要让大模型记住之前的对话内容，也就是上下文，以便得到更好的输出；“流式输出”则是让大模型的回答逐步渐进的输出，一个字一个字的呈现，以便让AI Chat应用的交互性更好。不过笔者暂时不关心这个，以后有机会再试用一下。