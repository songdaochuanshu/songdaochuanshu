---
layout: post
title: 'Êé¢ÁßòTransformerÁ≥ªÂàó‰πãÔºà27Ôºâ--- MQA & GQA'
date: "2025-04-15T00:40:28Z"
---
Êé¢ÁßòTransformerÁ≥ªÂàó‰πãÔºà27Ôºâ--- MQA & GQA
=================================

‰ªéÈõ∂ÂºÄÂßãËß£ÊûêTransformerÔºåÁõÆÊ†áÊòØÔºö(1) Ëß£ÊûêTransformerÂ¶Ç‰ΩïËøê‰ΩúÔºå‰ª•Âèä‰∏∫‰ΩïÂ¶ÇÊ≠§Ëøê‰ΩúÔºåËÆ©Êñ∞ÂêåÂ≠¶ÂèØ‰ª•ÂÖ•Èó®Ôºõ(2) Âäõ‰∫âËûçÂÖ•‰∏Ä‰∫õÊØîËæÉÊñ∞ÁöÑÊàñËÄÖÊúâÁâπËâ≤ÁöÑËÆ∫ÊñáÊàñËÄÖÁêÜÂøµÔºåËÆ©ËÄÅÈ∏ü‰πüÂèØ‰ª•ÊúâÊâÄÊî∂Ëé∑„ÄÇ

Êé¢ÁßòTransformerÁ≥ªÂàó‰πãÔºà27Ôºâ--- MQA & GQA
=================================

ÁõÆÂΩï

*   [Êé¢ÁßòTransformerÁ≥ªÂàó‰πãÔºà27Ôºâ--- MQA & GQA](#Êé¢ÁßòtransformerÁ≥ªÂàó‰πã27----mqa--gqa)
    *   [0x00 Ê¶ÇËø∞](#0x00-Ê¶ÇËø∞)
    *   [0x01 MHA](#0x01-mha)
        *   [1.1 Ê¶ÇÂøµ](#11-Ê¶ÇÂøµ)
        *   [1.2 ÂÆûÁé∞](#12-ÂÆûÁé∞)
            *   [1.2.1 Âìà‰Ωõ](#121-Âìà‰Ωõ)
            *   [1.2.2 llm-foundry](#122-llm-foundry)
        *   [1.3 ËµÑÊ∫êÂç†Áî®](#13-ËµÑÊ∫êÂç†Áî®)
    *   [0x02 MQA](#0x02-mqa)
        *   [2.1 Ê¶ÇÂøµ](#21-Ê¶ÇÂøµ)
        *   [2.2 ÂÆûÁé∞](#22-ÂÆûÁé∞)
            *   [1.2.1 Á≤æÁÆÄÁâà](#121-Á≤æÁÆÄÁâà)
            *   [1.2.2 ÂÆåÊï¥Áâà](#122-ÂÆåÊï¥Áâà)
        *   [2.3 ÊïàÊûú](#23-ÊïàÊûú)
            *   [2.3.1 ÂÜÖÂ≠ò](#231-ÂÜÖÂ≠ò)
            *   [2.3.2 ÈÄüÂ∫¶](#232-ÈÄüÂ∫¶)
            *   [2.3.3 Ë°®ÂæÅËÉΩÂäõ](#233-Ë°®ÂæÅËÉΩÂäõ)
            *   [2.3.3 ÈÄö‰ø°](#233-ÈÄö‰ø°)
    *   [0x03 GQA](#0x03-gqa)
        *   [3.1 Ê¶ÇÂøµ](#31-Ê¶ÇÂøµ)
        *   [3.2 Êû∂ÊûÑÊØîÂØπ](#32-Êû∂ÊûÑÊØîÂØπ)
        *   [3.3 ÂÆûÁé∞](#33-ÂÆûÁé∞)
            *   [3.3.1 Á≤æÁÆÄÁâà](#331-Á≤æÁÆÄÁâà)
            *   [3.3.2 ÂÆåÊï¥Áâà](#332-ÂÆåÊï¥Áâà)
        *   [3.4 ÊïàÊûú](#34-ÊïàÊûú)
            *   [3.4.1 ÂÜÖÂ≠ò](#341-ÂÜÖÂ≠ò)
            *   [3.4.2 ÈÄüÂ∫¶](#342-ÈÄüÂ∫¶)
            *   [3.4.3 Ë°®ÂæÅËÉΩÂäõ](#343-Ë°®ÂæÅËÉΩÂäõ)
        *   [3.5 ËΩ¨Êç¢](#35-ËΩ¨Êç¢)
            *   [3.5.1 Âπ≥ÂùáÊ±†Âåñ](#351-Âπ≥ÂùáÊ±†Âåñ)
            *   [3.5.2 Âü∫‰∫éÊé©Á†Å](#352-Âü∫‰∫éÊé©Á†Å)
                *   [ÁΩëÁªúËΩ¨Êç¢](#ÁΩëÁªúËΩ¨Êç¢)
                *   [ÊâæÂà∞Êõ¥Â•ΩÁöÑÂàÜÁªÑÊñπÊ≥ï](#ÊâæÂà∞Êõ¥Â•ΩÁöÑÂàÜÁªÑÊñπÊ≥ï)
                *   [Ââ™ÊûùËÆ≠ÁªÉ](#Ââ™ÊûùËÆ≠ÁªÉ)
        *   [3.6 ‰ºòÂåñ](#36-‰ºòÂåñ)
    *   [0xFF ÂèÇËÄÉ](#0xff-ÂèÇËÄÉ)

0x00 Ê¶ÇËø∞
-------

Âú®ÂâçÊñá‚Äú‰ºòÂåñKV Cache"‰∏≠Êàë‰ª¨ÊèêÂà∞ËøáÔºåÂú®‚ÄùÂáèÂ∞ëÊ≥®ÊÑèÂäõÂ§¥ÁöÑÊï∞Èáè‚ÄúËøô‰∏™Áª¥Â∫¶‰∏äÔºåÁõÆÂâç‰∏ªË¶ÅÁöÑÁõ∏ÂÖ≥Â∑•‰ΩúÊúâ MQAÂíåGQA„ÄÇMQA Âíå GQA ÊòØÂú®ÁºìÂ≠òÂ§öÂ∞ëÊï∞ÈáèKVÁöÑÊÄùË∑Ø‰∏äËøõË°å‰ºòÂåñÔºöÁõ¥ËßâÊòØÂ¶ÇÊûúÁºìÂ≠òÁöÑKV‰∏™Êï∞Â∞ë‰∏Ä‰∫õÔºåÊòæÂ≠òÂ∞±Âç†Áî®Â∞ë‰∏Ä‰∫õÔºåÂ§ßÊ®°ÂûãËÉΩÂäõÁöÑÈôç‰ΩéÂèØ‰ª•ÈÄöËøáËøõ‰∏ÄÊ≠•ÁöÑËÆ≠ÁªÉÊàñËÄÖÂ¢ûÂä†FFN/GLUÁöÑËßÑÊ®°Êù•Âº•Ë°•„ÄÇ

Âõ†‰∏∫MQAÂíåGQAÊòØÂü∫‰∫éMHAËøõË°åÊîπËøõÔºåÊâÄ‰ª•Êàë‰ª¨Áî®‰∏ãÂõæÂ±ïÁ§∫‰∫Ü‰∏âËÄÖÁöÑÂå∫Âà´„ÄÇÂèØ‰ª•ÁúãÂà∞ÔºåÈÄöËøáÁº©ÂáèÊ≥®ÊÑèÂäõÂ§¥Êï∞ÁõÆÔºåMQA/GQA‰ºöÈôç‰ΩéKV CacheÂ≠òÂÇ®ÔºåËÆ©‰∏çÂêåÁöÑÊ≥®ÊÑèÂäõÂ§¥ÊàñËÄÖÂêå‰∏ÄÁªÑÁöÑÊ≥®ÊÑèÂäõÂ§¥ÂÖ±‰∫´‰∏Ä‰∏™KÂíåVÁöÑÈõÜÂêàÔºåÂõ†‰∏∫Âè™ÂçïÁã¨‰øùÁïô‰∫Ü‰∏Ä‰ªΩÔºàÊàñËÄÖÂá†‰ªΩÔºâÊü•ËØ¢ÂèÇÊï∞„ÄÇÂõ†Ê≠§KÂíåVÁöÑÁü©Èòµ‰ªÖÊúâ‰∏Ä‰ªΩÔºàÊàñËÄÖÂá†‰ªΩÔºâÔºåËøôÂ§ßÂπÖÂ∫¶ÂáèÂ∞ë‰∫ÜÊòæÂ≠òÂç†Áî®Ôºå‰ΩøÂÖ∂Êõ¥È´òÊïà„ÄÇÂè¶Â§ñÔºå‰º†ÁªüÁöÑÂü∫‰∫éMHAÁöÑAttentionÁÆóÂ≠êËøá‰∫éÂç°ËÆøÂ≠òÂ∏¶ÂÆΩÔºåMQAÂíåGQAÔºå‰πÉËá≥ÂêéÁª≠ÁöÑMLAÈÉΩÂèØ‰ª•ÊèêËÆ°ÁÆóËÆøÂ≠òÊØîÔºåËøôÊ†∑‰πüÊòØÂØπÊÄßËÉΩÁöÑÊûÅÂ§ßÊèêÂçá„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202825378-692989445.jpg)

* * *

Ê≥®Ôºö

*   ÂÖ®ÈÉ®ÊñáÁ´†ÂàóË°®Âú®ËøôÈáåÔºå‰º∞ËÆ°ÊúÄÁªàÂú®35ÁØáÂ∑¶Âè≥ÔºåÂêéÁª≠ÊØèÂèë‰∏ÄÁØáÊñáÁ´†Ôºå‰ºö‰øÆÊîπÊ≠§ÊñáÁ´†ÂàóË°®„ÄÇcnblogs [Êé¢ÁßòTransformerÁ≥ªÂàó‰πãÊñáÁ´†ÂàóË°®](https://www.cnblogs.com/rossiXYZ/p/18785601)
*   Êú¨Á≥ªÂàóÊòØÂØπËÆ∫Êñá„ÄÅÂçöÂÆ¢Âíå‰ª£Á†ÅÁöÑÂ≠¶‰π†ÂíåËß£ËØªÔºåÂÄüÈâ¥‰∫ÜÂæàÂ§öÁΩë‰∏äÊúãÂèãÁöÑÊñáÁ´†ÔºåÂú®Ê≠§Ë°®Á§∫ÊÑüË∞¢ÔºåÂπ∂‰∏î‰ºöÂú®ÂèÇËÄÉ‰∏≠ÂàóÂá∫„ÄÇÂõ†‰∏∫Êú¨Á≥ªÂàóÂèÇËÄÉÊñáÁ´†Â§™Â§öÔºåÂèØËÉΩÊúâÊºèÁªôÂá∫Â§ÑÁöÑÁé∞Ë±°„ÄÇÂ¶ÇÊûúÂéü‰ΩúËÄÖÂèëÁé∞ÔºåËøòËØ∑ÊåáÂá∫ÔºåÊàëÂú®ÂèÇËÄÉÊñáÁåÆ‰∏≠ËøõË°åÂ¢ûË°•„ÄÇ

* * *

0x01 MHA
--------

Âõ†‰∏∫MQAÔºåGQAÊòØÂü∫‰∫éMHAËøõË°å‰øÆÊîπÔºåÊâÄ‰ª•Êàë‰ª¨ÊúâÂøÖË¶ÅÂÖàÂõûÈ°æ‰∏ãMHA„ÄÇ

### 1.1 Ê¶ÇÂøµ

MHAÔºàÂç≥Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºâÂú®2017Âπ¥Â∞±ÈöèÁùÄTransformerÂéüÂßãËÆ∫Êñá"Attention Is All You Need"‰∏ÄËµ∑ÊèêÂá∫ÔºåÂÖ∂‰∏ªË¶ÅÂ∑•‰ΩúÊòØÔºöÊääÂéüÊù•‰∏Ä‰∏™Ê≥®ÊÑèÂäõËÆ°ÁÆóÊãÜÊàêÂ§ö‰∏™Â∞è‰ªΩÁöÑÊ≥®ÊÑèÂäõÂ§¥ÔºåÂç≥ÊääQ„ÄÅK„ÄÅVÂàÜÂà´ÊãÜÂàÜÊàêÂ§ö‰ªΩÔºåÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥‰ΩøÁî®Áã¨Á´ãÁöÑQ„ÄÅK„ÄÅVËøõË°åËÆ°ÁÆó„ÄÇËÄåÂ§ö‰∏™Â§¥ÂèØ‰ª•Âπ∂Ë°åËÆ°ÁÆóÔºåÂàÜÂà´ÂæóÂá∫ÁªìÊûúÔºåÊúÄÂêéÂÜçÂêàÂõûÂéüÊù•ÁöÑÁª¥Â∫¶„ÄÇ

Êàë‰ª¨ÈÄöËøá‰∏ãÂõæÊù•ÁúãÁúãMHAÁöÑÊµÅÁ®ãÔºåËøôÈáåËÆæ ùëë Ë°®Á§∫ËØçÂµåÂÖ•ÁöÑÁª¥Â∫¶Ôºå \\(ùëõ\_‚Ñé\\) Ë°®Á§∫Ê≥®ÊÑèÂäõÂ§¥ÁöÑÊï∞ÈáèÔºå \\(ùëë\_‚Ñé\\) Ë°®Á§∫ÊØè‰∏Ä‰∏™Â§¥ÁöÑÁª¥Â∫¶Ôºå \\(‚Ñé\_ùë°\\inùëÖ^ùëë\\) Ë°®Á§∫Á¨¨ ùë° ‰∏™tokenÂú®‰∏Ä‰∏™Ê≥®ÊÑèÂäõÂ±ÇÁöÑËæìÂÖ•Ôºå \\(ùëä^ùëÇ‚ààùëÖ^{ùëë√óùëë\_‚Ñéùëõ\_‚Ñé}\\) Ë°®Á§∫ËæìÂá∫Êò†Â∞ÑÁü©Èòµ„ÄÇÂàôMHAÂèØ‰ª•ÂàÜ‰∏∫‰ª•‰∏ãÂõõÊ≠•Ôºö

1.  ÈÄöËøá3‰∏™ÂèÇÊï∞Áü©Èòµ \\(ùëä^ùëÑ,ùëä^ùêæ,ùëä^ùëâ‚ààùëÖ^{ùëë\_‚Ñéùëõ\_h\\times d}\\) Â∞±ÂèØ‰ª•ÂæóÂà∞ \\(ùëû\_ùë°,ùëò\_ùë°,ùë£\_ùë°‚ààùëÖ^{ùëë\_‚Ñéùëõ\_h}\\) „ÄÇ
2.  \\(ùëû\_ùë°,ùëò\_ùë°,ùë£\_ùë°\\) ‰ºöÂàÜÂâ≤Êàê \\(ùëõ\_‚Ñé\\) ‰∏™ÂêëÈáèÔºå\\(ùëû\_{ùë°,ùëñ},ùëò\_{ùë°,ùëñ},ùë£\_{ùë°,ùëñ}‚ààùëÖ^{ùëë\_‚Ñé}\\) ÂàÜÂà´Ë°®Á§∫Q„ÄÅKÂíåVÁöÑÁ¨¨ ùëñ ‰∏™ÂêëÈáèÔºåËøô‰∫õÊãÜÂàÜÂêéÁöÑÂêëÈáèÊàë‰ª¨ÂêéÁª≠Áß∞‰πã‰∏∫QÂ§¥ÔºåKÂ§¥ÂíåVÂ§¥„ÄÇ
3.  ÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥‰ºöÂà©Áî®Ëá™Â∑±Ëé∑ÂæóÁöÑQ„ÄÅK„ÄÅVÂêëÈáèËøõË°åÊ≥®ÊÑèÂäõËÆ°ÁÆó„ÄÇ
4.  Âà©Áî®\\(W^O\\)ÂØπÂ§öÂ§¥Ê≥®ÊÑèÂäõËÆ°ÁÆóÁªìÊûúËøõË°åÂêàÂπ∂„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202836695-310488105.jpg)

### 1.2 ÂÆûÁé∞

#### 1.2.1 Âìà‰Ωõ

Êàë‰ª¨ÂõûÈ°æ‰∏ã‚ÄúThe Annotated Transformer‚Äù‰∏≠MHA‰ª£Á†ÅÁöÑÂÆûÁé∞

    def attention(query, key, value, mask=None, dropout=None):
        "Compute 'Scaled Dot Product Attention'"
        d_k = query.size(-1)
        scores = torch.matmul(query, key.transpose(-2, -1)) \
                 / math.sqrt(d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        p_attn = F.softmax(scores, dim = -1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return torch.matmul(p_attn, value), p_attn
    
    class MultiHeadedAttention(nn.Module):
        def __init__(self, h, d_model, dropout=0.1):
            '''
            h: head number
            '''
            super(MultiHeadedAttention, self).__init__()
            assert d_model % h == 0
            # We assume d_v always equals d
            self.d = d_model // h
            self.h = h
            self.linears = clones(nn.Linear(d_model, d_model), 4)
            self.attn = None
            self.dropout = nn.Dropout(p=dropout)
            
        def forward(self, query, key, value, mask=None):
            if mask is not None:
                # Same mask applied to all h heads.
                mask = mask.unsqueeze(1)
            nbatches = query.size(0)
            
            # 1) Do all the linear projections in batch from d_model => h x d 
            query, key, value = \
                [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)
                 for l, x in zip(self.linears, (query, key, value))]
            
            # 2) Apply attention on all the projected vectors in batch. 
            x, self.attn = attention(query, key, value, mask=mask, 
                                     dropout=self.dropout)
            
            # 3) "Concat" using a view and apply a final linear. 
            x = x.transpose(1, 2).contiguous() \
                 .view(nbatches, -1, self.h * self.d)
            return self.linears[-1](x)
    

#### 1.2.2 llm-foundry

‰Ωú‰∏∫ÂØπÊØîÔºåÊàë‰ª¨ÁúãÁúãÂ∑•‰∏öÁïåÁöÑ‰∫ßÂìÅ„ÄÇ

[https://github.com/mosaicml/llm-foundry/blob/9c89ab263e72fb9610f28c8ab9cde5d2205b6bff/llmfoundry/models/layers/attention.py](https://github.com/mosaicml/llm-foundry/blob/9c89ab263e72fb9610f28c8ab9cde5d2205b6bff/llmfoundry/models/layers/attention.py)

    class MultiheadAttention(nn.Module):
        """Multi-head self attention.
    
        Using torch or triton attention implemetation enables user to also use
        additive bias.
        """
    
        def __init__(
            self,
            d_model: int,
            n_heads: int,
            attn_impl: str = 'triton',
            clip_qkv: Optional[float] = None,
            qk_ln: bool = False,
            softmax_scale: Optional[float] = None,
            attn_pdrop: float = 0.0,
            low_precision_layernorm: bool = False,
            verbose: int = 0,
            device: Optional[str] = None,
        ):
            super().__init__()
    
            self.attn_impl = attn_impl
            self.clip_qkv = clip_qkv
            self.qk_ln = qk_ln
    
            self.d_model = d_model
            self.n_heads = n_heads
            self.softmax_scale = softmax_scale
            if self.softmax_scale is None:
                self.softmax_scale = 1 / math.sqrt(self.d_model / self.n_heads)
            self.attn_dropout_p = attn_pdrop
    
            self.Wqkv = nn.Linear(self.d_model, 3 * self.d_model, device=device)
            # for param init fn; enables shape based init of fused layers
            fuse_splits = (d_model, 2 * d_model)
            self.Wqkv._fused = (0, fuse_splits)  # type: ignore
    
            if self.qk_ln:
                layernorm_class = LPLayerNorm if low_precision_layernorm else nn.LayerNorm
                self.q_ln = layernorm_class(self.d_model, device=device)
                self.k_ln = layernorm_class(self.d_model, device=device)
    
            if self.attn_impl == 'flash':
                self.attn_fn = flash_attn_fn
            elif self.attn_impl == 'triton':
                self.attn_fn = triton_flash_attn_fn
            elif self.attn_impl == 'torch':
                self.attn_fn = scaled_multihead_dot_product_attention
            else:
                raise ValueError(f'{attn_impl=} is an invalid setting.')
    
            self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
            self.out_proj._is_residual = True  # type: ignore
    
        def forward(
            self,
            x,
            past_key_value=None,
            attn_bias=None,
            attention_mask=None,
            is_causal=True,
            needs_weights=False,
        ):
            qkv = self.Wqkv(x)
    
            if self.clip_qkv:
                qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)
    
            query, key, value = qkv.chunk(3, dim=2)
    
            key_padding_mask = attention_mask
    
            if self.qk_ln:
                # Applying layernorm to qk
                dtype = query.dtype
                query = self.q_ln(query).to(dtype)
                key = self.k_ln(key).to(dtype)
    
            context, attn_weights, past_key_value = self.attn_fn(
                query,
                key,
                value,
                self.n_heads,
                past_key_value=past_key_value,
                softmax_scale=self.softmax_scale,
                attn_bias=attn_bias,
                key_padding_mask=key_padding_mask,
                is_causal=is_causal,
                dropout_p=self.attn_dropout_p,
                training=self.training,
                needs_weights=needs_weights,
            )
    
            return self.out_proj(context), attn_weights, past_key_value
    

scaled\_multihead\_dot\_product\_attention()‰ª£Á†ÅÂ¶Ç‰∏ã„ÄÇ

    def scaled_multihead_dot_product_attention(
        query,
        key,
        value,
        n_heads,
        past_key_value=None,
        softmax_scale=None,
        attn_bias=None,
        key_padding_mask=None,
        is_causal=False,
        dropout_p=0.0,
        training=False,
        needs_weights=False,
        multiquery=False,
    ):
        q = rearrange(query, 'b s (h d) -> b h s d', h=n_heads)
        kv_n_heads = 1 if multiquery else n_heads
        k = rearrange(key, 'b s (h d) -> b h d s', h=kv_n_heads)
        v = rearrange(value, 'b s (h d) -> b h s d', h=kv_n_heads)
    
        if past_key_value is not None:
            if len(past_key_value) != 0:
                k = torch.cat([past_key_value[0], k], dim=3)
                v = torch.cat([past_key_value[1], v], dim=2)
            past_key_value = (k, v)
    
        b, _, s_q, d = q.shape
        s_k = k.size(-1)
    
        if softmax_scale is None:
            softmax_scale = 1 / math.sqrt(d)
    
        attn_weight = q.matmul(k) * softmax_scale
    
        if attn_bias is not None:
            _s_q = max(0, attn_bias.size(2) - s_q)
            _s_k = max(0, attn_bias.size(3) - s_k)
            attn_bias = attn_bias[:, :, _s_q:, _s_k:]
            attn_weight = attn_weight + attn_bias
    
        min_val = torch.finfo(q.dtype).min
    
        if key_padding_mask is not None:
            attn_weight = attn_weight.masked_fill(
                ~key_padding_mask.view((b, 1, 1, s_k)), min_val)
    
        if is_causal and (not q.size(2) == 1):
            s = max(s_q, s_k)
            causal_mask = attn_weight.new_ones(s, s, dtype=torch.float16)
            causal_mask = causal_mask.tril()
            causal_mask = causal_mask.to(torch.bool)
            causal_mask = ~causal_mask
            causal_mask = causal_mask[-s_q:, -s_k:]
            attn_weight = attn_weight.masked_fill(causal_mask.view(1, 1, s_q, s_k),
                                                  min_val)
    
        attn_weight = torch.softmax(attn_weight, dim=-1)
    
        if dropout_p:
            attn_weight = torch.nn.functional.dropout(attn_weight,
                                                      p=dropout_p,
                                                      training=training,
                                                      inplace=True)
    
        out = attn_weight.matmul(v)
        out = rearrange(out, 'b h s d -> b s (h d)')
    
        if needs_weights:
            return out, attn_weight, past_key_value
        return out, None, past_key_value
    

### 1.3 ËµÑÊ∫êÂç†Áî®

Â¶ÇÊûúÊ®°ÂûãÁªìÊûÑÊòØMHAÔºåÂú®Êé®ÁêÜÊó∂ÔºåKV CacheÂØπ‰∫éÊØè‰∏™tokenÈúÄË¶ÅÁºìÂ≠òÁöÑÂèÇÊï∞Êúâ \\(2ùëõ\_‚Ñéùëë\_‚Ñéùëô\\)Ôºàùëô Ë°®Á§∫ÁΩëÁªúÂ±ÇÊï∞Ôºâ„ÄÇÂΩìÊ®°ÂûãÂ±ÇÊï∞Âä†Ê∑±ÂíåÂ§¥Êï∞ÂèòÂ§öÂêéÔºåÊ≥®ÊÑèÂäõËÆ°ÁÆóÊâÄÊ∂âÂèäÁöÑÁÆóÂäõ„ÄÅIOÂíåÂÜÖÂ≠òÈÉΩ‰ºöÂø´ÈÄüÂ¢ûÂä†„ÄÇ‰ΩÜÊòØÂØπËøô‰∫õËµÑÊ∫êÂç¥Âà©Áî®Âæó‰∏çÂ•Ω„ÄÇ

Â∞±‰∏ãÂõæËÄåË®ÄÔºåd Ë°®Á§∫ hidden sizeÔºåh Ë°®Á§∫ Head ‰∏™Êï∞Ôºål Ë°®Á§∫ÂΩìÂâçËæìÂÖ•Â∫èÂàó‰∏ÄÂÖ±Êúâ l ‰∏™ Token„ÄÇ

*   ÂΩì Batch Size ‰∏∫ 1 Êó∂ÔºåÂõæ‰∏≠Á∫¢Ëâ≤„ÄÅÁªøËâ≤„ÄÅËìùËâ≤ËôöÁ∫øÂúàÂ§ÑÁöÑ‰πòÊ≥ïÂÖ®ÈÉ®‰∏∫Áü©Èòµ‰πòÂêëÈáèÔºåÊòØÊòéÊòæÁöÑ Memory BoundÔºåÁÆóÊúØÂº∫Â∫¶‰∏çÂà∞ 1„ÄÇ
    
*   ÂΩì Batch Size Â§ß‰∫é 1 Êó∂ÔºàÊØîÂ¶Ç Continuous BatchingÔºâÔºö
    
*   *   Á∫¢Ëâ≤ÂíåËìùËâ≤ÈÉ®ÂàÜÔºöÁ∫øÊÄßÂ±ÇËÆ°ÁÆóÊòØÊùÉÈáç‰πò‰ª•ÊøÄÊ¥ªÔºå‰∏çÂêåËØ∑Ê±Ç‰πãÈó¥ÂèØ‰ª•ÂÖ±‰∫´ÊùÉÈáçÔºåÂõ†Ê≠§ÊòØÁü©Èòµ‰πòÁü©ÈòµÔºåÂπ∂‰∏î Batch Size Ë∂äÂ§ßÔºåÁÆóÊúØÂº∫Â∫¶Ë∂äÂ§ßÔºåË∂äË∂ãËøë‰∫éËÆ°ÁÆóÂØÜÈõÜÂûãÔºàFFN Â±Ç‰πüÁ±ª‰ººÔºâ„ÄÇ
    *   ÁªøËâ≤ÈÉ®ÂàÜÔºöÊ≥®ÊÑèÂäõËÆ°ÁÆóÊòØÊøÄÊ¥ª‰πò‰ª•ÊøÄÊ¥ª„ÄÇÂõ†‰∏∫‰∏çÂêåÁöÑËØ∑Ê±Ç‰πãÈó¥Ê≤°Êúâ‰ªª‰ΩïÁõ∏ÂÖ≥ÊÄßÔºåÂç≥‰Ωø BatchingÔºåÊ≠§Â§Ñ‰πüÊòØ Batched Áü©Èòµ‰πòÂêëÈáèÔºåÂπ∂‰∏îÂõ†‰∏∫Â∫èÂàóÈïøÂ∫¶ÂèØËÉΩ‰∏çÂêåÔºåËøôÈáå‰∏çÂêåËØ∑Ê±ÇÁöÑÁü©Èòµ‰πòÂêëÈáèÊòØ‰∏çËßÑÂàôÁöÑ„ÄÇÂç≥ÔºåËøôÈáåÁÆóÊúØÂº∫Â∫¶ÂßãÁªà‰∏çÂà∞ 1ÔºåÊòØÊòéÊòæÁöÑ Memory Bound„ÄÇ
*   Âõ†Ê≠§ÔºåÁªøËâ≤ÈÉ®ÂàÜÈöæ‰ª•‰ºòÂåñÔºåËæìÂÖ•Â∫èÂàóË∂äÈïøÔºåÊ≠§Â§ÑÁöÑÁì∂È¢àÂ∞±Ë∂äÂ§ß„ÄÇ
    

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250414200154304-642792608.jpg)

‰∏∫‰∫ÜÁºìËß£Ëøô‰∫õËµÑÊ∫êÂç†Áî®ÔºåÂêåÊó∂‰πüÂèØ‰ª•Êõ¥Â•ΩÁöÑÂà©Áî®ËµÑÊ∫êÔºåÁõ∏ÁªßÂá∫Áé∞‰∫ÜMQAÔºàMulti-Query AttentionÔºâ ÂíåGQAÔºàGrouped-Query Attention ÔºâÁ≠âÊñπÊ≥ïÔºåËøô‰∫õÊñπÊ≥ïÈÉΩÊòØÂõ¥Áªï‚ÄúÂ¶Ç‰ΩïÂáèÂ∞ëËµÑÊ∫êÂç†Áî®‰∏îÂ∞ΩÂèØËÉΩÂú∞‰øùËØÅÊïàÊûú‚ÄùËøô‰∏™‰∏ªÈ¢òÂèëÂ±ïËÄåÊù•ÁöÑ‰∫ßÁâ©„ÄÇ

0x02 MQA
--------

ÁõÆÂâçÁöÑÂü∫Êú¨ÂÅáËÆæÊòØÔºåÂú®Â§¥Áª¥Â∫¶‰∏äÂ≠òÂú®ÈùûÂ∏∏È´òÁöÑÁ®ÄÁñèÊÄßÔºåÊàë‰ª¨ÂèØ‰ª•ÊääÂ§¥ÁöÑÊï∞ÈáèÁº©ÂáèÂà∞Áõ∏ÂΩìÂ∞èÁöÑÊï∞ÁõÆ„ÄÇÂú®Ëøô‰∫õÊ≥®ÊÑèÂäõÂ§¥‰∏≠ÔºåÊúâ‰∏Ä‰∫õÂ§¥ÈÉ®‰∏ìÈó®Áî®‰∫éÊ£ÄÁ¥¢ÂíåÈïø‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ËÉΩÂäõÔºåÂõ†Ê≠§Â∫îËØ•‰øùÁïôËøô‰∫õÊ£ÄÁ¥¢Â§¥Âπ∂‰øÆÂâ™ÂÖ∂‰ªñÂ§¥„ÄÇÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÂ§¥ÈÉ®‰øÆÂâ™ÈÄöÂ∏∏ÂèëÁîüÂú®È¢ÑÂ°´ÂÖÖ‰πãÂêéÔºåËøôÊÑèÂë≥ÁùÄÂÆÉ‰ª¨Âè™‰ºöÊîπÂñÑËß£Á†Å„ÄÅÂπ∂ÂèëÊÄßÂíå‰∏ä‰∏ãÊñáÂàáÊç¢Ôºå‰ΩÜÂπ∂Ê≤°ÊúâÊîπÂñÑÈ¢ÑÂ°´ÂÖÖÈò∂ÊÆµ„ÄÇ

### 2.1 Ê¶ÇÂøµ

MQAÔºàMulti Queries AttentionÔºâÂá∫Ëá™ËÆ∫Êñá \[[2019\] Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150.pdf)„ÄÇÂú®MQA‰∏≠Ôºå‰øùÁïôqueryÁöÑÂ§öÂ§¥ÊÄßË¥®ÔºåÊâÄÊúâÊü•ËØ¢Â§¥ÂÖ±‰∫´Áõ∏ÂêåÁöÑÂçï‰∏ÄÈîÆÂíåÂÄºÂ§¥ÔºåËøôÁî®ÂèØ‰ª•ÂáèÂ∞ëKeyÂíåValueÁü©ÈòµÁöÑÊï∞ÈáèÔºå‰ªéËÄåÈôç‰ΩéËÆ°ÁÆóÂíåÂ≠òÂÇ®ÂºÄÈîÄ„ÄÇËøôÁõ∏ÂΩì‰∫éÊää‰∏çÂêåHeadÁöÑÊ≥®ÊÑèÂäõÂ∑ÆÂºÇÔºåÂÖ®ÈÉ®ÈÉΩÊîæÂú®‰∫ÜQuery‰∏äÔºåÈúÄË¶ÅÊ®°Âûã‰ªÖ‰ªé‰∏çÂêåÁöÑQuery Heads‰∏äÂ∞±ËÉΩÂ§üÂÖ≥Ê≥®Âà∞ËæìÂÖ•hidden states‰∏çÂêåÊñπÈù¢ÁöÑ‰ø°ÊÅØ„ÄÇ

MQAÁöÑÂÖ∑‰ΩìÁâπÁÇπÂ¶Ç‰∏ã„ÄÇ

*   Q ‰ªçÁÑ∂‰øùÊåÅÂéüÊù•ÁöÑÂ§¥Êï∞ÔºåÂç≥Á∫øÊÄßÂèòÊç¢‰πãÂêéÔºå‰æùÁÑ∂ÂØπQËøõË°åÂàáÂàÜÔºàÂÉèMHA‰∏ÄÊ†∑ÔºâÔºåÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥ÂçïÁã¨‰øùÁïô‰∫ÜËá™Â∑±ÁöÑQÂêëÈáè„ÄÇ
*   K Âíå V Âè™Êúâ‰∏Ä‰∏™Â§¥ÔºåÂÖ∑‰ΩìÊòØÂú®Á∫øÊÄßÂèòÊç¢Êó∂Áõ¥Êé•ÊääKÂíåVÁöÑÁª¥Â∫¶ÈôçÂà∞‰∫Ü\\(d\_{head}\\)ÔºåËÄå‰∏çÊòØÂÅöÂàáÂàÜÂèòÂ∞è„ÄÇ
*   ÊâÄÊúâÁöÑ Q Â§¥ÂÖ±‰∫´Ëøô‰∏™K Âíå V Â§¥ÔºåÊàñËÄÖÂèØ‰ª•ËÆ§‰∏∫ÊòØ k, vÁü©ÈòµÂèÇÊï∞ÂÖ±‰∫´„ÄÇÂÆûÁé∞‰∏äÔºåÂ∞±ÊòØÊîπ‰∏Ä‰∏ãÁ∫øÊÄßÂèòÊç¢Áü©ÈòµÔºåÁÑ∂ÂêéÊää K„ÄÅV ÁöÑÂ§ÑÁêÜ‰ªéÂàáÂàÜÂèòÊàêÂ§çÂà∂„ÄÇ
*   ÊâÄÊúâQÂ§¥ÈÉΩ‰ΩøÁî®Ëøô‰∏™Áõ∏ÂêåÁöÑKÂ§¥ËÆ°ÁÆóÂÆÉ‰ª¨ÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∞ÔºåÂπ∂‰∏îÊâÄÊúâÂ§¥ÁöÑËæìÂá∫ÈÉΩ‰ΩøÁî®Áõ∏ÂêåÁöÑVÂ§¥ËÆ°ÁÆóÔºà‰ΩÜÊ≥®ÊÑèÂäõÂàÜÊï∞‰∏çÂêåÔºâ„ÄÇ
*   ÊúÄÂêéÂ∞ÜÊØè‰∏™Â§¥ËÆ°ÁÆóÁöÑÁªìÊûúÊãºÊé•Ëµ∑Êù•„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202855494-1240701416.jpg)

### 2.2 ÂÆûÁé∞

Êàë‰ª¨ËøòÊòØ‰ª•llm-foundry‰∏∫‰æãÊù•ËøõË°åÂàÜÊûê„ÄÇ

#### 1.2.1 Á≤æÁÆÄÁâà

Êàë‰ª¨ÂÖàÁªôÂá∫MHAÂíåMQAÁöÑÁ≤æÁÆÄÁâàÂØπÊØî„ÄÇËøôÈáåÂÅáËÆæ x (tensor): (batch, hidden\_state, d\_model) ÔºåÊØîÂ¶Ç (1, 512, 768) „ÄÇÂèØ‰ª•ÁúãÂà∞Ôºå‰∏§ËÄÖ‰∏ªË¶Å‰∏çÂêåÂú®‰∫éÔºö

*   WÁü©ÈòµÁöÑÁª¥Â∫¶‰∏çÂêå„ÄÇ
*   QKVÂàáÂàÜÊñπÂºè‰∏çÂêå„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202905108-376926323.jpg)

‰ªé‰ª£Á†Å‰∏≠ÂèØ‰ª•ÁúãÂà∞ÔºåÂØπ‰∫éMQAÊù•ËØ¥ÔºåÊâÄÊúâÂ§¥‰πãÈó¥ÂÖ±‰∫´‰∏Ä‰ªΩ key Âíå value ÁöÑÂèÇÊï∞Ôºå‰ΩÜÊòØÂ¶Ç‰ΩïÂ∞ÜËøô 1 ‰ªΩÂèÇÊï∞ÂêåÊó∂ËÆ© 8 ‰∏™Â§¥ÈÉΩ‰ΩøÁî®Âë¢ÔºüÂú®scaled\_multihead\_dot\_product\_attention()ÂáΩÊï∞ÁöÑ‰ª£Á†Å‰ºö‰ΩøÁî®Áü©Èòµ‰πòÊ≥ï matmulÊù•ÂπøÊí≠Ôºå‰ΩøÂæóÊØè‰∏™Â§¥ÈÉΩ‰πò‰ª•ËøôÂêå‰∏Ä‰∏™Âº†ÈáèÔºå‰ª•Ê≠§Êù•ÂÆûÁé∞ÂèÇÊï∞ÂÖ±‰∫´„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202915394-772507533.jpg)

MQAÁöÑÊÄª‰ΩìÊµÅÁ®ãÂèØ‰ª•ÂèÇËßÅ‰∏ãÂõæ„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202923601-315033145.jpg)

#### 1.2.2 ÂÆåÊï¥Áâà

Êàë‰ª¨ÂÜçÁªôÂá∫ÂÆåÊï¥ÁâàÊú¨‰ª£Á†Å„ÄÇ

    class MultiQueryAttention(nn.Module):
        """Multi-Query self attention.
    
        Using torch or triton attention implemetation enables user to also use
        additive bias.
        """
    
        def __init__(
            self,
            d_model: int,
            n_heads: int,
            attn_impl: str = 'triton',
            clip_qkv: Optional[float] = None,
            qk_ln: bool = False,
            softmax_scale: Optional[float] = None,
            attn_pdrop: float = 0.0,
            low_precision_layernorm: bool = False,
            verbose: int = 0,
            device: Optional[str] = None,
        ):
            super().__init__()
    
            self.attn_impl = attn_impl
            self.clip_qkv = clip_qkv
            self.qk_ln = qk_ln
    
            self.d_model = d_model
            self.n_heads = n_heads
            self.head_dim = d_model // n_heads
            self.softmax_scale = softmax_scale
            if self.softmax_scale is None:
                self.softmax_scale = 1 / math.sqrt(self.head_dim)
            self.attn_dropout_p = attn_pdrop
    
            # NOTE: if we ever want to make attn TensorParallel, I'm pretty sure we'll
            # want to split Wqkv into Wq and Wkv where Wq can be TensorParallel but
            # Wkv shouldn't be TensorParallel
            # - vchiley
            self.Wqkv = nn.Linear(
                d_model,
                d_model + 2 * self.head_dim,
                device=device,
            )
            # for param init fn; enables shape based init of fused layers
            fuse_splits = (d_model, d_model + self.head_dim)
            self.Wqkv._fused = (0, fuse_splits)  # type: ignore
    
            if self.qk_ln:
                layernorm_class = LPLayerNorm if low_precision_layernorm else nn.LayerNorm
                self.q_ln = layernorm_class(d_model, device=device)
                self.k_ln = layernorm_class(self.head_dim, device=device)
    
            if self.attn_impl == 'flash':
                self.attn_fn = flash_attn_fn
            elif self.attn_impl == 'triton':
                self.attn_fn = triton_flash_attn_fn
            elif self.attn_impl == 'torch':
                self.attn_fn = scaled_multihead_dot_product_attention
            else:
                raise ValueError(f'{attn_impl=} is an invalid setting.')
    
            self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
            self.out_proj._is_residual = True  # type: ignore
    
        def forward(
            self,
            x,
            past_key_value=None,
            attn_bias=None,
            attention_mask=None,
            is_causal=True,
            needs_weights=False,
        ):
            qkv = self.Wqkv(x)
    
            if self.clip_qkv:
                qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)
    
            query, key, value = qkv.split(
                [self.d_model, self.head_dim, self.head_dim], dim=2)
    
            key_padding_mask = attention_mask
    
            if self.qk_ln:
                # Applying layernorm to qk
                dtype = query.dtype
                query = self.q_ln(query).to(dtype)
                key = self.k_ln(key).to(dtype)
    
            context, attn_weights, past_key_value = self.attn_fn(
                query,
                key,
                value,
                self.n_heads,
                past_key_value=past_key_value,
                softmax_scale=self.softmax_scale,
                attn_bias=attn_bias,
                key_padding_mask=key_padding_mask,
                is_causal=is_causal,
                dropout_p=self.attn_dropout_p,
                training=self.training,
                needs_weights=needs_weights,
                multiquery=True,
            )
    
            return self.out_proj(context), attn_weights, past_key_value
    

### 2.3 ÊïàÊûú

#### 2.3.1 ÂÜÖÂ≠ò

MQAÈúÄË¶ÅÁºìÂ≠òÁöÑ K„ÄÅV ÂÄº‰ªéÊâÄÊúâÂ§¥ÂèòÊàê‰∏Ä‰∏™Â§¥ÔºåÂõ†Ê≠§Áõ¥Êé•Â∞ÜKV CacheÂáèÂ∞ëÂà∞‰∫ÜÂéüÊù•ÁöÑ1/‚Ñé„ÄÇMHAÁöÑÂçï‰∏™TokenÈúÄË¶Å‰øùÂ≠òÁöÑKVÊï∞Ôºà \\(2‚àóùëô‚àóùëõ\_‚Ñé\\) ÔºâÔºåËÄåMQAÂáèÂ∞ëÂà∞‰∫ÜÔºà 2√óùëô Ôºâ‰∏™ÔºåÂç≥ÊØè‰∏ÄÂ±ÇÂÖ±‰∫´‰ΩøÁî®‰∏Ä‰∏™ ùëò ÂêëÈáèÂíå‰∏Ä‰∏™ ùë£ ÂêëÈáè„ÄÇ

#### 2.3.2 ÈÄüÂ∫¶

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202935498-1452899910.jpg)

ËÆ∫Êñá‰ΩúËÄÖÂÅö‰∫Ü‰∏ÄÁ≥ªÂàóÊµãËØïÔºåÂÖ∑‰ΩìÂèÇËßÅ‰∏äË°®ÔºàÊï∞ÂÄºÊòØÂπ≥ÂùáÁîüÊàêÊØè‰∏™tokenÊâÄÈúÄË¶ÅÁöÑÊØ´ÁßíÊï∞Ôºâ„ÄÇÈúÄË¶ÅÊ≥®ÊÑèÁöÑÂá†‰∏™ÁÇπÊòØÔºö

1.  ËÆ≠ÁªÉÈÄüÂ∫¶Âá†‰πéÊ≤°ÊúâÂèòÂåñ„ÄÇ
2.  Êé®ÁêÜÊó∂Èó¥ÂíåBeam searchÊó∂Èó¥ÈÉΩÊòæËëóÁº©Áü≠„ÄÇ
3.  Êé®ÁêÜÈÄüÂ∫¶‰∏≠ÔºåencoderÁöÑÊé®ÁêÜÈÄüÂ∫¶Âü∫Êú¨‰∏çÂèòÔºådecoderÁöÑÊé®ÁêÜÂø´‰∫ÜÂæàÂ§ö„ÄÇ

ËôΩÁÑ∂MQAÂè™Êúâ‰∏ÄÁªÑKVÂ§¥Ôºå‰ΩÜÂÆûÈôÖ‰∏äMQAÊòØËØªÂèñËøôÁªÑKVÂ§¥‰πãÂêéÔºåÂ§çÂà∂ÁªôÊâÄÊúâQÂ§¥‰ΩøÁî®ÔºåÂõ†Ê≠§ÊåâÁÖßÈÅìÁêÜÊù•ËØ¥ÔºåMQAÂè™ËÉΩÈôç‰ΩéÊòæÂ≠òÁöÑ‰ΩøÁî®ÔºåËøêÁÆóÈáèÂπ∂Ê≤°ÊúâÂáèÂ∞ëÔºå‰∏∫Âï•ÈÄüÂ∫¶ËÉΩÊèêÈ´òËøô‰πàÂ§öÔºüÂÖ∂ÂÆû‰∏ªË¶ÅÊî∂ÁõäÊòØÂõ†‰∏∫Èôç‰Ωé‰∫ÜKV CacheËÄåÂ∏¶Êù•ËÆ°ÁÆóÈáèÁöÑÂáèÂ∞ëÔºåÂÖ∑‰ΩìÂ¶Ç‰∏ãÔºö

*   KV-CacheÁ©∫Èó¥Âç†Áî®Èôç‰Ωé„ÄÇÂõ†‰∏∫Â§¥Êï∞ÈáèÁöÑÂáèÂ∞ëÔºåÊâÄ‰ª•ÈúÄË¶ÅÂ≠òÂÇ®Âú®GPUÂÜÖÂ≠ò‰∏≠ÁöÑÂº†Èáè‰πüÂáèÂ∞ë‰∫ÜÔºàÂÅáËÆæ‰πãÂâçË¶ÅÂ≠òÂÇ®32‰∏™Â§¥ÁöÑKV CacheÔºåÁõÆÂâçÂè™ÈúÄË¶ÅÂ≠òÂÇ®1‰∏™Â§¥ÁöÑKV CacheÔºâ„ÄÇËäÇÁúÅÁöÑÁ©∫Èó¥ÂèØ‰ª•Áî®Êù•Â¢ûÂä†ÊâπÊ¨°Â§ßÂ∞èÔºåÊèêÂçáÂêûÂêêÔºå‰ªéËÄåÊèêÈ´òÊïàÁéáÔºàËôΩÁÑ∂ÂçïÊù°ËØ∑Ê±ÇÁöÑÊÄªÊó∂Âª∂‰ºöÂ¢ûÂä†Ôºå‰ΩÜÊúçÂä°ÁöÑÊÄªÂêûÂêêÈáèÊòØÊòéÊòæÂ¢ûÂä†Ôºâ„ÄÇ
*   Èôç‰ΩéÂÜÖÂ≠òËØªÂèñÊ®°ÂûãÊùÉÈáçÁöÑÊó∂Èó¥ÂºÄÈîÄ„ÄÇÂõ†‰∏∫Â§¥Êï∞ÈáèÁöÑÂáèÂ∞ëÔºåÊâÄ‰ª•ÂáèÂ∞ë‰∫Ü‰ªéÊòæÂ≠ò‰∏≠ËØªÂèñÁöÑÊï∞ÊçÆÈáèÔºåÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÂçïÂÖÉÁöÑÁ≠âÂæÖÊó∂Èó¥Ôºå‰ªéÂÜÖÂ≠òÂØÜÈõÜÂûãË∂ãËøë‰∫éËÆ°ÁÆóÂØÜÈõÜÂûã„ÄÇÂè¶Â§ñÔºåÂêå‰∏Ä‰∏™ Request ‰∏≠ÁöÑ‰∏çÂêå Head ÂèØ‰ª•ÂÖ±‰∫´ÔºåËøôÂ∞±ÊèêÂçá‰∫Ü Q„ÄÅK Âíå V ÁöÑ Attention ËÆ°ÁÆóÁöÑÁÆóÊúØÂº∫Â∫¶„ÄÇ

#### 2.3.3 Ë°®ÂæÅËÉΩÂäõ

Âõ†‰∏∫ÁõÆÂâçÂè™Êúâ‰∏Ä‰∏™ÂÖ±‰∫´ÁöÑKVÂ§¥ÔºåÊâÄ‰ª•ÂéüÂÖàÂ§öQKVÂ§¥Â∏¶Êù•ÁöÑÊ≥®ÊÑèÂäõÂ∑ÆÂºÇÈÉΩÈúÄË¶Å‰ªÖ‰ªÖ‰æùÈù†Â§ö‰∏™QÂ§¥ÂÆåÊàêÔºåËøôÊ†∑ÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑË°®ÂæÅËÉΩÂäõÔºåÂõ†Ê≠§MQAËôΩÁÑ∂ËÉΩÂ•ΩÂú∞ÊîØÊåÅÊé®ÁêÜÂä†ÈÄüÔºå‰ΩÜÊòØÂú®ÊïàÊûú‰∏äÊØîMHAÁï•Â∑Æ„ÄÇ‰∏∫‰∫ÜÂº•Ë°•ÂÖ±‰∫´KVÂ∏¶Êù•ÁöÑÂèÇÊï∞ÈáèÂáèÂ∞ëÔºå‰∫∫‰ª¨ÂæÄÂæÄ‰ºöÁõ∏Â∫îÂú∞Â¢ûÂ§ßFFN/GLUÁöÑËßÑÊ®°Ôºå‰ª•Ê≠§Êù•Áª¥ÊåÅÊ®°ÂûãÊÄªÂèÇÊï∞ÈáèÁöÑ‰∏çÂèòÔºåËøõËÄåÂº•Ë°•‰∏ÄÈÉ®ÂàÜÊïàÊûúÊçüÂ§±„ÄÇ

Âè¶Â§ñÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÁî±‰∫éMQAÂíåGQAÊîπÂèò‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÁªìÊûÑÔºåÂõ†Ê≠§Ê®°ÂûãÈÄöÂ∏∏ÈúÄË¶Å‰ªéËÆ≠ÁªÉÂºÄÂßãÂ∞±ÊîØÊåÅ MQAÊàñËÄÖGQA „ÄÇÂ¶ÇÊûúÊ®°ÂûãÂ∑≤ÁªèËÆ≠ÁªÉÂ•Ω‰∫ÜÔºåÂ∞ÜKV CacheÂº∫Ë°åÊç¢ÊàêËøô‰∏§‰∏™ÊñπÊ≥ïÔºåÊïàÊûú‰ºöÂæàÂ∑ÆÔºåÂõ†Ê≠§ÈúÄË¶ÅÈúÄË¶ÅÂÄüÂä©ÂæÆË∞ÉÊù•Âº•Ë°•„ÄÇÊúâÁ†îÁ©∂Ë°®ÊòéÈúÄË¶ÅÁ∫¶ 5% ÁöÑÂéüÂßãËÆ≠ÁªÉÊï∞ÊçÆÈáèÂ∞±ÂèØ‰ª•ËææÂà∞‰∏çÈîôÁöÑÊïàÊûú„ÄÇ

#### 2.3.3 ÈÄö‰ø°

Âú®Â§öÂç°Âπ∂Ë°åÊÉÖÂÜµ‰∏ãÔºåMQAÂáèÂ∞ë‰∫ÜËÆøÂ≠òÔºå‰ΩÜÊòØÂ¢ûÂä†‰∫ÜÂπ∂Ë°åÈÄö‰ø°ÂºÄÈîÄ„ÄÇÂõ†‰∏∫KÂíåVÂº†ÈáèÂú®ÊâÄÊúâÂ§¥ÈÉ®‰πãÈó¥ÂÖ±‰∫´ÔºåÊØè‰∏™GPU‰∏äÈÉΩÈúÄË¶ÅÊúâËá™Â∑±ÁöÑÂ§á‰ªΩ„ÄÇ‰∏é‰∏ãÂõæ(a)‰∏≠MHAÂπ∂Ë°åÁ≠ñÁï•Áõ∏ÊØîÔºåMQAÈúÄË¶Å‰ΩøÁî®all-to-allÂØπËøõË°åËæìÂÖ•ËæìÂá∫ÊøÄÊ¥ªÂº†ÈáèreshardingÔºå‰ªéËÄå‰∫ßÁîüÈ¢ùÂ§ñÁöÑÈÄö‰ø°ÊàêÊú¨„ÄÇÂÖ∑‰ΩìÂ¶Ç‰∏ãÂõæ(b)ÊâÄÁ§∫„ÄÇÂè¶Â§ñÔºåÂõ†‰∏∫ÊØè‰∏™Âç°‰∏äÈÉΩÊúâÂ§á‰ªΩÔºåËøôÂèØËÉΩ‰ºöÂØºËá¥MQAÁöÑÂÜÖÂ≠òÊàêÊú¨ËäÇÁúÅÂ∞Ü‰ºö‰∏ßÂ§±„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202947399-47036567.jpg)

0x03 GQA
--------

ÂØπ‰∫éÊõ¥Â§ßÁöÑÊ®°ÂûãËÄåË®ÄÔºåÂΩªÂ∫ïÂâ•Á¶ªÊâÄÊúâÂ§¥Ëøá‰∫éÊøÄËøõ„ÄÇ‰æãÂ¶ÇÔºåÁõ∏ÊØî‰ªé32ÂáèÂ∞ëÂà∞1ÔºåÂ∞ÜÂ§¥Êï∞‰ªé64ÂáèÂ∞ëÂà∞1Âú®Ê®°ÂûãÁöÑË°®ÂæÅËÉΩÂäõ‰∏äÊòØ‰∏Ä‰∏™Êõ¥Â§ßÁöÑÂâäÂáè„ÄÇËÄå‰∏îÊ†πÊçÆGQAËÆ∫ÊñáÁöÑÂÆûÈ™åËØ¥ÔºåMQAËôΩÁÑ∂‚Äùdrastically‚ÄúÊèêÂçá‰∫Üdecoder‰∏≠ÁöÑÊé®ÁêÜÊÄßËÉΩÔºå‰ΩÜËøôÊ†∑ÂÅö‰ºöÂ∏¶Êù•ÁîüÊàêË¥®ÈáèÁöÑÊòæËëó‰∏ãÈôç‰ª•ÂèäÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö„ÄÇÊâÄ‰ª•‰∏∫‰∫ÜÂú®Áâ∫Áâ≤Êõ¥Â∞èÊÄßËÉΩÂâçÊèê‰∏ãÂä†ÈÄüÔºåGQAÂ∫îËøêËÄåÁîü„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203000275-1984809569.jpg)

‰∏äÂõæÊòæÁ§∫‰∫Ü‰ªé2022Âπ¥Âà∞2024Âπ¥ÊúüÈó¥Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊºîÂèòË∂ãÂäø„ÄÇÂèØ‰ª•ÁúãÂá∫ÔºåMHA Ê≠£Âú®ÈÄêÊ≠•Ê∑òÊ±∞ÔºåÂπ∂Ë¢´ GQA ÊâÄÂèñ‰ª£„ÄÇ

### 3.1 Ê¶ÇÂøµ

GQAÔºàGrouped Query Attention/ÂàÜÁªÑÊü•ËØ¢Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºâÁî±ËÆ∫Êñá‚ÄúGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints‚ÄùÊèêÂá∫ÔºåÂÆÉÈÄöËøáÂàÜÁªÑÊü•ËØ¢ÁöÑÊñπÂºèÊù•ÊèêÈ´ò‰ø°ÊÅØÂ§ÑÁêÜÁöÑÊïàÁéáÂíåÊïàÊûú„ÄÇGQAÁöÑÊ†∏ÂøÉÊîπËøõÁÇπÂú®‰∫éÔºöËÆ© Â§ö‰∏™ Query ÂÖ±‰∫´Â∞ëÈáèÁöÑ Key Âíå ValueÔºåÂáèÂ∞ëËÆ°ÁÆóÂºÄÈîÄÔºåÂπ∂ÈÄöËøá ÂàÜÁªÑÊú∫Âà∂ÔºàGrouping MechanismÔºâ ËøõË°åÊõ¥È´òÊïàÁöÑËÆ°ÁÆó„ÄÇ

GQAÊòØMHAÂíåMQA ‰πãÈó¥ÁöÑÊ≥õÂåñÔºåÊàñËÄÖËØ¥ÊòØ‰ªã‰∫éMHAÂíåMQA‰πãÈó¥ÁöÑÊäò‰∏≠ÊñπÊ°à„ÄÇMHA Êúâ H ‰∏™ query„ÄÅkey Âíå value Â§¥„ÄÇMQA Âú®ÊâÄÊúâ query Â§¥‰∏≠ÂÖ±‰∫´Âçï‰∏™ key Âíå value Â§¥„ÄÇËÄåGQA‰∏çÂÜçËÆ©ÊâÄÊúâÊü•ËØ¢Â§¥ÂÖ±‰∫´Áõ∏ÂêåÁöÑÂîØ‰∏ÄKVÂ§¥ÔºåËÄåÊòØÂ∞ÜÊâÄÊúâÁöÑQÂ§¥ÂàÜÊàêgÁªÑÔºåÂêå‰∏ÄÁªÑÁöÑQÂ§¥ÂÖ±‰∫´‰∏Ä‰∏™KÂ§¥ÔºàKey HeadÔºâÂíå‰∏Ä‰∏™VÂ§¥ÔºàValue HeadÔºâ„ÄÇ

‰∏ãÂõæ‰∏≠4‰∏™QÂ§¥ÔºàQuery HeadsÔºâË¢´ÂàÜÊàê2ÁªÑÔºåÊØè‰∏™ÁªÑÂåÖÂê´2‰∏™QÂ§¥ÔºåÊØèÁªÑÂèàÂØπÂ∫î‰∏Ä‰∏™KÂ§¥Ôºå‰∏Ä‰∏™VÂ§¥„ÄÇÂõæ‰∏äÊ†áÂè∑1‰∏∫‰∏ÄÁªÑÔºåÊ†áÂè∑2‰∏∫Âè¶Â§ñ‰∏ÄÁªÑ„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203151535-621628829.jpg)

‰∏ãÂõæÊòØGQAÁöÑÂÖ¨ÂºèÂíåÊµÅÁ®ã„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203200605-1893859507.jpg)

ËãèÁ•ûÂàôÊåáÂá∫ÔºåGQAÂÖ∂ÂÆûÊòØ‰∏Ä‰∏™\\(x\_i\\)ÁöÑ‰ΩéÁß©ÊäïÂΩ±„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203210935-534637606.jpg)

### 3.2 Êû∂ÊûÑÊØîÂØπ

GQAÂ∑ßÂ¶ôÂú∞ÁªìÂêà‰∫ÜMHAÂíåMQAÁöÑÂÖÉÁ¥†ÔºåÂàõÈÄ†‰∫Ü‰∏ÄÁßçÊõ¥ÊúâÊïàÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂„ÄÇGQAÊòØÂú®MHAÂíåMQA‰πãÈó¥ËøõË°åÊèíÂÄºÔºåÂ∞ÜKVÂ§¥ÁöÑÊï∞Èáè‰ªé\\(n\\\_heads\\)ÂáèÂ∞ëÂà∞\\(1<g<n\\\_heads\\)ÔºåËÄå‰∏çÊòØÂ∞ÜÂ§¥Êï∞‰ªé\\(n\\\_heads\\)ÂáèÂ∞ëÂà∞1‰∏™KVÂ§¥„ÄÇËøô‰∏™Êñ∞ÂèÇÊï∞gÂèØ‰ª•Ëøô‰πàË°®ËææÔºö

\\\[g = \\frac{Ê≥®ÊÑèÂäõÂ§¥Êï∞}{KVÂ§¥Êï∞} \\\]

ÂºïÂÖ•Ëøô‰∏™ÂèÇÊï∞g‰πãÂêéÔºåGQAÂ∞±ÊûÑÊàê‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄËßÜËßí„ÄÇÂú®Ëøô‰∏™ËßÜËßí‰∏ãÔºåMHAÂíåMQAÈÉΩÊòØGQAÁöÑÁâπÊÆäÊÉÖÂÜµÔºàÂàÜÂà´ÂØπÂ∫î‰∫ég=1Âíå g=\\(n\\\_heads\\)Ôºâ„ÄÇ

*   g = 1ÔºöÁõ∏ÂΩì‰∫éMQAÔºåÂç≥Âú®ÊâÄÊúâ N ‰∏™Â§¥‰∏≠‰ΩøÁî®ÂÖ±‰∫´ÁöÑÈîÆÂíåÂÄºÊäïÂΩ±„ÄÇ
*   g = Ê≥®ÊÑèÂäõÂ§¥Êï∞ÔºöÁõ∏ÂΩì‰∫éMHA„ÄÇ

GQAËÉΩÊõ¥È°∫ÁïÖÂú∞Âú®Ê®°ÂûãÂáÜÁ°ÆÊÄß/KVÁºìÂ≠òÂ§ßÂ∞èÔºà‰∏éÊó∂Âª∂ÂíåÂêûÂêêÈáèÊúâÂÖ≥ÔºâÔºåÂíåMHA‰ª•ÂèäMQAËøô‰∏§‰∏™ÊûÅÁ´ØÁî®‰æãÈó¥ËøõË°åÊùÉË°°„ÄÇÊàñËÄÖËØ¥ÔºåGQAÊØè‰∏™ÁªÑÂÜÖÊòØ‰∏Ä‰∏™Â∞èÂûãÁöÑMQAÔºåËÄåÁªÑÈó¥ÊòØ‰º†ÁªüÁöÑMHA„ÄÇ

Â§ßÂûãÊ®°ÂûãÁöÑMHA‰ºöÂ∞ÜÂçï‰∏™ÈîÆÂíåÂÄºÂ§¥Â§çÂà∂Âà∞Ê®°ÂûãÂàÜÂå∫ÁöÑÊï∞ÈáèÔºåMQA‰ª£Ë°®‰∫ÜÂÜÖÂ≠òÂ∏¶ÂÆΩÂíåÂÆπÈáèÁöÑÊõ¥Â§ßÂπÖÂ∫¶ÁöÑÂâäÂáèÔºåËÄåGQA ‰ΩøÊàë‰ª¨ËÉΩÂ§üÈöèÁùÄÊ®°ÂûãÂ§ßÂ∞èÁöÑÂ¢ûÂä†‰øùÊåÅÂ∏¶ÂÆΩÂíåÂÆπÈáèÁöÑÁõ∏ÂêåÊØî‰æã‰∏ãÈôçÔºåÂèØ‰ª•‰∏∫ËæÉÂ§ßÁöÑÊ®°ÂûãÊèê‰æõÁâπÂà´Â•ΩÁöÑÊùÉË°°„ÄÇGQA Ê∂àÈô§‰∫ÜËøôÁßçÂàÜÁâáÂ∏¶Êù•ÁöÑÊµ™Ë¥π„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨È¢ÑËÆ° GQA Â∞Ü‰∏∫ËæÉÂ§ßÁöÑÊ®°ÂûãÊèê‰æõÁâπÂà´Â•ΩÁöÑÊùÉË°°„ÄÇ

‰∏ãÂõæÂàôÁªôÂá∫‰∫Ü‰∏âËÄÖÊû∂ÊûÑ‰∏äÁöÑÂå∫Âà´„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203221768-1617176553.jpg)

### 3.3 ÂÆûÁé∞

Âú®ÁõÆÂâçÂ§ßÈÉ®ÂàÜ‰∏ªÊµÅËÆ≠Êé®Ê°ÜÊû∂ÊàñÁÆóÊ≥ïÔºåÈÉΩÂ∑≤ÁªèÊîØÊåÅMQA/GQAÔºåÊØîÂ¶ÇFlashAttention‰∏≠Ôºå‰πüÊîØÊåÅMQAÂíåGQA„ÄÇÂØπ‰∫éMQAÂíåGQAÁöÑÊÉÖÂΩ¢ÔºåFlashAttentionÈááÁî®IndexingÁöÑÊñπÂºèÔºåËÄå‰∏çÊòØÁõ¥Êé•Â§çÂà∂Â§ö‰ªΩKV HeadÁöÑÂÜÖÂÆπÂà∞ÊòæÂ≠òÁÑ∂ÂêéÂÜçËøõË°åËÆ°ÁÆó„ÄÇIndexingÔºåÂç≥ÈÄöËøá‰º†ÂÖ•KV/KV HeadÁ¥¢ÂºïÂà∞Kernel‰∏≠ÔºåÁÑ∂ÂêéËÆ°ÁÆóÂÜÖÂ≠òÂú∞ÂùÄÔºåÁõ¥Êé•‰ªéÂÜÖÂ≠ò‰∏≠ËØªÂèñKV„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203230087-2060532392.jpg)

È°∫Â∏¶‰∏ÄÊèêÔºåGQA ‰∏çÂ∫îÁî®‰∫éÁºñÁ†ÅÂô®Ëá™Ê≥®ÊÑèÂäõÂ±ÇÔºåÁºñÁ†ÅÂô®Ë°®Á§∫ÊòØÂπ∂Ë°åËÆ°ÁÆóÁöÑÔºåÂõ†Ê≠§ÂÜÖÂ≠òÂ∏¶ÂÆΩÈÄöÂ∏∏‰∏çÊòØ‰∏ªË¶ÅÁì∂È¢à„ÄÇ

Êàë‰ª¨‰ΩøÁî®llama3ÁöÑ‰ª£Á†ÅÊù•ËøõË°åÂàÜÊûê„ÄÇÈ¶ñÂÖàÁªôÂá∫Âà©‰∫éÂ≠¶‰π†ÁöÑÁ≤æÁÆÄÁâàÔºåÁÑ∂ÂêéÁªôÂá∫ÂÆåÊï¥Áâà„ÄÇ

#### 3.3.1 Á≤æÁÆÄÁâà

‰∏∫‰∫ÜÊõ¥Â•ΩÁöÑÂàÜÊûêÔºåÊàë‰ª¨ÁªôÂá∫Á≤æÁÆÄÁâà‰ª£Á†ÅÂ¶Ç‰∏ã„ÄÇ

Êú¨Êù• MHA ‰∏≠ Query, Key, Value ÁöÑÁü©ÈòµÁöÑÂ§ßÂ∞è‰∏∫ (batch\_size, n\_head, seq\_length, hidden\_size)„ÄÇËÄå GQA ‰∏≠ Query ÁöÑÂ§ßÂ∞è‰øùÊåÅ‰∏çÂèòÔºåKey, Value ÁöÑÁü©ÈòµÁöÑÂ§ßÂ∞èÂèò‰∏∫ (batch\_size, n\_head / group\_size, seq\_length, hidden\_size)„ÄÇÂç≥ÔºåÂú®GQA‰∏≠ÔºåkeyÂíåvalueÈÉΩË¶ÅÊØîqueryÂ∞ègroupÂÄç„ÄÇ‰∏∫‰∫ÜÂú®ÂêéÁª≠ÂÅöÁü©Èòµ‰πòÊ≥ïÔºå‰∏ÄËà¨Êúâ‰∏§ÁßçÂÅöÊ≥ïÔºö

*   Âà©‰∫éÂπøÊí≠Êú∫Âà∂ÊääQKVÁöÑÂΩ¢Áä∂ËøõË°åË∞ÉÊï¥ÔºåÂç≥Query : (batch\_size, n\_head / group\_size, group\_size, seq\_length, hidden\_size)ÔºåKey : (batch\_size, n\_head / group\_size, 1, seq\_length, hidden\_size)ÔºåValue : (batch\_size, n\_head / group\_size, 1, seq\_length, hidden\_size)„ÄÇ‰ΩÜÊòØËøôÊ†∑ÈúÄË¶ÅÂÅöÂπøÊí≠ÂíåÊúÄÁªàÂêàÂπ∂ÁöÑÂ§ÑÁêÜÔºåË¶ÅÂØπ MHA ÁöÑ‰ª£Á†ÅËøõË°åÂ§öÂ§Ñ‰øÆÊîπ„ÄÇ
    
*   ÊääGQAÊãìÂ±ïÂà∞MHAÂÜçËøõË°åËÆ°ÁÆóÔºåÂç≥ÂÖàÊää`key`Âíå`value`ÁöÑ`head`Âà©Áî®expandÊâ©Â±ïÂº†ÈáèÂà∞Âíå`query`Áõ∏ÂêåÁöÑÁª¥Â∫¶ÔºåÁÑ∂ÂêéËøõË°åËÆ°ÁÆó„ÄÇ
    

    class Attention(nn.Module):
        def __init__(self, args: ModelArgs):
            self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
            model_parallel_size = fs_init.get_model_parallel_world_size()
            self.n_local_heads = args.n_heads // model_parallel_size
            self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
            self.n_rep = self.n_local_heads // self.n_local_kv_heads # ËÆæÂÆöÁªÑÊï∞ÁõÆ
            self.head_dim = args.dim // args.n_heads
    
            # Áî®self.n_kv_heads * self.head_dimÂàùÂßãÂåñÔºåÂΩìn_kv_headsÂ∞è‰∫én_headsÊó∂ÔºåÂèÇÊï∞ÈáèÂèòÂ∞ë
            self.wq = ColumnParallelLinear(args.dim, args.n_heads * self.head_dim,)
            self.wk = ColumnParallelLinear(args.dim, self.n_kv_heads * self.head_dim,)
            self.wv = ColumnParallelLinear(args.dim, self.n_kv_heads * self.head_dim,)
            self.wo = RowParallelLinear(args.n_heads * self.head_dim, args.dim,)
    
            self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len,
                    self.n_local_kv_heads, self.head_dim,)).cuda()
            self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len,
                    self.n_local_kv_heads, self.head_dim,)).cuda()
    
        def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor,
            mask: Optional[torch.Tensor],
        ):
            bsz, seqlen, _ = x.shape
            xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
            xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
            xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
            xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
            xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
    
            self.cache_k = self.cache_k.to(xq)
            self.cache_v = self.cache_v.to(xq)
            self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
            self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv
       
            keys = self.cache_k[:bsz, : start_pos + seqlen]
            values = self.cache_v[:bsz, : start_pos + seqlen]
    
            '''
            self.n_rep = q_heads // kv_heads
            queryÂ§¥Êï∞Â§ß‰∫éKVÁöÑÂ§¥Êï∞Ôºå‰∏ÄÂØπKVÂØπÂ∫îÂ§ö‰∏™queryÔºåÈúÄË¶ÅÊääÊØè‰∏™KVÂ§çÂà∂n_rep‰ªΩÔºåËøôÊ†∑Á¨¨2‰∏™Áª¥Â∫¶Â∞±Âíåq‰∏ÄÊ†∑‰∫Ü
            Âç≥Ôºånum_key_value_headsÂ∞±ÊòØq_heads // kv_heads
            repeat_kvÊñπÊ≥ïÂ∞Ühidden states‰ªé(batch, num_key_value_heads, seqlen, head_dim) ÂèòÊàê (batch, num_attention_heads, seqlen, head_dim)ÔºåÁõ∏ÂΩì‰∫éÊòØÂ§çÂà∂‰∫Üself.num_key_value_groups‰ªΩ
            '''            
            # repeat k/v heads if n_kv_heads < n_heads
            keys = repeat_kv(keys, self.n_rep
            )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
            values = repeat_kv(values, self.n_rep
            )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
    
            xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
            keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
            values = values.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
            scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
            if mask is not None:
                scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)     
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)
            output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
            return self.wo(output)
    

repeat\_kv()ÂáΩÊï∞‰ª£Á†ÅÂ¶Ç‰∏ã„ÄÇ‰∏∫‰ªÄ‰πàË¶ÅÁî®expand‰πãÂêéÂÜçreshapeËÄå‰∏çËÉΩÁõ¥Êé•Áî®tensorËá™Â∏¶ÁöÑrepeatÔºüÂõ†‰∏∫‰ΩøÁî®expand()ÂáΩÊï∞ÂèØ‰ª•Âú®ËøêÁÆóÁöÑÊó∂ÂÄôËäÇÁúÅÂæàÂ§öÊòæÂ≠ò„ÄÇ

*   `expand` ÊñπÊ≥ïÁî®‰∫éÂØπÂº†ÈáèËøõË°åÊâ©Â±ïÔºå‰ΩÜ‰∏çÂÆûÈôÖÂàÜÈÖçÊñ∞ÁöÑÂÜÖÂ≠ò„ÄÇÂÆÉËøîÂõûÁöÑÂº†Èáè‰∏éÂéüÂßãÂº†ÈáèÂÖ±‰∫´Áõ∏ÂêåÁöÑÊï∞ÊçÆ
*   `repeat` ÊñπÊ≥ïÈÄöËøáÂÆûÈôÖÂ§çÂà∂Êï∞ÊçÆÊù•Êâ©Â±ïÂº†Èáè„ÄÇÂÆÉËøîÂõûÁöÑÊñ∞Âº†Èáè‰∏ç‰∏éÂéüÂßãÂº†ÈáèÂÖ±‰∫´Êï∞ÊçÆÔºåÊâ©Â±ïÂêéÁöÑÂº†ÈáèÂç†Áî®‰∫ÜÊõ¥Â§öÁöÑÂÜÖÂ≠ò„ÄÇ

    # ÂÆö‰πâËæìÂÖ•xÔºå n_repÊòØÈúÄË¶ÅÈáçÂ§çÁöÑÊ¨°Êï∞ÔºåÂú®ËøôÈáå‰∏ÄËà¨ÊòØÁªÑÊï∞
    def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
        """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
        bs, slen, n_kv_heads, head_dim = x.shape
        if n_rep == 1:
            return x
        return (
            # Á¨¨4Áª¥ËøõË°åÊâ©Áª¥ÔºåÊâ©Â±ïÊàê5Áª¥
            x[:, :, :, None, :] 
             # first we expand x to (bs, seq_len, head, group, head_dim)ÔºåÂç≥Á¨¨4Áª¥‰ªé1Êâ©Â±ï‰∏∫n_rep
            .expand(bs, slen, n_kv_heads, n_rep, head_dim)  # ËøõË°åÂπøÊí≠ÔºåkÔºåvÂêëÈáèÂÖ±‰∫´
             # reshape make head -> head * groupÔºåÁº©Êàê4Áª¥ÔºåÂç≥ÊääÁ¨¨3Áª¥‰ªén_kv_headsÊâ©Â±ïn_rep‰ªΩ
             # ËøôÊ†∑Á¨¨3‰∏™Áª¥Â∫¶Â∞±Âíåq‰∏ÄÊ†∑‰∫Ü
            .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
        )
    

#### 3.3.2 ÂÆåÊï¥Áâà

ÂÆåÊï¥Áâà‰ª£Á†ÅÂ¶Ç‰∏ã„ÄÇ

    class Attention(nn.Module):
        def __init__(self, args: ModelArgs):
            super().__init__()
            self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
            model_parallel_size = fs_init.get_model_parallel_world_size()
            self.n_local_heads = args.n_heads // model_parallel_size
            self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
            self.n_rep = self.n_local_heads // self.n_local_kv_heads
            self.head_dim = args.dim // args.n_heads
    
            self.wq = ColumnParallelLinear(
                args.dim,
                args.n_heads * self.head_dim,
                bias=False,
                gather_output=False,
                init_method=lambda x: x,
            )
            self.wk = ColumnParallelLinear(
                args.dim,
                self.n_kv_heads * self.head_dim,
                bias=False,
                gather_output=False,
                init_method=lambda x: x,
            )
            self.wv = ColumnParallelLinear(
                args.dim,
                self.n_kv_heads * self.head_dim,
                bias=False,
                gather_output=False,
                init_method=lambda x: x,
            )
            self.wo = RowParallelLinear(
                args.n_heads * self.head_dim,
                args.dim,
                bias=False,
                input_is_parallel=True,
                init_method=lambda x: x,
            )
    
            self.cache_k = torch.zeros(
                (
                    args.max_batch_size,
                    args.max_seq_len,
                    self.n_local_kv_heads,
                    self.head_dim,
                )
            ).cuda()
            self.cache_v = torch.zeros(
                (
                    args.max_batch_size,
                    args.max_seq_len,
                    self.n_local_kv_heads,
                    self.head_dim,
                )
            ).cuda()
    
        def forward(
            self,
            x: torch.Tensor,
            start_pos: int,
            freqs_cis: torch.Tensor,
            mask: Optional[torch.Tensor],
        ):
            bsz, seqlen, _ = x.shape
            xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
    
            xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
            xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
            xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
    
            xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
    
            self.cache_k = self.cache_k.to(xq)
            self.cache_v = self.cache_v.to(xq)
    
            self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
            self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv
    
            keys = self.cache_k[:bsz, : start_pos + seqlen]
            values = self.cache_v[:bsz, : start_pos + seqlen]
    
            # repeat k/v heads if n_kv_heads < n_heads
            keys = repeat_kv(
                keys, self.n_rep
            )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
            values = repeat_kv(
                values, self.n_rep
            )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
    
            xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
            keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
            values = values.transpose(
                1, 2
            )  # (bs, n_local_heads, cache_len + seqlen, head_dim)
            scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
            if mask is not None:
                scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)
            output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
            return self.wo(output)
    

Âè¶Â§ñÔºåÂØπ‰∫éMQAÂíåGQAÁöÑËß£Á†ÅÈò∂ÊÆµÔºå‰∏ÄÁßçÂ∏∏Áî®ÁöÑ‰ºòÂåñÊäÄÂ∑ßÊòØÊääÂÖ±Áî®‰∏Ä‰∏™KVÂ§¥ÁöÑÊâÄÊúâQOÂ§¥Ôºå‰∏équeryÁöÑË°åÊï∞ËûçÂêàÔºàÂõ†‰∏∫‰ªñ‰ª¨ÈúÄË¶ÅË∑üÁõ∏ÂêåÁöÑKV-CacheÂÅöAttentionËÆ°ÁÆóÔºâ„ÄÇËøôÊ†∑ÁöÑÊïàÊûúÊòØÂ¢ûÂä†‰∫ÜÊúâÊïàÁöÑË°åÊï∞ÔºåÂ¢ûÂä†‰∫ÜÁÆóÂ≠êÂØÜÂ∫¶ÔºåËá™ÂõûÂΩíËß£Á†ÅÈò∂ÊÆµËôΩÁÑ∂ËØ¥Êü•ËØ¢ÁöÑÈïøÂ∫¶ÊòØ1Ôºå‰ΩÜÊòØÁªèËøáHead GroupËûçÂêà‰πãÂêéÔºåÊúâÊïàË°åÊï∞Â¢ûÂ§ßÂà∞ \\(H\_{QO}/H\_{KV}\\)„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203246431-805086330.jpg)

### 3.4 ÊïàÊûú

#### 3.4.1 ÂÜÖÂ≠ò

GQAÂú®Êé®ÁêÜÈò∂ÊÆµÂèØ‰ª•ÊòæËëóÈôç‰Ωé KV Cache ÁöÑÂ§ßÂ∞èÔºå‰∏∫Êõ¥Â§ßÁöÑ Batch Size Êèê‰æõ‰∫ÜÁ©∫Èó¥ÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÊèêÂçáÂêûÂêê„ÄÇ

Âú®MHA‰∏ãÔºåÂØπ‰∫éÊâÄÊúâËæìÂÖ•ÊâπÊ¨°ÂíåÂ∫èÂàó‰∏≠ÁöÑÊØè‰∏™tokenÔºåKVÁºìÂ≠òÁöÑÊÄªÂ§ßÂ∞èÂèØ‰ª•Áî®‰ª•‰∏ãÂÖ¨ÂºèË°®Á§∫Ôºö

\\\[2 \\times B \\times L \\times H \\times D \\times N \\\]

*   B‰ª£Ë°®batch sizeÔºå
*   L‰ª£Ë°®ÊÄªÂ∫èÂàóÈïøÂ∫¶Ôºåsequence lengthÔºàËæìÂÖ•Â∫èÂàó+ËæìÂá∫Â∫èÂàóÔºåÊàñËÄÖËØ¥ÊòØÊèêÁ§∫ + ÂÆåÊàêÈÉ®ÂàÜÔºâÔºå
*   H‰ª£Ë°®number of headÔºå
*   D‰ª£Ë°®size of headÔºåÊØè‰∏™headÁöÑÁª¥Â∫¶„ÄÇ
*   N‰ª£Ë°®Â±ÇÊï∞

Âú®MQA‰∏ãÔºåÊØè‰∏™tokenÁöÑÂØπÂ∫î‰∏∫Ôºö

\\\[2 \\times B \\times L\\times D \\times N \\\]

Âú®GQA‰∏ãÔºåÊØè‰∏™tokenÁöÑÂØπÂ∫î‰∏∫Ôºö

\\\[2 \\times B \\times L\\times G \\times D\\times N \\\]

ÂÖ∑‰ΩìÊØîÂØπ‰πüÂèØ‰ª•ÂèÇËÄÉ‰∏ãÂõæÔºåÂÖ∂‰∏≠ g ÊòØKVÂ§¥ÁöÑÁªÑÊï∞Ôºà\\(ùëõ\_‚Ñé/ùëî\\)‰∏™Head ÂÖ±‰∫´‰∏Ä‰∏™KVÔºâÔºåh ÊòØÊü•ËØ¢ÁöÑÂ§¥Êï∞ Ôºå\\(d\_k\\)ÊòØÂ§¥Áª¥Â∫¶Ôºål ÊòØÂ±ÇÊï∞Ôºås ÊòØÂ∫èÂàóÈïøÂ∫¶Ôºåb ÊòØbatch size„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203256198-1237072538.jpg)

GQAÂíåMQAÂú®GPU ‰∏äÁöÑÂÆûÁé∞Â∏¶Êù•ÁöÑÊî∂ÁõäÊù•‰∏ªË¶ÅËá™‰∫éKV cache ÁöÑÂáèÂ∞ëÔºå‰ªéËÄåËÉΩÊîæ‰∏ãÊõ¥Â§öÁöÑtoken„ÄÇ‰ΩÜÊòØÔºåGQAÂíåMQAÁöÑÊÄßËÉΩÂÆπÊòìÂèóÂà∞Âπ∂Ë°åÁ≠ñÁï•ÁöÑÂΩ±Âìç„ÄÇÂ¶ÇÊûúGQA kernelÂú®Q headÁª¥Â∫¶ÂÅöÂπ∂Ë°åÔºà‰∏Ä‰∏™Q headÊòØ‰∏Ä‰∏™blockÔºâÔºåÂàô‰ºöÂØºËá¥ÂÖ±‰∫´‰∏Ä‰∏™KV head ÁöÑblock Ë¢´Ë∞ÉÂ∫¶Âú®‰∏çÂêåÁöÑSM‰∏äÔºåÊØè‰∏™SM ÈÉΩ‰ºöÂØπÂêå‰∏Ä‰ªΩKV head ÂÅöÈáçÂ§çÂä†ËΩΩ„ÄÇÂàôÂÜÖÂ≠òÂáèÂ∞ëÁöÑÊî∂Áõä‰ºöÂ§ßÂ§ßÈôç‰Ωé„ÄÇÂè¶Â§ñÔºåÂä†ËΩΩ KV ÊòØMHA Âíå GQA ÁöÑÁì∂È¢à„ÄÇÂõ†Ê≠§ÈúÄË¶ÅÂáèÂ∞ëQ headÁöÑÂπ∂Ë°åÂ∫¶„ÄÇ

#### 3.4.2 ÈÄüÂ∫¶

GQAÂπ∂Ê≤°ÊúâÈôç‰ΩéAttentionÁöÑËÆ°ÁÆóÈáèÔºàFLOPsÔºâÔºåÂõ†‰∏∫Key„ÄÅValueÊò†Â∞ÑÁü©Èòµ‰ºö‰ª•ÂπøÊí≠ÂèòÈáèÁöÑÂΩ¢ÂºèÊãìÂ±ïÂà∞ÂíåMHAÂíå‰∏ÄÊ†∑ÔºåÂõ†Ê≠§ËÆ°ÁÆóÈáè‰∏çÂèòÔºåÂè™ÊòØKey„ÄÅValueÂèÇÊï∞ÂÖ±‰∫´„ÄÇ‰ΩÜÊòØÔºåÂõ†‰∏∫GQA Â∞ÜÊü•ËØ¢Áü©Èòµ Q ÂàÜÊàêÂ§ö‰∏™ÁªÑÔºåÊØè‰∏™ÁªÑÂàÜÂà´ËÆ°ÁÆóÊ≥®ÊÑèÂäõÂàÜÊï∞ÂíåÂä†ÊùÉÊ±ÇÂíå„ÄÇËøôÊ†∑‰∏ÄÊù•ÔºåÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥Âè™ÈúÄË¶ÅËÆ°ÁÆó‰∏ÄÈÉ®ÂàÜÊü•ËØ¢ÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∞Ôºå‰ªéËÄåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂„ÄÇÊâÄ‰ª•ÔºåËôΩÁÑ∂GQA ÁöÑ QKV ËÆ°ÁÆóÈáèÊ≤°ÊúâÂáèÂ∞ëÔºå‰ΩÜÊòØÈÄüÂ∫¶ÂæóÂà∞‰∫ÜÂæàÂ§ßÊèêÈ´òÔºåÈÄüÂ∫¶ÊèêÈ´òÁöÑÂéüÂõ†ÂíåMQAÁõ∏Âêå„ÄÇ

#### 3.4.3 Ë°®ÂæÅËÉΩÂäõ

GQAÊó¢‰øùÁïô‰∫ÜÂ§öÂ§¥Ê≥®ÊÑèÂäõÁöÑ‰∏ÄÂÆöË°®ËææËÉΩÂäõÔºåÂèàÈÄöËøáÂáèÂ∞ëÂÜÖÂ≠òËÆøÈóÆÂéãÂäõÊù•Âä†ÈÄüÊé®ÁêÜÈÄüÂ∫¶„ÄÇ

ËÆ∫Êñá‚ÄùGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints‚ÄúÁ†îÁ©∂‰∫ÜÊ®°ÂûãÁöÑÁ≤æÂ∫¶ÂíåÊé®ÁêÜÊïàÁéá„ÄÇËÆ∫Êñá‰ΩúËÄÖÈááÁî®T5Ê®°Âûã‰Ωú‰∏∫Á†îÁ©∂ÂØπË±°ÔºåÊ®°ÂûãÁâàÊú¨ÈááÁî®T5-LargeÂíåT5-XXL„ÄÇ‰∏ãÂõæ‰∏≠ÔºåÊ®™ËΩ¥‰ª£Ë°®Âπ≥ÂùáÊØèÊù°Ê†∑Êú¨ÁöÑÊé®ÁêÜËÄóÊó∂ÔºåË∂äÂ§ß‰ª£Ë°®Âª∂ËøüË∂äÂ§ßÔºåÁ∫µËΩ¥‰ª£Ë°®Âú®‰ºóÂ§öÊï∞ÊçÆÈõÜ‰∏äÁöÑËØÑ‰ª∑ÂæóÂàÜÔºåË∂äÂ§ß‰ª£Ë°®ÂæóÂàÜË∂äÈ´ò„ÄÇ

‰∏ãÂõæË°®ÊòéÔºåMQAÁï•ÂæÆÊçüÂ§±‰∫ÜÊ®°ÂûãÁ≤æÂ∫¶Ôºå‰ΩÜÊòØÁ°ÆÂÆûËÉΩÂ§üÂ§ßÂπÖÈôç‰ΩéÊé®ÁêÜÂºÄÈîÄÔºåËÄåÂ¶ÇÊûúÈÄâÊã©‰∫ÜÂêàÈÄÇÁöÑÂàÜÁªÑÊï∞ÔºåGQAËÉΩÂ§ü‰∏§ËÄÖÁöÜÂæó„ÄÇGQAÁöÑË°®ÂæÅËÉΩÂäõÊòæËëóÈ´ò‰∫éMQAÔºåÂá†‰πéË∑üMHA‰∏ÄËá¥ÔºàGQAËøòÊòØÊúâÂèØËÉΩÂØºËá¥Á≤æÂ∫¶ÁöÑÊçüÂ§±ÔºâÔºåËÄå‰∏îÊé®ÁêÜÈÄüÂ∫¶‰∏äGQAË∑üMQAÁöÑÂå∫Âà´‰∏çÂ§ßÔºåÊØîËµ∑MHA‰æùÊóßÊúâÊòæËëóÊèêÂçá„ÄÇÂÖ∂‰∏≠ÔºåGQAÁöÑÂàÜÁªÑÊï∞ÊòØ‰∏Ä‰∏™Ë∂ÖÂèÇÊï∞ÔºåÁªÑÊï∞Ë∂äÂ§ßË∂äÊé•ËøëMHAÔºåÊé®ÁêÜÂª∂ËøüË∂äÂ§ßÔºåÂêåÊó∂Ê®°ÂûãÁ≤æÂ∫¶‰πüË∂äÈ´ò„ÄÇÂè¶Â§ñÔºå‰πüÂèØ‰ª•Â¢ûÂä†Ê®°ÂûãÊ∑±Â∫¶Êù•ÁºìËß£Ê®°ÂûãÊïàÊûúÁöÑ‰∏ãÈôç„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203304079-756174985.jpg)

### 3.5 ËΩ¨Êç¢

ËôΩÁÑ∂ÊúÄÊñ∞ÁöÑÊ®°ÂûãÂü∫Êú¨ÈÉΩÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÈªòËÆ§ÈááÁî® GQAÔºåÊàë‰ª¨‰πüÂèØ‰ª•ÊÄùËÄÉ‰∏ãÔºåÂ¶Ç‰ΩïÂ∞ÜÂ∑≤ÁªèËÆ≠ÁªÉÂ•ΩÁöÑMHAÁªìÊûÑÁöÑÊ®°ÂûãËΩ¨Êç¢ÊàêMQAÊàñËÄÖGQAÔºü

#### 3.5.1 Âπ≥ÂùáÊ±†Âåñ

Â¶ÇÊûúÊòØ‰ªéÂ∑≤ÊúâÁöÑ multi-head model ÂºÄÂßãÁªßÁª≠ËÆ≠ÁªÉ multi-query model (Uptraining)ÔºåÊàë‰ª¨ÂèØ‰ª•ÂØπMHAÁöÑÂ§¥ËøõË°åÂàÜÁªÑÔºåÈÄöËøáÂØπËØ•ÁªÑ‰∏≠ÊâÄÊúâÂéüÂßãÂ§¥ËøõË°åÂπ≥ÂùáÊ±†ÂåñÔºàmean poolÔºâÊù•ÊûÑÂª∫ÊØè‰∏™ÁªÑÁöÑÈîÆÂíåÂÄºÂ§¥ÔºåÁÑ∂ÂêéÁªßÁª≠ËøõË°åÈ¢ÑËÆ≠ÁªÉÂç≥ÂèØ„ÄÇÂÆûÈ™åËØÅÊòémean poolÁöÑÊò†Â∞ÑÊïàÊûúÂ•Ω‰∫éÈÄâÂàôÁ¨¨‰∏Ä‰∏™headÊàñËÄÖ‰ªªÊÑèÂàùÂßãÂåñ„ÄÇ‰∫∫‰ª¨ÊääËøô‰∏™ËÆ≠ÁªÉËøáÁ®ãÂè´ÂÅöuptraining„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203312168-1095087345.jpg)

ÂÖ∑‰ΩìÂèÇËÄÉ‰ª£Á†ÅÂ¶Ç‰∏ã„ÄÇ

    import torch.nn as nn
    
    n_heads=4
    n_kv_heads=2
    hidden_size=3
    group = n_heads // n_kv_heads
    k_proj = nn.Linear(hidden_size, n_heads) 
    
    # mean poolÊìç‰Ωú
    k_proj_4d = k_proj.weight.data.unsqueeze(dim=0).unsqueeze(dim=0)
    pool=nn.AvgPool2d(kernel_size=(group,1))
    pool_out = pool(k_proj_4d).squeeze(dim=0).squeeze(dim=0)
    
    k_proj_gaq = nn.Linear(hidden_size, n_kv_heads)
    k_proj_gaq.weight.data = pool_out
    

#### 3.5.2 Âü∫‰∫éÊé©Á†Å

ËÆ∫Êñá‚ÄùAlign Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA‚ÄúÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ΩéÊàêÊú¨ÊñπÊ≥ïÔºåÂèØÂ∞Ü MHA Ê®°ÂûãÊåâ‰ªªÊÑè KV Head ÂéãÁº©ÊØî‰øÆÂâ™‰∏∫ GQA Ê®°Âûã„ÄÇËØ•ÊñπÊ≥ïÂü∫‰∫é \\(L\_0\\) Êé©Á†ÅÈÄêÊ≠•ÂâîÈô§ÂÜó‰ΩôÂèÇÊï∞„ÄÇÊ≠§Â§ñÔºåÂú®‰∏çÊîπÂèòÊ®°ÂûãÁöÑÂâçÊèê‰∏ãÔºåÂØπÊ≥®ÊÑèÂäõÂ§¥ÊñΩÂä†Ê≠£‰∫§ÂèòÊç¢Ôºå‰ª•Âú®‰øÆÂâ™ËÆ≠ÁªÉÂâçÊèêÂçá Attention Head Èó¥ÁöÑÁõ∏‰ººÂ∫¶Ôºå‰ªéËÄåËøõ‰∏ÄÊ≠•‰ºòÂåñÊ®°ÂûãÊÄßËÉΩ„ÄÇ

ÂÖ∑‰ΩìÊñπÊ°àÂàÜ‰∏∫Â¶Ç‰∏ãÂá†Ê≠•ÔºöÁΩëÁªúËΩ¨Êç¢ÔºõËøõË°åÂàÜÁªÑÔºõÂâ™ÊûùËÆ≠ÁªÉ„ÄÇ

##### ÁΩëÁªúËΩ¨Êç¢

Ëøô‰∏ÄÊ≠•ÊòØÂú®Ââ™ÊûùËÆ≠ÁªÉ‰πãÂâçÔºåÂØπÊ®°ÂûãËøõË°åËΩ¨Êç¢„ÄÇÂÖ∑‰ΩìÁöÑËøáÁ®ãÂ§ßÊ¶Ç‰∏∫Ôºö

*   ‰ΩøÁî®ÈÉ®ÂàÜ C4 ÁöÑËÆ≠ÁªÉÈõÜÊù•Êî∂ÈõÜÁõ∏Â∫îÁöÑ KV CacheÔºåËøôÊ†∑ÊâçËÉΩÂØπKV CacheËøõË°åÊõ¥ÊúâÊïàÁöÑÂàÜÊûê„ÄÇ
*   Âü∫‰∫é‰ΩôÂº¶Áõ∏‰ººÊÄßÊàñËÄÖÊ¨ßÊ∞èË∑ùÁ¶ªÔºåËÆ°ÁÆóÊúÄ‰ºòÁöÑÊ≠£‰∫§Áü©Èòµ„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203324257-2099402571.jpg)

*   Â∞ÜËÆ°ÁÆóÂæóÂà∞ÁöÑÊ≠£‰∫§Áü©ÈòµËûçÂêàÂà∞ÂØπÂ∫îÁöÑ Q„ÄÅK„ÄÅV ÊäïÂΩ±Áü©Èòµ‰∏≠Ôºå‰øùËØÅËÆ°ÁÆó‰∏çÂèòÊÄß„ÄÇÂõ†‰∏∫RoPEÁöÑÂéüÂõ†ÔºåÊâÄ‰ª•ÂØπ‰∫é Q Âíå K ÁöÑÊäïÂΩ±Áü©ÈòµÔºåÂàÜÂà´Âú®Â≠êÁ©∫Èó¥Â∫îÁî®Ê≠£‰∫§ÂèòÊç¢„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203333938-279084538.jpg)

ÈÄöËøáÊ≠£‰∫§ÂèòÊç¢ÔºåÂèØ‰ª•‰ΩøÂæóÂêå‰∏ÄÁªÑÂÜÖ‰∏çÂêå Attention Head Âú®ÁâπÂæÅÁ©∫Èó¥‰∏≠Êõ¥Âä†Êé•ËøëÔºå‰ªéËÄåÂú®ÂêéÁª≠ÁöÑÂâ™ÊûùËÆ≠ÁªÉËøáÁ®ã‰∏≠Êõ¥ÂÆπÊòìÊâæÂà∞ÂêàÈÄÇÁöÑÂèÇÊï∞ÂÖ±‰∫´ÊñπÂºèÔºåÊèêÈ´òÊ®°ÂûãÁöÑÂéãÁº©ÊïàÊûúÂíåÊÄßËÉΩ„ÄÇ

##### ÊâæÂà∞Êõ¥Â•ΩÁöÑÂàÜÁªÑÊñπÊ≥ï

Âú®Ëé∑Âèñ‰∫ÜÊØèÂØπ Attention Head ‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶ËØÑÂàÜÂêéÔºåÂèØ‰æùÊçÆËøô‰∫õËØÑÂàÜÂØπ Attention Head ËøõË°åÈáçÊñ∞ÂàÜÁªÑ„ÄÇÂçï‰∏™ÁªÑÁöÑÁõ∏‰ººÂ∫¶ËØÑÂàÜÊòØËØ•ÁªÑÂÜÖÊØèÂØπ Attention Head ‰πãÈó¥Áõ∏‰ººÂ∫¶ËØÑÂàÜÁöÑÊÄªÂíåÔºåËÄåÊØèÁßçÂàÜÁªÑÁªìÊûúÁöÑÊÄªÁõ∏‰ººÂ∫¶ËØÑÂàÜÂàôÊòØÊâÄÊúâÁªÑÁõ∏‰ººÂ∫¶ËØÑÂàÜÁöÑÁ¥ØÂä†„ÄÇÁÆóÊ≥ïÁöÑÁõÆÊ†áÊòØÊâæÂà∞ÂæóÂàÜÊúÄÈ´òÁöÑÂàÜÁªÑÊñπÊ≥ï„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203344477-728978103.jpg)

ÂêàÁêÜÁöÑÂàÜÁªÑÊñπÂºèÂèØ‰ª•‰ΩøÂæóÂêå‰∏ÄÁªÑÂÜÖÁöÑ Attention Head Âú®ÁâπÂæÅÁ©∫Èó¥‰∏≠Êõ¥Âä†Áõ∏‰ººÔºå‰ªéËÄåÂú®Ââ™ÊûùÊó∂Êõ¥ÂÆπÊòìÊâæÂà∞ÂêàÈÄÇÁöÑÂèÇÊï∞ÂÖ±‰∫´ÊñπÂºèÔºåÊèêÈ´òÊ®°ÂûãÁöÑÂéãÁº©ÊïàÊûúÂíåÊÄßËÉΩ„ÄÇ

##### Ââ™ÊûùËÆ≠ÁªÉ

Ê≠§Ê≠•È™§‰ºöÈÄöËøáÂâ™ÊûùËÆ≠ÁªÉÔºåÈÄêÊ≠•Â∞ÜÂéüÂßãÁöÑ KV Head ËΩ¨ÁßªÂà∞Êñ∞ÁöÑ KV Head ‰∏äÔºåÂêåÊó∂‰øùÊåÅÊ®°ÂûãÊÄßËÉΩ„ÄÇÂ¶Ç‰∏ãÂõæ ÊâÄÁ§∫ÔºåÂÖ∑‰ΩìËøáÁ®ãÂåÖÊã¨Ôºö

*   Ê∑ªÂä†Êñ∞ÁöÑÊäïÂΩ±Áü©ÈòµÔºöÂú®ÊØèÁªÑÂÜÖ‰ΩøÁî® Mean Pooling ÂàùÂßãÂåñÊñ∞ÁöÑÊäïÂΩ±Áü©Èòµ„ÄÇ
*   Â∫îÁî® \\(L\_0\\) Êé©Á†ÅÔºöÂºïÂÖ• \\(L\_0\\) Êé©Á†ÅÊù•ÊéßÂà∂ÂéüÂßã KV Head ÂíåÊñ∞ KV Head ‰πãÈó¥ÁöÑËΩ¨Êç¢„ÄÇÂàùÂßãÊó∂ÔºåÊé©Á†ÅÂÄº‰∏∫ 1ÔºåË°®Á§∫‰ΩøÁî®ÂéüÂßã KV HeadÔºõÂú®Ââ™ÊûùËøáÁ®ã‰∏≠ÔºåÈÄêÊ≠•Â∞ÜÊé©Á†ÅÂÄºÁ∫¶Êùü‰∏∫ 0ÔºàË°®Á§∫‰ΩøÁî®Êñ∞ÁöÑ KV HeadÔºâ„ÄÇ
*   Áü•ËØÜËí∏È¶èÔºö‰ΩøÁî® KL ÊçüÂ§±Âíå BiLD ÊçüÂ§±ÔºåÈºìÂä±Â≠¶ÁîüÊ®°Âûã‰∏éÊïôÂ∏àÊ®°ÂûãÁöÑËæìÂá∫ÂØπÈΩêÔºå‰ªéËÄå‰øùÊåÅÊ®°ÂûãÊÄßËÉΩ„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203351548-1094520492.jpg)

### 3.6 ‰ºòÂåñ

ËÆ∫Êñá‚ÄúA Survey on Large Language Model Acceleration based on KV Cache Management‚ÄùÁªôÂá∫‰∫ÜMQA„ÄÅGQA‰ª•ÂèäÂÖ∂ÊîπËøõÊñπÊ°àÁöÑÊÄªÁªìÔºåÂÖ∑‰ΩìÂèÇËßÅ‰∏ãÂõæ„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203400480-1535487143.jpg)

Âá†ÁßçÊîπËøõÊñπÊ°àÂÖ∑‰ΩìÂ¶Ç‰∏ã„ÄÇ

*   Âä†ÊùÉGQAÔºàWeighted GQAÔºâ‰∏∫ÊØè‰∏™ÈîÆÂíåÂÄºÂ§¥ÂºïÂÖ•‰∫ÜÈ¢ùÂ§ñÁöÑÂèØËÆ≠ÁªÉÊùÉÈáçÔºåËøô‰∫õÊùÉÈáçÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Áé∞ÊúâÁöÑGQAÊ®°Âûã‰∏≠„ÄÇÈÄöËøáÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ë∞ÉÊï¥ÊùÉÈáçÔºåÂÆÉÂèØ‰ª•Âú®‰∏çÂ¢ûÂä†È¢ùÂ§ñÊé®ÁêÜÂºÄÈîÄÁöÑÊÉÖÂÜµ‰∏ãÊèêÈ´òÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203412355-945813625.jpg)

*   AsymGQAÈÄöËøáÊèêÂá∫ÊøÄÊ¥ªÈÄöÁü•ÂêàÂπ∂Á≠ñÁï•Ôºàactivationinformed merging strategyÔºâÊù•Êâ©Â±ïGQA„ÄÇAsymGQA‰∏çÊòØÈÄöËøáÁªü‰∏ÄËÅöÁ±ªÔºàuniform clusteringÔºâÂØπÂ§¥ËøõË°åÂàÜÁªÑÔºåËÄåÊòØÊ†πÊçÆËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÊøÄÊ¥ªÁõ∏‰ººÊÄßÊù•Âä®ÊÄÅÁ°ÆÂÆöÂ¶Ç‰ΩïÂàÜÁªÑÔºåÂπ∂ÊûÑÂª∫‰∏çÂØπÁß∞ÁöÑÁªÑÔºå‰ªéËÄåÂÆûÁé∞Êõ¥Â•ΩÁöÑ‰ºòÂåñÂíåÊ≥õÂåñ„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203419675-2098432331.jpg)

*   QCQAÂà©Áî®ËøõÂåñÔºàevolutionaryÔºâÁÆóÊ≥ïÊù•ËØÜÂà´GQAÁöÑÊúÄ‰Ω≥Êü•ËØ¢Â§¥ÂàÜÁªÑÔºåËØ•ÁÆóÊ≥ïÁî±‰∏Ä‰∏™ËÆ°ÁÆóÈ´òÊïàÁöÑÈÄÇÂ∫îÂ∫¶Ôºàcomputationally efficient fitnessÔºâÂáΩÊï∞ÊåáÂØºÔºåËØ•ÂáΩÊï∞Âà©Áî®ÊùÉÈáçÂÖ±‰∫´Ôºàweight-sharingÔºâËØØÂ∑ÆÂíåKVÁºìÂ≠òÊù•ËØÑ‰º∞ÊñáÊú¨ÁîüÊàêË¥®ÈáèÂíåÂÜÖÂ≠òÂÆπÈáè„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203428957-701936971.jpg)

*   KDGQAËÆ§‰∏∫ÔºåGQAÁöÑËÆ∏Â§öÂèò‰ΩìÈááÁî®Âõ∫ÂÆöÁöÑÂàÜÁªÑÁ≠ñÁï•ÔºåÂõ†Ê≠§Áº∫‰πèÂØπËÆ≠ÁªÉËøáÁ®ã‰∏≠ÈîÆÂÄº‰∫§‰∫íÊºîÂèòÁöÑÂä®ÊÄÅÈÄÇÂ∫îÊÄß„ÄÇ‰ªñ‰ª¨ÁöÑDynamic Key-Driven GQAÈÄöËøáÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠‰ΩøÁî®key head normsËá™ÈÄÇÂ∫îÂú∞ÂàÜÁªÑÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ªéËÄå‰∫ßÁîü‰∫Ü‰∏ÄÁßçÁÅµÊ¥ªÁöÑÁ≠ñÁï•Êù•Â∞ÜÊü•ËØ¢Â§¥ÂàÜÁªÑÂπ∂ÊèêÈ´òÊÄßËÉΩ„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203437938-572317046.jpg)

*   GQKVAÊèêÂá∫‰∫ÜÂàÜÁªÑÁ≠ñÁï•ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑÊü•ËØ¢„ÄÅÈîÆÂíåÂÄºÂàÜÁªÑÊú∫Âà∂„ÄÇÂÆÉÈ¶ñÂÖà‰ªãÁªç‰∫ÜMKVAÂíåGKVAÔºåÂÖ∂‰∏≠ÈîÆÂíåÂÄºË¢´ÂàÜÁªÑ‰ª•ÂÖ±‰∫´Âêå‰∏Ä‰∏™Êü•ËØ¢„ÄÇÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåËØ•ËÆ∫ÊñáÊèêÂá∫‰ΩøÁî®GQKVAÂ∞ÜÊü•ËØ¢ÂíåÈîÆÂÄºÂØπÂàÜÂºÄÂàÜÁªÑ„ÄÇÈÄöÂ∏∏ÔºåÊü•ËØ¢Ë¢´ÂàíÂàÜ‰∏∫\\(g\_q\\)ÁªÑÔºåÈîÆÂÄºË¢´ÂàíÂàÜ‰∏∫\\(g\_{kv}\\)ÁªÑÔºåÊü•ËØ¢ÂíåÈîÆÂÄºÂØπÁöÑÊØè‰∏™ÁªÑÂêàÈÉΩ‰ºö‰ΩøÁî®ÁÇπÁßØÊ≥®ÊÑèÂäõËøõË°å‰∫§‰∫í„ÄÇËøôÂØºËá¥\\(g\_q√óg\_{kv}\\)‰∫ßÁîü‰∏çÂêåÁöÑËæìÂá∫„ÄÇGQKVAÂú®Êü•ËØ¢„ÄÅÈîÆÂíåÂÄº‰∏äÊé®Âπø‰∫Ü‰∏çÂêåÁöÑÁªÑÁ≠ñÁï•ÔºåÂπ∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑËÆ°ÁÆóÊïàÁéáÂíå‰∏éMHAÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇ‰∏ãÂõæÂ±ïÁ§∫‰∫ÜÂú®Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÂØπÊü•ËØ¢„ÄÅÈîÆÂíåÂÄºËøõË°åÂàÜÁªÑÁöÑÂêÑÁßçÁ≠ñÁï•ÔºåÂåÖÊã¨Vanilla MHA„ÄÅMQA„ÄÅGQA„ÄÅMKVA„ÄÅGKVAÂíåGQKVA„ÄÇ

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203445313-1209222049.jpg)

0xFF ÂèÇËÄÉ
-------

[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpointsarxiv.org/pdf/2305.13245.pdf](https://arxiv.org/pdf/2305.13245.pdf)

[„ÄêLLM Âä†ÈÄüÊäÄÂ∑ß„ÄëMuti Query Attention Âíå Attention with Linear BiasÔºàÈôÑÊ∫êÁ†ÅÔºâ](https://zhuanlan.zhihu.com/p/634236135) [‰ΩïÊûù](https://www.zhihu.com/people/who-u)

[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)

[2‰∏áÂ≠óÈïøÊñáÔºÅ‰∏ÄÊñá‰∫ÜËß£AttentionÔºå‰ªéMHAÂà∞DeepSeek MLAÔºåÂ§ßÈáèÂõæËß£ÔºåÈùûÂ∏∏ËØ¶ÁªÜÔºÅ](https://mp.weixin.qq.com/s?__biz=MzUzOTgwNDMzOQ==&mid=2247502844&idx=1&sn=067165341bbfeba775fa4301a9d1095e&chksm=fb47e670f9237047b5c94d0657212d88d53640c54f05784fbce8bba9e086deb11d5ad0f7e84c&mpshare=1&scene=1&srcid=0228ATe44326dTsGNtdpPNl2&sharer_shareinfo=f6c641fcb6b4d0caf449ce78dc907e41&sharer_shareinfo_first=f6c641fcb6b4d0caf449ce78dc907e41#rd) ShuYini \[AINLPer\](javascript:void(0)üòâ

[‰ªéMHA„ÄÅMQA„ÄÅGQAÂà∞MLA](https://zhuanlan.zhihu.com/p/700588653) [ËãèÂâëÊûó](https://www.zhihu.com/people/su-jian-lin-22)

[ÈòøÈáå‰∏ÄÈù¢‰ª£Á†ÅÈ¢òÔºö"ÂÆûÁé∞‰∏Ä‰∏ã GQA"](https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247487324&idx=1&sn=cc79e02b124278f3d07067c355390abc&chksm=fb845df40f104ca547107d0dfaeb7424bc67e3c49d30961eecf5b98673df8c93794121484b21&mpshare=1&scene=1&srcid=0213jOpb0yYHTU7MRuvxFq7x&sharer_shareinfo=669a538ad10f3b46605953dd65cb7500&sharer_shareinfo_first=669a538ad10f3b46605953dd65cb7500#rd) ÁúãÂõæÂ≠¶ \[ÁúãÂõæÂ≠¶\](javascript:void(0)üòâ

[MHA -> GQAÔºöÊèêÂçá LLM Êé®ÁêÜÊïàÁéá](https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==&mid=2247488906&idx=1&sn=e2038e8b907c9b703354481ed0193af9&chksm=c2437164308b699ffe83a81842f17e611351c867b5b51fc3ee58bd6e7628a5b0c7716e52c26e&mpshare=1&scene=1&srcid=0115XLnq4kZjRAdZ8tI4DzYD&sharer_shareinfo=6f5890ca41e9b97d037f34b4c9518848&sharer_shareinfo_first=6f5890ca41e9b97d037f34b4c9518848#rd) AIÈó≤Ë∞à \[AIÈó≤Ë∞à\](javascript:void(0)üòâ

[Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA](https://arxiv.org/abs/2412.20677)

[FLASHINFER: EFFICIENT AND CUSTOMIZABLE ATTENTION ENGINE FOR LLM INFERENCE SERVING](https://arxiv.org/pdf/2501.01005)

[FlashInfer‰∏≠DeepSeek MLAÁöÑÂÜÖÊ†∏ËÆæËÆ°](https://zhuanlan.zhihu.com/p/25920092499) [yzh119](https://www.zhihu.com/people/wuyu-98-91)

[Â§ßÊ®°ÂûãÂπ∂Ë°åÊé®ÁêÜÁöÑÂ§™Á•ñÈïøÊã≥ÔºöËß£ËØªJeff DeanÁΩ≤ÂêçMLSys 23Êù∞Âá∫ËÆ∫Êñá](https://zhuanlan.zhihu.com/p/660715870) Êñπ‰Ω≥Áëû

[Áî±GQAÊÄßËÉΩÊï∞ÊçÆÂºÇÂ∏∏ÂºïÂèëÁöÑÂØπMHAÔºåGQAÔºåMQA Âú®GPU‰∏äÁöÑÊÑüÊÄßÂàÜÊûê](https://zhuanlan.zhihu.com/p/708776013) [‰ª£Á†ÅÊê¨ËøêÂ∑•](https://www.zhihu.com/people/fly-zhai)

[MHA->MQA->GQA->MLAÁöÑÊºîËøõ‰πãË∑Ø](https://zhuanlan.zhihu.com/p/22590523172) [ÂÅáÂ¶ÇÁªôÊàë‰∏ÄÂè™AI](https://www.zhihu.com/people/ai-81-85-59)

Y. Chen, C. Zhang, X. Gao, R. D. Mullins, G. A. Constantinides, and Y. Zhao, ‚ÄúOptimised Grouped-Query Attention Mechanism for Transformers,‚Äù in Workshop on Efficient Systems for Foundation Models II @ ICML2024, Jul. 2024. \[Online\]. Available: [https://openreview.net/forum?id=13MMghY6Kh](https://openreview.net/forum?id=13MMghY6Kh)

S. S. Chinnakonduru and A. Mohapatra, ‚ÄúWeighted Grouped Query Attention in Transformers,‚Äù Jul. 2024. \[Online\]. Available: [http://arxiv.org/abs/2407.10855](http://arxiv.org/abs/2407.10855)

V. Joshi, P. Laddha, S. Sinha, O. J. Omer, and S. Subramoney, ‚ÄúQCQA: Quality and Capacity-aware grouped Query Attention,‚Äù Jun. 2024. \[Online\]. Available: [http://arxiv.org/abs/2406.10247](http://arxiv.org/abs/2406.10247)

Z. Khan, M. Khaquan, O. Tafveez, B. Samiwala, and A. A. Raza, ‚ÄúBeyond Uniform Query Distribution: Key-Driven Grouped Query Attention,‚Äù Aug. 2024. \[Online\]. Available: [http://arxiv.org/abs/2408.08454](http://arxiv.org/abs/2408.08454)

F. Javadi, W. Ahmed, H. Hajimolahoseini, F. Ataiefard, M. Hassanpour, S. Asani, A. Wen, O. M. Awad, K. Liu, and Y. Liu, ‚ÄúGQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values,‚Äù Dec. 2023. \[Online\]. Available: [http://arxiv.org/abs/2311.03426](http://arxiv.org/abs/2311.03426)