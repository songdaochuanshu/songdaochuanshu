---
layout: post
title: 'æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ27ï¼‰--- MQA & GQA'
date: "2025-04-15T00:40:28Z"
---
æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ27ï¼‰--- MQA & GQA
=================================

ä»é›¶å¼€å§‹è§£æTransformerï¼Œç›®æ ‡æ˜¯ï¼š(1) è§£æTransformerå¦‚ä½•è¿ä½œï¼Œä»¥åŠä¸ºä½•å¦‚æ­¤è¿ä½œï¼Œè®©æ–°åŒå­¦å¯ä»¥å…¥é—¨ï¼›(2) åŠ›äº‰èå…¥ä¸€äº›æ¯”è¾ƒæ–°çš„æˆ–è€…æœ‰ç‰¹è‰²çš„è®ºæ–‡æˆ–è€…ç†å¿µï¼Œè®©è€é¸Ÿä¹Ÿå¯ä»¥æœ‰æ‰€æ”¶è·ã€‚

æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ27ï¼‰--- MQA & GQA
=================================

ç›®å½•

*   [æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ27ï¼‰--- MQA & GQA](#æ¢ç§˜transformerç³»åˆ—ä¹‹27----mqa--gqa)
    *   [0x00 æ¦‚è¿°](#0x00-æ¦‚è¿°)
    *   [0x01 MHA](#0x01-mha)
        *   [1.1 æ¦‚å¿µ](#11-æ¦‚å¿µ)
        *   [1.2 å®ç°](#12-å®ç°)
            *   [1.2.1 å“ˆä½›](#121-å“ˆä½›)
            *   [1.2.2 llm-foundry](#122-llm-foundry)
        *   [1.3 èµ„æºå ç”¨](#13-èµ„æºå ç”¨)
    *   [0x02 MQA](#0x02-mqa)
        *   [2.1 æ¦‚å¿µ](#21-æ¦‚å¿µ)
        *   [2.2 å®ç°](#22-å®ç°)
            *   [1.2.1 ç²¾ç®€ç‰ˆ](#121-ç²¾ç®€ç‰ˆ)
            *   [1.2.2 å®Œæ•´ç‰ˆ](#122-å®Œæ•´ç‰ˆ)
        *   [2.3 æ•ˆæœ](#23-æ•ˆæœ)
            *   [2.3.1 å†…å­˜](#231-å†…å­˜)
            *   [2.3.2 é€Ÿåº¦](#232-é€Ÿåº¦)
            *   [2.3.3 è¡¨å¾èƒ½åŠ›](#233-è¡¨å¾èƒ½åŠ›)
            *   [2.3.3 é€šä¿¡](#233-é€šä¿¡)
    *   [0x03 GQA](#0x03-gqa)
        *   [3.1 æ¦‚å¿µ](#31-æ¦‚å¿µ)
        *   [3.2 æ¶æ„æ¯”å¯¹](#32-æ¶æ„æ¯”å¯¹)
        *   [3.3 å®ç°](#33-å®ç°)
            *   [3.3.1 ç²¾ç®€ç‰ˆ](#331-ç²¾ç®€ç‰ˆ)
            *   [3.3.2 å®Œæ•´ç‰ˆ](#332-å®Œæ•´ç‰ˆ)
        *   [3.4 æ•ˆæœ](#34-æ•ˆæœ)
            *   [3.4.1 å†…å­˜](#341-å†…å­˜)
            *   [3.4.2 é€Ÿåº¦](#342-é€Ÿåº¦)
            *   [3.4.3 è¡¨å¾èƒ½åŠ›](#343-è¡¨å¾èƒ½åŠ›)
        *   [3.5 è½¬æ¢](#35-è½¬æ¢)
            *   [3.5.1 å¹³å‡æ± åŒ–](#351-å¹³å‡æ± åŒ–)
            *   [3.5.2 åŸºäºæ©ç ](#352-åŸºäºæ©ç )
                *   [ç½‘ç»œè½¬æ¢](#ç½‘ç»œè½¬æ¢)
                *   [æ‰¾åˆ°æ›´å¥½çš„åˆ†ç»„æ–¹æ³•](#æ‰¾åˆ°æ›´å¥½çš„åˆ†ç»„æ–¹æ³•)
                *   [å‰ªæè®­ç»ƒ](#å‰ªæè®­ç»ƒ)
        *   [3.6 ä¼˜åŒ–](#36-ä¼˜åŒ–)
    *   [0xFF å‚è€ƒ](#0xff-å‚è€ƒ)

0x00 æ¦‚è¿°
-------

åœ¨å‰æ–‡â€œä¼˜åŒ–KV Cache"ä¸­æˆ‘ä»¬æåˆ°è¿‡ï¼Œåœ¨â€å‡å°‘æ³¨æ„åŠ›å¤´çš„æ•°é‡â€œè¿™ä¸ªç»´åº¦ä¸Šï¼Œç›®å‰ä¸»è¦çš„ç›¸å…³å·¥ä½œæœ‰ MQAå’ŒGQAã€‚MQA å’Œ GQA æ˜¯åœ¨ç¼“å­˜å¤šå°‘æ•°é‡KVçš„æ€è·¯ä¸Šè¿›è¡Œä¼˜åŒ–ï¼šç›´è§‰æ˜¯å¦‚æœç¼“å­˜çš„KVä¸ªæ•°å°‘ä¸€äº›ï¼Œæ˜¾å­˜å°±å ç”¨å°‘ä¸€äº›ï¼Œå¤§æ¨¡å‹èƒ½åŠ›çš„é™ä½å¯ä»¥é€šè¿‡è¿›ä¸€æ­¥çš„è®­ç»ƒæˆ–è€…å¢åŠ FFN/GLUçš„è§„æ¨¡æ¥å¼¥è¡¥ã€‚

å› ä¸ºMQAå’ŒGQAæ˜¯åŸºäºMHAè¿›è¡Œæ”¹è¿›ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨ä¸‹å›¾å±•ç¤ºäº†ä¸‰è€…çš„åŒºåˆ«ã€‚å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ç¼©å‡æ³¨æ„åŠ›å¤´æ•°ç›®ï¼ŒMQA/GQAä¼šé™ä½KV Cacheå­˜å‚¨ï¼Œè®©ä¸åŒçš„æ³¨æ„åŠ›å¤´æˆ–è€…åŒä¸€ç»„çš„æ³¨æ„åŠ›å¤´å…±äº«ä¸€ä¸ªKå’ŒVçš„é›†åˆï¼Œå› ä¸ºåªå•ç‹¬ä¿ç•™äº†ä¸€ä»½ï¼ˆæˆ–è€…å‡ ä»½ï¼‰æŸ¥è¯¢å‚æ•°ã€‚å› æ­¤Kå’ŒVçš„çŸ©é˜µä»…æœ‰ä¸€ä»½ï¼ˆæˆ–è€…å‡ ä»½ï¼‰ï¼Œè¿™å¤§å¹…åº¦å‡å°‘äº†æ˜¾å­˜å ç”¨ï¼Œä½¿å…¶æ›´é«˜æ•ˆã€‚å¦å¤–ï¼Œä¼ ç»Ÿçš„åŸºäºMHAçš„Attentionç®—å­è¿‡äºå¡è®¿å­˜å¸¦å®½ï¼ŒMQAå’ŒGQAï¼Œä¹ƒè‡³åç»­çš„MLAéƒ½å¯ä»¥æè®¡ç®—è®¿å­˜æ¯”ï¼Œè¿™æ ·ä¹Ÿæ˜¯å¯¹æ€§èƒ½çš„æå¤§æå‡ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202825378-692989445.jpg)

* * *

æ³¨ï¼š

*   å…¨éƒ¨æ–‡ç« åˆ—è¡¨åœ¨è¿™é‡Œï¼Œä¼°è®¡æœ€ç»ˆåœ¨35ç¯‡å·¦å³ï¼Œåç»­æ¯å‘ä¸€ç¯‡æ–‡ç« ï¼Œä¼šä¿®æ”¹æ­¤æ–‡ç« åˆ—è¡¨ã€‚cnblogs [æ¢ç§˜Transformerç³»åˆ—ä¹‹æ–‡ç« åˆ—è¡¨](https://www.cnblogs.com/rossiXYZ/p/18785601)
*   æœ¬ç³»åˆ—æ˜¯å¯¹è®ºæ–‡ã€åšå®¢å’Œä»£ç çš„å­¦ä¹ å’Œè§£è¯»ï¼Œå€Ÿé‰´äº†å¾ˆå¤šç½‘ä¸Šæœ‹å‹çš„æ–‡ç« ï¼Œåœ¨æ­¤è¡¨ç¤ºæ„Ÿè°¢ï¼Œå¹¶ä¸”ä¼šåœ¨å‚è€ƒä¸­åˆ—å‡ºã€‚å› ä¸ºæœ¬ç³»åˆ—å‚è€ƒæ–‡ç« å¤ªå¤šï¼Œå¯èƒ½æœ‰æ¼ç»™å‡ºå¤„çš„ç°è±¡ã€‚å¦‚æœåŸä½œè€…å‘ç°ï¼Œè¿˜è¯·æŒ‡å‡ºï¼Œæˆ‘åœ¨å‚è€ƒæ–‡çŒ®ä¸­è¿›è¡Œå¢è¡¥ã€‚

* * *

0x01 MHA
--------

å› ä¸ºMQAï¼ŒGQAæ˜¯åŸºäºMHAè¿›è¡Œä¿®æ”¹ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰å¿…è¦å…ˆå›é¡¾ä¸‹MHAã€‚

### 1.1 æ¦‚å¿µ

MHAï¼ˆå³å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼‰åœ¨2017å¹´å°±éšç€TransformeråŸå§‹è®ºæ–‡"Attention Is All You Need"ä¸€èµ·æå‡ºï¼Œå…¶ä¸»è¦å·¥ä½œæ˜¯ï¼šæŠŠåŸæ¥ä¸€ä¸ªæ³¨æ„åŠ›è®¡ç®—æ‹†æˆå¤šä¸ªå°ä»½çš„æ³¨æ„åŠ›å¤´ï¼Œå³æŠŠQã€Kã€Våˆ†åˆ«æ‹†åˆ†æˆå¤šä»½ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´ä½¿ç”¨ç‹¬ç«‹çš„Qã€Kã€Vè¿›è¡Œè®¡ç®—ã€‚è€Œå¤šä¸ªå¤´å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œåˆ†åˆ«å¾—å‡ºç»“æœï¼Œæœ€åå†åˆå›åŸæ¥çš„ç»´åº¦ã€‚

æˆ‘ä»¬é€šè¿‡ä¸‹å›¾æ¥çœ‹çœ‹MHAçš„æµç¨‹ï¼Œè¿™é‡Œè®¾ ğ‘‘ è¡¨ç¤ºè¯åµŒå…¥çš„ç»´åº¦ï¼Œ \\(ğ‘›\_â„\\) è¡¨ç¤ºæ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œ \\(ğ‘‘\_â„\\) è¡¨ç¤ºæ¯ä¸€ä¸ªå¤´çš„ç»´åº¦ï¼Œ \\(â„\_ğ‘¡\\inğ‘…^ğ‘‘\\) è¡¨ç¤ºç¬¬ ğ‘¡ ä¸ªtokenåœ¨ä¸€ä¸ªæ³¨æ„åŠ›å±‚çš„è¾“å…¥ï¼Œ \\(ğ‘Š^ğ‘‚âˆˆğ‘…^{ğ‘‘Ã—ğ‘‘\_â„ğ‘›\_â„}\\) è¡¨ç¤ºè¾“å‡ºæ˜ å°„çŸ©é˜µã€‚åˆ™MHAå¯ä»¥åˆ†ä¸ºä»¥ä¸‹å››æ­¥ï¼š

1.  é€šè¿‡3ä¸ªå‚æ•°çŸ©é˜µ \\(ğ‘Š^ğ‘„,ğ‘Š^ğ¾,ğ‘Š^ğ‘‰âˆˆğ‘…^{ğ‘‘\_â„ğ‘›\_h\\times d}\\) å°±å¯ä»¥å¾—åˆ° \\(ğ‘\_ğ‘¡,ğ‘˜\_ğ‘¡,ğ‘£\_ğ‘¡âˆˆğ‘…^{ğ‘‘\_â„ğ‘›\_h}\\) ã€‚
2.  \\(ğ‘\_ğ‘¡,ğ‘˜\_ğ‘¡,ğ‘£\_ğ‘¡\\) ä¼šåˆ†å‰²æˆ \\(ğ‘›\_â„\\) ä¸ªå‘é‡ï¼Œ\\(ğ‘\_{ğ‘¡,ğ‘–},ğ‘˜\_{ğ‘¡,ğ‘–},ğ‘£\_{ğ‘¡,ğ‘–}âˆˆğ‘…^{ğ‘‘\_â„}\\) åˆ†åˆ«è¡¨ç¤ºQã€Kå’ŒVçš„ç¬¬ ğ‘– ä¸ªå‘é‡ï¼Œè¿™äº›æ‹†åˆ†åçš„å‘é‡æˆ‘ä»¬åç»­ç§°ä¹‹ä¸ºQå¤´ï¼ŒKå¤´å’ŒVå¤´ã€‚
3.  æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¼šåˆ©ç”¨è‡ªå·±è·å¾—çš„Qã€Kã€Vå‘é‡è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚
4.  åˆ©ç”¨\\(W^O\\)å¯¹å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ç»“æœè¿›è¡Œåˆå¹¶ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202836695-310488105.jpg)

### 1.2 å®ç°

#### 1.2.1 å“ˆä½›

æˆ‘ä»¬å›é¡¾ä¸‹â€œThe Annotated Transformerâ€ä¸­MHAä»£ç çš„å®ç°

    def attention(query, key, value, mask=None, dropout=None):
        "Compute 'Scaled Dot Product Attention'"
        d_k = query.size(-1)
        scores = torch.matmul(query, key.transpose(-2, -1)) \
                 / math.sqrt(d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        p_attn = F.softmax(scores, dim = -1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return torch.matmul(p_attn, value), p_attn
    
    class MultiHeadedAttention(nn.Module):
        def __init__(self, h, d_model, dropout=0.1):
            '''
            h: head number
            '''
            super(MultiHeadedAttention, self).__init__()
            assert d_model % h == 0
            # We assume d_v always equals d
            self.d = d_model // h
            self.h = h
            self.linears = clones(nn.Linear(d_model, d_model), 4)
            self.attn = None
            self.dropout = nn.Dropout(p=dropout)
            
        def forward(self, query, key, value, mask=None):
            if mask is not None:
                # Same mask applied to all h heads.
                mask = mask.unsqueeze(1)
            nbatches = query.size(0)
            
            # 1) Do all the linear projections in batch from d_model => h x d 
            query, key, value = \
                [l(x).view(nbatches, -1, self.h, self.d).transpose(1, 2)
                 for l, x in zip(self.linears, (query, key, value))]
            
            # 2) Apply attention on all the projected vectors in batch. 
            x, self.attn = attention(query, key, value, mask=mask, 
                                     dropout=self.dropout)
            
            # 3) "Concat" using a view and apply a final linear. 
            x = x.transpose(1, 2).contiguous() \
                 .view(nbatches, -1, self.h * self.d)
            return self.linears[-1](x)
    

#### 1.2.2 llm-foundry

ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬çœ‹çœ‹å·¥ä¸šç•Œçš„äº§å“ã€‚

[https://github.com/mosaicml/llm-foundry/blob/9c89ab263e72fb9610f28c8ab9cde5d2205b6bff/llmfoundry/models/layers/attention.py](https://github.com/mosaicml/llm-foundry/blob/9c89ab263e72fb9610f28c8ab9cde5d2205b6bff/llmfoundry/models/layers/attention.py)

    class MultiheadAttention(nn.Module):
        """Multi-head self attention.
    
        Using torch or triton attention implemetation enables user to also use
        additive bias.
        """
    
        def __init__(
            self,
            d_model: int,
            n_heads: int,
            attn_impl: str = 'triton',
            clip_qkv: Optional[float] = None,
            qk_ln: bool = False,
            softmax_scale: Optional[float] = None,
            attn_pdrop: float = 0.0,
            low_precision_layernorm: bool = False,
            verbose: int = 0,
            device: Optional[str] = None,
        ):
            super().__init__()
    
            self.attn_impl = attn_impl
            self.clip_qkv = clip_qkv
            self.qk_ln = qk_ln
    
            self.d_model = d_model
            self.n_heads = n_heads
            self.softmax_scale = softmax_scale
            if self.softmax_scale is None:
                self.softmax_scale = 1 / math.sqrt(self.d_model / self.n_heads)
            self.attn_dropout_p = attn_pdrop
    
            self.Wqkv = nn.Linear(self.d_model, 3 * self.d_model, device=device)
            # for param init fn; enables shape based init of fused layers
            fuse_splits = (d_model, 2 * d_model)
            self.Wqkv._fused = (0, fuse_splits)  # type: ignore
    
            if self.qk_ln:
                layernorm_class = LPLayerNorm if low_precision_layernorm else nn.LayerNorm
                self.q_ln = layernorm_class(self.d_model, device=device)
                self.k_ln = layernorm_class(self.d_model, device=device)
    
            if self.attn_impl == 'flash':
                self.attn_fn = flash_attn_fn
            elif self.attn_impl == 'triton':
                self.attn_fn = triton_flash_attn_fn
            elif self.attn_impl == 'torch':
                self.attn_fn = scaled_multihead_dot_product_attention
            else:
                raise ValueError(f'{attn_impl=} is an invalid setting.')
    
            self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
            self.out_proj._is_residual = True  # type: ignore
    
        def forward(
            self,
            x,
            past_key_value=None,
            attn_bias=None,
            attention_mask=None,
            is_causal=True,
            needs_weights=False,
        ):
            qkv = self.Wqkv(x)
    
            if self.clip_qkv:
                qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)
    
            query, key, value = qkv.chunk(3, dim=2)
    
            key_padding_mask = attention_mask
    
            if self.qk_ln:
                # Applying layernorm to qk
                dtype = query.dtype
                query = self.q_ln(query).to(dtype)
                key = self.k_ln(key).to(dtype)
    
            context, attn_weights, past_key_value = self.attn_fn(
                query,
                key,
                value,
                self.n_heads,
                past_key_value=past_key_value,
                softmax_scale=self.softmax_scale,
                attn_bias=attn_bias,
                key_padding_mask=key_padding_mask,
                is_causal=is_causal,
                dropout_p=self.attn_dropout_p,
                training=self.training,
                needs_weights=needs_weights,
            )
    
            return self.out_proj(context), attn_weights, past_key_value
    

scaled\_multihead\_dot\_product\_attention()ä»£ç å¦‚ä¸‹ã€‚

    def scaled_multihead_dot_product_attention(
        query,
        key,
        value,
        n_heads,
        past_key_value=None,
        softmax_scale=None,
        attn_bias=None,
        key_padding_mask=None,
        is_causal=False,
        dropout_p=0.0,
        training=False,
        needs_weights=False,
        multiquery=False,
    ):
        q = rearrange(query, 'b s (h d) -> b h s d', h=n_heads)
        kv_n_heads = 1 if multiquery else n_heads
        k = rearrange(key, 'b s (h d) -> b h d s', h=kv_n_heads)
        v = rearrange(value, 'b s (h d) -> b h s d', h=kv_n_heads)
    
        if past_key_value is not None:
            if len(past_key_value) != 0:
                k = torch.cat([past_key_value[0], k], dim=3)
                v = torch.cat([past_key_value[1], v], dim=2)
            past_key_value = (k, v)
    
        b, _, s_q, d = q.shape
        s_k = k.size(-1)
    
        if softmax_scale is None:
            softmax_scale = 1 / math.sqrt(d)
    
        attn_weight = q.matmul(k) * softmax_scale
    
        if attn_bias is not None:
            _s_q = max(0, attn_bias.size(2) - s_q)
            _s_k = max(0, attn_bias.size(3) - s_k)
            attn_bias = attn_bias[:, :, _s_q:, _s_k:]
            attn_weight = attn_weight + attn_bias
    
        min_val = torch.finfo(q.dtype).min
    
        if key_padding_mask is not None:
            attn_weight = attn_weight.masked_fill(
                ~key_padding_mask.view((b, 1, 1, s_k)), min_val)
    
        if is_causal and (not q.size(2) == 1):
            s = max(s_q, s_k)
            causal_mask = attn_weight.new_ones(s, s, dtype=torch.float16)
            causal_mask = causal_mask.tril()
            causal_mask = causal_mask.to(torch.bool)
            causal_mask = ~causal_mask
            causal_mask = causal_mask[-s_q:, -s_k:]
            attn_weight = attn_weight.masked_fill(causal_mask.view(1, 1, s_q, s_k),
                                                  min_val)
    
        attn_weight = torch.softmax(attn_weight, dim=-1)
    
        if dropout_p:
            attn_weight = torch.nn.functional.dropout(attn_weight,
                                                      p=dropout_p,
                                                      training=training,
                                                      inplace=True)
    
        out = attn_weight.matmul(v)
        out = rearrange(out, 'b h s d -> b s (h d)')
    
        if needs_weights:
            return out, attn_weight, past_key_value
        return out, None, past_key_value
    

### 1.3 èµ„æºå ç”¨

å¦‚æœæ¨¡å‹ç»“æ„æ˜¯MHAï¼Œåœ¨æ¨ç†æ—¶ï¼ŒKV Cacheå¯¹äºæ¯ä¸ªtokenéœ€è¦ç¼“å­˜çš„å‚æ•°æœ‰ \\(2ğ‘›\_â„ğ‘‘\_â„ğ‘™\\)ï¼ˆğ‘™ è¡¨ç¤ºç½‘ç»œå±‚æ•°ï¼‰ã€‚å½“æ¨¡å‹å±‚æ•°åŠ æ·±å’Œå¤´æ•°å˜å¤šåï¼Œæ³¨æ„åŠ›è®¡ç®—æ‰€æ¶‰åŠçš„ç®—åŠ›ã€IOå’Œå†…å­˜éƒ½ä¼šå¿«é€Ÿå¢åŠ ã€‚ä½†æ˜¯å¯¹è¿™äº›èµ„æºå´åˆ©ç”¨å¾—ä¸å¥½ã€‚

å°±ä¸‹å›¾è€Œè¨€ï¼Œd è¡¨ç¤º hidden sizeï¼Œh è¡¨ç¤º Head ä¸ªæ•°ï¼Œl è¡¨ç¤ºå½“å‰è¾“å…¥åºåˆ—ä¸€å…±æœ‰ l ä¸ª Tokenã€‚

*   å½“ Batch Size ä¸º 1 æ—¶ï¼Œå›¾ä¸­çº¢è‰²ã€ç»¿è‰²ã€è“è‰²è™šçº¿åœˆå¤„çš„ä¹˜æ³•å…¨éƒ¨ä¸ºçŸ©é˜µä¹˜å‘é‡ï¼Œæ˜¯æ˜æ˜¾çš„ Memory Boundï¼Œç®—æœ¯å¼ºåº¦ä¸åˆ° 1ã€‚
    
*   å½“ Batch Size å¤§äº 1 æ—¶ï¼ˆæ¯”å¦‚ Continuous Batchingï¼‰ï¼š
    
*   *   çº¢è‰²å’Œè“è‰²éƒ¨åˆ†ï¼šçº¿æ€§å±‚è®¡ç®—æ˜¯æƒé‡ä¹˜ä»¥æ¿€æ´»ï¼Œä¸åŒè¯·æ±‚ä¹‹é—´å¯ä»¥å…±äº«æƒé‡ï¼Œå› æ­¤æ˜¯çŸ©é˜µä¹˜çŸ©é˜µï¼Œå¹¶ä¸” Batch Size è¶Šå¤§ï¼Œç®—æœ¯å¼ºåº¦è¶Šå¤§ï¼Œè¶Šè¶‹è¿‘äºè®¡ç®—å¯†é›†å‹ï¼ˆFFN å±‚ä¹Ÿç±»ä¼¼ï¼‰ã€‚
    *   ç»¿è‰²éƒ¨åˆ†ï¼šæ³¨æ„åŠ›è®¡ç®—æ˜¯æ¿€æ´»ä¹˜ä»¥æ¿€æ´»ã€‚å› ä¸ºä¸åŒçš„è¯·æ±‚ä¹‹é—´æ²¡æœ‰ä»»ä½•ç›¸å…³æ€§ï¼Œå³ä½¿ Batchingï¼Œæ­¤å¤„ä¹Ÿæ˜¯ Batched çŸ©é˜µä¹˜å‘é‡ï¼Œå¹¶ä¸”å› ä¸ºåºåˆ—é•¿åº¦å¯èƒ½ä¸åŒï¼Œè¿™é‡Œä¸åŒè¯·æ±‚çš„çŸ©é˜µä¹˜å‘é‡æ˜¯ä¸è§„åˆ™çš„ã€‚å³ï¼Œè¿™é‡Œç®—æœ¯å¼ºåº¦å§‹ç»ˆä¸åˆ° 1ï¼Œæ˜¯æ˜æ˜¾çš„ Memory Boundã€‚
*   å› æ­¤ï¼Œç»¿è‰²éƒ¨åˆ†éš¾ä»¥ä¼˜åŒ–ï¼Œè¾“å…¥åºåˆ—è¶Šé•¿ï¼Œæ­¤å¤„çš„ç“¶é¢ˆå°±è¶Šå¤§ã€‚
    

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250414200154304-642792608.jpg)

ä¸ºäº†ç¼“è§£è¿™äº›èµ„æºå ç”¨ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æ›´å¥½çš„åˆ©ç”¨èµ„æºï¼Œç›¸ç»§å‡ºç°äº†MQAï¼ˆMulti-Query Attentionï¼‰ å’ŒGQAï¼ˆGrouped-Query Attention ï¼‰ç­‰æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•éƒ½æ˜¯å›´ç»•â€œå¦‚ä½•å‡å°‘èµ„æºå ç”¨ä¸”å°½å¯èƒ½åœ°ä¿è¯æ•ˆæœâ€è¿™ä¸ªä¸»é¢˜å‘å±•è€Œæ¥çš„äº§ç‰©ã€‚

0x02 MQA
--------

ç›®å‰çš„åŸºæœ¬å‡è®¾æ˜¯ï¼Œåœ¨å¤´ç»´åº¦ä¸Šå­˜åœ¨éå¸¸é«˜çš„ç¨€ç–æ€§ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå¤´çš„æ•°é‡ç¼©å‡åˆ°ç›¸å½“å°çš„æ•°ç›®ã€‚åœ¨è¿™äº›æ³¨æ„åŠ›å¤´ä¸­ï¼Œæœ‰ä¸€äº›å¤´éƒ¨ä¸“é—¨ç”¨äºæ£€ç´¢å’Œé•¿ä¸Šä¸‹æ–‡ç›¸å…³èƒ½åŠ›ï¼Œå› æ­¤åº”è¯¥ä¿ç•™è¿™äº›æ£€ç´¢å¤´å¹¶ä¿®å‰ªå…¶ä»–å¤´ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¤´éƒ¨ä¿®å‰ªé€šå¸¸å‘ç”Ÿåœ¨é¢„å¡«å……ä¹‹åï¼Œè¿™æ„å‘³ç€å®ƒä»¬åªä¼šæ”¹å–„è§£ç ã€å¹¶å‘æ€§å’Œä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œä½†å¹¶æ²¡æœ‰æ”¹å–„é¢„å¡«å……é˜¶æ®µã€‚

### 2.1 æ¦‚å¿µ

MQAï¼ˆMulti Queries Attentionï¼‰å‡ºè‡ªè®ºæ–‡ \[[2019\] Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150.pdf)ã€‚åœ¨MQAä¸­ï¼Œä¿ç•™queryçš„å¤šå¤´æ€§è´¨ï¼Œæ‰€æœ‰æŸ¥è¯¢å¤´å…±äº«ç›¸åŒçš„å•ä¸€é”®å’Œå€¼å¤´ï¼Œè¿™ç”¨å¯ä»¥å‡å°‘Keyå’ŒValueçŸ©é˜µçš„æ•°é‡ï¼Œä»è€Œé™ä½è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚è¿™ç›¸å½“äºæŠŠä¸åŒHeadçš„æ³¨æ„åŠ›å·®å¼‚ï¼Œå…¨éƒ¨éƒ½æ”¾åœ¨äº†Queryä¸Šï¼Œéœ€è¦æ¨¡å‹ä»…ä»ä¸åŒçš„Query Headsä¸Šå°±èƒ½å¤Ÿå…³æ³¨åˆ°è¾“å…¥hidden statesä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚

MQAçš„å…·ä½“ç‰¹ç‚¹å¦‚ä¸‹ã€‚

*   Q ä»ç„¶ä¿æŒåŸæ¥çš„å¤´æ•°ï¼Œå³çº¿æ€§å˜æ¢ä¹‹åï¼Œä¾ç„¶å¯¹Qè¿›è¡Œåˆ‡åˆ†ï¼ˆåƒMHAä¸€æ ·ï¼‰ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´å•ç‹¬ä¿ç•™äº†è‡ªå·±çš„Qå‘é‡ã€‚
*   K å’Œ V åªæœ‰ä¸€ä¸ªå¤´ï¼Œå…·ä½“æ˜¯åœ¨çº¿æ€§å˜æ¢æ—¶ç›´æ¥æŠŠKå’ŒVçš„ç»´åº¦é™åˆ°äº†\\(d\_{head}\\)ï¼Œè€Œä¸æ˜¯åšåˆ‡åˆ†å˜å°ã€‚
*   æ‰€æœ‰çš„ Q å¤´å…±äº«è¿™ä¸ªK å’Œ V å¤´ï¼Œæˆ–è€…å¯ä»¥è®¤ä¸ºæ˜¯ k, vçŸ©é˜µå‚æ•°å…±äº«ã€‚å®ç°ä¸Šï¼Œå°±æ˜¯æ”¹ä¸€ä¸‹çº¿æ€§å˜æ¢çŸ©é˜µï¼Œç„¶åæŠŠ Kã€V çš„å¤„ç†ä»åˆ‡åˆ†å˜æˆå¤åˆ¶ã€‚
*   æ‰€æœ‰Qå¤´éƒ½ä½¿ç”¨è¿™ä¸ªç›¸åŒçš„Kå¤´è®¡ç®—å®ƒä»¬çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¹¶ä¸”æ‰€æœ‰å¤´çš„è¾“å‡ºéƒ½ä½¿ç”¨ç›¸åŒçš„Vå¤´è®¡ç®—ï¼ˆä½†æ³¨æ„åŠ›åˆ†æ•°ä¸åŒï¼‰ã€‚
*   æœ€åå°†æ¯ä¸ªå¤´è®¡ç®—çš„ç»“æœæ‹¼æ¥èµ·æ¥ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202855494-1240701416.jpg)

### 2.2 å®ç°

æˆ‘ä»¬è¿˜æ˜¯ä»¥llm-foundryä¸ºä¾‹æ¥è¿›è¡Œåˆ†æã€‚

#### 1.2.1 ç²¾ç®€ç‰ˆ

æˆ‘ä»¬å…ˆç»™å‡ºMHAå’ŒMQAçš„ç²¾ç®€ç‰ˆå¯¹æ¯”ã€‚è¿™é‡Œå‡è®¾ x (tensor): (batch, hidden\_state, d\_model) ï¼Œæ¯”å¦‚ (1, 512, 768) ã€‚å¯ä»¥çœ‹åˆ°ï¼Œä¸¤è€…ä¸»è¦ä¸åŒåœ¨äºï¼š

*   WçŸ©é˜µçš„ç»´åº¦ä¸åŒã€‚
*   QKVåˆ‡åˆ†æ–¹å¼ä¸åŒã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202905108-376926323.jpg)

ä»ä»£ç ä¸­å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºMQAæ¥è¯´ï¼Œæ‰€æœ‰å¤´ä¹‹é—´å…±äº«ä¸€ä»½ key å’Œ value çš„å‚æ•°ï¼Œä½†æ˜¯å¦‚ä½•å°†è¿™ 1 ä»½å‚æ•°åŒæ—¶è®© 8 ä¸ªå¤´éƒ½ä½¿ç”¨å‘¢ï¼Ÿåœ¨scaled\_multihead\_dot\_product\_attention()å‡½æ•°çš„ä»£ç ä¼šä½¿ç”¨çŸ©é˜µä¹˜æ³• matmulæ¥å¹¿æ’­ï¼Œä½¿å¾—æ¯ä¸ªå¤´éƒ½ä¹˜ä»¥è¿™åŒä¸€ä¸ªå¼ é‡ï¼Œä»¥æ­¤æ¥å®ç°å‚æ•°å…±äº«ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202915394-772507533.jpg)

MQAçš„æ€»ä½“æµç¨‹å¯ä»¥å‚è§ä¸‹å›¾ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202923601-315033145.jpg)

#### 1.2.2 å®Œæ•´ç‰ˆ

æˆ‘ä»¬å†ç»™å‡ºå®Œæ•´ç‰ˆæœ¬ä»£ç ã€‚

    class MultiQueryAttention(nn.Module):
        """Multi-Query self attention.
    
        Using torch or triton attention implemetation enables user to also use
        additive bias.
        """
    
        def __init__(
            self,
            d_model: int,
            n_heads: int,
            attn_impl: str = 'triton',
            clip_qkv: Optional[float] = None,
            qk_ln: bool = False,
            softmax_scale: Optional[float] = None,
            attn_pdrop: float = 0.0,
            low_precision_layernorm: bool = False,
            verbose: int = 0,
            device: Optional[str] = None,
        ):
            super().__init__()
    
            self.attn_impl = attn_impl
            self.clip_qkv = clip_qkv
            self.qk_ln = qk_ln
    
            self.d_model = d_model
            self.n_heads = n_heads
            self.head_dim = d_model // n_heads
            self.softmax_scale = softmax_scale
            if self.softmax_scale is None:
                self.softmax_scale = 1 / math.sqrt(self.head_dim)
            self.attn_dropout_p = attn_pdrop
    
            # NOTE: if we ever want to make attn TensorParallel, I'm pretty sure we'll
            # want to split Wqkv into Wq and Wkv where Wq can be TensorParallel but
            # Wkv shouldn't be TensorParallel
            # - vchiley
            self.Wqkv = nn.Linear(
                d_model,
                d_model + 2 * self.head_dim,
                device=device,
            )
            # for param init fn; enables shape based init of fused layers
            fuse_splits = (d_model, d_model + self.head_dim)
            self.Wqkv._fused = (0, fuse_splits)  # type: ignore
    
            if self.qk_ln:
                layernorm_class = LPLayerNorm if low_precision_layernorm else nn.LayerNorm
                self.q_ln = layernorm_class(d_model, device=device)
                self.k_ln = layernorm_class(self.head_dim, device=device)
    
            if self.attn_impl == 'flash':
                self.attn_fn = flash_attn_fn
            elif self.attn_impl == 'triton':
                self.attn_fn = triton_flash_attn_fn
            elif self.attn_impl == 'torch':
                self.attn_fn = scaled_multihead_dot_product_attention
            else:
                raise ValueError(f'{attn_impl=} is an invalid setting.')
    
            self.out_proj = nn.Linear(self.d_model, self.d_model, device=device)
            self.out_proj._is_residual = True  # type: ignore
    
        def forward(
            self,
            x,
            past_key_value=None,
            attn_bias=None,
            attention_mask=None,
            is_causal=True,
            needs_weights=False,
        ):
            qkv = self.Wqkv(x)
    
            if self.clip_qkv:
                qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)
    
            query, key, value = qkv.split(
                [self.d_model, self.head_dim, self.head_dim], dim=2)
    
            key_padding_mask = attention_mask
    
            if self.qk_ln:
                # Applying layernorm to qk
                dtype = query.dtype
                query = self.q_ln(query).to(dtype)
                key = self.k_ln(key).to(dtype)
    
            context, attn_weights, past_key_value = self.attn_fn(
                query,
                key,
                value,
                self.n_heads,
                past_key_value=past_key_value,
                softmax_scale=self.softmax_scale,
                attn_bias=attn_bias,
                key_padding_mask=key_padding_mask,
                is_causal=is_causal,
                dropout_p=self.attn_dropout_p,
                training=self.training,
                needs_weights=needs_weights,
                multiquery=True,
            )
    
            return self.out_proj(context), attn_weights, past_key_value
    

### 2.3 æ•ˆæœ

#### 2.3.1 å†…å­˜

MQAéœ€è¦ç¼“å­˜çš„ Kã€V å€¼ä»æ‰€æœ‰å¤´å˜æˆä¸€ä¸ªå¤´ï¼Œå› æ­¤ç›´æ¥å°†KV Cacheå‡å°‘åˆ°äº†åŸæ¥çš„1/â„ã€‚MHAçš„å•ä¸ªTokenéœ€è¦ä¿å­˜çš„KVæ•°ï¼ˆ \\(2âˆ—ğ‘™âˆ—ğ‘›\_â„\\) ï¼‰ï¼Œè€ŒMQAå‡å°‘åˆ°äº†ï¼ˆ 2Ã—ğ‘™ ï¼‰ä¸ªï¼Œå³æ¯ä¸€å±‚å…±äº«ä½¿ç”¨ä¸€ä¸ª ğ‘˜ å‘é‡å’Œä¸€ä¸ª ğ‘£ å‘é‡ã€‚

#### 2.3.2 é€Ÿåº¦

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202935498-1452899910.jpg)

è®ºæ–‡ä½œè€…åšäº†ä¸€ç³»åˆ—æµ‹è¯•ï¼Œå…·ä½“å‚è§ä¸Šè¡¨ï¼ˆæ•°å€¼æ˜¯å¹³å‡ç”Ÿæˆæ¯ä¸ªtokenæ‰€éœ€è¦çš„æ¯«ç§’æ•°ï¼‰ã€‚éœ€è¦æ³¨æ„çš„å‡ ä¸ªç‚¹æ˜¯ï¼š

1.  è®­ç»ƒé€Ÿåº¦å‡ ä¹æ²¡æœ‰å˜åŒ–ã€‚
2.  æ¨ç†æ—¶é—´å’ŒBeam searchæ—¶é—´éƒ½æ˜¾è‘—ç¼©çŸ­ã€‚
3.  æ¨ç†é€Ÿåº¦ä¸­ï¼Œencoderçš„æ¨ç†é€Ÿåº¦åŸºæœ¬ä¸å˜ï¼Œdecoderçš„æ¨ç†å¿«äº†å¾ˆå¤šã€‚

è™½ç„¶MQAåªæœ‰ä¸€ç»„KVå¤´ï¼Œä½†å®é™…ä¸ŠMQAæ˜¯è¯»å–è¿™ç»„KVå¤´ä¹‹åï¼Œå¤åˆ¶ç»™æ‰€æœ‰Qå¤´ä½¿ç”¨ï¼Œå› æ­¤æŒ‰ç…§é“ç†æ¥è¯´ï¼ŒMQAåªèƒ½é™ä½æ˜¾å­˜çš„ä½¿ç”¨ï¼Œè¿ç®—é‡å¹¶æ²¡æœ‰å‡å°‘ï¼Œä¸ºå•¥é€Ÿåº¦èƒ½æé«˜è¿™ä¹ˆå¤šï¼Ÿå…¶å®ä¸»è¦æ”¶ç›Šæ˜¯å› ä¸ºé™ä½äº†KV Cacheè€Œå¸¦æ¥è®¡ç®—é‡çš„å‡å°‘ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

*   KV-Cacheç©ºé—´å ç”¨é™ä½ã€‚å› ä¸ºå¤´æ•°é‡çš„å‡å°‘ï¼Œæ‰€ä»¥éœ€è¦å­˜å‚¨åœ¨GPUå†…å­˜ä¸­çš„å¼ é‡ä¹Ÿå‡å°‘äº†ï¼ˆå‡è®¾ä¹‹å‰è¦å­˜å‚¨32ä¸ªå¤´çš„KV Cacheï¼Œç›®å‰åªéœ€è¦å­˜å‚¨1ä¸ªå¤´çš„KV Cacheï¼‰ã€‚èŠ‚çœçš„ç©ºé—´å¯ä»¥ç”¨æ¥å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œæå‡ååï¼Œä»è€Œæé«˜æ•ˆç‡ï¼ˆè™½ç„¶å•æ¡è¯·æ±‚çš„æ€»æ—¶å»¶ä¼šå¢åŠ ï¼Œä½†æœåŠ¡çš„æ€»ååé‡æ˜¯æ˜æ˜¾å¢åŠ ï¼‰ã€‚
*   é™ä½å†…å­˜è¯»å–æ¨¡å‹æƒé‡çš„æ—¶é—´å¼€é”€ã€‚å› ä¸ºå¤´æ•°é‡çš„å‡å°‘ï¼Œæ‰€ä»¥å‡å°‘äº†ä»æ˜¾å­˜ä¸­è¯»å–çš„æ•°æ®é‡ï¼Œå‡å°‘äº†è®¡ç®—å•å…ƒçš„ç­‰å¾…æ—¶é—´ï¼Œä»å†…å­˜å¯†é›†å‹è¶‹è¿‘äºè®¡ç®—å¯†é›†å‹ã€‚å¦å¤–ï¼ŒåŒä¸€ä¸ª Request ä¸­çš„ä¸åŒ Head å¯ä»¥å…±äº«ï¼Œè¿™å°±æå‡äº† Qã€K å’Œ V çš„ Attention è®¡ç®—çš„ç®—æœ¯å¼ºåº¦ã€‚

#### 2.3.3 è¡¨å¾èƒ½åŠ›

å› ä¸ºç›®å‰åªæœ‰ä¸€ä¸ªå…±äº«çš„KVå¤´ï¼Œæ‰€ä»¥åŸå…ˆå¤šQKVå¤´å¸¦æ¥çš„æ³¨æ„åŠ›å·®å¼‚éƒ½éœ€è¦ä»…ä»…ä¾é å¤šä¸ªQå¤´å®Œæˆï¼Œè¿™æ ·é™åˆ¶äº†æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ï¼Œå› æ­¤MQAè™½ç„¶èƒ½å¥½åœ°æ”¯æŒæ¨ç†åŠ é€Ÿï¼Œä½†æ˜¯åœ¨æ•ˆæœä¸Šæ¯”MHAç•¥å·®ã€‚ä¸ºäº†å¼¥è¡¥å…±äº«KVå¸¦æ¥çš„å‚æ•°é‡å‡å°‘ï¼Œäººä»¬å¾€å¾€ä¼šç›¸åº”åœ°å¢å¤§FFN/GLUçš„è§„æ¨¡ï¼Œä»¥æ­¤æ¥ç»´æŒæ¨¡å‹æ€»å‚æ•°é‡çš„ä¸å˜ï¼Œè¿›è€Œå¼¥è¡¥ä¸€éƒ¨åˆ†æ•ˆæœæŸå¤±ã€‚

å¦å¤–éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äºMQAå’ŒGQAæ”¹å˜äº†æ³¨æ„åŠ›æœºåˆ¶çš„ç»“æ„ï¼Œå› æ­¤æ¨¡å‹é€šå¸¸éœ€è¦ä»è®­ç»ƒå¼€å§‹å°±æ”¯æŒ MQAæˆ–è€…GQA ã€‚å¦‚æœæ¨¡å‹å·²ç»è®­ç»ƒå¥½äº†ï¼Œå°†KV Cacheå¼ºè¡Œæ¢æˆè¿™ä¸¤ä¸ªæ–¹æ³•ï¼Œæ•ˆæœä¼šå¾ˆå·®ï¼Œå› æ­¤éœ€è¦éœ€è¦å€ŸåŠ©å¾®è°ƒæ¥å¼¥è¡¥ã€‚æœ‰ç ”ç©¶è¡¨æ˜éœ€è¦çº¦ 5% çš„åŸå§‹è®­ç»ƒæ•°æ®é‡å°±å¯ä»¥è¾¾åˆ°ä¸é”™çš„æ•ˆæœã€‚

#### 2.3.3 é€šä¿¡

åœ¨å¤šå¡å¹¶è¡Œæƒ…å†µä¸‹ï¼ŒMQAå‡å°‘äº†è®¿å­˜ï¼Œä½†æ˜¯å¢åŠ äº†å¹¶è¡Œé€šä¿¡å¼€é”€ã€‚å› ä¸ºKå’ŒVå¼ é‡åœ¨æ‰€æœ‰å¤´éƒ¨ä¹‹é—´å…±äº«ï¼Œæ¯ä¸ªGPUä¸Šéƒ½éœ€è¦æœ‰è‡ªå·±çš„å¤‡ä»½ã€‚ä¸ä¸‹å›¾(a)ä¸­MHAå¹¶è¡Œç­–ç•¥ç›¸æ¯”ï¼ŒMQAéœ€è¦ä½¿ç”¨all-to-allå¯¹è¿›è¡Œè¾“å…¥è¾“å‡ºæ¿€æ´»å¼ é‡reshardingï¼Œä»è€Œäº§ç”Ÿé¢å¤–çš„é€šä¿¡æˆæœ¬ã€‚å…·ä½“å¦‚ä¸‹å›¾(b)æ‰€ç¤ºã€‚å¦å¤–ï¼Œå› ä¸ºæ¯ä¸ªå¡ä¸Šéƒ½æœ‰å¤‡ä»½ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´MQAçš„å†…å­˜æˆæœ¬èŠ‚çœå°†ä¼šä¸§å¤±ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413202947399-47036567.jpg)

0x03 GQA
--------

å¯¹äºæ›´å¤§çš„æ¨¡å‹è€Œè¨€ï¼Œå½»åº•å‰¥ç¦»æ‰€æœ‰å¤´è¿‡äºæ¿€è¿›ã€‚ä¾‹å¦‚ï¼Œç›¸æ¯”ä»32å‡å°‘åˆ°1ï¼Œå°†å¤´æ•°ä»64å‡å°‘åˆ°1åœ¨æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ä¸Šæ˜¯ä¸€ä¸ªæ›´å¤§çš„å‰Šå‡ã€‚è€Œä¸”æ ¹æ®GQAè®ºæ–‡çš„å®éªŒè¯´ï¼ŒMQAè™½ç„¶â€drasticallyâ€œæå‡äº†decoderä¸­çš„æ¨ç†æ€§èƒ½ï¼Œä½†è¿™æ ·åšä¼šå¸¦æ¥ç”Ÿæˆè´¨é‡çš„æ˜¾è‘—ä¸‹é™ä»¥åŠå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚æ‰€ä»¥ä¸ºäº†åœ¨ç‰ºç‰²æ›´å°æ€§èƒ½å‰æä¸‹åŠ é€Ÿï¼ŒGQAåº”è¿è€Œç”Ÿã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203000275-1984809569.jpg)

ä¸Šå›¾æ˜¾ç¤ºäº†ä»2022å¹´åˆ°2024å¹´æœŸé—´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ¼”å˜è¶‹åŠ¿ã€‚å¯ä»¥çœ‹å‡ºï¼ŒMHA æ­£åœ¨é€æ­¥æ·˜æ±°ï¼Œå¹¶è¢« GQA æ‰€å–ä»£ã€‚

### 3.1 æ¦‚å¿µ

GQAï¼ˆGrouped Query Attention/åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼‰ç”±è®ºæ–‡â€œGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpointsâ€æå‡ºï¼Œå®ƒé€šè¿‡åˆ†ç»„æŸ¥è¯¢çš„æ–¹å¼æ¥æé«˜ä¿¡æ¯å¤„ç†çš„æ•ˆç‡å’Œæ•ˆæœã€‚GQAçš„æ ¸å¿ƒæ”¹è¿›ç‚¹åœ¨äºï¼šè®© å¤šä¸ª Query å…±äº«å°‘é‡çš„ Key å’Œ Valueï¼Œå‡å°‘è®¡ç®—å¼€é”€ï¼Œå¹¶é€šè¿‡ åˆ†ç»„æœºåˆ¶ï¼ˆGrouping Mechanismï¼‰ è¿›è¡Œæ›´é«˜æ•ˆçš„è®¡ç®—ã€‚

GQAæ˜¯MHAå’ŒMQA ä¹‹é—´çš„æ³›åŒ–ï¼Œæˆ–è€…è¯´æ˜¯ä»‹äºMHAå’ŒMQAä¹‹é—´çš„æŠ˜ä¸­æ–¹æ¡ˆã€‚MHA æœ‰ H ä¸ª queryã€key å’Œ value å¤´ã€‚MQA åœ¨æ‰€æœ‰ query å¤´ä¸­å…±äº«å•ä¸ª key å’Œ value å¤´ã€‚è€ŒGQAä¸å†è®©æ‰€æœ‰æŸ¥è¯¢å¤´å…±äº«ç›¸åŒçš„å”¯ä¸€KVå¤´ï¼Œè€Œæ˜¯å°†æ‰€æœ‰çš„Qå¤´åˆ†æˆgç»„ï¼ŒåŒä¸€ç»„çš„Qå¤´å…±äº«ä¸€ä¸ªKå¤´ï¼ˆKey Headï¼‰å’Œä¸€ä¸ªVå¤´ï¼ˆValue Headï¼‰ã€‚

ä¸‹å›¾ä¸­4ä¸ªQå¤´ï¼ˆQuery Headsï¼‰è¢«åˆ†æˆ2ç»„ï¼Œæ¯ä¸ªç»„åŒ…å«2ä¸ªQå¤´ï¼Œæ¯ç»„åˆå¯¹åº”ä¸€ä¸ªKå¤´ï¼Œä¸€ä¸ªVå¤´ã€‚å›¾ä¸Šæ ‡å·1ä¸ºä¸€ç»„ï¼Œæ ‡å·2ä¸ºå¦å¤–ä¸€ç»„ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203151535-621628829.jpg)

ä¸‹å›¾æ˜¯GQAçš„å…¬å¼å’Œæµç¨‹ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203200605-1893859507.jpg)

è‹ç¥åˆ™æŒ‡å‡ºï¼ŒGQAå…¶å®æ˜¯ä¸€ä¸ª\\(x\_i\\)çš„ä½ç§©æŠ•å½±ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203210935-534637606.jpg)

### 3.2 æ¶æ„æ¯”å¯¹

GQAå·§å¦™åœ°ç»“åˆäº†MHAå’ŒMQAçš„å…ƒç´ ï¼Œåˆ›é€ äº†ä¸€ç§æ›´æœ‰æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚GQAæ˜¯åœ¨MHAå’ŒMQAä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œå°†KVå¤´çš„æ•°é‡ä»\\(n\\\_heads\\)å‡å°‘åˆ°\\(1<g<n\\\_heads\\)ï¼Œè€Œä¸æ˜¯å°†å¤´æ•°ä»\\(n\\\_heads\\)å‡å°‘åˆ°1ä¸ªKVå¤´ã€‚è¿™ä¸ªæ–°å‚æ•°gå¯ä»¥è¿™ä¹ˆè¡¨è¾¾ï¼š

\\\[g = \\frac{æ³¨æ„åŠ›å¤´æ•°}{KVå¤´æ•°} \\\]

å¼•å…¥è¿™ä¸ªå‚æ•°gä¹‹åï¼ŒGQAå°±æ„æˆäº†ä¸€ä¸ªç»Ÿä¸€è§†è§’ã€‚åœ¨è¿™ä¸ªè§†è§’ä¸‹ï¼ŒMHAå’ŒMQAéƒ½æ˜¯GQAçš„ç‰¹æ®Šæƒ…å†µï¼ˆåˆ†åˆ«å¯¹åº”äºg=1å’Œ g=\\(n\\\_heads\\)ï¼‰ã€‚

*   g = 1ï¼šç›¸å½“äºMQAï¼Œå³åœ¨æ‰€æœ‰ N ä¸ªå¤´ä¸­ä½¿ç”¨å…±äº«çš„é”®å’Œå€¼æŠ•å½±ã€‚
*   g = æ³¨æ„åŠ›å¤´æ•°ï¼šç›¸å½“äºMHAã€‚

GQAèƒ½æ›´é¡ºç•…åœ°åœ¨æ¨¡å‹å‡†ç¡®æ€§/KVç¼“å­˜å¤§å°ï¼ˆä¸æ—¶å»¶å’Œååé‡æœ‰å…³ï¼‰ï¼Œå’ŒMHAä»¥åŠMQAè¿™ä¸¤ä¸ªæç«¯ç”¨ä¾‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ–è€…è¯´ï¼ŒGQAæ¯ä¸ªç»„å†…æ˜¯ä¸€ä¸ªå°å‹çš„MQAï¼Œè€Œç»„é—´æ˜¯ä¼ ç»Ÿçš„MHAã€‚

å¤§å‹æ¨¡å‹çš„MHAä¼šå°†å•ä¸ªé”®å’Œå€¼å¤´å¤åˆ¶åˆ°æ¨¡å‹åˆ†åŒºçš„æ•°é‡ï¼ŒMQAä»£è¡¨äº†å†…å­˜å¸¦å®½å’Œå®¹é‡çš„æ›´å¤§å¹…åº¦çš„å‰Šå‡ï¼Œè€ŒGQA ä½¿æˆ‘ä»¬èƒ½å¤Ÿéšç€æ¨¡å‹å¤§å°çš„å¢åŠ ä¿æŒå¸¦å®½å’Œå®¹é‡çš„ç›¸åŒæ¯”ä¾‹ä¸‹é™ï¼Œå¯ä»¥ä¸ºè¾ƒå¤§çš„æ¨¡å‹æä¾›ç‰¹åˆ«å¥½çš„æƒè¡¡ã€‚GQA æ¶ˆé™¤äº†è¿™ç§åˆ†ç‰‡å¸¦æ¥çš„æµªè´¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¢„è®¡ GQA å°†ä¸ºè¾ƒå¤§çš„æ¨¡å‹æä¾›ç‰¹åˆ«å¥½çš„æƒè¡¡ã€‚

ä¸‹å›¾åˆ™ç»™å‡ºäº†ä¸‰è€…æ¶æ„ä¸Šçš„åŒºåˆ«ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203221768-1617176553.jpg)

### 3.3 å®ç°

åœ¨ç›®å‰å¤§éƒ¨åˆ†ä¸»æµè®­æ¨æ¡†æ¶æˆ–ç®—æ³•ï¼Œéƒ½å·²ç»æ”¯æŒMQA/GQAï¼Œæ¯”å¦‚FlashAttentionä¸­ï¼Œä¹Ÿæ”¯æŒMQAå’ŒGQAã€‚å¯¹äºMQAå’ŒGQAçš„æƒ…å½¢ï¼ŒFlashAttentioné‡‡ç”¨Indexingçš„æ–¹å¼ï¼Œè€Œä¸æ˜¯ç›´æ¥å¤åˆ¶å¤šä»½KV Headçš„å†…å®¹åˆ°æ˜¾å­˜ç„¶åå†è¿›è¡Œè®¡ç®—ã€‚Indexingï¼Œå³é€šè¿‡ä¼ å…¥KV/KV Headç´¢å¼•åˆ°Kernelä¸­ï¼Œç„¶åè®¡ç®—å†…å­˜åœ°å€ï¼Œç›´æ¥ä»å†…å­˜ä¸­è¯»å–KVã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203230087-2060532392.jpg)

é¡ºå¸¦ä¸€æï¼ŒGQA ä¸åº”ç”¨äºç¼–ç å™¨è‡ªæ³¨æ„åŠ›å±‚ï¼Œç¼–ç å™¨è¡¨ç¤ºæ˜¯å¹¶è¡Œè®¡ç®—çš„ï¼Œå› æ­¤å†…å­˜å¸¦å®½é€šå¸¸ä¸æ˜¯ä¸»è¦ç“¶é¢ˆã€‚

æˆ‘ä»¬ä½¿ç”¨llama3çš„ä»£ç æ¥è¿›è¡Œåˆ†æã€‚é¦–å…ˆç»™å‡ºåˆ©äºå­¦ä¹ çš„ç²¾ç®€ç‰ˆï¼Œç„¶åç»™å‡ºå®Œæ•´ç‰ˆã€‚

#### 3.3.1 ç²¾ç®€ç‰ˆ

ä¸ºäº†æ›´å¥½çš„åˆ†æï¼Œæˆ‘ä»¬ç»™å‡ºç²¾ç®€ç‰ˆä»£ç å¦‚ä¸‹ã€‚

æœ¬æ¥ MHA ä¸­ Query, Key, Value çš„çŸ©é˜µçš„å¤§å°ä¸º (batch\_size, n\_head, seq\_length, hidden\_size)ã€‚è€Œ GQA ä¸­ Query çš„å¤§å°ä¿æŒä¸å˜ï¼ŒKey, Value çš„çŸ©é˜µçš„å¤§å°å˜ä¸º (batch\_size, n\_head / group\_size, seq\_length, hidden\_size)ã€‚å³ï¼Œåœ¨GQAä¸­ï¼Œkeyå’Œvalueéƒ½è¦æ¯”queryå°groupå€ã€‚ä¸ºäº†åœ¨åç»­åšçŸ©é˜µä¹˜æ³•ï¼Œä¸€èˆ¬æœ‰ä¸¤ç§åšæ³•ï¼š

*   åˆ©äºå¹¿æ’­æœºåˆ¶æŠŠQKVçš„å½¢çŠ¶è¿›è¡Œè°ƒæ•´ï¼Œå³Query : (batch\_size, n\_head / group\_size, group\_size, seq\_length, hidden\_size)ï¼ŒKey : (batch\_size, n\_head / group\_size, 1, seq\_length, hidden\_size)ï¼ŒValue : (batch\_size, n\_head / group\_size, 1, seq\_length, hidden\_size)ã€‚ä½†æ˜¯è¿™æ ·éœ€è¦åšå¹¿æ’­å’Œæœ€ç»ˆåˆå¹¶çš„å¤„ç†ï¼Œè¦å¯¹ MHA çš„ä»£ç è¿›è¡Œå¤šå¤„ä¿®æ”¹ã€‚
    
*   æŠŠGQAæ‹“å±•åˆ°MHAå†è¿›è¡Œè®¡ç®—ï¼Œå³å…ˆæŠŠ`key`å’Œ`value`çš„`head`åˆ©ç”¨expandæ‰©å±•å¼ é‡åˆ°å’Œ`query`ç›¸åŒçš„ç»´åº¦ï¼Œç„¶åè¿›è¡Œè®¡ç®—ã€‚
    

    class Attention(nn.Module):
        def __init__(self, args: ModelArgs):
            self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
            model_parallel_size = fs_init.get_model_parallel_world_size()
            self.n_local_heads = args.n_heads // model_parallel_size
            self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
            self.n_rep = self.n_local_heads // self.n_local_kv_heads # è®¾å®šç»„æ•°ç›®
            self.head_dim = args.dim // args.n_heads
    
            # ç”¨self.n_kv_heads * self.head_dimåˆå§‹åŒ–ï¼Œå½“n_kv_headså°äºn_headsæ—¶ï¼Œå‚æ•°é‡å˜å°‘
            self.wq = ColumnParallelLinear(args.dim, args.n_heads * self.head_dim,)
            self.wk = ColumnParallelLinear(args.dim, self.n_kv_heads * self.head_dim,)
            self.wv = ColumnParallelLinear(args.dim, self.n_kv_heads * self.head_dim,)
            self.wo = RowParallelLinear(args.n_heads * self.head_dim, args.dim,)
    
            self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len,
                    self.n_local_kv_heads, self.head_dim,)).cuda()
            self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len,
                    self.n_local_kv_heads, self.head_dim,)).cuda()
    
        def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor,
            mask: Optional[torch.Tensor],
        ):
            bsz, seqlen, _ = x.shape
            xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
            xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
            xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
            xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
            xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
    
            self.cache_k = self.cache_k.to(xq)
            self.cache_v = self.cache_v.to(xq)
            self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
            self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv
       
            keys = self.cache_k[:bsz, : start_pos + seqlen]
            values = self.cache_v[:bsz, : start_pos + seqlen]
    
            '''
            self.n_rep = q_heads // kv_heads
            queryå¤´æ•°å¤§äºKVçš„å¤´æ•°ï¼Œä¸€å¯¹KVå¯¹åº”å¤šä¸ªqueryï¼Œéœ€è¦æŠŠæ¯ä¸ªKVå¤åˆ¶n_repä»½ï¼Œè¿™æ ·ç¬¬2ä¸ªç»´åº¦å°±å’Œqä¸€æ ·äº†
            å³ï¼Œnum_key_value_headså°±æ˜¯q_heads // kv_heads
            repeat_kvæ–¹æ³•å°†hidden statesä»(batch, num_key_value_heads, seqlen, head_dim) å˜æˆ (batch, num_attention_heads, seqlen, head_dim)ï¼Œç›¸å½“äºæ˜¯å¤åˆ¶äº†self.num_key_value_groupsä»½
            '''            
            # repeat k/v heads if n_kv_heads < n_heads
            keys = repeat_kv(keys, self.n_rep
            )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
            values = repeat_kv(values, self.n_rep
            )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
    
            xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
            keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
            values = values.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
            scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
            if mask is not None:
                scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)     
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)
            output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
            return self.wo(output)
    

repeat\_kv()å‡½æ•°ä»£ç å¦‚ä¸‹ã€‚ä¸ºä»€ä¹ˆè¦ç”¨expandä¹‹åå†reshapeè€Œä¸èƒ½ç›´æ¥ç”¨tensorè‡ªå¸¦çš„repeatï¼Ÿå› ä¸ºä½¿ç”¨expand()å‡½æ•°å¯ä»¥åœ¨è¿ç®—çš„æ—¶å€™èŠ‚çœå¾ˆå¤šæ˜¾å­˜ã€‚

*   `expand` æ–¹æ³•ç”¨äºå¯¹å¼ é‡è¿›è¡Œæ‰©å±•ï¼Œä½†ä¸å®é™…åˆ†é…æ–°çš„å†…å­˜ã€‚å®ƒè¿”å›çš„å¼ é‡ä¸åŸå§‹å¼ é‡å…±äº«ç›¸åŒçš„æ•°æ®
*   `repeat` æ–¹æ³•é€šè¿‡å®é™…å¤åˆ¶æ•°æ®æ¥æ‰©å±•å¼ é‡ã€‚å®ƒè¿”å›çš„æ–°å¼ é‡ä¸ä¸åŸå§‹å¼ é‡å…±äº«æ•°æ®ï¼Œæ‰©å±•åçš„å¼ é‡å ç”¨äº†æ›´å¤šçš„å†…å­˜ã€‚

    # å®šä¹‰è¾“å…¥xï¼Œ n_repæ˜¯éœ€è¦é‡å¤çš„æ¬¡æ•°ï¼Œåœ¨è¿™é‡Œä¸€èˆ¬æ˜¯ç»„æ•°
    def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
        """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
        bs, slen, n_kv_heads, head_dim = x.shape
        if n_rep == 1:
            return x
        return (
            # ç¬¬4ç»´è¿›è¡Œæ‰©ç»´ï¼Œæ‰©å±•æˆ5ç»´
            x[:, :, :, None, :] 
             # first we expand x to (bs, seq_len, head, group, head_dim)ï¼Œå³ç¬¬4ç»´ä»1æ‰©å±•ä¸ºn_rep
            .expand(bs, slen, n_kv_heads, n_rep, head_dim)  # è¿›è¡Œå¹¿æ’­ï¼Œkï¼Œvå‘é‡å…±äº«
             # reshape make head -> head * groupï¼Œç¼©æˆ4ç»´ï¼Œå³æŠŠç¬¬3ç»´ä»n_kv_headsæ‰©å±•n_repä»½
             # è¿™æ ·ç¬¬3ä¸ªç»´åº¦å°±å’Œqä¸€æ ·äº†
            .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
        )
    

#### 3.3.2 å®Œæ•´ç‰ˆ

å®Œæ•´ç‰ˆä»£ç å¦‚ä¸‹ã€‚

    class Attention(nn.Module):
        def __init__(self, args: ModelArgs):
            super().__init__()
            self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
            model_parallel_size = fs_init.get_model_parallel_world_size()
            self.n_local_heads = args.n_heads // model_parallel_size
            self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
            self.n_rep = self.n_local_heads // self.n_local_kv_heads
            self.head_dim = args.dim // args.n_heads
    
            self.wq = ColumnParallelLinear(
                args.dim,
                args.n_heads * self.head_dim,
                bias=False,
                gather_output=False,
                init_method=lambda x: x,
            )
            self.wk = ColumnParallelLinear(
                args.dim,
                self.n_kv_heads * self.head_dim,
                bias=False,
                gather_output=False,
                init_method=lambda x: x,
            )
            self.wv = ColumnParallelLinear(
                args.dim,
                self.n_kv_heads * self.head_dim,
                bias=False,
                gather_output=False,
                init_method=lambda x: x,
            )
            self.wo = RowParallelLinear(
                args.n_heads * self.head_dim,
                args.dim,
                bias=False,
                input_is_parallel=True,
                init_method=lambda x: x,
            )
    
            self.cache_k = torch.zeros(
                (
                    args.max_batch_size,
                    args.max_seq_len,
                    self.n_local_kv_heads,
                    self.head_dim,
                )
            ).cuda()
            self.cache_v = torch.zeros(
                (
                    args.max_batch_size,
                    args.max_seq_len,
                    self.n_local_kv_heads,
                    self.head_dim,
                )
            ).cuda()
    
        def forward(
            self,
            x: torch.Tensor,
            start_pos: int,
            freqs_cis: torch.Tensor,
            mask: Optional[torch.Tensor],
        ):
            bsz, seqlen, _ = x.shape
            xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
    
            xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
            xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
            xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
    
            xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
    
            self.cache_k = self.cache_k.to(xq)
            self.cache_v = self.cache_v.to(xq)
    
            self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
            self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv
    
            keys = self.cache_k[:bsz, : start_pos + seqlen]
            values = self.cache_v[:bsz, : start_pos + seqlen]
    
            # repeat k/v heads if n_kv_heads < n_heads
            keys = repeat_kv(
                keys, self.n_rep
            )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
            values = repeat_kv(
                values, self.n_rep
            )  # (bs, cache_len + seqlen, n_local_heads, head_dim)
    
            xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)
            keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)
            values = values.transpose(
                1, 2
            )  # (bs, n_local_heads, cache_len + seqlen, head_dim)
            scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
            if mask is not None:
                scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)
            output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
            return self.wo(output)
    

å¦å¤–ï¼Œå¯¹äºMQAå’ŒGQAçš„è§£ç é˜¶æ®µï¼Œä¸€ç§å¸¸ç”¨çš„ä¼˜åŒ–æŠ€å·§æ˜¯æŠŠå…±ç”¨ä¸€ä¸ªKVå¤´çš„æ‰€æœ‰QOå¤´ï¼Œä¸queryçš„è¡Œæ•°èåˆï¼ˆå› ä¸ºä»–ä»¬éœ€è¦è·Ÿç›¸åŒçš„KV-CacheåšAttentionè®¡ç®—ï¼‰ã€‚è¿™æ ·çš„æ•ˆæœæ˜¯å¢åŠ äº†æœ‰æ•ˆçš„è¡Œæ•°ï¼Œå¢åŠ äº†ç®—å­å¯†åº¦ï¼Œè‡ªå›å½’è§£ç é˜¶æ®µè™½ç„¶è¯´æŸ¥è¯¢çš„é•¿åº¦æ˜¯1ï¼Œä½†æ˜¯ç»è¿‡Head Groupèåˆä¹‹åï¼Œæœ‰æ•ˆè¡Œæ•°å¢å¤§åˆ° \\(H\_{QO}/H\_{KV}\\)ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203246431-805086330.jpg)

### 3.4 æ•ˆæœ

#### 3.4.1 å†…å­˜

GQAåœ¨æ¨ç†é˜¶æ®µå¯ä»¥æ˜¾è‘—é™ä½ KV Cache çš„å¤§å°ï¼Œä¸ºæ›´å¤§çš„ Batch Size æä¾›äº†ç©ºé—´ï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡ååã€‚

åœ¨MHAä¸‹ï¼Œå¯¹äºæ‰€æœ‰è¾“å…¥æ‰¹æ¬¡å’Œåºåˆ—ä¸­çš„æ¯ä¸ªtokenï¼ŒKVç¼“å­˜çš„æ€»å¤§å°å¯ä»¥ç”¨ä»¥ä¸‹å…¬å¼è¡¨ç¤ºï¼š

\\\[2 \\times B \\times L \\times H \\times D \\times N \\\]

*   Bä»£è¡¨batch sizeï¼Œ
*   Lä»£è¡¨æ€»åºåˆ—é•¿åº¦ï¼Œsequence lengthï¼ˆè¾“å…¥åºåˆ—+è¾“å‡ºåºåˆ—ï¼Œæˆ–è€…è¯´æ˜¯æç¤º + å®Œæˆéƒ¨åˆ†ï¼‰ï¼Œ
*   Hä»£è¡¨number of headï¼Œ
*   Dä»£è¡¨size of headï¼Œæ¯ä¸ªheadçš„ç»´åº¦ã€‚
*   Nä»£è¡¨å±‚æ•°

åœ¨MQAä¸‹ï¼Œæ¯ä¸ªtokençš„å¯¹åº”ä¸ºï¼š

\\\[2 \\times B \\times L\\times D \\times N \\\]

åœ¨GQAä¸‹ï¼Œæ¯ä¸ªtokençš„å¯¹åº”ä¸ºï¼š

\\\[2 \\times B \\times L\\times G \\times D\\times N \\\]

å…·ä½“æ¯”å¯¹ä¹Ÿå¯ä»¥å‚è€ƒä¸‹å›¾ï¼Œå…¶ä¸­ g æ˜¯KVå¤´çš„ç»„æ•°ï¼ˆ\\(ğ‘›\_â„/ğ‘”\\)ä¸ªHead å…±äº«ä¸€ä¸ªKVï¼‰ï¼Œh æ˜¯æŸ¥è¯¢çš„å¤´æ•° ï¼Œ\\(d\_k\\)æ˜¯å¤´ç»´åº¦ï¼Œl æ˜¯å±‚æ•°ï¼Œs æ˜¯åºåˆ—é•¿åº¦ï¼Œb æ˜¯batch sizeã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203256198-1237072538.jpg)

GQAå’ŒMQAåœ¨GPU ä¸Šçš„å®ç°å¸¦æ¥çš„æ”¶ç›Šæ¥ä¸»è¦è‡ªäºKV cache çš„å‡å°‘ï¼Œä»è€Œèƒ½æ”¾ä¸‹æ›´å¤šçš„tokenã€‚ä½†æ˜¯ï¼ŒGQAå’ŒMQAçš„æ€§èƒ½å®¹æ˜“å—åˆ°å¹¶è¡Œç­–ç•¥çš„å½±å“ã€‚å¦‚æœGQA kernelåœ¨Q headç»´åº¦åšå¹¶è¡Œï¼ˆä¸€ä¸ªQ headæ˜¯ä¸€ä¸ªblockï¼‰ï¼Œåˆ™ä¼šå¯¼è‡´å…±äº«ä¸€ä¸ªKV head çš„block è¢«è°ƒåº¦åœ¨ä¸åŒçš„SMä¸Šï¼Œæ¯ä¸ªSM éƒ½ä¼šå¯¹åŒä¸€ä»½KV head åšé‡å¤åŠ è½½ã€‚åˆ™å†…å­˜å‡å°‘çš„æ”¶ç›Šä¼šå¤§å¤§é™ä½ã€‚å¦å¤–ï¼ŒåŠ è½½ KV æ˜¯MHA å’Œ GQA çš„ç“¶é¢ˆã€‚å› æ­¤éœ€è¦å‡å°‘Q headçš„å¹¶è¡Œåº¦ã€‚

#### 3.4.2 é€Ÿåº¦

GQAå¹¶æ²¡æœ‰é™ä½Attentionçš„è®¡ç®—é‡ï¼ˆFLOPsï¼‰ï¼Œå› ä¸ºKeyã€Valueæ˜ å°„çŸ©é˜µä¼šä»¥å¹¿æ’­å˜é‡çš„å½¢å¼æ‹“å±•åˆ°å’ŒMHAå’Œä¸€æ ·ï¼Œå› æ­¤è®¡ç®—é‡ä¸å˜ï¼Œåªæ˜¯Keyã€Valueå‚æ•°å…±äº«ã€‚ä½†æ˜¯ï¼Œå› ä¸ºGQA å°†æŸ¥è¯¢çŸ©é˜µ Q åˆ†æˆå¤šä¸ªç»„ï¼Œæ¯ä¸ªç»„åˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°å’ŒåŠ æƒæ±‚å’Œã€‚è¿™æ ·ä¸€æ¥ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´åªéœ€è¦è®¡ç®—ä¸€éƒ¨åˆ†æŸ¥è¯¢çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»è€Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿åºåˆ—æ—¶ã€‚æ‰€ä»¥ï¼Œè™½ç„¶GQA çš„ QKV è®¡ç®—é‡æ²¡æœ‰å‡å°‘ï¼Œä½†æ˜¯é€Ÿåº¦å¾—åˆ°äº†å¾ˆå¤§æé«˜ï¼Œé€Ÿåº¦æé«˜çš„åŸå› å’ŒMQAç›¸åŒã€‚

#### 3.4.3 è¡¨å¾èƒ½åŠ›

GQAæ—¢ä¿ç•™äº†å¤šå¤´æ³¨æ„åŠ›çš„ä¸€å®šè¡¨è¾¾èƒ½åŠ›ï¼Œåˆé€šè¿‡å‡å°‘å†…å­˜è®¿é—®å‹åŠ›æ¥åŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚

è®ºæ–‡â€GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpointsâ€œç ”ç©¶äº†æ¨¡å‹çš„ç²¾åº¦å’Œæ¨ç†æ•ˆç‡ã€‚è®ºæ–‡ä½œè€…é‡‡ç”¨T5æ¨¡å‹ä½œä¸ºç ”ç©¶å¯¹è±¡ï¼Œæ¨¡å‹ç‰ˆæœ¬é‡‡ç”¨T5-Largeå’ŒT5-XXLã€‚ä¸‹å›¾ä¸­ï¼Œæ¨ªè½´ä»£è¡¨å¹³å‡æ¯æ¡æ ·æœ¬çš„æ¨ç†è€—æ—¶ï¼Œè¶Šå¤§ä»£è¡¨å»¶è¿Ÿè¶Šå¤§ï¼Œçºµè½´ä»£è¡¨åœ¨ä¼—å¤šæ•°æ®é›†ä¸Šçš„è¯„ä»·å¾—åˆ†ï¼Œè¶Šå¤§ä»£è¡¨å¾—åˆ†è¶Šé«˜ã€‚

ä¸‹å›¾è¡¨æ˜ï¼ŒMQAç•¥å¾®æŸå¤±äº†æ¨¡å‹ç²¾åº¦ï¼Œä½†æ˜¯ç¡®å®èƒ½å¤Ÿå¤§å¹…é™ä½æ¨ç†å¼€é”€ï¼Œè€Œå¦‚æœé€‰æ‹©äº†åˆé€‚çš„åˆ†ç»„æ•°ï¼ŒGQAèƒ½å¤Ÿä¸¤è€…çš†å¾—ã€‚GQAçš„è¡¨å¾èƒ½åŠ›æ˜¾è‘—é«˜äºMQAï¼Œå‡ ä¹è·ŸMHAä¸€è‡´ï¼ˆGQAè¿˜æ˜¯æœ‰å¯èƒ½å¯¼è‡´ç²¾åº¦çš„æŸå¤±ï¼‰ï¼Œè€Œä¸”æ¨ç†é€Ÿåº¦ä¸ŠGQAè·ŸMQAçš„åŒºåˆ«ä¸å¤§ï¼Œæ¯”èµ·MHAä¾æ—§æœ‰æ˜¾è‘—æå‡ã€‚å…¶ä¸­ï¼ŒGQAçš„åˆ†ç»„æ•°æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œç»„æ•°è¶Šå¤§è¶Šæ¥è¿‘MHAï¼Œæ¨ç†å»¶è¿Ÿè¶Šå¤§ï¼ŒåŒæ—¶æ¨¡å‹ç²¾åº¦ä¹Ÿè¶Šé«˜ã€‚å¦å¤–ï¼Œä¹Ÿå¯ä»¥å¢åŠ æ¨¡å‹æ·±åº¦æ¥ç¼“è§£æ¨¡å‹æ•ˆæœçš„ä¸‹é™ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203304079-756174985.jpg)

### 3.5 è½¬æ¢

è™½ç„¶æœ€æ–°çš„æ¨¡å‹åŸºæœ¬éƒ½åœ¨é¢„è®­ç»ƒé˜¶æ®µé»˜è®¤é‡‡ç”¨ GQAï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ€è€ƒä¸‹ï¼Œå¦‚ä½•å°†å·²ç»è®­ç»ƒå¥½çš„MHAç»“æ„çš„æ¨¡å‹è½¬æ¢æˆMQAæˆ–è€…GQAï¼Ÿ

#### 3.5.1 å¹³å‡æ± åŒ–

å¦‚æœæ˜¯ä»å·²æœ‰çš„ multi-head model å¼€å§‹ç»§ç»­è®­ç»ƒ multi-query model (Uptraining)ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹MHAçš„å¤´è¿›è¡Œåˆ†ç»„ï¼Œé€šè¿‡å¯¹è¯¥ç»„ä¸­æ‰€æœ‰åŸå§‹å¤´è¿›è¡Œå¹³å‡æ± åŒ–ï¼ˆmean poolï¼‰æ¥æ„å»ºæ¯ä¸ªç»„çš„é”®å’Œå€¼å¤´ï¼Œç„¶åç»§ç»­è¿›è¡Œé¢„è®­ç»ƒå³å¯ã€‚å®éªŒè¯æ˜mean poolçš„æ˜ å°„æ•ˆæœå¥½äºé€‰åˆ™ç¬¬ä¸€ä¸ªheadæˆ–è€…ä»»æ„åˆå§‹åŒ–ã€‚äººä»¬æŠŠè¿™ä¸ªè®­ç»ƒè¿‡ç¨‹å«åšuptrainingã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203312168-1095087345.jpg)

å…·ä½“å‚è€ƒä»£ç å¦‚ä¸‹ã€‚

    import torch.nn as nn
    
    n_heads=4
    n_kv_heads=2
    hidden_size=3
    group = n_heads // n_kv_heads
    k_proj = nn.Linear(hidden_size, n_heads) 
    
    # mean poolæ“ä½œ
    k_proj_4d = k_proj.weight.data.unsqueeze(dim=0).unsqueeze(dim=0)
    pool=nn.AvgPool2d(kernel_size=(group,1))
    pool_out = pool(k_proj_4d).squeeze(dim=0).squeeze(dim=0)
    
    k_proj_gaq = nn.Linear(hidden_size, n_kv_heads)
    k_proj_gaq.weight.data = pool_out
    

#### 3.5.2 åŸºäºæ©ç 

è®ºæ–‡â€Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQAâ€œæå‡ºäº†ä¸€ç§ä½æˆæœ¬æ–¹æ³•ï¼Œå¯å°† MHA æ¨¡å‹æŒ‰ä»»æ„ KV Head å‹ç¼©æ¯”ä¿®å‰ªä¸º GQA æ¨¡å‹ã€‚è¯¥æ–¹æ³•åŸºäº \\(L\_0\\) æ©ç é€æ­¥å‰”é™¤å†—ä½™å‚æ•°ã€‚æ­¤å¤–ï¼Œåœ¨ä¸æ”¹å˜æ¨¡å‹çš„å‰æä¸‹ï¼Œå¯¹æ³¨æ„åŠ›å¤´æ–½åŠ æ­£äº¤å˜æ¢ï¼Œä»¥åœ¨ä¿®å‰ªè®­ç»ƒå‰æå‡ Attention Head é—´çš„ç›¸ä¼¼åº¦ï¼Œä»è€Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚

å…·ä½“æ–¹æ¡ˆåˆ†ä¸ºå¦‚ä¸‹å‡ æ­¥ï¼šç½‘ç»œè½¬æ¢ï¼›è¿›è¡Œåˆ†ç»„ï¼›å‰ªæè®­ç»ƒã€‚

##### ç½‘ç»œè½¬æ¢

è¿™ä¸€æ­¥æ˜¯åœ¨å‰ªæè®­ç»ƒä¹‹å‰ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè½¬æ¢ã€‚å…·ä½“çš„è¿‡ç¨‹å¤§æ¦‚ä¸ºï¼š

*   ä½¿ç”¨éƒ¨åˆ† C4 çš„è®­ç»ƒé›†æ¥æ”¶é›†ç›¸åº”çš„ KV Cacheï¼Œè¿™æ ·æ‰èƒ½å¯¹KV Cacheè¿›è¡Œæ›´æœ‰æ•ˆçš„åˆ†æã€‚
*   åŸºäºä½™å¼¦ç›¸ä¼¼æ€§æˆ–è€…æ¬§æ°è·ç¦»ï¼Œè®¡ç®—æœ€ä¼˜çš„æ­£äº¤çŸ©é˜µã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203324257-2099402571.jpg)

*   å°†è®¡ç®—å¾—åˆ°çš„æ­£äº¤çŸ©é˜µèåˆåˆ°å¯¹åº”çš„ Qã€Kã€V æŠ•å½±çŸ©é˜µä¸­ï¼Œä¿è¯è®¡ç®—ä¸å˜æ€§ã€‚å› ä¸ºRoPEçš„åŸå› ï¼Œæ‰€ä»¥å¯¹äº Q å’Œ K çš„æŠ•å½±çŸ©é˜µï¼Œåˆ†åˆ«åœ¨å­ç©ºé—´åº”ç”¨æ­£äº¤å˜æ¢ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203333938-279084538.jpg)

é€šè¿‡æ­£äº¤å˜æ¢ï¼Œå¯ä»¥ä½¿å¾—åŒä¸€ç»„å†…ä¸åŒ Attention Head åœ¨ç‰¹å¾ç©ºé—´ä¸­æ›´åŠ æ¥è¿‘ï¼Œä»è€Œåœ¨åç»­çš„å‰ªæè®­ç»ƒè¿‡ç¨‹ä¸­æ›´å®¹æ˜“æ‰¾åˆ°åˆé€‚çš„å‚æ•°å…±äº«æ–¹å¼ï¼Œæé«˜æ¨¡å‹çš„å‹ç¼©æ•ˆæœå’Œæ€§èƒ½ã€‚

##### æ‰¾åˆ°æ›´å¥½çš„åˆ†ç»„æ–¹æ³•

åœ¨è·å–äº†æ¯å¯¹ Attention Head ä¹‹é—´çš„ç›¸ä¼¼åº¦è¯„åˆ†åï¼Œå¯ä¾æ®è¿™äº›è¯„åˆ†å¯¹ Attention Head è¿›è¡Œé‡æ–°åˆ†ç»„ã€‚å•ä¸ªç»„çš„ç›¸ä¼¼åº¦è¯„åˆ†æ˜¯è¯¥ç»„å†…æ¯å¯¹ Attention Head ä¹‹é—´ç›¸ä¼¼åº¦è¯„åˆ†çš„æ€»å’Œï¼Œè€Œæ¯ç§åˆ†ç»„ç»“æœçš„æ€»ç›¸ä¼¼åº¦è¯„åˆ†åˆ™æ˜¯æ‰€æœ‰ç»„ç›¸ä¼¼åº¦è¯„åˆ†çš„ç´¯åŠ ã€‚ç®—æ³•çš„ç›®æ ‡æ˜¯æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„åˆ†ç»„æ–¹æ³•ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203344477-728978103.jpg)

åˆç†çš„åˆ†ç»„æ–¹å¼å¯ä»¥ä½¿å¾—åŒä¸€ç»„å†…çš„ Attention Head åœ¨ç‰¹å¾ç©ºé—´ä¸­æ›´åŠ ç›¸ä¼¼ï¼Œä»è€Œåœ¨å‰ªææ—¶æ›´å®¹æ˜“æ‰¾åˆ°åˆé€‚çš„å‚æ•°å…±äº«æ–¹å¼ï¼Œæé«˜æ¨¡å‹çš„å‹ç¼©æ•ˆæœå’Œæ€§èƒ½ã€‚

##### å‰ªæè®­ç»ƒ

æ­¤æ­¥éª¤ä¼šé€šè¿‡å‰ªæè®­ç»ƒï¼Œé€æ­¥å°†åŸå§‹çš„ KV Head è½¬ç§»åˆ°æ–°çš„ KV Head ä¸Šï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚å¦‚ä¸‹å›¾ æ‰€ç¤ºï¼Œå…·ä½“è¿‡ç¨‹åŒ…æ‹¬ï¼š

*   æ·»åŠ æ–°çš„æŠ•å½±çŸ©é˜µï¼šåœ¨æ¯ç»„å†…ä½¿ç”¨ Mean Pooling åˆå§‹åŒ–æ–°çš„æŠ•å½±çŸ©é˜µã€‚
*   åº”ç”¨ \\(L\_0\\) æ©ç ï¼šå¼•å…¥ \\(L\_0\\) æ©ç æ¥æ§åˆ¶åŸå§‹ KV Head å’Œæ–° KV Head ä¹‹é—´çš„è½¬æ¢ã€‚åˆå§‹æ—¶ï¼Œæ©ç å€¼ä¸º 1ï¼Œè¡¨ç¤ºä½¿ç”¨åŸå§‹ KV Headï¼›åœ¨å‰ªæè¿‡ç¨‹ä¸­ï¼Œé€æ­¥å°†æ©ç å€¼çº¦æŸä¸º 0ï¼ˆè¡¨ç¤ºä½¿ç”¨æ–°çš„ KV Headï¼‰ã€‚
*   çŸ¥è¯†è’¸é¦ï¼šä½¿ç”¨ KL æŸå¤±å’Œ BiLD æŸå¤±ï¼Œé¼“åŠ±å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºå¯¹é½ï¼Œä»è€Œä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203351548-1094520492.jpg)

### 3.6 ä¼˜åŒ–

è®ºæ–‡â€œA Survey on Large Language Model Acceleration based on KV Cache Managementâ€ç»™å‡ºäº†MQAã€GQAä»¥åŠå…¶æ”¹è¿›æ–¹æ¡ˆçš„æ€»ç»“ï¼Œå…·ä½“å‚è§ä¸‹å›¾ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203400480-1535487143.jpg)

å‡ ç§æ”¹è¿›æ–¹æ¡ˆå…·ä½“å¦‚ä¸‹ã€‚

*   åŠ æƒGQAï¼ˆWeighted GQAï¼‰ä¸ºæ¯ä¸ªé”®å’Œå€¼å¤´å¼•å…¥äº†é¢å¤–çš„å¯è®­ç»ƒæƒé‡ï¼Œè¿™äº›æƒé‡å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„GQAæ¨¡å‹ä¸­ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´æƒé‡ï¼Œå®ƒå¯ä»¥åœ¨ä¸å¢åŠ é¢å¤–æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203412355-945813625.jpg)

*   AsymGQAé€šè¿‡æå‡ºæ¿€æ´»é€šçŸ¥åˆå¹¶ç­–ç•¥ï¼ˆactivationinformed merging strategyï¼‰æ¥æ‰©å±•GQAã€‚AsymGQAä¸æ˜¯é€šè¿‡ç»Ÿä¸€èšç±»ï¼ˆuniform clusteringï¼‰å¯¹å¤´è¿›è¡Œåˆ†ç»„ï¼Œè€Œæ˜¯æ ¹æ®è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¿€æ´»ç›¸ä¼¼æ€§æ¥åŠ¨æ€ç¡®å®šå¦‚ä½•åˆ†ç»„ï¼Œå¹¶æ„å»ºä¸å¯¹ç§°çš„ç»„ï¼Œä»è€Œå®ç°æ›´å¥½çš„ä¼˜åŒ–å’Œæ³›åŒ–ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203419675-2098432331.jpg)

*   QCQAåˆ©ç”¨è¿›åŒ–ï¼ˆevolutionaryï¼‰ç®—æ³•æ¥è¯†åˆ«GQAçš„æœ€ä½³æŸ¥è¯¢å¤´åˆ†ç»„ï¼Œè¯¥ç®—æ³•ç”±ä¸€ä¸ªè®¡ç®—é«˜æ•ˆçš„é€‚åº”åº¦ï¼ˆcomputationally efficient fitnessï¼‰å‡½æ•°æŒ‡å¯¼ï¼Œè¯¥å‡½æ•°åˆ©ç”¨æƒé‡å…±äº«ï¼ˆweight-sharingï¼‰è¯¯å·®å’ŒKVç¼“å­˜æ¥è¯„ä¼°æ–‡æœ¬ç”Ÿæˆè´¨é‡å’Œå†…å­˜å®¹é‡ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203428957-701936971.jpg)

*   KDGQAè®¤ä¸ºï¼ŒGQAçš„è®¸å¤šå˜ä½“é‡‡ç”¨å›ºå®šçš„åˆ†ç»„ç­–ç•¥ï¼Œå› æ­¤ç¼ºä¹å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­é”®å€¼äº¤äº’æ¼”å˜çš„åŠ¨æ€é€‚åº”æ€§ã€‚ä»–ä»¬çš„Dynamic Key-Driven GQAé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨key head normsè‡ªé€‚åº”åœ°åˆ†ç»„æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ç§çµæ´»çš„ç­–ç•¥æ¥å°†æŸ¥è¯¢å¤´åˆ†ç»„å¹¶æé«˜æ€§èƒ½ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203437938-572317046.jpg)

*   GQKVAæå‡ºäº†åˆ†ç»„ç­–ç•¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šç”¨çš„æŸ¥è¯¢ã€é”®å’Œå€¼åˆ†ç»„æœºåˆ¶ã€‚å®ƒé¦–å…ˆä»‹ç»äº†MKVAå’ŒGKVAï¼Œå…¶ä¸­é”®å’Œå€¼è¢«åˆ†ç»„ä»¥å…±äº«åŒä¸€ä¸ªæŸ¥è¯¢ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¯¥è®ºæ–‡æå‡ºä½¿ç”¨GQKVAå°†æŸ¥è¯¢å’Œé”®å€¼å¯¹åˆ†å¼€åˆ†ç»„ã€‚é€šå¸¸ï¼ŒæŸ¥è¯¢è¢«åˆ’åˆ†ä¸º\\(g\_q\\)ç»„ï¼Œé”®å€¼è¢«åˆ’åˆ†ä¸º\\(g\_{kv}\\)ç»„ï¼ŒæŸ¥è¯¢å’Œé”®å€¼å¯¹çš„æ¯ä¸ªç»„åˆéƒ½ä¼šä½¿ç”¨ç‚¹ç§¯æ³¨æ„åŠ›è¿›è¡Œäº¤äº’ã€‚è¿™å¯¼è‡´\\(g\_qÃ—g\_{kv}\\)äº§ç”Ÿä¸åŒçš„è¾“å‡ºã€‚GQKVAåœ¨æŸ¥è¯¢ã€é”®å’Œå€¼ä¸Šæ¨å¹¿äº†ä¸åŒçš„ç»„ç­–ç•¥ï¼Œå¹¶ä¿æŒäº†è‰¯å¥½çš„è®¡ç®—æ•ˆç‡å’Œä¸MHAç›¸å½“çš„æ€§èƒ½ã€‚ä¸‹å›¾å±•ç¤ºäº†åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å¯¹æŸ¥è¯¢ã€é”®å’Œå€¼è¿›è¡Œåˆ†ç»„çš„å„ç§ç­–ç•¥ï¼ŒåŒ…æ‹¬Vanilla MHAã€MQAã€GQAã€MKVAã€GKVAå’ŒGQKVAã€‚

![](https://img2024.cnblogs.com/blog/1850883/202504/1850883-20250413203445313-1209222049.jpg)

0xFF å‚è€ƒ
-------

[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpointsarxiv.org/pdf/2305.13245.pdf](https://arxiv.org/pdf/2305.13245.pdf)

[ã€LLM åŠ é€ŸæŠ€å·§ã€‘Muti Query Attention å’Œ Attention with Linear Biasï¼ˆé™„æºç ï¼‰](https://zhuanlan.zhihu.com/p/634236135) [ä½•æ](https://www.zhihu.com/people/who-u)

[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)

[2ä¸‡å­—é•¿æ–‡ï¼ä¸€æ–‡äº†è§£Attentionï¼Œä»MHAåˆ°DeepSeek MLAï¼Œå¤§é‡å›¾è§£ï¼Œéå¸¸è¯¦ç»†ï¼](https://mp.weixin.qq.com/s?__biz=MzUzOTgwNDMzOQ==&mid=2247502844&idx=1&sn=067165341bbfeba775fa4301a9d1095e&chksm=fb47e670f9237047b5c94d0657212d88d53640c54f05784fbce8bba9e086deb11d5ad0f7e84c&mpshare=1&scene=1&srcid=0228ATe44326dTsGNtdpPNl2&sharer_shareinfo=f6c641fcb6b4d0caf449ce78dc907e41&sharer_shareinfo_first=f6c641fcb6b4d0caf449ce78dc907e41#rd) ShuYini \[AINLPer\](javascript:void(0)ğŸ˜‰

[ä»MHAã€MQAã€GQAåˆ°MLA](https://zhuanlan.zhihu.com/p/700588653) [è‹å‰‘æ—](https://www.zhihu.com/people/su-jian-lin-22)

[é˜¿é‡Œä¸€é¢ä»£ç é¢˜ï¼š"å®ç°ä¸€ä¸‹ GQA"](https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&mid=2247487324&idx=1&sn=cc79e02b124278f3d07067c355390abc&chksm=fb845df40f104ca547107d0dfaeb7424bc67e3c49d30961eecf5b98673df8c93794121484b21&mpshare=1&scene=1&srcid=0213jOpb0yYHTU7MRuvxFq7x&sharer_shareinfo=669a538ad10f3b46605953dd65cb7500&sharer_shareinfo_first=669a538ad10f3b46605953dd65cb7500#rd) çœ‹å›¾å­¦ \[çœ‹å›¾å­¦\](javascript:void(0)ğŸ˜‰

[MHA -> GQAï¼šæå‡ LLM æ¨ç†æ•ˆç‡](https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==&mid=2247488906&idx=1&sn=e2038e8b907c9b703354481ed0193af9&chksm=c2437164308b699ffe83a81842f17e611351c867b5b51fc3ee58bd6e7628a5b0c7716e52c26e&mpshare=1&scene=1&srcid=0115XLnq4kZjRAdZ8tI4DzYD&sharer_shareinfo=6f5890ca41e9b97d037f34b4c9518848&sharer_shareinfo_first=6f5890ca41e9b97d037f34b4c9518848#rd) AIé—²è°ˆ \[AIé—²è°ˆ\](javascript:void(0)ğŸ˜‰

[Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA](https://arxiv.org/abs/2412.20677)

[FLASHINFER: EFFICIENT AND CUSTOMIZABLE ATTENTION ENGINE FOR LLM INFERENCE SERVING](https://arxiv.org/pdf/2501.01005)

[FlashInferä¸­DeepSeek MLAçš„å†…æ ¸è®¾è®¡](https://zhuanlan.zhihu.com/p/25920092499) [yzh119](https://www.zhihu.com/people/wuyu-98-91)

[å¤§æ¨¡å‹å¹¶è¡Œæ¨ç†çš„å¤ªç¥–é•¿æ‹³ï¼šè§£è¯»Jeff Deanç½²åMLSys 23æ°å‡ºè®ºæ–‡](https://zhuanlan.zhihu.com/p/660715870) æ–¹ä½³ç‘

[ç”±GQAæ€§èƒ½æ•°æ®å¼‚å¸¸å¼•å‘çš„å¯¹MHAï¼ŒGQAï¼ŒMQA åœ¨GPUä¸Šçš„æ„Ÿæ€§åˆ†æ](https://zhuanlan.zhihu.com/p/708776013) [ä»£ç æ¬è¿å·¥](https://www.zhihu.com/people/fly-zhai)

[MHA->MQA->GQA->MLAçš„æ¼”è¿›ä¹‹è·¯](https://zhuanlan.zhihu.com/p/22590523172) [å‡å¦‚ç»™æˆ‘ä¸€åªAI](https://www.zhihu.com/people/ai-81-85-59)

Y. Chen, C. Zhang, X. Gao, R. D. Mullins, G. A. Constantinides, and Y. Zhao, â€œOptimised Grouped-Query Attention Mechanism for Transformers,â€ in Workshop on Efficient Systems for Foundation Models II @ ICML2024, Jul. 2024. \[Online\]. Available: [https://openreview.net/forum?id=13MMghY6Kh](https://openreview.net/forum?id=13MMghY6Kh)

S. S. Chinnakonduru and A. Mohapatra, â€œWeighted Grouped Query Attention in Transformers,â€ Jul. 2024. \[Online\]. Available: [http://arxiv.org/abs/2407.10855](http://arxiv.org/abs/2407.10855)

V. Joshi, P. Laddha, S. Sinha, O. J. Omer, and S. Subramoney, â€œQCQA: Quality and Capacity-aware grouped Query Attention,â€ Jun. 2024. \[Online\]. Available: [http://arxiv.org/abs/2406.10247](http://arxiv.org/abs/2406.10247)

Z. Khan, M. Khaquan, O. Tafveez, B. Samiwala, and A. A. Raza, â€œBeyond Uniform Query Distribution: Key-Driven Grouped Query Attention,â€ Aug. 2024. \[Online\]. Available: [http://arxiv.org/abs/2408.08454](http://arxiv.org/abs/2408.08454)

F. Javadi, W. Ahmed, H. Hajimolahoseini, F. Ataiefard, M. Hassanpour, S. Asani, A. Wen, O. M. Awad, K. Liu, and Y. Liu, â€œGQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values,â€ Dec. 2023. \[Online\]. Available: [http://arxiv.org/abs/2311.03426](http://arxiv.org/abs/2311.03426)