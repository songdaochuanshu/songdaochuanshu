---
layout: post
title: 'SAM3模型来了，手把手带你运行SAM3模型代码，SAM3模型初探！'
date: "2025-12-07T00:48:19Z"
---
SAM3模型来了，手把手带你运行SAM3模型代码，SAM3模型初探！
==================================

![SAM3模型来了，手把手带你运行SAM3模型代码，SAM3模型初探！](https://img2024.cnblogs.com/blog/3687401/202512/3687401-20251206172547401-124163000.png) Meta开源SAM3图像分割模型，支持文本提示精准分割目标。本文提供Windows本地部署详细教程，包含环境配置、依赖安装、权重下载等完整步骤，并解决triton缺失和权重访问等常见问题。通过修改源码加载本地权重文件sam3.pt，即可实现文本提示分割功能。教程附带测试代码示例，支持自定义文本提示词进行目标分割。作者还提供了后续进阶玩法预告，包括视频追踪等应用场景。

[SAM3模型来了，手把手带你运行SAM3模型代码，SAM3模型初探！](https://www.bilibili.com/video/BV1YVUWBrEDC)

大家好，我是 Coding 茶水间。

Meta 终于把 SAM3 开源了！ 虽然论文和模型架构早就放出来了，但权重一直锁着，直到前两天才真正公开。 我第一时间冲去跑通，效果真的逆天——纯文本提示就能精准分割多个同类目标，还带 ID 和置信度，完爆 SAM2 的点+框交互方式。

先直接上干货，下方是原始图像：

输入提示词 "shoe"，结果：

再换提示词 "child"，结果：

这才是真正的“语义理解+分割”啊！ 支持文本、参考图像、视频追踪等多模态提示，潜力巨大。

下面我把整个 Windows 本地跑通过程手把手写成最详细图文教程，包含我踩过的所有坑 + 解决方案，跟着做，100%能跑起来。

### 环境要求（必须满足）

*   NVIDIA 独显（AMD/核显无解）
*   CUDA ≥ 12.6（强烈建议更新到最新显卡驱动）
*   已安装 Conda（Miniconda 或 Anaconda 都行）

### 完整步骤（已亲测 4090 + Win11 完美运行）

1.  创建并激活环境

    conda create -n sam3 python=3.12 -y
    conda activate sam3

2.  安装 PyTorch（必须这个版本，官方指定）

    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126

3.  下载源码（推荐直接下载 zip，避免 git 克隆不稳定）

GitHub 地址：[https://github.com/facebookresearch/sam3](https://github.com/facebookresearch/sam3)

下载 ZIP → 解压 → 打开文件夹

4.  进入项目目录，安装核心依赖

    cd path\to\sam3-main
    pip install -e .

5.  安装示例所需额外包

    pip install opencv-python matplotlib pandas tqdm

### 重大坑位一：Windows 下缺少 triton 包

直接运行会报 No module named 'triton' 官方 triton 不支持 Windows，但社区大佬已编译好替代版：

    pip install triton-windows==3.3.0.post19

装完这步基本解决 90% 人的卡死问题

### 重大坑位二：权重无法下载（最致命）

SAM3 权重放在 Hugging Face，需要申请访问权限。 我申请了 → 被拒了 很多人也一样被拒，目前 Meta 不知道按什么标准批。

好在社区已经有人放出了直链，我下载好了： 文件：sam3.pt 大小约 3.2GB

把 sam3.pt 复制到项目根目录（和 sam3 文件夹同级）

然后修改源码，让它加载本地权重（否则默认去 HF 下载，会一直网络错误）

打开文件：sam3/model\_builder.py

找到以下两处（Ctrl+F 搜索关键字）：

    load_from_hf = True   → 改成 False

    checkpoint_path = None   → 改成 "sam3.pt"

改完保存。

### 最终测试代码

新建 main.py 文件，粘贴以下代码：

    import torch
    import matplotlib.pyplot as plt
    from PIL import Image
    
    from sam3.model_builder import build_sam3_image_model
    from sam3.model.sam3_image_processor import Sam3Processor
    from sam3.visualization_utils import plot_results
    
    # 加载模型（会自动读取本地 sam3.pt）
    model = build_sam3_image_model()
    processor = Sam3Processor(model)
    
    # 加载测试图片
    image = Image.open("assets/images/test_image.jpg")
    
    # 设置图像（这一步会做全图编码）
    inference_state = processor.set_image(image)
    
    # 文本提示分割（换成你想要的词）
    inference_state = processor.set_text_prompt(state=inference_state, prompt="child")
    # 或者分割鞋子：prompt="shoe"
    # 或者试试：prompt="foot" / "sock" / "person" / "hat" 都好使
    
    # 可视化结果（我修复了官方 plot_results 没 plt.show() 的 bug）
    plot_results(image, inference_state)
    plt.show()  # 加上这句才能弹出图片

运行 python main.py

第一次运行会稍微慢一点（加载模型），之后就很快了。

至此，SAM3 就在你本地完美跑起来了！ 后续我还会继续出：

*   参考图像分割
*   视频目标追踪
*   结合 CLIP 做更复杂提示
*   自定义数据集微调等进阶玩法

系列都会持续更新，建议关注 + 收藏，随时来看最新进展～

### 最后：关于权重文件 sam3.pt

我这里已经下载好了（3.2GB，完整官方权重）

需要的朋友请： 三连 + 关注 + 在评论区或私信留邮箱

我看到会统一打包发给大家（免费，手动发，可能稍微慢一点，耐心等哈）

手动码字+截图三小时，全程干货，如果觉得有用，麻烦给个三连鼓励一下～

我们下期见！