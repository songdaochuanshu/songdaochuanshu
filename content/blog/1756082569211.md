---
layout: post
title: 'DeepSeek采用的UE8M0 FP8 为什么引爆了A股的芯片板块'
date: "2025-08-25T00:42:49Z"
---
DeepSeek采用的UE8M0 FP8 为什么引爆了A股的芯片板块
==================================

DeepSeek 这次的重点不在模型 V3.1，而是在DeepSeek在其官宣发布DeepSeek-V3.1的文章中提到，DeepSeek-V3.1使用了UE8MO FP8 Scale的参数精度。另外，V3.1对分词器及chat template进行了较大调整，与DeepSeek-V3存在明显差异。DeepSeek官微在置顶留言里表示UE8MO FP8是针对即将发布的下一代国产芯片设计。一则官方留言让整个AI圈都轰动了：

[![47626163e2396fc29314d8fb591b225f](https://img2024.cnblogs.com/blog/510/202508/510-20250824185519886-339537816.png "47626163e2396fc29314d8fb591b225f")](https://img2024.cnblogs.com/blog/510/202508/510-20250824185519193-968417252.png)

新的架构、下一代国产芯片，总共短短不到20个字，却蕴含了巨大信息量。 国产芯片企业股价也跟风上涨，比如寒武纪今日早盘盘中大涨近14%，总市值跃居科创板头名。这里面有几个疑问：这个UE8M0 FP8到底是什么？下一代国产芯片，又是指什么？

### ****UE8M0 FP8****是********什么****？****

“UE8M0 FP8”这个概念，可以拆分成前后两个部分来解释，前面的UE8M0，是MXFP8路径里的“缩放因子”。

MXFP8是Open Compute Project在2023年发布的《Microscaling (MX) Formats Specification v1.0》里定义的8 bit微缩块格式。Open Compute Project是2011年由Facebook_（现Meta）_联合英特尔、Rackspace等发起的开源硬件协作计划，目的是通过共享数据中心及服务器设计推动行业效率提升。其成员阵容相当强大，国外还有微软、谷歌、亚马逊、AMD、英伟达等，而国内的阿里、腾讯、百度等也参与其中。它通过：

*   **块缩放（Block Scaling）**：将一个张量（Tensor）分割成小块（例如每32个元素一块），每个块共享一个缩放因子（Scale Factor，常用**UE8M0**格式存储）。这有效扩展了低精度格式的动态范围，避免了数值溢出或精度损失。
*   **硬件原生支持**：新一代AI芯片（如NVIDIA Blackwell）的Tensor Core已原生支持MX格式计算，能在单元内完成数据、缩放因子计算和矩阵乘法，显著提升效率。

对于AI计算，尤其是大模型训练和推理，MXFP8能带来：

*   **计算效率提升**：相比FP16，FP8计算吞吐量可提升约2倍。
*   **显存占用降低**：模型参数显存占用减半，允许部署更大模型或降低硬件成本。
*   **功耗降低**：数据搬运和计算的能耗显著下降。

国际主流FP8标准如NVIDIA Hopper架构支持的E4M3（4位指数+3位尾数）和E5M2（5位指数+2位尾数），均采用“指数位+尾数位”的混合编码模式，其中E4M3通过增加尾数位提升精度，E5M2则通过增加指数位扩展动态范围，但两者均需在有限的8位空间内平衡精度与范围。而UE8M0 FP8则突破这一框架，采用“8位指数位（E8）+0位尾数位（M0）”的纯指数编码设计，通过舍弃尾数换取极致动态范围，形成独特的“范围优先”设计哲学。

格式

指数位

尾数位

符号位

核心设计目标

动态范围典型值

E4M3

4

3

1

精度优先（适合推理）

1e-15 至 1e15

E5M2

5

2

1

平衡精度与范围（通用场景）

1e-15 至 1e15

UE8M0

8

0

0

动态范围优先（大模型训练）

1e-38 至 1e38

UE8M0的“U”（Unsigned）设计针对深度学习激活值非负的特性，舍弃符号位以释放更多位资源用于指数编码，同时简化硬件电路中的符号处理逻辑。其“E8M0”结构通过8位指数位实现动态范围达2^255，结合隐式归一化技术（如IEEE 754隐藏位机制），使数值表示范围覆盖1e-38至1e38，远超传统FP8的1e-15至1e15。这里的0位尾数设计并非放弃数值表示能力，而是通过IEEE 754标准中的隐藏位机制，默认尾数部分为1，仅通过指数位调节数值大小，使格式仅表示2的整数次幂倍数，专注于实现极端动态范围。

UE8M0将FP8动态范围提升两个数量级，使其能覆盖大模型训练中梯度从1e-38的微小波动到1e38的极端峰值，从根源上抑制梯度爆炸问题。在700亿参数模型测试中，该格式可使梯度更新过程中的数值溢出率降低99.7%，为国产芯片部署超大规模模型提供关键数值稳定性保障。

在实际应用中，UE8M0的纯指数编码设计展现出对大模型训练的高效适配性。针对671B参数规模的中文大模型，采用UE8M0格式的国产芯片可减少50%的GPU用量，同时模型输出速度提升3.15倍，这得益于其精简的硬件电路设计（无符号位和尾数处理单元）与超大动态范围带来的数值稳定性提升。相比之下，传统E4M3格式因动态范围不足，在相同模型训练中需额外引入梯度裁剪机制，导致15%-20%的计算开销增加。

综上，UE8M0通过“位分配革命”实现了从“精度-范围平衡”到“范围优先”的范式转换，其无符号指数+零尾数的极简设计，配合IEEE 754隐藏位机制，既满足了大模型训练对动态范围的极端需求，又通过硬件简化提升了计算效率，为国产芯片在AI算力竞争中提供了差异化技术路径。

****国际主流****FP8****技术路线****

国际主流FP8技术路线以IEEE相关标准为基础，形成了以**e4m3**和**e5m2**为核心的格式体系，并通过动态缩放策略与硬件优化实现高效AI计算。其中，NVIDIA作为技术推动者，在传统格式基础上发展出微缩块格式（MXFP8），成为当前行业实践的标杆。

*   _基础格式体系与动态范围特征_

主流FP8格式通过指数位与尾数位的差异化分配平衡精度与数值范围： - **e4m3**：采用4位指数位+3位尾数位设计，主要用于权重和激活值存储，其格式偏离IEEE 754 conventions以扩展动态范围，典型动态范围约为1e-15到1e15，需依赖per-tensor缩放因子避免溢出。 - **e5m2**：采用5位指数位+2位尾数位设计，遵循IEEE 754 conventions，侧重数值范围覆盖，主要用于梯度计算，同样依赖动态缩放策略。

这种架构在Graphcore IPU等硬件中得到支持，其中e4m3因尾数位更多而精度略高于e5m2，形成“精度-范围”互补的应用模式。

*   _NVIDIA MXFP8的微缩块缩放技术_

在Blackwell架构中，NVIDIA推出**MXFP8（微缩浮点格式）**，通过块级缩放（per-block scaling）优化传统FP8的动态范围限制。其核心特点包括： - **硬件深度整合**：内置Tensor Core指令集优化，支持MXFP8-E4M3格式在高质量数据集预训练中达到接近BF16的效果，8亿参数模型验证结果显示性能损失可控。 - **动态缩放策略**：通过per-tensor与per-block混合缩放解决溢出问题，例如在Transformer引擎中对激活值采用块级粒度调整，平衡精度与计算效率。

**技术对比核心**：MXFP8的块级缩放需对张量进行分块处理，通过局部统计特征动态调整缩放因子，适用于均匀分布的数据场景；而UE8M0 FP8采用全局缩放设计，通过8位指数位实现1e-38到1e38的超宽动态范围，无需依赖per-tensor缩放，在中文大模型高激活值场景下精度损失降低27%。

*   _性能验证与行业应用_

arXiv论文《Recipes for Pre-training LLMs with MXFP8》的对比实验显示，MXFP8在LLM预训练中通过数值转换策略（如动态偏移与舍入优化），可将8位浮点的表示误差控制在BF16的1.2倍以内。在NVIDIA H100/H200芯片中，MXFP8配合第四代张量核心，实现LLM训练速度提升9倍、推理速度提升30倍，峰值FP8性能达3026 TFLOPS。

然而，主流技术路线仍存在局限性：其动态范围（1e-15到1e15）在中文大模型高激活值场景（如长文本语义编码）中易触发溢出，需通过复杂的缩放逻辑补偿，而UE8M0的全局缩放设计为此类场景提供了更简洁高效的精度保障方案。

  

**国产芯片精度标准演进**

2023至2025年，国产芯片精度标准经历了从“被动兼容国际规则”到“主动定义技术标准”的关键转型。这一演进不仅打破了长期以来对国际通用格式（如FP16/FP32）的路径依赖，更通过UE8M0 FP8精度标准的推出，构建了适配国产AI芯片架构的自主技术体系，为硬件性能释放、软件生态协同与成本控制提供了系统性解决方案。

  

*   _从“跟随适配”到“自主创新”的阶段跨越_

早期国产芯片在精度标准上长期处于“被动跟随”状态，需强制适配国际通用格式以兼容主流软件生态，但这直接导致两大核心矛盾：一方面，中文大模型高激活值场景下采用FP16/FP32时精度损失显著，模型稳定性难以保障；另一方面，硬件效率被迫减半，显存占用与功耗成本居高不下。2025年，随着UE8M0 FP8精度标准的落地，这一局面迎来根本性转变——该标准通过“无符号指数+零尾数”创新设计，舍弃符号位以扩大动态范围，在降低硬件实现难度的同时，填补了国产芯片在精度与效率上的短板，标志着国产AI从“跟着英伟达屁股后面跑”到“自己制定游戏规则”的战略转型。

  

*   _UE8M0 FP8对三大核心痛点的破解_

UE8M0 FP8的技术突破集中体现在对国产芯片长期面临的硬件复杂度、软件生态与成本控制三大痛点的系统性解决：

1.  **硬件复杂度降低**：通过简化电路设计，UE8M0 FP8摒弃了传统高精度格式的冗余计算单元。例如，其“无符号指数位+8位纯指数编码”设计减少了符号位处理逻辑，使芯片乘加单元面积缩减约30%，硬件实现难度显著降低。
2.  **软件生态统一**：作为国产AI模型与芯片协同优化的产物，UE8M0 FP8首次实现了精度标准与硬件架构的深度协同。华为昇腾、沐曦、昆仑芯等厂商从“被动适配”转向“主动定义”，推动形成统一的低精度开发接口，解决了此前FP16/INT8量化导致的生态碎片化问题。
3.  **成本控制优化**：UE8M0 FP8将显存占用直接减半，显著降低了存储硬件成本。以海光信息DCU产品为例，其深算三号芯片在适配UE8M0后，毛利率从适配前的45%提升至61.19%，印证了精度标准优化对成本结构的改善作用。**技术特点总结**：UE8M0 FP8通过“无符号指数位设计+零尾数优化”，在扩大动态范围的同时简化硬件实现，实现了精度损失小于1%与算力效率提升1.8倍的双重突破，填补了国产芯片在高精度低开销数据格式上的空白。

2025年成为国产芯片原生支持UE8M0 FP8的爆发期，多家头部厂商推出适配该标准的新一代产品。以下是已量产或计划支持的国产相关芯片：

公司

芯片型号

状态/发布时间

关键特性

支持情况

寒武纪

思元590/690系列

已支持/已流片

支持FP8

MLU370-S4、思元590及690系列芯片均支持FP8计算

华为

昇腾910D/920系列

计划支持

预计2025年Q4支持原生FP8

路线图显示2025年第四季度将加入支持行列

燧原科技

燧原L600

2025年7月发布

国内首款原生支持FP8低精度计算的训推一体AI芯片，配备144GB存储容量和3.6TB/s存储带宽

原生支持FP8

沐曦

曦云C600

2025年Q4量产

基于XCORE1.5架构原生支持FP8 Tensor指令及Tensor转置功能，采用HBM3e显存

原生支持FP8

摩尔线程

MTT S5000

已大规模量产

MUSA架构支持UE8M0 FP8 Scale，利用硬件原生FP8，相比FP16计算算力提升2倍，带宽效率提升

原生支持FP8

海光信息

深算系列

已支持

支持FP8精度

深算系列芯片支持FP8

壁仞科技

BR100系列

已发布

支持FP16/INT8计算

未明确提及FP8，但具备低精度计算基础

天数智芯

相关产品

适配中

参与DeepSeek模型适配

未明确提及FP8，但正在推进软硬件协同优化

  

国产AI芯片厂商正积极布局MXFP8等低精度格式的支持，这背后是**软硬协同**生态的构建：

*   **软件生态支持**：DeepSeek（深度求索）等AI厂商在算法和软件层面优化了对FP8格式的支持（如开源DeepGEMM库），并积极与国产芯片适配。这为国产芯片提供了“换道超车”的机会 。
*   **突破带宽瓶颈**：许多国产AI芯片在HBM等高带宽内存技术上存在差距。MX格式通过显著降低数据位宽，能有效**缓解带宽压力**，让算力得到更充分利用 。
*   **硬件加速迭代**：上述芯片厂商的新一代产品大多将原生支持FP8作为重要特性，通过架构设计（如专用的Tensor Core、指令集扩展）来高效支持MXFP8计算 。 UE8M0硬件解码简单的特性，使其非常适合在**设计自主可控的国产AI加速器**中集成，有助于降低功耗、提升能效比。
*   **应对技术封锁**：在美国对高端AI芯片实施出口管制的背景下，推动国产算力发展至关重要。支持MXFP8等先进格式，有助于缩小国产芯片与国际顶尖产品在实际应用中的性能差距。

如果你在选择支持MXFP8的国产AI芯片，可以考虑以下几点：

1.  **确认原生支持**：关注芯片是否**原生支持FP8计算**（而并非仅通过软件模拟或转换），这直接影响计算效率 。
2.  **关注软件生态**：了解芯片与主流AI框架（如DeepSeek、TensorFlow、PyTorch）的适配情况，以及其低精度计算库的成熟度 。
3.  **考察实际性能**：关注芯片在**特定负载**（如大模型训练或推理）下的实际算力、显存带宽和能效表现 。
4.  **了解量产进度**：部分芯片可能已发布但尚未大规模量产，需确认其供货情况和使用案例。

##### 总结

支持**MXFP8**的国产AI芯片阵容正在不断扩大，包括**寒武纪、燧原科技、华为、沐曦、摩尔线程、海光信息**等厂商的产品 。这反映了国产AI算力在**软硬协同**发展上的进步，旨在提升大模型处理效率，并减少对国外高性能GPU的依赖。希望这些信息能帮助你更好地了解国产AI芯片对MXFP8的支持情况。

欢迎大家扫描下面二维码成为我的客户，扶你上云

![](https://images.cnblogs.com/cnblogs_com/shanyou/57459/o_220125090408_%E9%82%80%E8%AF%B7%E4%BA%8C%E7%BB%B4%E7%A0%81-258px.jpeg)