---
layout: post
title: 'åŸºäºæ¢¯åº¦ç»„åˆçš„å¤šä»»åŠ¡ / å¤šç›®æ ‡å­¦ä¹ '
date: "2026-01-12T00:48:50Z"
---
åŸºäºæ¢¯åº¦ç»„åˆçš„å¤šä»»åŠ¡ / å¤šç›®æ ‡å­¦ä¹ 
==================

é¢å¯¹å¤šä»»åŠ¡ / å¤šç›®æ ‡å­¦ä¹ æ—¶ï¼Œå¯èƒ½ç›¸äº’å†²çªçš„æ¢¯åº¦ä¿¡å·ï¼Œç°æœ‰æ–¹æ³•é€šè¿‡åŠ æƒã€æŠ•å½±ã€ç»Ÿä¸€ç¬¦å·ç­‰æ–¹æ³•ï¼Œè°ƒåˆè¿™äº›æ¢¯åº¦ä¿¡å·ã€‚

  

å¤šä»»åŠ¡å­¦ä¹ ä¸€ç›´æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªè¯±äººæ„¿æ™¯ï¼šè®©å•ä¸ªæ¨¡å‹åŒæ—¶æŒæ¡å¤šé¡¹æŠ€èƒ½ï¼Œåƒäººç±»ä¸€æ ·ä¸¾ä¸€åä¸‰ï¼Œæå‡æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸­ï¼ŒåŒæ—¶å­¦ä¹ å¤šä¸ªä»»åŠ¡ï¼Œæ•ˆæœæœ‰æ—¶è¿˜ä¸å¦‚ä¸ºæ¯ä¸ªä»»åŠ¡å•ç‹¬è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚

å…¶æ ¸å¿ƒçŸ›ç›¾åœ¨äºï¼šä¸åŒä»»åŠ¡çš„æ¢¯åº¦ï¼ˆæŒ‡å¯¼æ¨¡å‹æ›´æ–°çš„æ–¹å‘ï¼‰ç»å¸¸â€œæ‰“æ¶â€ã€‚æœ‰çš„æ¢¯åº¦å¹…å€¼å¤§ï¼Œæœ‰çš„æ–¹å‘å®Œå…¨ç›¸åã€‚ç®€å•åœ°å°†æ¢¯åº¦åŠ èµ·æ¥æ›´æ–°ï¼Œæ¨¡å‹å°±ä¼šè¢«å¤§æ¢¯åº¦æˆ–æŸä¸ªç‰¹å®šä»»åŠ¡â€œå¸¦åâ€ï¼Œå¯¼è‡´å…¶ä»–ä»»åŠ¡å­¦ä¸å¥½ã€‚

å­¦æœ¯ç•Œæå‡ºäº†ä¸€ç³»åˆ—åŸºäºæ¢¯åº¦ç»„åˆçš„æ–¹æ³•ï¼Œå®ƒä»¬é€šè¿‡åŠ æƒã€æŠ•å½±ã€ç»Ÿä¸€ç¬¦å·ç­‰æ–¹å¼ï¼Œè°ƒå’Œæ¢¯åº¦å†²çªï¼Œå¹¶ä¿è¯æ¢¯åº¦ä¼˜åŒ–å¯ä»¥æ”¶æ•›åˆ°å¤šä»»åŠ¡å­¦ä¹ çš„çº³ä»€å‡è¡¡è§£ã€‚

* * *

ç›®å½•

*   [ç»å…¸æ–‡ç« ](#ç»å…¸æ–‡ç« )
    *   [\[ICML 2018\] GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks](#icml-2018-gradnorm-gradient-normalization-for-adaptive-loss-balancing-in-deep-multitask-networks)
    *   [\[NeurIPS 2018\] Multi-Task Learning as Multi-Objective Optimization (MGDA-UB)](#neurips-2018-multi-task-learning-as-multi-objective-optimization-mgda-ub)
    *   [\[NeurIPS 2020\] Just Pick a Sign: Optimizing Deep Multitask Models with Gradient Sign Dropout (GradDrop)](#neurips-2020-just-pick-a-sign-optimizing-deep-multitask-models-with-gradient-sign-dropout-graddrop)
    *   [\[NeurIPS 2020\] Gradient Surgery for Multi-Task Learning (PCGrad)](#neurips-2020-gradient-surgery-for-multi-task-learning-pcgrad)
    *   [\[NeurIPS 2021\] Conflict-Averse Gradient Descent for Multi-task learning (CAGrad)](#neurips-2021-conflict-averse-gradient-descent-for-multi-task-learning-cagrad)
    *   [\[ICML 2022\] Multi-Task Learning as a Bargaining Game (Nash-MTL)](#icml-2022-multi-task-learning-as-a-bargaining-game-nash-mtl)
    *   [\[NeurIPS 2023\] Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms (SDMGrad)](#neurips-2023-direction-oriented-multi-objective-learning-simple-and-provable-stochastic-algorithms-sdmgrad)
    *   [\[NeurIPS 2023\] FAMO: Fast Adaptive Multitask Optimization](#neurips-2023-famo-fast-adaptive-multitask-optimization)
*   [å°†æ¢¯åº¦ç»„åˆæ–¹æ³•åº”ç”¨åœ¨ LLM é¢†åŸŸ](#å°†æ¢¯åº¦ç»„åˆæ–¹æ³•åº”ç”¨åœ¨-llm-é¢†åŸŸ)

* * *

ç»å…¸æ–‡ç« 
----

### \[ICML 2018\] GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks

*   arxivï¼š[https://arxiv.org/abs/1711.02257](https://arxiv.org/abs/1711.02257)
*   å‚è€ƒåšå®¢ï¼š[CSDN | GradNormï¼šå¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ¢¯åº¦å¹³è¡¡æ–¹æ³•](https://blog.csdn.net/MoonOutCloudBack/article/details/156614639)

æ ¹æ®å„ä¸ªä»»åŠ¡ loss ä¸‹é™çš„é€Ÿåº¦ï¼ŒåŠ¨æ€è°ƒèŠ‚æ¯ä¸ªä»»åŠ¡çš„æƒé‡ã€‚å¸Œæœ›å­¦å¾—æ…¢çš„ä»»åŠ¡è·å¾—æ›´å¤§çš„æ¢¯åº¦ï¼Œå­¦å¾—å¿«çš„ä»»åŠ¡åˆ™å‡å°æ¢¯åº¦ã€‚

### \[NeurIPS 2018\] Multi-Task Learning as Multi-Objective Optimization (MGDA-UB)

*   arxivï¼š[https://arxiv.org/abs/1810.04650](https://arxiv.org/abs/1810.04650)
*   å‚è€ƒåšå®¢ï¼š[CSDN | MGDA-UBï¼šå¯»æ‰¾å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„å¸•ç´¯æ‰˜æœ€ä¼˜ï¼Œç¼“è§£ä»»åŠ¡å†²çªé—®é¢˜](https://blog.csdn.net/MoonOutCloudBack/article/details/156518736)

æ˜¯ MGDA æ–¹æ³•çš„ç›´æ¥æ”¹è¿›ã€‚

MGDA å¸Œæœ›æ±‚è§£å„ä¸ªæ¢¯åº¦ \\(\\nabla\_\\theta L\_i\\) ä¹‹é—´çš„å‡¸ç»„åˆï¼šæ‰¾æƒé‡ \\(\\alpha\\)ï¼Œ\\(\\sum\_i \\alpha\_i = 1\\)ï¼Œè®©åŠ æƒåçš„æ¢¯åº¦ \\(\\|\\sum\_i \\alpha\_i \\nabla\_\\theta L\_i\\|^2\\) çš„äºŒèŒƒæ•°æœ€å°ã€‚

### \[NeurIPS 2020\] Just Pick a Sign: Optimizing Deep Multitask Models with Gradient Sign Dropout (GradDrop)

*   arxivï¼š[https://arxiv.org/abs/2010.06808](https://arxiv.org/abs/2010.06808)
*   å‚è€ƒåšå®¢ï¼š[CSDN | GradDropï¼šè®©å¤šä»»åŠ¡å­¦ä¹ ä¸å†â€œæ¢¯åº¦æ‹”æ²³â€](https://blog.csdn.net/MoonOutCloudBack/article/details/156618026)

å¯¹äºæ¯ä¸€ä¸ªå‚æ•°ï¼ŒGradDrop åªå…è®¸ä¸€ä¸ªâ€œæ–¹å‘â€çš„æ›´æ–°ï¼ˆè¦ä¹ˆå…¨å¢åŠ ï¼Œè¦ä¹ˆå…¨å‡å°‘ï¼‰ï¼Œä½†ä¿ç•™è¿™ä¸ªæ–¹å‘ä¸Šæ‰€æœ‰çš„â€œåŠ›é‡â€ã€‚ä¸å…¶è®©æ­£è´Ÿæ¢¯åº¦åœ¨â€œæ‹”æ²³â€ä¸­ç›¸äº’æŠµæ¶ˆï¼Œä¸å¦‚åœ¨æ¯æ¬¡æ›´æ–°æ—¶ï¼Œç»Ÿä¸€æ‰€æœ‰æ¢¯åº¦çš„æ–¹å‘ï¼ˆç¬¦å·ï¼‰ã€‚

å¯¹äºæ¯ä¸ªå‚æ•°ä½ç½®ï¼Œæˆ‘ä»¬æ±‡é›†æ‰€æœ‰ä»»åŠ¡åœ¨è¯¥å¤„çš„æ¢¯åº¦å€¼ï¼Œè®¡ç®—ä¸€ä¸ªæ¢¯åº¦ç¬¦å·çº¯åº¦åˆ†æ•°ã€‚ç„¶åï¼Œç”Ÿæˆä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„éšæœºæ•°ï¼Œä¸çº¯åº¦æ¯”è¾ƒï¼Œå¦‚æœçº¯åº¦ > éšæœºæ•°ï¼Œåˆ™ä¿ç•™æ‰€æœ‰æ­£æ¢¯åº¦ï¼Œä¸¢å¼ƒæ‰€æœ‰è´Ÿæ¢¯åº¦ï¼Œçº¯åº¦ < éšæœºæ•°åˆ™ç›¸åã€‚

### \[NeurIPS 2020\] Gradient Surgery for Multi-Task Learning (PCGrad)

*   arxivï¼š[https://arxiv.org/abs/2001.06782](https://arxiv.org/abs/2001.06782)
*   GitHubï¼š[https://github.com/WeiChengTseng/Pytorch-PCGrad](https://github.com/WeiChengTseng/Pytorch-PCGrad)
*   å‚è€ƒåšå®¢ï¼š[CSDN | PCGradï¼šé€šè¿‡æ¢¯åº¦æ‰‹æœ¯ï¼Œè®©å¤šä»»åŠ¡å­¦ä¹ ä¸å†â€œå·¦å³äº’æâ€](https://blog.csdn.net/MoonOutCloudBack/article/details/156618799)

åˆæ˜¯ tianhe yu çš„å·¥ä½œã€‚

åšäº† RL taskã€‚

å¦‚æœä¸¤ä¸ªä»»åŠ¡çš„æ¢¯åº¦æ–¹å‘å†²çªï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ < 0ï¼Œå³å¤¹è§’ > 90Â°ï¼‰ï¼Œå°±æŠŠæ¯ä¸ªæ¢¯åº¦æŠ•å½±åˆ°å¦ä¸€ä¸ªæ¢¯åº¦çš„â€œå‚ç›´å¹³é¢â€ä¸Šï¼Œå»æ‰å†²çªéƒ¨åˆ†ã€‚

### \[NeurIPS 2021\] Conflict-Averse Gradient Descent for Multi-task learning (CAGrad)

*   arxivï¼š[https://arxiv.org/abs/2110.14048](https://arxiv.org/abs/2110.14048)
*   OpenReviewï¼š[https://openreview.net/forum?id=](https://openreview.net/forum?id=)_61Qh8tULj_
*   GitHubï¼š[https://github.com/Cranial-XIX/CAGrad](https://github.com/Cranial-XIX/CAGrad)
*   MTRL çš„ GitHubï¼š[https://github.com/facebookresearch/mtrl](https://github.com/facebookresearch/mtrl)
*   å‚è€ƒåšå®¢ï¼š[CSDN | CAGradï¼šä¿è¯æ”¶æ•›åˆ°å¹³å‡æŸå¤±æœ€å°çš„å¤šä»»åŠ¡æ¢¯åº¦ç®—æ³•](https://blog.csdn.net/MoonOutCloudBack/article/details/156617297)

åšäº† RL taskï¼Œå¥½åƒ RL task çš„ä»£ç å¼€æºäº†ã€‚

CAGrad åœ¨å¹³å‡æ¢¯åº¦é™„è¿‘å¯»æ‰¾ä¸€ä¸ªæ›´æ–°æ–¹å‘ï¼Œè®©æ‰€æœ‰ä»»åŠ¡ä¸­æŸå¤±ä¸‹é™æœ€æ…¢çš„é‚£ä¸ªä»»åŠ¡ ä¹Ÿèƒ½å¾—åˆ°æå‡ï¼Œä»è€Œå¹³è¡¡å„ä»»åŠ¡ï¼ŒåŒæ—¶è¿˜èƒ½ä¿è¯æœ€ç»ˆæ”¶æ•›åˆ°å¹³å‡æŸå¤±çš„æœ€å°å€¼ã€‚

### \[ICML 2022\] Multi-Task Learning as a Bargaining Game (Nash-MTL)

*   arxivï¼š[https://arxiv.org/abs/2202.01017](https://arxiv.org/abs/2202.01017)
*   GitHubï¼š[https://github.com/AvivNavon/nash-mtl](https://github.com/AvivNavon/nash-mtl)
*   å‚è€ƒåšå®¢ï¼š[CSDN | Nash-MTLï¼šåœ¨å¤šä»»åŠ¡æ¢¯åº¦ç»„åˆä¸­å¼•å…¥çº³ä»€è°ˆåˆ¤è§£](https://blog.csdn.net/MoonOutCloudBack/article/details/156730958)

åšäº† RL taskã€‚

Nash-MTL çš„ç†è®ºï¼šå¯¹å„ä¸ª task iï¼Œå¸Œæœ›æ±‚è§£æ¢¯åº¦ \\(\\Delta\\theta\\) æœ€å¤§åŒ– \\(\\sum\\log g\_i^\\top \\Delta\\theta\\) ã€‚è¿™æ ·çš„ \\(\\Delta\\theta\\) æ–¹å‘æ˜¯å”¯ä¸€çš„ï¼Œæ¨¡é•¿ åŸæ–‡æœ‰è¯´æ³•ã€‚

### \[NeurIPS 2023\] Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms (SDMGrad)

*   arxivï¼š[https://arxiv.org/abs/2305.18409](https://arxiv.org/abs/2305.18409)
*   GitHubï¼š[https://github.com/OptMN-Lab/SDMGrad](https://github.com/OptMN-Lab/SDMGrad)
*   ä¸»è¦å†…å®¹ï¼šæ„Ÿè§‰æ˜¯ CAGrad çš„ç›´æ¥æ”¹è¿›ï¼Œä½†æŠŠ CAGrad é™åˆ¶æ¢¯åº¦ä¸€å®šè¦åœ¨å¹³å‡ loss ä¸‹é™æ–¹å‘çš„ä¸€ä¸ªçƒå†…ï¼Œè¿™ä¸ªçº¦æŸæ¢æˆäº† Î» åŠ æƒçš„æƒ©ç½šï¼Œå› ä¸ºè¿™ä¸ªæ–°å½¢å¼å¤©ç„¶å…è®¸æ„é€ ä¸€ä¸ªæ— åçš„éšæœºæ¢¯åº¦ä¼°è®¡å™¨ã€‚

åšäº† RL taskã€‚

### \[NeurIPS 2023\] FAMO: Fast Adaptive Multitask Optimization

*   arxivï¼š[https://arxiv.org/abs/2306.03792](https://arxiv.org/abs/2306.03792)
*   GitHubï¼š[https://github.com/Cranial-XIX/FAMO](https://github.com/Cranial-XIX/FAMO)

å¯èƒ½ç›´æ¥æœ‰ Nash-MTL çš„ MTRL ä»£ç ï¼Œä¸ç¡®å®šã€‚

è®ºæ–‡è¿˜æ²¡çœ‹ã€‚

å°†æ¢¯åº¦ç»„åˆæ–¹æ³•åº”ç”¨åœ¨ LLM é¢†åŸŸ
-----------------

ä»¥ä¸‹æ–‡ç« éƒ½æ˜¯å‘è¡¨åœ¨è´¨é‡é«˜çš„ä¼šè®®ä¸Šçš„ï¼Œåœ¨å­¦æœ¯çš„è§’åº¦ï¼Œåº”è¯¥å¯ä»¥ç®—æ­£æ ·æœ¬ã€‚

è¿˜æ²¡å…·ä½“çœ‹ã€‚

*   ğŸ¯ Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models -- æ¢¯åº¦è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼šè¿ˆå‘å¤§è¯­è¨€æ¨¡å‹çš„å¤šç›®æ ‡å¯¹é½
*   ä¿¡æ¯ï¼šACL 2025 (main)ï¼Œ[https://arxiv.org/abs/2507.01915](https://arxiv.org/abs/2507.01915)
*   å…³é”®è¯ï¼šå°†äººç±»ä»·å€¼è§‚å¯¹é½é—®é¢˜æ„å»ºä¸ºä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œæ¢¯åº¦è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ– (GAPO)ï¼Œè‡ªé€‚åº”åœ°é‡æ–°è°ƒæ•´æ¯ä¸ªç›®æ ‡çš„æ¢¯åº¦ï¼Œå¼•å…¥ç”¨æˆ· preferenceï¼ˆæƒé‡å‘é‡ï¼‰ï¼Œæ”¶æ•›åˆ°ä¸€ä¸ªå¤šç›®æ ‡éæ”¯é…è§£ï¼ŒMistral-7B
*   ğŸ¯ Pareto Multi-Objective Alignment for Language Models -- è¯­è¨€æ¨¡å‹çš„å¸•ç´¯æ‰˜å¤šç›®æ ‡å¯¹é½
*   ä¿¡æ¯ï¼šECML/PKDD 2025ï¼Œ[https://arxiv.org/abs/2508.07768](https://arxiv.org/abs/2508.07768)
*   å…³é”®è¯ï¼šæå‡ºäº†å¸•ç´¯æ‰˜å¤šç›®æ ‡å¯¹é½ï¼ˆPAMAï¼‰ï¼Œå°† O(n^2\*d) å¤æ‚åº¦é™ä½åˆ° O(n)ï¼Œæ”¶æ•›åˆ°ä¸€ä¸ª Pareto ç¨³å®šç‚¹ï¼Œä» 125M åˆ° 7B å‚æ•°èŒƒå›´
*   ğŸ¯ GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining -- GRAPE: ä¼˜åŒ–æ•°æ®æ··åˆï¼Œä»¥å®ç°ç¾¤ä½“é²æ£’å¤šç›®æ ‡è‡ªé€‚åº”é¢„è®­ç»ƒ
*   ä¿¡æ¯ï¼šNeurIPS 2025ï¼Œ[https://arxiv.org/abs/2505.20380](https://arxiv.org/abs/2505.20380)
*   å…³é”®è¯ï¼šGRAPE åŠ¨æ€è°ƒæ•´æºåŸŸï¼ˆé¢†åŸŸæƒé‡ï¼‰çš„é‡‡æ ·æƒé‡ï¼ŒåŒæ—¶è°ƒèŠ‚å„ä¸ªä»»åŠ¡çš„æƒé‡ï¼Œå»ºæ¨¡ä¸ºä¸€ä¸ªæå°æå¤§ä¼˜åŒ–é—®é¢˜ï¼Œå®éªŒéªŒè¯äº† ClimbLabã€SlimPajama æ•°æ®é›†å’Œå¤šè¯­è¨€ç›®æ ‡
*   ğŸ¯ CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models -- CoBa: ç”¨äºå¤šä»»åŠ¡å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹çš„æ”¶æ•›å¹³è¡¡å™¨
*   ä¿¡æ¯ï¼šEMNLP 2024 (main)ï¼Œ[https://arxiv.org/abs/2410.06741](https://arxiv.org/abs/2410.06741)
*   å…³é”®è¯ï¼šä¸åŒä»»åŠ¡æ”¶æ•›é€Ÿåº¦å·®å¼‚å·¨å¤§ï¼Œæœ‰çš„å…ˆâ€œå­¦å®Œâ€å¼€å§‹è¿‡æ‹Ÿåˆï¼Œæœ‰çš„è¿˜å‡ ä¹æ²¡å­¦åˆ°ï¼Œcoba å¸Œæœ›å„ä¸ªä»»åŠ¡æœ€ç»ˆä¸€èµ·æ”¶æ•›ï¼›è®¡ç®—å®Œå…¨åœ¨ loss çº§åˆ«ï¼Œé¿å…æ˜¾å¼æ±‚å¤šä»»åŠ¡æ¢¯åº¦
*   ğŸ¯ AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models -- AMoPO: æ— éœ€å¥–åŠ±æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„è‡ªé€‚åº”å¤šç›®æ ‡åå¥½ä¼˜åŒ–
*   ä¿¡æ¯ï¼šACL 2025ï¼Œ[https://arxiv.org/abs/2506.07165](https://arxiv.org/abs/2506.07165)
*   å…³é”®è¯ï¼šalignment çš„ç›®æ ‡å¯ä»¥é€šè¿‡è¾“å‡ºç‰¹æ€§æŒ‡æ ‡ï¼ˆå¦‚ç¤¼è²Œæ€§ã€ç®€æ´åº¦ã€çœŸå®æ€§ï¼‰é—´æ¥åˆ»ç”»ï¼Œä¸ä¸€å®šæ¯ä¸ªéƒ½è¦ reward æ¨¡å‹ï¼›æŠŠè¿™äº›â€œç»´åº¦æ„ŸçŸ¥çš„ç”ŸæˆæŒ‡æ ‡â€å½“æˆéšå¼ rewardï¼Œæ„å»ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼›åœ¨â€œåå¥½æƒé‡ç©ºé—´â€å‡è®¾ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼ŒåŠ¨æ€ä»ä¸­é‡‡æ ·æƒé‡å‘é‡ï¼Œä½œä¸ºæ¯æ¬¡æ›´æ–°çš„æ ‡é‡åŒ–æƒé‡ï¼›7Bã€14B å’Œ 32B æ¨¡å‹çš„å®éªŒï¼›å»æ‰æ˜¾å¼ reward/reference æ¨¡å‹ï¼Œå¤§å¹…å‡è½»å·¥ç¨‹è´Ÿæ‹…
*   ğŸ¯ PiKE: Adaptive Data Mixing for Large-Scale Multi-Task Learning Under Low Gradient Conflicts -- PiKE: é€‚ç”¨äºä½æ¢¯åº¦å†²çªä¸‹å¤§è§„æ¨¡å¤šä»»åŠ¡å­¦ä¹ çš„è‡ªé€‚åº”æ•°æ®æ··åˆ
*   ä¿¡æ¯ï¼šNeurIPS 2025 spotlightï¼Œ[https://arxiv.org/abs/2502.06244](https://arxiv.org/abs/2502.06244)
*   å…³é”®è¯ï¼šå¤šä»»åŠ¡æ¢¯åº¦å…¶å®å¤§éƒ¨åˆ†æ—¶é—´æ˜¯â€œä½å†²çª/é«˜åº¦æ­£å¯¹é½â€çš„ï¼Œä¸æ˜¯æˆ‘ä»¬åœ¨ CV å°æ¨¡å‹ä¸Šå¸¸è§çš„é‚£ç§å¼ºè´Ÿç›¸å…³åœºæ™¯ï¼›ä¼°è®¡å„ä»»åŠ¡æ¢¯åº¦çš„æœŸæœ›ä¸‹é™é‡å’Œæ–¹å·®ï¼Œæ¨å¯¼å‡ºæ¯æ­¥æœŸæœ› loss é™ä½çš„ä¸Šç•Œï¼Œç„¶åé€‰æ‹©èƒ½æœ€å¤§åŒ–è¿™ä¸ªä¸Šç•Œçš„ä»»åŠ¡é‡‡æ ·åˆ†å¸ƒï¼ˆå³â€œä¸‹ä¸ª batch é€‰å“ªä¸ªä»»åŠ¡çš„æ•°æ®â€ï¼‰
*   ğŸ¯ LDC-MTL: Balancing Multi-Task Learning through Scalable Loss Discrepancy Control -- LDC-MTL: é€šè¿‡å¯æ‰©å±•æŸå¤±å·®å¼‚æ§åˆ¶ï¼Œå¹³è¡¡å¤šä»»åŠ¡å­¦ä¹ 
*   ä¿¡æ¯ï¼šICLR 2026 åˆ†æ•° 6644ï¼Œ[https://arxiv.org/abs/2502.08585](https://arxiv.org/abs/2502.08585)
*   å…³é”®è¯ï¼šæŠŠ MTL å†™æˆåŒå±‚ä¼˜åŒ–é—®é¢˜ï¼Œæ”¶æ•›åˆ° Îµâ€‘Pareto stationary ç‚¹ï¼ŒåŒæ—¶æ§åˆ¶ loss ä¹‹é—´çš„å·®è·