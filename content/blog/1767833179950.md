---
layout: post
title: '吴恩达深度学习课程五：自然语言处理  第一周：循环神经网络 （三）语言模型'
date: "2026-01-08T00:46:19Z"
---
吴恩达深度学习课程五：自然语言处理 第一周：循环神经网络 （三）语言模型
====================================

此分类用于记录吴恩达深度学习课程的学习笔记。  
课程相关信息链接如下：

1.  原课程视频链接：[\[双语字幕\]吴恩达深度学习deeplearning.ai](https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&from_spmid=playlist.playlist-detail.0.0&is_story_h5=false&mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1713085655&unique_k=DfBgvFW&up_id=8654113&vd_source=e035e9878d32f414b4354b839a4c31a4)
2.  github课程资料，含课件与笔记:[吴恩达深度学习教学资料](https://github.com/robbertliu/deeplearning.ai-andrewNG)
3.  课程配套练习（中英）与答案：[吴恩达深度学习课后习题与答案](https://blog.csdn.net/u013733326/article/details/79827273)

本篇为第五课的第一周内容，[1.5](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=155)到[1.7](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=157)的内容。

* * *

本周为第五课的第一周内容，与 CV 相对应的，这一课所有内容的中心只有一个：**自然语言处理（Natural Language Processing，NLP）**。  
应用在深度学习里，它是专门用来进行**文本与序列信息建模**的模型和技术，本质上是在全连接网络与统计语言模型基础上的一次“结构化特化”，也是人工智能中**最贴近人类思维表达方式**的重要研究方向之一。  
**这一整节课同样涉及大量需要反复消化的内容，横跨机器学习、概率统计、线性代数以及语言学直觉。**  
语言不像图像那样“直观可见”，更多是抽象符号与上下文关系的组合，因此**理解门槛反而更高**。  
因此，我同样会尽量补足必要的背景知识，尽可能用比喻和实例降低理解难度。  
本篇的内容关于**语言模型**，是在了解 RNN 原理上的进一步应用。

1\. 不同类型的 RNN
=============

我们在[上一篇](https://www.cnblogs.com/Goblinscholar/p/19449622)中用命名实体识别作为示例来演示 RNN 的基本原理，但我们也提到了：  
**模型输入序列的长度 \\(T\_x\\) 和输出序列的长度 \\(T\_y\\) 并不都是相等的。而二者的对应关系不同，往往就代表了不同的任务类型。**  
因此，我们以 \\(T\_x\\) 和 \\(T\_y\\) 的不同对应关系，划分了不同类型的 RNN，常见情况可以概括为以下几类：

1.  **一对一（One-to-One）**
2.  **多对一（Many-to-One）**
3.  **一对多（One-to-Many）**
4.  **多对多（Many-to-Many，等长）**
5.  **多对多（Many-to-Many，非等长）**

![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260107215709176-1109618344.png)  
现在，来**逐个简单展开**一下：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260107220014271-1535817236.png)  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260107220011950-1342522143.png)  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260107220010979-2118108004.png)  
现在，我们就知道了：如果我们要完成之前用来演示的命名实体识别任务，那一般需要构建等长的多对多 RNN 模型。  
而语言模型，同样也是一种等长的多对多 RNN 模型，由此，我们正式开始引入语言模型。

2.语言模型（Language Model, LM）
==========================

先摆一下语言模型较为官方的定义：

> **语言模型**是一种用于刻画自然语言中**词（或字符）序列概率分布**的模型。其核心目标是：**对一个序列中下一个符号出现的可能性进行建模**，即在已知前文的条件下，预测当前或下一时刻的词。

而通俗点来说，语言模型所实现的功能是：**猜你输入的下一个字更可能是什么**。  
比如输入“我爱”时，它可以预测到下一个字更可能是“你”，而在输入“我爱你”时，模型会预测**一个表示句子结束的特殊符号**的概率较高。

显然，问题来了：**什么样的训练逻辑可以得到拥有这种预测功能的语言模型呢？**  
先给出一个较简洁的结论，然后我们来进行演示：**在每一个时间步 \\(t\\)，把前 \\(t-1\\) 个词作为输入，让模型去预测第 \\(t\\) 个词，计算损失，不断优化参数，实现学习。**  
来看看它的具体运行过程：

2.1 语言模型的数据准备
-------------

![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260107215708710-1370975958.png)  
在这里，你可能会有这样一个问题：我们定义了一个控制信号 `<EOS>` 来表示序列的终止，但是**在语言直觉上，句号本身不就是终止的意思吗？能不能直接用句号来代表终止信号？**

答案当然是不能的，使用`<EOS>`并非是多此一举，不能使用句号本身直接代替的`<EOS>`的原因可以简单概括为一句话：**语言结束并不代表序列结束。**  
来看这样几个例子：

语言

句子示例

说明

为什么句号不能直接代表 `<EOS>`

中文

我真的爱你。我真的饿了。

句子中包含多个子句。

第一个句号是子句结束，句号是语法结束，但整个序列可能还没结束，无法作为统一序列终止信号。

中文

他爱你吗？

句子中没有句号。

不使用句号也可以代表结束。

英文

“Dr. Smith works at St. Mary’s Hospital.”

英文中缩写、专有名词中包含句点。

句点不一定表示句子或序列结束，例如缩写中的“Dr.”，如果直接用句号作为 `<EOS>`，会误判序列结束。

从这些例子来看，`<EOS>` 并不是语言符号，而是模型训练与生成过程中必不可少的**控制信号**。  
因此，你会发现：**我们需要一个明确不歧义，在各语言中统一且唯一的控制信号来代表序列的结束，这就是 `<EOS>` 。**

在完成了数据准备之后，我们就正式来看看语言模型的传播逻辑。

2.2 语言模型的传播过程
-------------

回忆我们最开始简述的正向传播逻辑：**在每一个时间步 \\(t\\)，把前 \\(t-1\\) 个词作为输入，让模型去预测第 \\(t\\) 个词。**  
现在就来先展开看看语言模型的正常传播过程：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260107220012733-870010096.png)  
用语言模型的正向传播过程并不复杂，但要真正明白它的学习逻辑，自然还离不开它的反向传播，而一个需要再次强调的就是：**模型每一步的输出是一个概率分布，也就是“可能性”。**  
带着这一点，我们来看看反向传播的过程：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260107220013578-590433542.png)  
仍然是反向传播不变的基本逻辑：**计算损失，计算梯度，更新参数，实现学习。**  
我们在[上一篇](https://www.cnblogs.com/Goblinscholar/p/19449622)中也展开了 RNN 具体的反向传播逻辑，这里就不再多提了。

至此，我们已经从训练视角理解了语言模型是如何学习到序列建模能力的。  
在模型训练完成之后，如何利用这个语言模型真正生成新的文本序列，就成为接下来需要回答的问题，其中相关的一门技术叫做**新序列采样**。

3.新序列采样
=======

当我们拥有一个训练好的语言模型时，有时我们会想具体地看看这个模型都学到了什么，它的“文风”是什么样的，其中一种方法就是新序列采样。  
这个概念同样不难理解：

> **新序列采样**，指的是：在语言模型生成文本时，**不再固定选择概率最大的 token**，而是根据模型给出的概率分布，从中 **“抽样”生成下一个 token**，从而逐步生成一整段新序列的过程。

再通俗一点：**新序列采样不是“选最可能的那个”，而是“按可能性来抽一个”，让模型生成更自然、多样的文本。**  
它的具体过程是这样的：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260107220012413-1576332189.png)  
再复述一下这个过程：  
给定一个已训练好的语言模型，在生成阶段，模型以起始符号作为输入，**在每个时间步根据当前上下文输出一个 token 的概率分布，并通过采样策略选取下一个 token**，将其作为下一步输入，直到生成 `<EOS>` 或达到设定长度为止。

而在实际应用中，常见的新序列采样策略包括：

*   **随机采样（Sampling）**
*   **Top-k 采样**：只在概率最高的 \\(k\\) 个 token 中采样。
*   **Top-p（Nucleus）采样**：在累计概率达到 \\(p\\) 的最小 token 集合中采样。

这样，我们便可以较直观地观察模型**在语言层面所学习到的统计规律与生成偏好**。  
在具体部署中合理使用新序列采样策略，也可以在保证基本语言合理性的前提下，提高生成文本的多样性。

最后，吴恩达老师还提及了**基于字符的语言模型**。这类模型**不再以词或子词作为基本单位**，而是直接以**字符序列**作为输入与预测对象，如**字母、空格及标点符号**等。  
字符级建模的优势在于不依赖分词规则、天然不存在未登录词问题，但代价也十分明显：单个字符所携带的语义信息极弱，序列长度显著变长，模型需要先“学会拼词”，再学习句法与语义结构，训练难度和计算成本都更高。  
因此，这种模型在实际部署中的价值并不高，实际上也并不流行，我们就不再展开了。

4\. 总结
======

概念

原理

比喻

不同类型的 RNN

根据输入序列长度 \\(T\_x\\) 与输出序列长度 \\(T\_y\\) 的对应关系，RNN 可适配不同任务结构，如分类、序列标注、生成等

不同规格的传送带：有的只收一件吐一件，有的收一排给一个结果

语言模型（LM）

建模序列的条件概率分布 \\(P(w\_t \\mid w\_{<t})\\)，在已知前文的情况下预测下一个 token

根据已经写下的内容，猜作者下一笔会写什么

语言模型的训练逻辑

在时间步 \\(t\\)，用前 \\(t-1\\) 个 token 作为输入，预测第 \\(t\\) 个 token，通过损失函数和反向传播更新参数

做完一句话的“完形填空”，错了就记住，下次改正

`<EOS>` 结束符

使用一个不歧义的特殊符号明确标记序列结束，而不是依赖语言中的标点

像文件里的“结束标志”，而不是文章里的句号

语言模型输出

每一步输出的是对整个词表的概率分布，而不是一个确定结果

给出一张“可能性排行榜”，而不是直接拍板

新序列采样

在生成阶段，不固定选择概率最大的 token，而是根据概率分布进行抽样生成

不是每次都选第一名，而是按权重抽签

字符级语言模型

以字符而非词或子词为建模单位，逐字符预测

先学拼字母，再学组词、造句

字符级模型不流行原因

序列过长、语义单位过弱，训练和推理成本高，工程性价比低

用积木一粒一粒搭摩天楼，理论可行但太慢