---
layout: post
title: 'JuiceFS + MinIO：Ariste AI 量化投资高性能存储实践'
date: "2025-12-09T00:43:50Z"
---
JuiceFS + MinIO：Ariste AI 量化投资高性能存储实践
=====================================

Ariste AI 是一家专注于 AI 驱动交易的公司，业务涵盖自营交易、资产管理、高频做市等多个领域。在量化交易研究中，数据的读取速度和存储效率，往往直接决定了研究迭代的速度。

Ariste AI 团队在构建量化研究基础设施的过程中，面对总规模超过 500TB，行情与因子数据，经历了从本地盘到最终选择在 MinIO 对象存储之上叠加 JuiceFS 文件系统的四个阶段。通过缓存机制与分层架构，团队实现了高频数据的快速访问与集中管理。**这一实践验证了“缓存加速、弹性对象存储与 POSIX 兼容”三位一体方案在量化场景下的可行性**，希望这一经验能为同行提供一些参考。

01 量化投资存储挑战：规模、速度与协作的平衡
-----------------------

量化投资流程依次包括数据层、因子与信号层、策略与仓位层及执行与交易层，构成从数据获取到交易执行的完整闭环。

在整个过程中，存储系统面临多重挑战，主要体现在以下几个方面：

*   **数据规模与增速**：量化研究所需处理的数据总量较大，涵盖历史行情数据、新闻数据以及自行计算的因子数据等。目前，这些数据的总量已接近 500T。并且，企业每日新增的行情数据也达数百 GB。若采用传统磁盘进行存储，显然无法满足如此巨大的数据存储需求。
*   **高频访问与低延迟要求**：高频的数据访问依赖于低延迟的数据读取。数据读取的速率直接决定了研究效率的高低。若数据读取速度较快，研究进程便能迅速推进；反之，则会导致研究效率低下。
*   **多团队并行与数据治理**：在量化研究过程中，通常会有多个团队同时开展不同的实验。为确保各团队研究工作的独立性与数据安全性，需要进行安全的隔离，以避免数据混淆与泄露。

为应对上述量化全流程对数据存储的需求，打造面向未来的存储系统，**我们的目标是实现：高性能、易扩展与可治理，三者有机统一**：

*   高性能：单节点读写带宽突破 500MB/s，访问延迟低于本地磁盘感知阈值；
*   易扩展：支持存储与计算资源按需水平扩容，业务无需改造即可实现平滑弹性伸缩；
*   可治理：提供细粒度权限控制、操作审计与数据生命周期策略的一站式管理能力。

02 存储架构的演进
----------

### 阶段一：本地盘极速起步

在项目初期，我们采用了 Quantrabyte 研究框架，该框架内置了 ETF 模块，可直接将数据存储在本地磁盘上，数据读取速度较快。研究员可根据自身需求，直接运行所需数据，迭代过程较为迅速。然而，这一阶段也存在一些问题：

*   重复下载造成资源浪费：多个研究员若使用相同数据，会进行多次下载。
*   存储容量不足：研究服务器的存储容量有限，仅约 15T，难以满足日益增长的数据存储需求。
*   协作困难：当需要复用他人的研究结果时，操作过程不够便捷。

### 阶段二：MinIO 集中管理的双刃剑

为解决第一阶段存在的问题，我们引入了 MinIO 进行集中管理。将所有存储数据集中在 MinIO 上，通过拆分出的模块将数据全部存入。同时，将具体因子数据也存入 MinIO，实现公共数据的统一下载。并通过权限隔离，实现多团队数据共享，提升存储空间利用率。

然而，这一阶段也出现了新的瓶颈：

*   高频随机读延迟大：在进行高频数据 I/O 操作时延迟较大，影响数据读取速度。
*   无缓存导致读写慢：由于 MinIO 社区版无缓存功能，读写高频公共数据时速度较慢。

### 阶段三：JuiceFS 引入缓存加速

为解决上述瓶颈，经充分调研，我们最终引入 JuiceFS 的缓存加速方案。该方案通过客户端本地 RAID5 存储进行挂载，借助高效的缓存机制，**成功将读写性能提升约三倍，显著改善了高频共享数据的访问体验**。

随着业务数据量突破 300TB，本地存储的扩容瓶颈逐渐显现。由于数据存储在本地，扩容需重新配置存储设备，而 RAID5 架构下扩容速度缓慢且风险较高，难以满足业务持续增长的需求。

### 阶段四：JuiceFS + MinIO 集群终局架构

为解决扩容难题，我们最终采用了JuiceFS+MinIO 集群架构。该方案具备以下优势：

*   持续高性能：JuiceFS 提供充足的缓存能力，充分满足高频数据访问场景的性能需求；
*   便捷集群扩展：基于集群化方案，可快速实现横向扩容，仅需添加同类型磁盘即可灵活提升存储容量，大幅增强系统扩展性。

**通过四阶段演进，我们验证了缓存加速、弹性对象存储与 POSIX 兼容三位一体方案在量化场景的可行性**。此方案可为同行业提供可复制、可落地的最佳实践范本，在性能、成本与治理之间取得了卓越平衡。

03 性能与成本收益
----------

通过采用 JuiceFS 与 MinIO相 结合的存储架构，系统带宽与资源利用效率得到质的飞跃，目前已完全满足研究业务对存储性能的需求。引入 JuiceFS 缓存层后，**回测任务执行效率大幅提高，1 亿条 Tick 数据回测耗时由之前的数小时降至数十分钟**。

同时，基于我们完整的数据生命周期分层存储体系策略，实现存储单价由高到低的平滑过渡，整体存储成本下降40% 以上。

04 运维实践与展望
----------

### 多租户治理

在数据隔离与权限管理方面，我们建立了完善的管理体系：

通过命名空间实现逻辑隔离，采用类似 `/factor/A`、`/factor/B` 的路径规划，确保各业务数据边界清晰。在权限控制层面，支持用户、团队、项目三个维度的精细化管理，并与 POSIX ACL 权限体系无缝对接。同时建立完整的审计日志系统，实现访问行为的实时追踪与变更历史回溯，全面满足合规性要求。

### 可观测性与自动化运维

**我们围绕四大核心指标构建了完整的监控体系：缓存命中率、I/O 吞吐量、I/O 延迟与写入重试率，系统在指标异常时可自动触发告警**。

基于 Grafana 实现了运维闭环管理，持续监控节点健康状态与存储容量。在每次扩容前，会通过模拟压测验证系统承载能力，确保业务无感知。整体运维体系实现了自动化、可预测、可回滚的高标准运维目标。

### 回测系统中的数据更新设计

我们在回测系统设计中采用基于 DAG（Directed Acyclic Graph，有向无环图）的架构，以提升系统的计算效率与可维护性。**该框架以计算节点和依赖关系为核心，将数据处理、特征计算、信号生成等环节抽象为节点，并通过依赖图统一管理**。系统内置版本控制机制，当数据版本更新时，可依托依赖图自动识别受影响的节点，精确定位需重算部分，从而实现高效的增量更新与结果追溯。

未来展望
----

在未来规划中，我们将从以下三个方向持续优化存储架构：

1.  元数据高可用升级：计划将元数据存储从 Redis 迁移至 TiKV 或 PostgreSQL，以构建跨机房高可用架构，显著提升系统容灾与快速恢复能力。
2.  混合云分层存储：通过对接公有云 S3 与 Glacier 存储服务，构建智能冷热分层体系，在实现存储容量无限弹性的同时，达成成本最优化目标。
3.  研究数据湖统一治理：计划构建统一的研究数据湖平台，集成 Schema 注册、自动数据清洗与统一目录治理等核心服务，全面提升数据资产的发现与管理效率。

我们希望本文中的一些实践经验，能为正在面临类似问题的开发者提供参考，如果有其他疑问欢迎加入 [JuiceFS 社区](https://juicefs.com/zh-cn/)与大家共同交流。