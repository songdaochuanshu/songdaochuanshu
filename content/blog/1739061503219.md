---
layout: post
title: 'Huggingface使用'
date: "2025-02-09T00:38:23Z"
---
Huggingface使用
=============

目录

*   [1\. Transformer模型](#1-transformer模型)
    *   [1.1 核心组件](#11-核心组件)
    *   [1.2 模型结构](#12-模型结构)
    *   [1.3 Transformer 使用](#13-transformer-使用)
        *   [1.3.1 使用 Hugging Face Transformers 库](#131-使用-hugging-face-transformers-库)
        *   [1.3.2 自定义 Transformer 模型](#132-自定义-transformer-模型)
        *   [1.3.3 Transformer 的 Demo](#133-transformer-的-demo)
            *   [1.3.3.1 安装依赖](#1331-安装依赖)
            *   [1.3.3.2 代码实现](#1332-代码实现)
            *   [1.3.3.3 输出示例](#1333--输出示例)
*   [2\. Huggingface](#2-huggingface)
    *   [2.1 Huggingface 的具体介绍](#21-huggingface-的具体介绍)
    *   [2.2 Huggingface 的 Models](#22-huggingface-的-models)
    *   [2.3 模型的使用](#23-模型的使用)
        *   [2.3.1 使用方法-1](#231-使用方法-1)
        *   [2.3.2 使用方法-2](#232--使用方法-2)
    *   [2.4 Huggingface的Datasets](#24--huggingface的datasets)
        *   [2.4.1 导入数据集的方法](#241-导入数据集的方法)
        *   [2.4.2 有了数据后训练模型方法](#242--有了数据后训练模型方法)
    *   [2.5 Huggingface的Spaces](#25--huggingface的spaces)

1\. Transformer模型
-----------------

**Transformer** 是一种基于自注意力机制（Self-Attention）的深度学习模型，最初由 Vaswani 等人在 2017 年的论文《Attention is All You Need》中提出。Transformer 模型在自然语言处理（NLP）任务中表现出色，逐渐取代了传统的循环神经网络（RNN）和卷积神经网络（CNN）模型，成为 NLP 领域的主流架构。

![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208142028813-343225409.png)

### 1.1 核心组件

**自注意力机制（Self-Attention）:**

自注意力机制允许模型在处理输入序列时，关注序列中的不同部分，从而捕捉序列内部的依赖关系。

通过计算每个词与其他词的相关性，模型可以动态地调整每个词的表示。

**多头注意力（Multi-Head Attention）:**

多头注意力机制通过并行计算多个自注意力头，捕捉不同子空间的信息，增强模型的表达能力。

**位置编码（Positional Encoding）:**

由于 Transformer 模型没有显式的序列信息（如 RNN 中的时间步），位置编码被引入以提供序列中每个词的位置信息。

**前馈神经网络（Feed-Forward Neural Network）:**

每个 Transformer 层包含一个前馈神经网络，用于进一步处理自注意力机制的输出。

残差连接和层归一化（Residual Connection and Layer Normalization）:

残差连接有助于缓解梯度消失问题，层归一化则用于稳定训练过程。

### 1.2 模型结构

Transformer 模型通常由编码器（Encoder）和解码器（Decoder）两部分组成：

编码器：由多个相同的层堆叠而成，每层包含一个多头自注意力机制和一个前馈神经网络。

解码器：同样由多个相同的层堆叠而成，每层包含一个多头自注意力机制、一个编码器-解码器注意力机制和一个前馈神经网络。

### 1.3 Transformer 使用

Transformer 模型广泛应用于各种 NLP 任务，如机器翻译、文本生成、文本分类、问答系统等。以下是使用 Transformer 模型的基本步骤：

#### 1.3.1 使用 Hugging Face Transformers 库

首先，确保安装了必要的深度学习框架，如 PyTorch 或 TensorFlow。此外，可以使用 Hugging Face 的 transformers 库，它提供了预训练的 Transformer 模型和简单的接口。

    from transformers import pipeline
    
    # 加载预训练的文本生成模型（如 GPT-2）
    generator = pipeline("text-generation", model="gpt2")
    
    # 生成文本
    prompt = "Once upon a time"
    output = generator(prompt, max_length=50, num_return_sequences=1)
    
    print(output[0]['generated_text'])
    

#### 1.3.2 自定义 Transformer 模型

如果需要从头实现 Transformer，可以参考以下步骤：  
示例：使用 PyTorch 实现 Transformer

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    class Transformer(nn.Module):
        def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):
            super(Transformer, self).__init__()
            self.embedding = nn.Embedding(input_dim, model_dim)
            self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, model_dim))  # 假设最大序列长度为 1000
            self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads)
            self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)
            self.fc = nn.Linear(model_dim, output_dim)
    
        def forward(self, src):
            src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]
            output = self.transformer_encoder(src)
            output = self.fc(output.mean(dim=1))  # 取序列的平均值作为输出
            return output
    
    # 示例用法
    input_dim = 10000  # 词汇表大小
    model_dim = 512    # 模型维度
    num_heads = 8      # 注意力头数
    num_layers = 6     # 编码器层数
    output_dim = 10    # 输出类别数
    
    model = Transformer(input_dim, model_dim, num_heads, num_layers, output_dim)
    src = torch.randint(0, input_dim, (32, 100))  # 输入序列 (batch_size, seq_len)
    output = model(src)
    print(output.shape)  # 输出形状: (batch_size, output_dim)
    

#### 1.3.3 Transformer 的 Demo

以下是一个简单的文本分类任务的 Demo，使用 Hugging Face 的预训练模型。

##### 1.3.3.1 安装依赖

    pip install transformers torch
    

##### 1.3.3.2 代码实现

    from transformers import pipeline
    
    # 加载预训练的文本分类模型
    classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")
    
    # 输入文本
    text = "I love using transformers, it's so easy and powerful!"
    
    # 进行分类
    result = classifier(text)
    print(result)
    

##### 1.3.3.3 输出示例

    [{'label': 'POSITIVE', 'score': 0.9998}]
    

2\. Huggingface
---------------

Huggingface 既是网站名也是其公司名，随着 transformer 浪潮，Huggingface 逐步收纳了众多最前沿的模型和数据集等有趣的工  
作，与 transformers 库结合，可以快速使用学习这些模型。目前提到 NLP 必然绕不开 Huggingface。

### 2.1 Huggingface 的具体介绍

进入 Huggingface 网站,如下图所示  
![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208142552341-2014217342.png)

其主要包含：  
Models（模型），包括各种处理 CV 和 NLP 等任务的模型，上面模型都是可以免费获得  
Datasets（数据集），包括很多数据集  
Spaces（分享空间），包括社区空间下最新的一些有意思的分享，可以理解为 huggingface 朋友圈  
Docs（文档，各种模型算法文档），包括各种模型算法等说明使用文档  
Solutions（解决方案，体验等），包括 others

### 2.2 Huggingface 的 Models

点开 Models。可以看到下图的任务  
![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208142616223-737215713.png)

其中，主要包括计算机视觉、自然语言处理、语音处理、多模态、表格处理、强化学习。

**展开介绍：**

**Computer Vision（计算机视觉任务）**：包括 lmage Classification（图像分类），lmage Segmentation（图像分割）、zero-Shot lmage Classification（零样本图像分类）、lmage-to-Image（图像到图像的任务）、Unconditional lmage Generation（无条件图像生成）、Object Detection(目标检测)、Video Classification（视频分类）、Depth Estimation(深度估计，估计拍摄者距离图像各处的距离)

**Natural Language Processing（自然语言处理）**：包括 Translation（机器翻译）、Fill-Mask(填充掩码，预测句子中被遮掩的词)、Token Classification（词分类）、Sentence Similarity（句子相似度）、Question Answering（问答系统），Summarization（总结，缩句）、Zero-Shot Classification (零样本分类)、Text Classification（文本分类）、Text2Tex（t 文本到文本的生成）、Text Generation  
（文本生成）、Conversational（聊天）、Table Question Answer（表问答，1.预测表格中被遮掩单词 2.数字推理，判断句子是否被表格数据支持）

**Audio（语音）**：Automatic Speech Recognition（语音识别）、Audio Classification（语音分类）、Text-to-Speech（文本到语音的生成）、Audio-to-Audio（语音到语音的生成）、Voice Activity Detection（声音检测、检测识别出需要的声音部分）

**Multimodal（多模态）**：Feature Extraction（特征提取）、Text-to-Image（文本到图像）、Visual Question Answering（视觉问答）、Image2Text（图像到文本）、Document Question Answering（文档问答）

**Tabular（表格）**：Tabular Classification（表分类）、Tabular Regression（表回归）

**Reinforcement Learning（强化学习）**：Reinforcement Learning（强化学习）、Robotics（机器人）

### 2.3 模型的使用

一般来说，页面上会给出模型的介绍。例如，我们打开其中一个 fill-mask 任务下下载最多的模型 bert-base-uncased  
![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144035743-697048616.png)

可以看到模型描述：  
![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144059960-626331135.png)

#### 2.3.1 使用方法-1

需要提前安装 transformers 库，可以直接 pip install transformers 安装。还有 Pytorch 或 TensorFlow 库，读者自行下载。

下载完后可以使用 pipeline 直接简单的使用这些模型。第一次执行时 pipeline 会加载模型，模型会自动下载到本地，可以直接用。

第一个参数是任务类型，第二个是具体模型名字

    from transformers import pipeline
    
    unmasker = pipeline('fill-mask', model='bert-base-uncased')
    
    unmasker("Hello I'm a [MASK] model.")
    

运行结果：

    [
        {
            "score": 0.10731087625026703,
            "token": 4827,
            "token_str": "fashion",
            "sequence": "hello i ' m a fashion model."
        },
        {
            "score": 0.08774463832378387,
            "token": 2535,
            "token_str": "role",
            "sequence": "hello i ' m a role model."
        },
        {
            "score": 0.053383927792310715,
            "token": 2047,
            "token_str": "new",
            "sequence": "hello i ' m a new model."
        },
        {
            "score": 0.046672236174345016,
            "token": 3565,
            "token_str": "super",
            "sequence": "hello i ' m a super model."
        },
        {
            "score": 0.027095887809991837,
            "token": 2986,
            "token_str": "fine",
            "sequence": "hello i ' m a fine model."
        }
    ]
    
    

模型下载在这个地方:

C:\\Users\\用户\\.cache\\huggingface\\hub

不同模型使用方法略有区别，直接通过页面学习或文档学习最好  
![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144143279-2093376612.png)

**可以自定义加载输入分词器:使用 AutoTokenizer**

    from transformers import AutoTokenizer 
    #下面这种方式可以自动加载 bert-base-uncased 中使用的分词器
    tokenizer=AutoTokenizer.from_pretrained("bert-base-uncased")
    

**可以自定义加载模型结构:使用 AutoModel ， 不包括输入分词器和输出部分！！！**

    from transformers import AutoModel
    #下面这种方式可以自动加载 bert-base-uncased 中使用的模型，没有最后的全连接输出层和 softmax
    model=AutoModel.from_pretrained("bert-base-uncased")
    

**可以自定义加载模型和输出部分:使用 AutoModelForSequenceClassification 等**

    from transformers import AutoModelForSequenceClassification
    #下面这种方式可以自动加载 bert-base-uncased 中使用的模型（包括了输出部分），有最后的全连接输出层
    model=AutoModel.AutoModelForSequenceClassification("bert-base-uncased")
    

**模型保存**

    model.save_pretrained("./")#保持到当前目录
    

**一个简单的流程例子：**

代码接收一个句子列表，对其进行分词，将其传递给一个预训练的情感分析模型，然后处理输出以获得每个类别的预测概率。最后，将结果打印到控制台

    input=['The first sentence!','The second sentence!']
    
    from transformers import AutoTokenizer
    
    #从 Transformers 库中导入 AutoTokenizer 类，用于对输入句子进行分词。
    #分词是将文本转换为数值标记的过程，这些标记可以被模型理解。from_pretrained 方法加载一个预训练的分词器
    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    
    #使用 tokenizer 对象对输入句子进行分词。padding=True 参数确保所有句子都被填充到相同的长度，
    #truncation=True 截断过长的句子，return_tensors='pt' 返回 PyTorch 张量
    input = tokenizer(input, padding=True, truncation=True, return_tensors='pt')
    
    from transformers import AutoModelForSequenceClassification
    # 模型加载：使用 AutoModelForSequenceClassification 类加载一个预训练的序列分类模型。
    # from_pretrained 方法加载模型
    model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    
    print(model)
    
    # 模型推理
    output = model(**input)
    
    print(output.logits.shape)
    
    import torch
    
    predictions = torch.nn.functional.softmax(output.logits, dim=-1)
    
    # 打印预测概率
    print(predictions)
    
    # ID 到标签的映射
    print(model.config.id2label)
    

#### 2.3.2 使用方法-2

![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144356312-1005121862.png)

下面以 ChatGLM2-6B 为例（见上图），先在 github 上 git 下 ChatGLM2-6B 除模型外的相关文件

    git clone git@github.com:THUDM/ChatGLM2-6B.git
    
    cd ChatGLM2-6B-main
    

安装好相关依赖

    pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
    

类似刚才的方法一直接执行下面代码，会在网上自动下载模型文件

    >>> from transformers import AutoTokenizer, AutoModel
    >>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
    >>> model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True, device='cuda')
    >>> model = model.eval()
    >>> response, history = model.chat(tokenizer, "你好", history=[])
    >>> print(response)
    你好👋!我是人工智能助手 ChatGLM2-6B,很高兴见到你,欢迎问我任何问题。
    >>> response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
    >>> print(response)
    晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:
    
    1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。
    2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。
    3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。
    4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。
    5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。
    6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。
    
    如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。
    

也可以方法二，找到 huggingface 上 ChatGLM2-6B 模型地址，直接 git

    git clone https://huggingface.co/THUDM/chatglm2-6b
    

然后打开刚才的 ChatGLM2-6B 里的 web\_demo.py，修改里面的模型和 AutoTokenizer 目录，为刚才 git 模型的目录，例如我在ChatGLM2-6B 里新建了一个 model，在 model 目录下 git 模型的，所以我的目录修改为下图

    tokenizer = AutoTokenizer.from_pretrained("model/chatglm2-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("model/chatglm2-6b", trust_remote_code=True).cuda()
    

最后，在终端直接执行下面代码

    python web_demo.py
    

点击启动后的链接，即可使用 web版本的ChatGLM2-6B

### 2.4 Huggingface的Datasets

可以看到有如下任务的数据集。读者可自行打开学习

![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144438996-1503040558.png)

例如，我们打开 Text Classification 任务的 glue 数据集,可以看到下图，里面会有数据集的介绍、相关信息和下载方式，读者自行查看。  
![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144449012-902800212.png)

#### 2.4.1 导入数据集的方法

提前 `pip install datasets`

    from datasets import load_dataset
    
    datasets = load_dataset('glue', 'mrpc')  # 加载glue数据集
    print(datasets)  # 打印数据集
    
    print(datasets['train'][0])  # 打印第一个样本
    

#### 2.4.2 有了数据后训练模型方法

下面给出 bert-base-uncased 的例子，实现对两个句子的相似度计算

    # 导入tokenizer
    
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    
    # input = tokenizer("The first sentence!", "The second sentence!")
    #
    # print(tokenizer.convert_ids_to_tokens(input['input_ids']))
    
    
    # 实际使用 tokenizer 的方法，得到 tokenizer_data
    def tokenize_function(examples):
        return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)
    
    
    from datasets import load_dataset
    datasets = load_dataset("glue", "mrpc")
    tokenizer_data = datasets.map(tokenize_function, batched=True)
    print(tokenizer_data)
    
    # 训练参数
    from transformers import TrainingArguments
    
    # https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments
    training_args = TrainingArguments("test_trainer")
    print(training_args)#看下默认值
    
    # 导入模型
    from transformers import AutoModelForSequenceClassification
    model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
    
    #导入数据处理的一个东西 DataCollatorWithPadding，变成一个一个 batch
    from transformers import DataCollatorWithPadding
    
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    #导入训练器，进行训练,API : https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Traine
    
    from transformers import Trainer
    trainer = Trainer(
        model,
        training_args,
        train_dataset=tokenizer_data["train"],
        eval_dataset=tokenizer_data["validation"],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    trainer.train()
    

### 2.5 Huggingface的Spaces

点开如下图所示。里面有些近些天有趣的东西火热的 apps  
![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144530402-881224256.png)

比如下面的一个统一的多模态理解和生成模型  
![image](https://img2024.cnblogs.com/blog/682547/202502/682547-20250208144537707-1862109150.png)

### 微信公众号

### ![](https://images.cnblogs.com/cnblogs_com/bigdata1024/2014130/o_221015130328_%E6%89%AB%E7%A0%81_%E6%90%9C%E7%B4%A2%E8%81%94%E5%90%88%E4%BC%A0%E6%92%AD%E6%A0%B7%E5%BC%8F-%E6%A0%87%E5%87%86%E8%89%B2%E7%89%88.png)

### 作者：[chaplinthink](https://www.cnblogs.com/bigdata1024/) [\===> \[欢迎赞赏作者， 您的赞赏，是我前进的动力🙂\]](https://www.cnblogs.com/bigdata1024/p/16795143.html)

### 出处：[https://www.cnblogs.com/bigdata1024/p/18704285](https://www.cnblogs.com/bigdata1024/p/18704285)

### 本文以学习、研究和分享为主，如需转载，请联系本人，标明作者和出处，非商业用途!