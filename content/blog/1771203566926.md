---
layout: post
title: '万字长文深度解析 RAG'
date: "2026-02-16T00:59:26Z"
---
万字长文深度解析 RAG
============

随着大语言模型（LLM）在私域知识问答领域的广泛应用，检索增强生成（RAG）已成为解决模型幻觉与知识时效性问题的核心技术。本文对 RAG 技术进行了全景式的深度纵述，包括：RAG的本质、RAG的演进和主流形态、RAG的全流程技术细节、RAG的评测。

1\. 引言
======

> 本章节将以笔者的理解方式带你认识RAG

1.1 为什么需要 RAG？
--------------

检索增强生成（Retrieval-Augmented Generation，RAG）是一种将大语言模型（LLM）与外部知识库相结合的技术范式，其核心目标在于：

*   **知识局限**：解决大模型知识局限于训练数据的问题（“参数中的知识”）
*   **幻觉问题**：缓解模型“幻觉”（无中生有地生成错误信息）
*   **数据安全**：在不上传私域数据至第三方平台的前提下，实现安全、可控的智能问答

1.2 RAG 的本质
-----------

graph LR %% 定义全局样式 classDef highlight fill:#e1f5fe,stroke:#01579b,stroke-width:2px; classDef warning fill:#fff3e0,stroke:#ef6c00,stroke-dasharray: 5 5; classDef success fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px; %% 传统 LLM 流程 subgraph Traditional \["传统 LLM (闭卷考试)"\] direction TB T1\[用户提问 Query\] --> T2\["LLM 内部大脑<br/>(参数化知识 Parametric Memory)"\] T2 --> T3\["直接生成答案<br/>(依赖模型记忆)"\] T3 -.-> T4{{"❌ 风险：幻觉、知识过时<br/>无法访问私有文档"}} end %% RAG 流程 subgraph RAG \["RAG (开卷考试)"\] direction TB R1\[用户提问 Query\] --> R2\["R: 检索 (Retrieval)<br/>从图书馆/数据库查资料"\] R2 --> R3\["A: 增强 (Augmented)<br/>将相关资料放在模型案头"\] R3 --> R4\["G: 生成 (Generation)<br/>LLM 结合资料进行逻辑推导"\] R4 --> R5\["✅ 优点：事实准确、引用溯源<br/>实时性高、私有数据受控"\] end %% 样式应用 class R1,R2,R3,R4 highlight; class T4 warning; class R5 success; %% 核心区别标注 Traditional -.- RAG annotate\["<b>本质区别：知识存储与逻辑推理的解耦</b>"\]

**RAG是“一场 LLM 的“开卷考试”**

*   **传统 LLM（闭卷考试）**：模型依靠预训练阶段记忆在参数里的知识（Parametric Memory）来回答问题。如果考题涉及它没读过的私有文档或训练日期之后的时政，它就会为了“交卷”而编造答案，产生幻觉。
*   **RAG（开卷考试）**：在回答问题前，先允许模型去图书馆（外部知识库）查阅相关资料，把资料放在案头，再结合题目给出答案。

通过拆解 **R、A、G** 这三个字母，我们可以更深刻地看到这一技术的本质：

### 1.2.1 R（Retrieval / 检索）：解决“去哪找”的问题

**R** 是整个流程的起点，它的本质是**从海量非结构化数据中精准定位**。

*   **从“模糊”到“精确”**：LLM 内部的知识是高度压缩且模糊的（概率预测），而外部检索到的文档是确定性的（事实）。
*   **非参数化存储**：R 将知识的存储从模型参数（很难更新）中剥离出来，转存到向量数据库或搜索引擎中（极易更新）。
*   **核心挑战**：检索的质量决定了 RAG 的上限。如果第一步找错了“参考书”，后面的回答就会南辕北辙。

### 1.2.2 A（Augmented / 增强）：解决“怎么用”的问题

**A** 其实是 RAG 的灵魂纽带，它是**联结检索结果与生成模型的“桥梁”**。

*   **上下文增强**：检索回来的原始文档通常是零散、冗余甚至有噪声的。**Augmented** 的过程就是对这些原始材料进行“精加工”——包括重排序（Rerank）、清洗、甚至长文本压缩。
*   **Prompt 的重构**：它将用户的问题（Query）与检索到的知识（Context）进行有机融合，构建出一个功能强大的 Prompt。
*   **本质逻辑**：它是通过**注入外部上下文**，改变了 LLM 的推理环境。它把一个简单的“问答”变成了一个“基于给定事实进行推理”的任务。

### 1.2.3 G（Generation / 生成）：解决“怎么说”的问题

**G** 是最终的产出。此时的 LLM 不再扮演“百科全书”，而是扮演一个 **“具备逻辑分析能力的阅读理解专家”**。

*   **约束生成**：在 RAG 场景下，我们对 G 的要求不是博学，而是**忠实（Faithfulness）**。我们希望它在给定的资料范围内组织语言，避免发散。
*   **推理引擎**：利用 LLM 强大的语言组织和逻辑推导能力，将碎片化的检索信息转化为连贯、易读、符合人类逻辑的答案。
*   **本质转变**：G 将 LLM 的角色从“知识库”转化为了“推理机”。

### 1.2.4 总结：RAG 的哲学公式

如果用一个公式来表达 RAG 的本质，那就是：

\\\[RAG = 检索R（寻找事实） + 增强A（对齐上下文） + 生成G（逻辑表达） \\\]

*   **没有 R**：LLM 是无米之炊，容易一本正经胡说八道。
*   **没有 A**：知识与模型之间存在隔阂，容易出现“消化不良”。
*   **没有 G**：用户拿到的是一堆文档碎片，无法直接获取答案。

**RAG 的出现，本质上是解耦了“知识存储”与“逻辑推理”**：让数据库负责记住知识，让大模型负责理解和运用知识。

2\. RAG 的演进
===========

> 本章节将介绍RAG近些年的演进，同时向你展示当下RAG的几种主要形态

timeline title RAG 技术演变简图 2020 : Naive RAG : 概念奠基 : 检索-增强-生成线性架构 2023 : Advanced RAG : 策略优化 : 重排序 / 混合检索 / 意图改写 2023 : Modular RAG : 范式升级 : 插件化模块 / 路由分发 / 灵活编排 2023 : GraphRAG : 拓扑增强 : 知识图谱 / 社区发现 / 全局总结 2024 : Agentic RAG : 智能进化 : 自主规划 / 多步推理 / 自我修复

2.1 Naive RAG
-------------

![NaiveRAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125316686-57296797.png)

> 摘自[\[2501.09136\] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://arxiv.org/abs/2501.09136)

### **2.1.1 概念介绍**

Naive RAG 是 RAG 技术最原始、最标准的形态。它出现于 2020 年左右（由 Lewis 等人提出），旨在通过一个简单的线性流水线，将外部静态知识库与预训练大语言模型（LLM）结合。它不涉及复杂的逻辑判断或多轮检索，只是机械地完成“搜索并喂给模型”的任务。

### **2.1.2 机制原理**

Naive RAG 的工作流程是一个典型的**线性单向流水线**，分为两个阶段：

*   **离线阶段（数据准备/索引）**：
    
    1.  **清洗与分块（Chunking）**：将原始文档（PDF、Markdown等）切分成固定大小的文本块（如每块 512 字符）。
    2.  **向量化（Embedding）**：利用 Embedding 模型将每个文本块转换为高维向量。
    3.  **构建索引**：将向量存入向量数据库（Vector DB），建立索引以备查询。
*   **在线阶段（推理/生成）**：
    
    1.  **检索（Retrieval）**：将用户的提问（Query）转换成同样的向量，在数据库中进行余弦相似度计算，召回最相似的 Top-K 个文本块。
    2.  **增强（Augmentation）**：将这 K 个文本块直接拼接到 Prompt 中，作为“背景知识”。
    3.  **生成（Generation）**：LLM 阅读 Prompt 中的上下文，输出最终答案。

### **2.1.3 优点**

*   **实现简单**：几乎不需要复杂的工程架构，利用 LangChain 或 LlamaIndex 几行代码即可搭建。
*   **低成本**：不涉及多轮推理或复杂的重排序模型，API 调用次数少，响应速度快。
*   **透明度高**：流程极其直观，容易追踪哪一部分文档被召回用于辅助生成。
*   **通用性强**：无需对模型进行微调（Fine-tuning），即可在不同领域的私有数据上快速上线。

### **2.1.4 缺点**

*   **检索精度低（Low Precision）**：
    *   由于仅依赖语义向量，容易召回与 Query 语义相近但事实无关的噪声内容。
    *   **语义偏差**：向量搜索对关键词（如专有名词、产品型号）不够敏感。
*   **召回率不足（Low Recall）**：
    *   如果相关的知识分散在多个分块中，固定长度的切分可能导致关键上下文断裂。
*   **生成质量受限**：
    *   **上下文过载**：如果召回的 Top-K 内容相互矛盾或含有大量冗余，会导致模型产生“幻觉”或忽略关键信息（Lost in the middle）。
    *   **缺乏逻辑**：无法处理需要多步推理的问题，因为它只进行一次性检索。
*   **知识时效性滞后**：虽然比纯模型好，但如果外部知识库很大，索引更新（增删改）的同步成本和一致性在 Naive 架构下较难处理。

_Naive RAG 就像是一个“直肠子”：你问什么，它就去翻书，翻到哪页算哪页，然后一股脑念给你听。虽然解决了“从无到有”的问题，但在处理企业级复杂业务时，往往显得过于“笨拙”。这也是为什么后来演化出了 Advanced RAG。_

2.2 Advanced RAG
----------------

![Advanced RAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125422209-70779315.png)

> 摘自[\[2501.09136\] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://arxiv.org/abs/2501.09136)

#### **2.2.1 概念介绍**

Advanced RAG 并非改变了 RAG 的基本范式，而是通过“精细化工程”对流水线的每一个环节进行了增强。它的核心逻辑是：**不仅要搜到，还要搜得准；不仅要喂给模型，还要喂精华。** 它解决了向量检索在关键词匹配上的弱点，以及召回内容过多导致的干扰问题。

#### **2.2.2 机制原理**

Advanced RAG 在传统的 R-A-G 链路中插入了多个关键优化节点，下面简单介绍一些：

*   **预检索优化 (Pre-Retrieval Strategies)**：
    *   **查询重写 (Query Rewriting)**：利用 LLM 将用户模糊的提问转化为更适合检索的表达。
    *   **多查询并行 (Multi-Query)**：从不同角度生成多个相似问题并行检索，扩大搜索范围。
    *   **假设性文档嵌入 (HyDE)**：让 LLM 先生成一个“虚假但理想”的答案，再用这个答案去向量库搜真正的参考文档（解决 Query 与 Doc 之间的语义鸿沟）。
*   **索引增强 (Indexing Optimization)**：
    *   **混合检索 (Hybrid Search)**：结合 **向量检索（语义）** 与 **关键词检索（BM25/全文搜索）**，确保专有名词不丢失。
    *   **精细化分块**：采用滑动窗口或父子分块（Parent-Document Retrieval），检索小块以保证精度，返回大块以保证上下文完整。
*   **后检索处理 (Post-Retrieval Strategies)**：
    *   **重排序 (Reranking)**：这是 Advanced RAG 的灵魂。先粗筛出 100 条，再利用高精度的 **Cross-Encoder 模型** 对其进行精排，只取前 5-10 条。
    *   **上下文压缩 (Context Compression)**：剔除召回文档中的废话，只保留与问题最相关的句子，减少 Token 消耗并降低干扰。

#### **2.2.3 优点**

*   **检索精度极高**：通过 Rerank 和混合检索，大幅降低了无关噪声的干扰。
*   **解决“中间失落”问题**：通过压缩和精选，避免了由于输入上下文过长导致 LLM 忽略中间有效信息的现象（Lost in the Middle）。
*   **更好的语义对齐**：HyDE 和 Query Rewrite 让模型能更好地理解用户的真实意图，即便用户问得很模糊。
*   **工程落地成熟**：是目前大多数企业级 RAG 应用的主流选择，平衡了性能与复杂度。

#### **2.2.4 缺点**

*   **响应延迟增加**：Query 重写、多次检索以及 Rerank 都会消耗额外的时间，首字响应（TTFT）显著变慢。
*   **Token 成本上升**：在生成最终答案前，往往需要多次调用 LLM 进行重写或压缩，增加了 API 费用。
*   **流水线复杂化**：需要维护多个模型（Embedding, Reranker, LLM）以及复杂的业务逻辑，对系统稳定性挑战更大。
*   **依然是线性思维**：虽然环节变多了，但本质还是“一次性”的流水线，难以处理需要多步拆解或全局总结的极复杂任务。

_如果说 Naive RAG 是在图书馆里随便找书，Advanced RAG 就是先请一位专家帮你优化搜索词，找出一堆书后再由专人精读、划重点，最后把精华笔记交到你手上。这显著提升了答案的靠谱程度，但也意味着你得支付更多的“专家费”并等待更久。_

2.3 GraphRAG
------------

![GraphRAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125446665-681665676.png)

> 摘自[\[2501.09136\] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://arxiv.org/abs/2501.09136)

### **2.3.1 概念介绍**

GraphRAG 是由微软（Microsoft）等机构推向主流的一种范式，旨在解决传统向量 RAG 的两个核心局限：**缺乏全局理解力**和**难以处理跨文档的多跳逻辑**。

传统的向量 RAG 擅长处理“局部”信息（点对点检索），而 GraphRAG 通过引入**知识图谱（Knowledge Graph, KG）**，将非结构化的文本转化为结构化的语义网络。它不仅能找到具体的“点”，还能理解点与点之间的“线”，甚至是通过 **社区发现（Community Detection）** 技术看清全局的“面”。

### **2.3.2 机制原理**

GraphRAG 的本质是 **“以图聚类，以文摘要”** 。其核心架构可以拆解为两个阶段：**层次化索引构建**与**多模态查询策略**。

#### **2.3.2.1 索引阶段：从非结构化文本到语义社区**

这是 GraphRAG 最耗时也最核心的部分，分为四个关键层级：

1.  **实体与关系提取（Extraction）**：
    *   利用 LLM 扫描原始文本块，提取出**实体（Entities）**、**关系（Relationships）** 和**声明（Claims）**。
    *   **关键技术**：实体消歧（Entity Resolution）。例如，将文中出现的“马斯克”和“Elon Musk”识别并归并为同一个节点。
2.  **图谱构建与图压缩（Graph Construction）**：
    *   将提取的实体作为节点，关系作为边，构建初始图谱。
    *   计算节点权重（度数）和边的重要性，对图进行精简。
3.  **社区发现（Community Detection）**：
    *   使用 **Leiden 算法** 或 **Louvain 算法** 对图进行层次化聚类。
        
        *   **Louvain 算法**：这是一种基于“模块化（Modularity）最大化”的经典贪心算法，通过不断迭代合并邻近节点来发现图谱中的社区结构，以极高的计算效率在大规模网络中挖掘出紧密的关联群体。
        
        *   **Leiden 算法**：作为 Louvain 的升级版，它通过引入重构机制解决了 Louvain 可能产生不连贯或低质量社区的缺陷，能够以更快的速度识别出逻辑更严密、结构更合理的语义社区。
    *   将联系紧密的实体划分为不同的**社区（Communities）**。这一步模拟了人类对知识的分类（如：从“特斯拉”延伸出“电动汽车社区”、“财报分析社区”、“自动驾驶技术社区”）。
4.  **层次化社区摘要（Community Summarization）**：
    *   **自下而上生成**：针对每个社区及其包含的边和节点，利用 LLM 生成一份详尽的报告（Summary）。
    *   这些摘要分层存储（从微观社区到中观社区再到宏观主题），形成了知识库的“全局大纲”。

#### **2.3.2.2 查询阶段：双路径检索模型**

根据用户问题的粒度，GraphRAG 动态选择两种检索路径：

*   **路径 A：全局查询（Global Search）** —— 处理概括性/宏观问题
    *   **场景**：如“这个项目的核心风险点有哪些？”
    *   **机制**：采用类似 **Map-Reduce** 的逻辑。
        *   **Map**：并行检索所有预生成的社区摘要，生成中间回答。
        *   **Reduce**：对所有中间回答进行评分和汇总，生成最终的全局综述。
*   **路径 B：局部查询（Local Search）** —— 处理具体事实/多跳问题
    *   **场景**：如“A 公司的 CTO 是如何评价 B 技术的？”
    *   **机制**：结合向量检索与图遍历。
        *   **起始点**：通过向量搜索找到与 Query 相关的实体节点。
        *   **顺藤摸瓜**：沿着图谱的边扩展，寻找相邻节点、关联关系及相关的原始文档块。
        *   **融合生成**：将找到的结构化图数据与非结构化文本块组合，作为上下文喂给 LLM。

### **2.3.3 优点**

*   **全局视野（Global Understanding）**：它是目前回答总结性、对比性问题表现最好的 RAG 架构。
*   **复杂推理链条（Multi-hop Reasoning）**：通过图谱的“边”，它能轻松连接原本分布在不同文档、不同页码的信息片段。
*   **抗噪声能力强**：社区摘要过程本质上是一种“去粗取精”的信息过滤，降低了低相关文档对生成的干扰。
*   **数据可溯源性**：每一个生成的结论都可以精准定位到图谱中的特定节点（实体）和边（关系），提供了比纯向量检索更强的可解释性。

### **2.3.4 缺点**

*   **极高的计算开销（Cost & Time）**：
    *   索引阶段需要 LLM 扫描每一行文本并提取关系，其 Token 消耗量通常是传统 RAG 的 **50x - 100x**。
    *   对于海量文档，构建一次索引可能耗时数天且成本昂贵。
*   **索引更新困难**：由于图谱和社区摘要是全局耦合的，新增文档时往往难以进行“增量更新”，有时需要重构部分社区，维护成本高。
*   **提取阶段的信息损耗**：如果 LLM 在初期提取实体和关系时出错（Schema 偏移或漏掉关键关系），后续的检索无论多么精妙也无法补救。
*   **技术栈异构化**：需要管理向量数据库（如 Milvus）和图数据库（如 Neo4j/NebulaGraph），增加了运维复杂性。

_如果说传统的向量 RAG 是一本地图册，你可以查到具体的坐标（经纬度）；那么 GraphRAG 就是一个城市大脑，它不仅知道每个建筑的位置，还深谙它们之间的交通流向、隶属关系以及整个城市的区域划分。它让大模型真正具备了“全局观”，但你必须为构建这个大脑支付高昂的计算电费。_

2.4 Modular RAG
---------------

原文地址：[\[2407.21059\] Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks](https://arxiv.org/abs/2407.21059)  
编写参考：[【RAG技术论文】《模块化RAG（Modular RAG）: 将RAG系统转变为乐高玩具一样的可重构框架》 - 知乎](https://zhuanlan.zhihu.com/p/722159912)

![Modular RAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125532522-2144387055.png)

> 摘自[\[2501.09136\] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://arxiv.org/abs/2501.09136)

![RAG Overview](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125610210-1030968787.png)

> 摘自论文：[\[2407.21059\] Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks](https://arxiv.org/abs/2407.21059)

### **2.4.1 概念介绍**

Modular RAG 是在 RAG 技术走向成熟过程中提出的更高阶范式。它的核心思想是：**将复杂的 RAG（检索增强生成）系统解耦为独立、可复用的功能单元，并通过编排（Orchestration） 这些单元来构建灵活多变的处理流程（称为 RAG Flow）**。它不再局限于传统的“检索-然后-生成”线性范式，而是提供了一个类似“乐高积木”的可重构框架。

这种架构允许开发者针对不同的业务场景，灵活地插入、替换或重复某些处理步骤。

### **2.4.2 机制原理**

Modular RAG 的运作不再是“一条路走到底”，而是基于**模块化组件**进行流程编排：

> 论文中给出的一个典型示例  
> ![ModularRAG示例](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125743632-1539400684.png)

#### 2.4.2.1 三层架构设计

论文提出了一个清晰的三层架构，从宏观到微观定义了系统的组成：

*   **L1: 模块 (Module)**：这是最高层级，关注RAG系统中的关键阶段，将每个阶段视为独立模块，继承了高级RAG的主要流程并引入了编排模块来协调流程：
    *   **索引 (Indexing)**：处理文档的分块、向量化和存储。支持高级策略如分层索引、知识图谱索引等。
    *   **检索前 (Pre-retrieval)**：优化查询本身，如查询重写（Rewrite）、查询扩展（Multi-Query）、假设性文档嵌入（HyDE）、查询构造（Text-to-SQL/Cypher）等。
    *   **检索 (Retrieval)**：执行实际的检索操作。支持多种检索器（稀疏、稠密、混合）以及微调策略（监督微调、语言模型监督微调、适配器等）。
    *   **检索后 (Post-retrieval)**：对检索结果进行后处理，如重排序（Rerank）、压缩（Compression）和选择（Selection），以去除噪声、冗余并提升关键信息的可见性。
    *   **生成 (Generation)**：利用 LLM 和检索到的上下文生成答案。支持生成器的微调（指令微调、强化学习）和验证（基于知识库或模型的验证）。
    *   **编排 (Orchestration)**：这是 Modular RAG 的“大脑”，负责动态控制整个流程的走向。
*   **L2: 子模块 (Sub-module)**：在每个模块内部，功能可以被进一步细化为子模块，用于更精细的优化。
*   **L3: 操作符 (Operator)**：这是最基础的执行单元，是具体功能的实现。例如，在“检索前”模块中，`HyDE`、`Query Rewrite` 等都是独立的操作符。这些操作符是系统可维护性和可理解性的基石。

#### 2.4.2.2 编排控制机制

这是 Modular RAG 超越线性架构的关键，由三个核心组件构成：

*   **路由 (Routing)**：
    *   **概念**：RAG系统根据不同查询将其路由到特定处理管道，这是适应多场景RAG架构的关键功能。需要决策机制来确定将使用哪些模块，依据模型输入或元数据信息进行选择。不同提示或组件有不同的路由机制。例如：一个关于法律的问题可能被路由到一个包含法律知识图谱索引和专业法律验证的流程，而一个普通常识问题则走一个更通用的流程。
    *   **路由机制**：
        *   **元数据路由**：从查询中提取关键术语或实体，并结合块中的元数据优化路由。
        *   **语义路由**：根据查询的语义信息，将其路由到不同的模块。
        *   **混合路由**： 结合语义分析和基于元数据的路由方法，以改进查询路由。
*   **调度 (Scheduling)**：
    *   **概念**：随着RAG系统的复杂性和适应性的提升，调度模块能够有效地管理整个流程。它在模块化RAG系统中发挥着关键作用，帮助识别需要外部数据检索的关键时刻，评估生成结果的质量，并决定是否需要进一步的处理。调度模块常用于递归、迭代和自适应检索场景，确保系统能够在合适的时机停止生成或启动新的检索过程。
    *   **判断机制**：
        *   **规则判断**：后续步骤由一组预设规则控制。如根据结果的置信度阈值判断是否继续。
        *   **LLM判断**：LLM能够独立判断后续操作的流程。通过提示词工程或者微调LLM，使其生成特定的token来触发操作。
        *   **知识引导调度**：利用知识图谱引导检索和生成流程，具体做法是从知识图谱中提取与问题相关的信息并构建推理链。推理链由一系列逻辑相关的节点组成，每个节点提供解决问题的关键信息。基于这些节点的信息，可以分别执行检索和内容生成。这种方法不仅提高了问题解决的效率和准确性，还使生成的解释更加清晰。
*   **融合 (Fusion)**：
    *   **概念**：随着RAG流程超越了传统的线性模式，常常需要扩展检索范围或通过多个处理管道来增加多样性。因此，在扩展到不同分支后，融合模块用于整合信息，确保生成的回答全面且一致。融合模块不仅合并答案，还确保输出内容丰富，并能充分反映问题的多维度特性。
    *   **融合机制**：
        
        *   **LLM融合**： 多分支信息整合的一种直接方法是利用LLM强大的能力，分析并整合来自不同分支的信息。答案长度超出上下文窗口限制时，先逐个进行总结，在长度限制内保留关键信息，再进行融合。
        *   **加权集成**：根据不同分支生成的token加权，进行最终答案的选择。权重由召回结果（result）和查询（query）的相似度得分决定
        
        *   **互惠排序融合（RRF）**：RRF采用加权平均的方法，提升整体预测性能和排名精度。其优势在于动态调整权重，基于分支间的相互作用来优化输出。RRF在模型或数据源异质性较大的情况下，表现出色，能够显著提高预测的准确性。

#### 2.4.2.3 RAG Flow与模式（Pattern）

通过组合模块和操作符，并应用编排逻辑，可以构建出不同的 RAG Flow。论文总结了四种典型模式：

*   **线性模式** (Linear)：最简单的顺序执行，即 Naive 或 Advanced RAG。 ![Linear Pattern](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125806700-2024838393.png)
*   **条件模式** (Conditional)：根据路由结果，选择一条特定的流程执行。 ![Conditional Pattern](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125821383-418936754.png)
*   **分支模式** (Branching)：并行执行多条流程（如多查询检索或多文档分别生成），最后进行融合。 ![Branching Pattern](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125837219-1394756349.png)
*   **循环模式** (Looping)：包括迭代（固定轮次）、递归（树状结构深度探索）和自适应/主动（由 LLM 动态决定何时检索和终止）三种形式。![Loop Pattern](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125850243-928049273.png)
*   **调优模式** (Tuning): RAG系统持续整合更多与LLM相关的技术。在模块化RAG中，许多组件由可训练的语言模型组成。通过微调，可以进一步优化组件性能，并增强其与整体流程的协调性。

### **2.4.3 优点**

*   **极高的灵活性与可扩展性**：可以根据业务需求，像插拔组件一样更换 Embedding 模型、Rerank 模型或数据源，而无需重构整个系统。
*   **多源异构数据融合**：能够同时处理结构化（SQL）、非结构化（Doc）和实时（Web）数据，适用范围极广。
*   **针对性优化**：可以针对流程中的瓶颈点（如某类问题的检索率低）单独增加一个模块进行补强，而不影响其他环节。
*   **工程化友好**：符合现代软件工程的微服务/模块化思想，便于团队协作和版本迭代。

### **2.4.4 缺点**

*   **系统复杂性急剧增加**：多个模块之间的接口标准、数据流转逻辑需要精细设计，开发成本远高于前两种。
*   **调试与追踪困难**：当结果出错时，很难一眼看出是 Routing 选错了路，还是 Rewrite 改错了词，亦或是 Rerank 过滤掉了正确信息。
*   **潜在的延迟累积**：模块越多，意味着处理链路越长。如果编排不当（例如串行运行过多的 LLM 处理节点），会导致用户等待时间过长。
*   **冗余计算**：如果不加控制，复杂的模块编排可能会导致多次重复检索或多次 LLM 调用，推高成本。

_如果说 Naive RAG 是一条单向传送带，Modular RAG 就是一个配备了自动分拣系统、多条支线轨道和循环回路的现代化物流中心。它不预设路径，而是根据快递（Query）的类型，自动决定它该去哪个窗口处理，甚至可以送回上一个工序重跑。它是走向 Agentic RAG（智能体 RAG）的关键阶梯。_

2.5 Agentic RAG
---------------

![AgenticRAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215125946484-669374116.png)

> 摘自[\[2501.09136\] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://arxiv.org/abs/2501.09136)

### **2.5.1 概念介绍**

Agentic RAG 是 RAG 演进的终极形态。如果说 Modular RAG 是配置精良的“自动化流水线”，那么 Agentic RAG 就是一个 **“拥有自主意识的资深研究员”**。

它将大模型（LLM）从被动的“信息加工者”转变为主动的 **“系统控制器（Controller）”**。在 Agentic RAG 中，系统不再遵循预设的线性或分支路径，而是由 Agent 根据用户的问题，自主决定检索时机、选择检索工具、评估检索质量，并决定是否需要多轮迭代。它本质上是 **AI Agent（智能体）与 RAG 技术的深度融合**。

### **2.5.2 机制原理**

Agentic RAG 的核心逻辑在于 **“推理（Reasoning）与行动（Acting）的循环”**。

#### **2.5.2.1 核心架构组件**

参考智能体通用架构，Agentic RAG 同样可以拆解为以下三个关键层级：

*   **L1：大脑（The Brain / Planner）**：
    *   由强大的 LLM担任，负责**意图解析**与**任务拆解**。它不直接检索，而是先思考：“为了回答这个问题，我需要分几步？每一步需要什么工具？”
*   **L2：工具箱（Toolbox / Skill Set）**：
    *   Agent 调用的具体功能单元，包括但不限于：
        *   **Vector Search Tool**：检索私有向量库。
        *   **Web Search Tool**：访问互联网获取最新时政。
        *   **Code Interpreter**：执行 Python 代码进行数据统计分析。
        *   **Knowledge Graph Tool**：查询实体间的复杂关系。
*   **L3：反思机制（Self-Reflection / Critic）**：
    *   这是 Agentic RAG 的灵魂。Agent 会对检索到的内容进行“质检”：
        *   **相关性评估**：搜到的东西有用吗？
        *   **完备性评估**：信息够了吗？还需要再搜别的吗？
        *   **忠实度评估**：生成的结果是否基于事实而非幻觉？

#### **2.5.2.2 运作流程：Agentic Loop**

graph TD User((用户提问)) --> Parse\[意图解析与规划\] Parse --> Loop{Agent 决策循环} subgraph Action \[行动阶段\] Loop -- "调用工具" --> Tools\[向量库/Web/代码\] Tools --> Observation\[观察结果/获取上下文\] end subgraph Reflection \[反思阶段\] Observation --> Eval{自我评估} Eval -- "信息不足/有误" --> Replan\[修正规划并继续\] Replan -.-> Loop end Eval -- "证据充足" --> Final\[最终生成\] Final --> Output((回答用户))

### **2.5.3 常见的 Agentic 模式 (Patterns)**

1.  **单Agent**：一种集中式决策系统，其中单一智能体负责管理信息的检索、路由与整合。该架构通过将这些任务整合到一个统一的智能体中，简化了整个系统，特别适用于工具或数据源数量有限的场景。![SingleAgenticRAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215130029814-1334763044.png)
2.  **多Agent**：多智能体RAG是单智能体架构的一种模块化且可扩展的演进形式，旨在通过多个专业化智能体协同工作，处理复杂的工作流和多样化的查询类型。该系统不再依赖单一智能体来完成所有任务（如推理、检索和生成回答），而是将职责分散到多个智能体上，每个智能体都针对特定角色或数据源进行了优化。![Multi-Agent Agentic RAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215130042224-973341326.png)
3.  **分层式（Hierarchical Agentic RAG）**：采用一种结构化的多层级方法进行信息检索与处理，从而提升效率并增强战略性决策能力。智能体按层级组织，高层级智能体负责监督和指挥低层级智能体。这种结构支持多层次的决策机制，确保每个查询都能由最合适的资源进行处理。![Hierarchical Agentic RAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215130052972-1377020396.png)
4.  **纠错式（Agentic Corrective RAG）**：引入了自我修正检索结果的机制，从而提升文档利用率并改善生成回答的质量（如图19所示）。通过在工作流中嵌入智能代理，纠错式RAG能够对上下文文档和生成的回答进行迭代优化，最大限度地减少错误并提升相关性。![Agentic Corrective RAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215130102462-952039870.png)
5.  **自适应式（Adaptive Agentic RAG）**：通过根据输入查询的复杂度动态调整查询处理策略，提升了大语言模型（LLMs）的灵活性与效率。与静态的检索工作流不同，Adaptive RAG引入了一个分类器，用于评估查询的复杂程度，并据此选择最合适的方法——从单步检索、多步推理，到对简单查询直接跳过检索环节。![AdaptiveAgenticRAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215130110328-490855268.png)

### **2.5.4 优点**

*   **极强的复杂任务处理能力**：擅长处理需要多步推理、数据对比、深度调研的长任务。
*   **极高的准确度与可靠性**：通过内置的“反思-修正”循环，极大程度压低了 RAG 的幻觉率。
*   **动态适应性**：不再受限于固定的 Workflow，能够根据不同问题的深度自动调节“智力投入”。
*   **多工具融合**：打破了“文字找文字”的限制，可以结合代码计算、API 调用等手段，产出更专业的答案。

### **2.5.5 缺点**

*   **高延迟（Latency）**：由于存在多轮 LLM 推理和决策循环，首字响应（TTFT）和完整生成时间显著拉长。
*   **高成本（Token Consumption）**：每一轮“思考-反思”都在消耗大量 Token，尤其在使用顶尖大模型时，成本可能成倍增长。
*   **潜在的失控风险（Instability）**：智能体可能进入“死循环”（Looping）或逻辑漂移，导致给出的结果南辕北辙。
*   **调试难度极大**：决策路径是不确定的，很难像线性流程那样进行逐行 Debug。

_如果说前三者是在“造工具”，Modular RAG 是在“造生产线”，那么 Agentic RAG 就是在“造人”。它让 AI 从一个只会查字典的助手，变成了一个能独立思考、能自我怀疑、能不断修正的研究型数字员工。_

3\. RAG 全流程深挖
=============

> 本章节将深入探讨RAG各个阶段（基于AdvancedRAG），包括：作用、技术细节……

阶段划分方式多种多样，但基本符合下方的RAG链路，下文也基于本模型对技术点进行归类  
![RAG Chain](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215130131393-159934606.png)

3.1 Indexing
------------

### 3.1.1 数据提取（Loading）

#### 3.1.1.1 概念介绍

数据提取是指将企业私有或外部非结构化数据（如 PDF、Word、网页、数据库记录、视频字幕等）统一转换为程序可处理的纯文本格式，并提取关键元数据（如文件名、章节标题、创建时间等）的过程。该过程需处理格式解析、噪声清洗、结构保留等任务。

#### 3.1.1.2 作用

*   为后续分块、嵌入提供标准化输入；
*   元数据支持后期按来源、时间、权限等进行过滤；
*   保留原始文档结构（如标题层级）有助于提升检索语义完整性。

#### 3.1.1.3 原理与实现

常用工具包括：

*   Unstructured：支持 30+ 格式，含 OCR（需 Tesseract）；
*   LlamaIndex 的 SimpleDirectoryReader：自动识别目录下文件类型，返回统一 Document 对象；
*   LangChain 的 Document Loaders：模块化设计，每个格式对应一个 loader。

#### 3.1.1.4 优缺点

*   **优点**：多源兼容、元数据自动绑定、接口简洁。
*   **缺点**：扫描 PDF 依赖 OCR，准确率受限；复杂表格、图表难以结构化还原。

### 3.1.2 文本分块（Chunking）

#### 3.1.2.1 概念介绍

由于 Embedding 模型有最大输入长度限制（如 BERT 为 512 tokens，OpenAI ada-002 为 8191），必须将长文档切分为多个“块”（chunks）。但简单按字符或 token 切分会割裂语义（如“因为...所以...”被拆开），因此需在长度限制与语义完整性之间取得平衡。

#### 3.1.2.2 作用

*   适配 Embedding 模型输入要求；
*   控制检索粒度：块过大 → 信息冗余；块过小 → 信息碎片；
*   直接影响最终答案的准确性与相关性。

#### 3.1.2.3 原理与实现

常见策略包括：

1.  **固定长度分块**
    *   按 token 数切分（如 512），头尾增加 overlap（如 50 tokens）缓解边界断裂。
2.  **句子级分块**
    *   以句号、问号、换行符为边界，保留完整语义单元。
3.  **递归分块**（LlamaIndex 默认）
    *   优先按段落，再按句子，最后按字符，兼顾结构与长度。
4.  **文档结构分块**
    *   利用文档自身结构（如标题、段落、章节等）进行切分的策略，能有效保留语义完整性和上下文连贯性
5.  **层级分块**（父-子块）
    *   文档 → 父块（1024 tokens）→ 子块（256 tokens），用于“父文档检索器”。

#### 3.1.2.4 优缺点

**优点**：overlap 降低语义断裂风险；元数据保留块来源便于溯源。  
**缺点**：固定切分仍可能破坏逻辑结构；语义分块需 LLM 辅助，成本高。

#### 3.1.2.5 非文本类型

笔者特别说明：RAG处理的数据源并非全都是非结构化的文档，有时会通过其他方式构建一个Chunk，或者称之为**RAG所用的语义单元**更为合适。

例如我们有这样一个商品对象

id

name

price

description

created\_time

10011

索尼A6000

2000

一台二手的索尼微单相机

2014-1-1

笔者根据自己习惯，会将其拆分成三类对象：

*   **Key**：用于和query进行匹配，**召回Value**。
    *   vector index：用于embedding。这里选用description
    *   keywords index：用于构建倒排索引。这里选用name
*   **Value**：最终返回的返回结果。这里选用name+price+description（id对LLM而言无语义价值）
*   **Filter**：可用于过滤的字段，用于压缩检索空间，提升检索效果。这里选用price、created\_time

需要注意的问题，

*   Key的信息源（字段）、形式（json/md/扁平字符串）可能会影响embedding构建的特征，需要结合具体场景进行优化，从而达到更好的效果
*   Value不用一股脑返回所有信息，选择必要部分，避免上下文污染/噪声

### 3.1.3 数据入库（Indexing）

> 这里给自己开个坑，后续单独写一篇向量数据库相关的博客

#### 3.1.3.1 概念介绍

数据入库是将文本块及其向量写入向量数据库，并构建高效检索索引的过程。暴力搜索（Flat Index）在百万级数据下效率低下，因此现代系统采用近似最近邻（ANN）算法（如 HNSW、IVF）实现毫秒级检索。

#### 3.1.3.2 作用

*   支持高并发、低延迟的 Top-K 检索；
*   支持元数据过滤（如“只查 2024 年文件”）；
*   支持混合检索（向量 + 关键词）。

#### 3.1.3.3 原理与实现

主流向量数据库对比：

数据库

类型

特点

适用规模

FAISS

库

Meta 开源，纯内存，不支持动态增删

<100万

Chroma

轻量DB

易用，适合原型

<50万

Milvus

生产级

分布式、支持标量过滤、混合检索

百万~十亿

Pinecone

云服务

全托管，免运维

任意规模

索引类型：

*   HNSW：图结构，高召回 + 低延迟；
*   IVF：先聚类，再类内搜索，适合超大规模。

#### 3.1.3.4 优缺点

**优点**：ANN 索引在百万数据下仍可 <50ms；元数据过滤支持业务逻辑。  
**缺点**：FAISS 不支持在线增删，生产环境慎用；云服务有数据出境风险。

3.2 Pre-Retrieval
-----------------

### 3.2.1 查询转换（Query Transformation）

#### 3.2.1.1 概念介绍

查询转换是一类利用大语言模型（LLM）对原始查询进行语义改写、分解或抽象的技术，目的是提升检索阶段的召回覆盖率。典型方法包括多查询生成（RAG Fusion）、Step-back Prompting 和查询重写。

#### 3.2.1.2 作用

*   解决用户表述与知识库表述不一致的问题（如“手机没电快” vs “电池续航短”）；
*   将复杂问题拆解为多个可检索的子问题；
*   自动纠正拼写或补充上下文。

#### 3.2.1.3 原理与实现

##### RAG Fusion（多查询融合）：

1.  构造提示词模板：
2.  LLM 输出 4 个语义相关的子查询。例如：
    *   原始查询：“LangChain 和 LlamaIndex 哪个更适合企业级 RAG？”
    *   生成子查询：
        *   “LangChain 企业级 RAG 支持情况”
        *   “LlamaIndex 在生产环境中的稳定性”
        *   “LangChain 与 LlamaIndex 对比”
        *   “企业如何选择 RAG 框架”
3.  对每个子查询并行执行向量检索，得到多组 Top-K 结果；
4.  使用 RRF（Reciprocal Rank Fusion） 算法对所有结果重排序：
    
    \\\[\\text{Score}(d) = \\sum\_{i=1}^{n} \\frac{1}{k + \\text{rank}\_i(d)} \\\]
    
    其中 k通常取 60，\\(\\text{rank}\_i(d)\\)为文档 d 在第 i 个查询结果中的排名；
5.  返回 RRF 分数最高的 Top-K 作为最终检索输入。

##### Step-back Prompting：

*   让 LLM 生成一个更抽象的问题（如“RAG 的作用是什么？”），检索高层知识；
*   同时保留原始查询检索结果；
*   两组上下文共同注入 Prompt，提升答案深度。

#### 3.2.1.4 优缺点

**优点**：

*   显著提升复杂、模糊问题的召回率；
*   生成查询成本低（约 100 tokens），远低于最终答案生成（1000+ tokens）；
*   具备自动纠错与上下文补充能力。

**缺点**：

*   延迟增加：需一次额外 LLM 调用；
*   术语误解风险：当查询含领域专有词（如“注意力机制”）而 LLM 未理解上下文时，可能生成无关查询（如“如何集中注意力学习？”）；
*   收益不确定：对简单事实性问题可能无增益。

> 规避术语误解的方法：
> 
> *   在提示中限定角色：“你是一个解释 Transformer 架构的助手”；
> *   提供 few-shot 示例；
> *   微调小型 LLM 专门用于查询生成。
> *   提供全面的背景知识（例如针对企业内部信息）

> 适用场景：
> 
> *   比较类、多因素、多跳推理问题；
> *   用户表述高度口语化或模糊；
> *   知识库覆盖全面但表述多样。

### 3.2.2 假设性文档嵌入（HyDE, Hypothetical Document Embeddings）

![HyDE RAG](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215130155742-1763883895.png)

> 图摘自CSDN的某篇文章，文章找不到了，但是图中有标注作者

#### 3.2.2.1 概念介绍

**HyDE**（Hypothetical Document Embeddings）是一种“生成后检索”的技术。它不直接使用用户的原始查询进行检索，而是先让 LLM 生成一个伪造的“假设性文档”（即预估答案），然后利用该假设文档的向量去知识库中寻找真实的相似文本块。

#### 3.2.2.2 作用

*   **缩小语义鸿沟**：解决“问题”与“答案”在向量空间中距离较远的问题。例如，问题“如何解决数据库死锁？”与答案“通过优化事务顺序...”在语义表达上存在差异，但“假设答案”与“真实答案”在表达方式上高度接近。
*   **提升稠密检索性能**：在零样本（Zero-shot）场景下，比单纯的向量检索具有更强的召回精度。

#### 3.2.2.3 原理与实现

1.  **生成假设文档**：接收用户查询\\(q\\)，通过特定的 Prompt 指导 LLM 生成一个捕获相关模式的文档 \\(d\\)（无需保证事实准确性）；
    *   举一个粗暴的例子：查询“add(1, 2)”，LLM 生成假设答案“结果是 3”，使用“3”的向量去匹配知识库，比直接用“add(1, 2)”匹配效率更高。
2.  **向量化编码**：使用 Embedding 模型将\\(d\\)转化为向量\\(v\_d\\) ;
3.  **相似度检索**：在向量数据库中检索与\\(v\_d\\)最接近的真实文档块\\(d\\)；
4.  **生成最终答案**：将检索到的真实上下文送入 LLM 生成正式回答。

#### 3.2.2.4 优缺点

**优点**：

*   **语义对齐精准**：通过将“问题-文档”匹配转变为“文档-文档”匹配，显著提升召回质量；
*   **无需改动索引**：仅在查询侧增加一次 LLM 调用，无需重新构建向量库。  
    **缺点**：
*   **幻觉风险**：若 LLM 生成的假设文档包含严重的误导性信息，可能导致检索方向彻底偏离；
*   **成本与延迟**：增加了生成假设文档的时间开销和 Token 消耗。

> **适用场景**：
> 
> *   知识库文档较长，而用户提问极短的场景；
> *   缺乏微调数据，需要依赖预训练模型通用能力的场景。

* * *

### 3.2.3 聊天引擎（Chat Engine）

#### 3.2.3.1 概念介绍

聊天引擎是 RAG 系统支持多轮对话的核心组件。它通过引入“记忆（Memory）”机制，处理对话中的指代消解（Coreference Resolution）和上下文依赖，使用户能够以自然对话的方式与知识库交互。

#### 3.2.3.2 作用

*   **指代消解**：识别用户口中的“它”、“那个产品”具体指代历史对话中的哪个实体。
*   **上下文连贯**：允许用户基于前序回答进行追问（如“那它的价格呢？”），而无需重复提供背景信息。

#### 3.2.3.3 原理与实现

常见的实现模式包括以下两种：

1.  **Context Chat Mode（上下文模式）**：
    *   直接检索与当前查询相关的上下文；
    *   将“历史对话记录 + 检索到的上下文 + 当前查询”全部堆叠进 Prompt；
    *   **特点**：简单直接，保留了原始对话的完整语义。
2.  **Condense Plus Context Mode（压缩重写模式）**：
    *   **第一步（压缩）**：LLM 接收“历史对话 + 当前查询”，将其重写为一个独立的、语义完整的查询（Standalone Query）。
        *   示例：
            *   历史：“iPhone 17 Pro Max 怎么样？”
            *   当前：“它多少钱？”
            *   重写后：“iPhone 17 Pro Max 的官方售价是多少？”
    *   **第二步（检索）**：使用重写后的查询执行向量搜索。
    *   **第三步（生成）**：将检索结果送入 LLM 生成最终回答。

#### 3.2.3.4 优缺点

**优点**：

*   **交互自然**：提供了接近人类对话的体验；
*   **意图明确**：通过重写机制，可以有效过滤对话中的冗余噪音。  
    **缺点**：
*   **Token 膨胀**：随着对话轮数增加，Prompt 长度迅速增长，可能触及模型上下文上限；
*   **累积误差**：如果某一轮查询重写失败，会导致后续所有检索和生成出现偏差。

> **优化策略**：
> 
> *   **滑动窗口记忆**：仅保留最近\\(N\\)轮对话，防止 Token 过载；
> *   **摘要记忆**：定期让 LLM 对长对话进行摘要，作为压缩后的背景信息。

> **适用场景**：
> 
> *   智能客服、个人助理等需要深度交互的场景；
> *   涉及复杂决策、需要多步引导的问答任务。

3.3 Retrieval
-------------

### 3.3.1 关键词检索 (Keyword Search)

关键词检索是最传统和基础的搜索引擎技术，至今仍在许多场景下发挥着核心作用。

#### 3.3.1.1 概念介绍

关键词检索，也称为词法检索或全文检索，**是一种基于字面匹配的搜索方式**。它将用户的查询语句和文档内容都视为一系列独立的词语（关键词），通过匹配这些词语来寻找相关文档。这种方法不深入理解词语背后的真实含义或用户意图。

#### 3.3.1.2 作用

*   **精准性高**：对于专有名词、产品型号、代码、人名、错误代码等特定术语的查询，关键词检索能够提供精确的匹配结果。
*   **速度快且成本低**：技术成熟，通过倒排索引等机制可以实现非常快速的检索，计算资源消耗相对较小。
*   **结果可预测且易于解释**：检索结果的出现是基于明确的关键词匹配，用户很容易理解为什么某个文档会被返回。

#### 3.3.1.3 原理与实现

关键词检索的核心机制是**倒排索引 (Inverted Index)**。系统首先会对所有文档进行分词处理，然后创建一个索引，其中每个词都指向包含该词的文档列表。当用户输入查询时，系统同样对查询进行分词，然后在索引中查找这些词，快速定位到所有相关的文档。

此外，为了对检索到的文档进行排序，关键词检索通常会使用**排序算法**，如 **TF-IDF** (Term Frequency-Inverse Document Frequency) 或 **BM25** (Best Matching 25)。这些算法主要根据以下因素评估文档与查询的相关性：

*   **词频 (Term Frequency)**：关键词在文档中出现的频率。
*   **逆文档频率 (Inverse Document Frequency)**：关键词在整个文档库中的稀有程度。一个词越稀有，权重通常越高。

在向量空间模型中，关键词检索被视为使用**稀疏向量 (Sparse Vectors)** 的过程，向量的每一维度对应词典中的一个词，只有文档中出现的词，其对应的维度才会有值。

#### 3.3.1.4 适用场景

关键词检索非常适用于那些用户明确知道自己要查找的确切词语的场景，例如：

*   **代码或日志搜索**：在程序代码或系统日志中查找特定的函数名或错误信息。
*   **法律与合规文件查询**：查找包含特定法条或专有名词的法律文件。
*   **电商平台**：搜索特定的产品型号、SKU或品牌名称。
*   **数据库查询**：进行布尔查询或精确短语匹配。

### 3.3.2 语义检索 (Semantic Search)

随着人工智能和自然语言处理 (NLP) 技术的发展，语义检索应运而生，它旨在让搜索更“智能”。

#### 3.3.2.1 概念介绍

语义检索是一种更先进的搜索技术，它超越了字面匹配，致力于理解用户查询背后的**意图和上下文含义**。它可以识别同义词、近义词以及概念之间的关联，即使用户查询的措辞与文档中的内容不完全一致，也能找到高度相关的结果。

#### 3.3.2.2 作用

*   **相关性更高**：通过理解“意思”而非仅仅是“词语”，语义检索能返回更多虽然用词不同但内容相关的结果，有效提升召回率。
*   **支持自然语言和模糊查询**：用户可以使用日常对话式的语言进行提问，或者在忘记确切名称时进行模糊描述。
*   **跨语言能力**：先进的语义模型可以理解不同语言之间的概念关联，实现跨语言搜索。

#### 3.3.2.3 原理与实现

语义检索的核心是**向量嵌入 (Vector Embeddings)** 技术。它借助深度学习模型（如BERT等）将文本（无论是查询语句还是文档）转换为高维空间中的数学表示——**稠密向量 (Dense Vectors)**。

其工作流程如下：

1.  **文本向量化**：使用预训练的语言模型将查询和文档库中的所有文本内容都转换成向量。在这些向量中，语义相近的词语或句子在向量空间中的距离也更近。例如，“汽车”和“轿车”的向量会非常接近。
2.  **相似度计算**：当用户输入查询后，系统将其转换为一个查询向量，然后在向量数据库中计算这个查询向量与所有文档向量之间的“距离”（通常使用余弦相似度等算法）。
3.  **返回结果**：系统会返回与查询向量距离最近、即语义最相似的文档作为结果。  
    这种机制使得搜索不再局限于关键词，而是基于概念和意义的匹配。

#### 3.3.2.4 适用场景

语义检索特别适合那些用户意图比较复杂或模糊的场景：

*   **智能问答系统**：用户提出一个问题，系统需要理解问题并从知识库中找到对应的答案段落。
*   **企业知识库**：员工在公司内部庞大的知识库中查找解决方案或信息，他们可能不知道确切的文件名或术语。
*   **推荐系统**：根据用户正在浏览的内容，推荐语义上相关的其他产品或文章。
*   **概念性或研究性搜索**：用户探索一个概念，希望能找到所有相关的资料，而不仅仅是包含特定关键词的文档。

### 3.3.3 图检索 (Graph Search)

当数据之间的“**关系 (Relationship)**”比数据本身更重要时，图检索便展现出其独特的、不可替代的威力。

#### 3.3.3.1 概念介绍

图检索是在知识图谱 (Knowledge Graph) 或图数据库上进行的检索。它不只关注文档内容，更关注**实体 (Entity)**（如函数、开发者、服务）之间的**连接关系 (Edge)**（如调用、属于、负责）。查询的目标是探索这些实体间的复杂关系网络，回答“为什么”和“怎么样”的问题。

#### 3.3.3.2 作用

*   **深度洞察 (Deep Insights)**：能够回答复杂的、关于“关系”的问题，发现数据中隐藏的连接和模式。
*   **上下文感知 (Context-Aware)**：提供高度相关的上下文信息，而不仅仅是孤立的答案，呈现问题的全貌。
*   **精准推理 (Precise Reasoning)**：基于已有的关系，可以进行一定程度的逻辑推理，预测潜在影响。

#### 3.3.3.3 原理与实现

图检索的流程通常分为两步：

1.  **图谱构建 (Graph Construction)**：通过从各种结构化和非结构化数据源中提取实体（节点）和关系（边）来构建知识图谱，并存入图数据库（如Neo4j）。
2.  **图查询 (Graph Querying)**：当用户用自然语言提问时，系统将其转换为一种结构化的图查询语言（如Cypher或SPARQL），然后在图上执行遍历、匹配和查找操作，以找到答案。

#### 3.3.3.4 适用场景

*   **软件工程**：代码依赖分析（“修改这个函数会影响哪些下游服务？”）、故障根因分析。
*   **金融风控**：发现隐藏的关联交易、反欺诈网络分析。
*   **社交网络分析**：社群发现、影响力分析。
*   **智能推荐**：基于用户关系图谱和兴趣图谱进行更精准的推荐。

### 3.3.4 混合检索 (Hybrid Search)

混合检索结合了关键词检索和语义检索的优点，是现代搜索引擎架构的主流选择，旨在实现最佳的搜索效果。

#### 3.3.4.1 概念介绍

混合检索是一种将两种或多种检索技术结合起来的策略，最常见的组合就是关键词检索和语义检索。它也被称为多路召回或融合检索，目标是同时利用关键词的精确性和语义的上下文理解能力，以提供更全面、更准确的搜索结果。

#### 3.3.4.2 作用

*   **优势互补**：结合了关键词检索的精准匹配能力和语义检索的上下文理解能力，弥补了各自的短板。例如，语义检索可能无法处理训练数据中未见过的专有术语（如新产品名），而这正是关键词检索的强项。
*   **提升搜索质量和覆盖面**：能够同时处理精确查询和模糊查询，显著提升搜索结果的召回率和相关性。
*   **处理域外数据的能力强**：对于新出现的词汇、特定术语或公司内部行话，混合检索通过关键词部分仍能有效召回。

#### 3.3.4.3 原理与实现

混合检索的典型实现流程如下：

1.  **并行检索**：当收到用户查询时，系统会同时启动两条并行的检索路径：一条是基于关键词的稀疏向量检索，另一条是基于语义的稠密向量检索。
2.  **结果融合与重排序**：两条路径各自返回一个候选结果列表。然后，系统需要将这两个列表融合成一个统一的、经过优化的最终列表。最常用的融合算法是**倒数排序融合** (Reciprocal Rank Fusion, RRF)。RRF算法会综合考虑一个文档在两个不同列表中的排名，并给出一个最终的融合分数，然后根据这个分数对所有结果进行重新排序。
3.  （可选）**精排**：在某些高级应用中，还会在融合后的结果基础上再引入一个重排序模型 (Re-ranker)，对排名靠前的结果进行更精细的二次排序，进一步提升结果的精准度。

#### 3.3.4.4 适用场景

混合检索的适用性非常广泛，是构建高性能、现代化搜索引擎的首选方案：

*   **通用搜索引擎**：如网页搜索，需要应对用户五花八门的查询需求。
*   **电商搜索**：用户可能搜索精确的“iPhone 15 Pro Max”，也可能搜索模糊的“适合拍照的手机”。混合检索能很好地同时满足这两种需求。
*   **企业级搜索平台**：需要同时处理技术文档中的精确代码片段查询和业务文档中的概念性问题。
*   **检索增强生成 (RAG) 系统**：在大型语言模型的应用中，RAG需要从知识库中检索最相关的信息来辅助生成答案。混合检索能够显著提升信息召回的质量，从而改善最终生成内容的准确性。

### 3.3.5 四种检索技术对比

特性

关键词检索 (Keyword Search)

语义检索 (Semantic Search)

图检索 (Graph Search)

混合检索 (Hybrid Search)

核心思想

字面精确匹配 (Lexical Matching)

上下文含义匹配 (Conceptual Matching)

实体关系匹配 (Relational Matching)

优势互补，协同作战

技术原理

倒排索引 + BM25 (稀疏向量)

深度学习模型 + 向量嵌入 (稠密向量)

知识图谱 + 图查询语言

并行检索 + RRF融合重排

优势

精准、快速、可解释

相关性好、支持自然语言

深度洞察、上下文感知

效果最佳、覆盖广、鲁棒性强

劣势

词汇鸿沟、召回率低

成本高、对专有名词不敏感

构建成本高、查询复杂

系统复杂度高、对融合算法要求高

适用场景

代码搜索、日志检索、法律查询

智能问答、知识库搜索、内容推荐

依赖分析、风控、社交网络

通用搜索、企业智能平台、高级RAG

3.4 Post-Retrieval
------------------

### 3.4.1 重排序（Reranking）

#### 3.4.1.1 概念介绍

在召回阶段（Retrieval）获取的 Top-K 文档通常基于粗略的相似度计算。重排序阶段引入更精细的模型（如 Cross-Encoder），对召回的文档与查询进行二次匹配度打分并重新排序。

#### 3.4.1.2 作用

*   解决语义搜索中“看似相关但实际无关”的问题；
*   缩小送入 LLM 的上下文范围，提升生成准确性，降低 Token 开销。

#### 3.4.1.3 原理与实现

*   使用交叉编码器（Cross-Encoder）模型（如 BGE-Reranker、Cohere Rerank）；
*   将 `(Query, Chunk)` 对同时输入模型，捕捉两者间的深度交互特征；
*   根据模型输出的分数重新排列文档，选取相关性最高的前 N 个。

#### 3.4.1.4 优缺点

**优点**：显著提升检索精度，解决召回结果中的噪音问题；  
**缺点**：相比向量检索计算开销较大，增加系统响应延迟。

### 3.4.2 上下文压缩与过滤（Context Compression & Filtering）

#### 3.4.2.1 概念介绍

在将检索到的文本块送入 LLM 之前，剔除其中的冗余信息或无关片段，仅保留与问题核心相关的部分。

#### 3.4.2.2 作用

*   减少 LLM 需处理的 Token 数量，降低成本；
*   缓解“中间丢失（Lost in the Middle）”现象，让模型更关注核心上下文。

#### 3.4.2.3 原理与实现

*   **长文本压缩**：利用小型 LLM 或特定算法（如 LLMLingua）提取文本摘要或关键词；
*   **元数据过滤**：基于预定义的业务规则（如日期、权限、置信度阈值）过滤掉不合规的块。

#### 3.4.2.4 优缺点

**优点**：提高生成效率，减少模型受噪音干扰的概率；  
**缺点**：若压缩过度可能导致关键信息丢失。

### 3.4.3 语句窗口检索器（Sentence Window Retriever）

#### 3.4.3.1 概念介绍

该方法以单个句子为单位进行向量化和检索。检索到最相关句子后，扩展其前后若干句（如 k=2）作为完整上下文送入 LLM。

#### 3.4.3.2 作用

*   提高检索精度（句子粒度更细）；
*   保证送入 LLM 的上下文语义连贯。

#### 3.4.3.3 原理与实现

*   文档按句切分，每句独立嵌入；
*   检索返回最相关句子；
*   扩展前后 k 句组成段落；
*   整个段落作为上下文注入 Prompt。

#### 3.4.3.4 优缺点

**优点**：检索准、上下文完整；  
**缺点**：存储开销大（每句需存向量 + 原文）。

### 3.4.4 父文档检索器（Auto-Merging Retriever）

#### 3.4.4.1 概念介绍

文档被递归切分为父块（如 1024 tokens）和子块（如 256 tokens）。检索仅在子块索引中进行，但若多个子块属于同一父块，则用父块替代子块作为上下文。

#### 3.4.4.2 作用

*   子块保证检索细粒度；
*   父块保证上下文连贯性。

#### 3.4.4.3 原理与实现

*   构建父子块索引，子块含父块引用；
*   检索 Top-K 子块；
*   统计父块出现频次，若某父块下子块 ≥ n，则用该父块替代所有子块；
*   送入 LLM 的是合并后的父块。

#### 3.4.4.4 优缺点

**优点**：平衡检索精度与上下文完整性；  
**缺点**：实现复杂，需维护父子关系。

3.5 Generation
--------------

### 3.5.1 提示词优化（Prompt Engineering for RAG）

#### 3.5.1.1 概念介绍

通过设计结构化的提示词模板，将检索到的上下文与用户的查询有机结合，引导 LLM 严格基于所提供的知识进行回答。

#### 3.5.1.2 作用

*   约束模型生成行为，减少幻觉（Hallucination）；
*   规范输出格式（如 Markdown、JSON）和语言风格。

#### 3.5.1.3 原理与实现

*   典型的提示词结构：`[指令] + [检索到的参考上下文] + [用户查询] + [输出约束]`；
*   强调“若上下文中不包含答案，请回答不知道”。

#### 3.5.1.4 优缺点

**优点**：成本低、见效快，能显著改善生成质量；  
**缺点**：高度依赖模型指令遵循能力，Prompt 长度受模型上下文窗口限制。

### 3.5.2 引用与溯源（Citations & Attribution）

#### 3.5.2.1 概念介绍

在 LLM 生成的回答中，标注出具体信息来源于哪一个参考文档块（如使用 \[1\]\[2\] 等注脚）。

#### 3.5.2.2 作用

*   增加回复的可信度，方便用户核实原始信息；
*   辅助调试 RAG 系统，判断错误源于检索还是生成。

#### 3.5.2.3 原理与实现

*   在 Prompt 中要求模型返回答案的同时，附带来源索引；
*   通过解析模型输出，将索引映射回原始文档的元数据（如 URL、文件名）。

#### 3.5.2.4 优缺点

**优点**：增强系统透明度和安全性；  
**缺点**：增加了生成内容的复杂度，可能对生成流畅性有微小影响。

### 3.5.3 生成结果检验（Hallucination Detection / Evaluation）

#### 3.5.3.1 概念介绍

在最终回复输出给用户前，通过自动化手段评估生成内容是否忠实于检索到的上下文。

#### 3.5.3.2 作用

*   拦截错误的预测信息，防止误导用户；
*   实现系统的闭环监控。

#### 3.5.3.3 原理与实现

*   **RAGAS 框架**：评估 Faithfulness（忠实度）、Relevance（相关性）等指标；
*   **双模型校验**：利用性能更强的模型（如 GPT-4）对生成模型的结果进行一致性审核。

#### 3.5.3.4 优缺点

**优点**：极大降低业务风险，提升系统稳定性；  
**缺点**：引入额外的审核环节会增加系统整体延迟和成本。

4\. RAG 的度量与评测
==============

4.1 为什么需要评测？
------------

### 4.1.1 核心痛点：多级系统的“故障归因”

RAG 系统是一个复杂的流水线。当最终回答错误时，开发者面临 **“归因黑盒”** ：是向量库索引质量差？检索器找错了分块？还是大模型忽略了上下文？评测的本质是将这个黑盒拆解，量化每一环节的损耗。

### 4.1.2 关键目标

*   **量化性能表现**：在更换 Embedding 模型、改变 Chunk Size 或调整 Top-K 时，提供数据支撑的决策依据，而非依赖“感觉”。
*   **阻断幻觉外溢**：RAG 的初衷是抑制幻觉，但错误的检索逻辑反而会引入噪声。评测能确立 **“事实性基准”**，确保系统在给定上下文内输出。
*   **优化成本与延迟**：评测不仅关注质量，还通过量化 Token 消耗与 Latency，寻找性能与成本的最优平衡点（Pareto Frontier）。

4.2 用什么评测：评测集
-------------

### 4.2.1 评测集数据结构

RAG评测集的核心结构需包含四个关键要素，形成“问题-上下文-答案-基准”的闭环验证体系：

1.  **问题（Question）**  
    需覆盖不同难度和场景，如简单事实查询（如“第一届超级碗何时举行？”）、多文档推理题（综合多个文档信息）、噪声干扰题（含无关信息）等。例如在NoMIRACL数据集中，问题还需适配18种语言，测试多语言鲁棒性。
2.  **上下文（Contexts）**  
    通常为文档分块的列表，既包含与问题相关的正确信息，也可能混入无关内容以测试检索器的抗干扰能力。例如RGB基准会在上下文中掺入噪声，评估模型对冗余信息的过滤能力。
3.  **答案（Answer）**  
    由RAG系统生成的响应，需与上下文高度相关且避免幻觉。例如RAGTruth数据集通过逐词标注，专门检测答案中与上下文矛盾的“词级别幻觉”。
4.  **基准事实（Ground Truth）**  
    人工标注的标准答案，用于对比生成答案的准确性。例如RAGAS框架明确要求每个问题对应唯一基准事实，确保评估的客观性。  
    **典型数据样例**（以RAGAS格式为例）：

    {   
    	"question": ["第一届超级碗是什么时候举行的?"],   
    	"answer": ["第一届超级碗于1967年1月15日举行"],   
    	"contexts": [["第一届AFL-NFL世界冠军赛于1967年1月15日在洛杉矶纪念体育馆举行"]],   
    	"ground_truth": ["第一届超级碗于1967年1月15日举行"] 
    }
    

### 4.2.2 评测集构建方式

#### 4.2.2.1 传统基准测试集 (Public Benchmarks)

*   **概念**：使用学术界公开的、经过人工标注的大规模数据集。
*   **常见数据**：**HotpotQA**（多跳推理）、**MS MARCO**（真实搜索场景）、**Natural Questions**（事实性问答）。
*   **局限性**：无法覆盖企业的私域知识库，存在数据泄露给 LLM 训练集的风险。

#### 4.2.2.2 合成数据生成 (Synthetic Data Generation, SDG)

*   **机制**：利用强模型针对私有文档库自动生成测试集。
*   **核心流程 (Evol-Instruct 思路)**：
    1.  **种子提取**：从文档库中随机采样不同分布的文本块（Chunks）。
    2.  **反向提问**：LLM 根据文本块生成一个简单的 Question。
    3.  **复杂化演进**：通过 Prompt 工程，将简单问题重写为**推理型（Reasoning）**、**多上下文型（Multi-context）** 或 **条件约束型（Conditional）** 问题。
*   **作用**：极大地降低了标注成本，且能确保考题与实际业务知识高度相关。

4.3 评测关注什么：评测指标
---------------

**RAG 三元组（RAG Triad）** 是由 **TruLens**（由 Arize AI 开发的一个开源评估框架）提出的，用于系统化评估**检索增强生成（Retrieval-Augmented Generation, RAG）**应用质量的核心理论框架。

由于 RAG 系统比单纯的 LLM 对话更复杂（涉及外部知识库的检索），简单的“黑盒测试”很难发现问题出在哪里。RAG 三元组将评估过程分解为三个关键维度，帮助开发者精准定位是**检索阶段**还是**生成阶段**出现了问题。

### 4.3.1 RAG 三元组的核心结构

![RAG三元组](https://img2024.cnblogs.com/blog/3614123/202602/3614123-20260215130229041-1700867191.png)

RAG 的工作流程通常包含：**Query（查询）** → **Context（检索到的上下文）** → **Answer（生成的回答）**。

基于这三个环节，RAG 三元组定义了三条评估路径：

1.  **上下文相关性 (Context Relevance)**：Query↔️Context
2.  **诚实度/依据性 (Groundedness/Faithfulness)**：Context↔️Answer
3.  **回答相关性 (Answer Relevance)**：Query↔️Answer

### 4.3.2 详细拆解三元组指标

#### 4.3.2.1 检索端-上下文相关性 (Context Relevance)

*   **评估对象**：查询（Query）与 检索到的上下文（Context）之间。
*   **核心问题**：检索出来的这些文档片段，真的能回答用户的问题吗？
*   **评估目的**：衡量检索（Retrieval）环节的质量。
*   **低分原因**：
    *   向量数据库的索引质量差。
    *   Embedding 模型不匹配。
    *   检索策略（如 Top-K）设置不当。
    *   知识库中根本没有相关信息。

#### 4.3.2.2 生成端-诚实度/依据性 (Groundedness / Faithfulness)

*   **评估对象**：生成的回答（Answer）与 检索到的上下文（Context）之间。
*   **核心问题**：生成的答案是否完全基于检索到的内容？有没有胡编乱造（幻觉）？
*   **评估目的**：衡量生成（Generation）环节的忠诚度，防止大模型脱离事实。
*   **低分原因**：
    *   模型“幻觉”严重，利用了自身的训练数据而非提供的上下文。
    *   系统提示词（System Prompt）没有强制要求“只能根据上下文回答”。

#### 4.3.2.3 回答相关性 (Answer Relevance)

*   **评估对象**：生成的回答（Answer）与 原始查询（Query）之间。
*   **核心问题**：回答是否真正解决了用户提出的问题？
*   **评估目的**：衡量最终输出的有用性和逻辑一致性。
*   **低分原因**：
    *   模型虽然依据了上下文，但回答得词不达意、过于简略或绕过了核心问题。
    *   提示词工程（Prompt Engineering）引导不佳。

### 4.3.3 为什么需要 RAG 三元组？（调试价值）

RAG 三元组最大的作用是**故障诊断**。当你发现 RAG 系统效果不好时，可以通过三元组快速定位原因：

现象

问题根源

优化方向

**Context Relevance 低**

检索不到对的信息

优化 Embedding、切片（Chunking）策略、引入重排序（Rerank）

**Groundedness 低**

回答是瞎编的（有幻觉）

调整 Prompt 要求、更换推理能力更强的模型、降低 Temperature

**Answer Relevance 低**

回答驴唇不对马嘴

调整 Prompt 的结构、检查上下文是否干扰了模型理解问题

4.4 评测框架
--------

基于 Ragas、RAGChecker、Evalscope、TruLens、Aries 等主流开源框架的技术实现，剥离了具体的代码实现细节，将其通用架构抽象为**五个核心功能模块**。

RAG 评测框架旨在解决检索增强生成系统中的“黑盒”问题，即**量化衡量检索模块（Retriever）与生成模块（Generator）的各自表现及协同效果**。尽管各框架侧重点不同，但其底层架构均遵循 “**数据构建 -> 链路追踪 -> 指标计算 -> 诊断分析**” 的逻辑闭环。

### 4.4.1 模块一：测试数据集构建与管理 (Testset Generation & Management)

#### 4.4.1.1 概念说明

该模块负责管理评测所需的“考题”。一个标准的 RAG 评测数据样本通常包含三要素：

*   **Query (用户提问)**
*   **Context/Documents (参考文档/知识库切片)**
*   **Ground Truth (标准答案)** —— 注：部分无参考评测（Reference-free）不需要此项。

#### 4.4.1.2 作用说明

*   **基准确立**： 确立评测的输入标准，保证不同模型或版本在同一水平线上对比。
*   **场景模拟**： 通过构造不同类型的问题（如多跳推理、条件限制、负样本），测试 RAG 系统在复杂场景下的鲁棒性。

#### 4.4.1.3 常见实现机制

*   **合成数据生成** (Synthetic Data Generation)：
    *   **机制**： 这是 Ragas 等框架的核心特性。利用强模型（如 GPT-4）遍历知识库文档，反向生成 Query-Answer 对。
    *   **进化策略**： 采用类似于 `Evol-Instruct` 的方法，自动改写 Query，增加复杂度（如添加“如果...”、“除了...”等约束条件），生成 Simple, Reasoning, Multi-context, Conditional 等多种类型的问题。
*   **数据增强** (Data Augmentation)：
    *   **机制**： 对原始 Query 进行同义改写、引入噪声或拼写错误，用于测试检索器的抗干扰能力。
*   **动态切片采样**：
    *   **机制**： 从大规模向量库中随机或按规则抽取 Document chunks，作为生成问题的种子内容。

### 4.4.2 模块二：指标定义体系 (Metrics Definition System)

#### 4.4.2.1 概念说明

该模块定义了“好”的 RAG 系统应该具备的量化标准。为了精准定位问题，指标体系通常被解耦为 **检索端（Retrieval）** 和 **生成端（Generation）** 两个维度。

#### 4.4.2.2 作用说明

*   **检索评价**：衡量知识库检索的精准度，判断是否找对了文档。
*   **生成评价**：衡量大模型回答的质量，判断是否利用了文档且回答正确。

#### 4.4.2.3 常见实现机制

##### A. 检索端指标 (Retrieval Metrics)

*   **Context Precision (上下文精确率)**：
    *   机制： 计算检索到的前 K 个文档中，包含真实有用信息的文档所占比例。
*   **Context Recall (上下文召回率)**：
    *   机制： 衡量所有与问题相关的 Ground Truth 文档是否都被检索出来了。通常需要 LLM 辅助判断“该文档是否包含答案所需信息”。

##### B. 生成端指标 (Generation Metrics)

*   **Faithfulness / Groundedness (忠实度)**：
    *   **机制**： 检测“幻觉”。通过 NLI (自然语言推理) 任务逻辑，判断生成的 Claims（事实陈述）是否都能从 Context 中找到依据。
*   **Answer Relevance (答案相关性)**：
    *   **机制**： 计算生成的 Answer 与原始 Query 的向量相似度，或利用 LLM 反向生成 Query 并对比原 Query，判断是否答非所问。
*   **Answer Correctness (回答正确性)**：
    *   **机制**： 将生成答案与 Ground Truth 进行语义比对（不仅仅是关键词匹配），评估事实一致性。

##### C. 细粒度指标 (Fine-grained Metrics - RAGChecker 特有)

*   **Claim-level Metrics**：
    *   **机制**： 不再以整段文本为单位，而是将文本拆解为独立的“原子事实（Claims）”，计算 Claim 级别的 Precision 和 Recall，以区分“由于检索缺失导致的错误”和“模型推理导致的错误”。

### 4.4.3 模块三：评测执行与裁判引擎 (Evaluation Execution & Judge)

#### 4.4.3.1 概念说明

这是框架的计算核心，负责执行具体的打分逻辑。现代 RAG 评测已从传统的字符串匹配（BLEU/ROUGE）全面转向 **LLM-as-a-Judge** 模式。

#### 4.4.3.2 作用说明

*   **语义理解**： 利用大模型的语义理解能力，替代僵化的规则匹配，对复杂的开放式问答进行评分。
*   **自动化批处理**： 并发处理大量评测样本，输出分数。

#### 4.4.3.3 常见实现机制

*   **Prompt Engineering 模板库**：
    *   **机制**： 框架内置了大量针对不同指标（如忠实度、相关性）优化过的 Prompt 模板（Few-shot 或 CoT），指导裁判模型（如 GPT-4）进行打分。
*   **Pointwise Scoring (单点打分)**：
    *   **机制**： 给定 `(Q, A, C)`，让 LLM 直接输出 1-5 分或 Binary (0/1) 结果。
*   **Pairwise Comparison (成对比较)**：
    *   **机制**： (常见于 Evalscope/LMSYS 模式) 给定同一个问题的两个不同回答（来自不同 RAG 版本），让 LLM 选出更好的一个。这种方式通常比单点打分更符合人类直觉。
*   **本地模型支持**：
    *   **机制**： 支持通过 vLLM、Ollama 等接口调用本地开源模型（如 Qwen, Llama-3）作为裁判，以降低评测成本并保护数据隐私。

* * *

### 4.4.4 模块四：链路集成与数据抓取 (Pipeline Integration & Instrumentation)

#### 4.4.4.1 概念说明

为了进行分组件评测，框架必须能够介入 RAG 系统的运行流程，获取中间状态数据（特别是检索到的 `Retrieved Context`）。

#### 4.4.4.2 作用说明

*   **数据捕获**： 自动记录 Query 输入、Retriever 返回的文档列表、Generator 输出的文本以及 Latency（延迟）和 Token 消耗。
*   **无侵入性**： 尽量减少对原有业务代码的修改。

#### 4.4.4.3 常见实现机制

*   **基于回调的追踪 (Callback/Hooks)**：
    *   **机制**： 如 TruLens 利用 Python 的装饰器（Decorators）或 LangChain/LlamaIndex 的 Callback 系统，自动“挂钩”到特定的函数（如 `retrieve()`, `query()`），在代码运行时“偷”出中间变量。
*   **显式数据导入 (Explicit Import)**：
    *   **机制**： 如 RAGChecker 和 Ragas 的基础模式，接受标准格式的 JSON/DataFrame/CSV 文件。用户需自行在业务代码中打印日志，整理成框架要求的格式（如 `{"query":..., "retrieved_context":..., "response":...}`）后上传进行离线评测。

* * *

### 4.4.5 模块五：诊断与归因分析 (Diagnosis & Attribution Analysis)

#### 4.4.5.1 概念说明

评测的最终目的是优化。该模块负责将抽象的分数转化为可操作的改进建议，即“归因分析”。

#### 4.4.5.2 作用说明

*   **错误定位**： 明确回答错误是因为“没查到资料”（Retriever 锅）还是“查到了没看懂/乱说”（Generator 锅）。
*   **模型对比**： 横向对比不同参数（如 `Chunk Size`, `Top-K`）下的系统表现。

#### 4.4.5.3 常见实现机制

*   **归因四象限分析 (Quadrants Analysis)**：
    *   机制： (RAGChecker 典型功能) 将测试用例分为四类：
        1.  **Correct**: 检索对，生成对。
        2.  **Hallucination**: 检索对，生成错（模型未遵循上下文）。
        3.  **Misinformation**: 检索错（检索到错误信息），生成错（模型被误导）。
        4.  **Retrieval Failure**: 检索漏（没查到），导致生成错。
*   **可视化报表 (Visualization)**：
    *   机制： 生成雷达图（展示多维能力均衡度）、柱状图、散点图。
*   **Bad Case 导出**：
    *   机制： 自动筛选出低分样本，高亮显示检索到的 Context 与 Ground Truth 的差异，辅助人工复盘。

* * *

_目前的 RAG 评测框架在功能组成上高度趋同，差异主要体现在 Module 1（数据生成的质量） 和 Module 5（归因分析的深度） 上。_