---
layout: post
title: 'å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†'
date: "2025-04-23T00:39:47Z"
---
å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†
========================

æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åœ¨ \*\*OpenRLHF\*\*ä¸­æ¨¡å‹æ¡†æ¶è®¾è®¡ï¼Œä¸»è¦åˆ†ä¸º3ç±»æ¨¡å‹ï¼š1ã€\`actor model\`ï¼›2ã€\`critic model\`ï¼›3ã€\`reward model\`è¿™ä¸‰ç±»æ¨¡å‹ä¸­åˆ†åˆ«èµ·åˆ°ä½œç”¨ï¼š1ã€ç›´æ¥æ›´å…·promptè¾“å‡ºresponseï¼›2ã€è¾“å‡ºtokençš„è¯„åˆ†ï¼ˆ\`action\_values = values\[:, -3:\]\`ï¼‰ï¼›3ã€è¿”å›æ•´å¥è¾“å‡ºè¯„åˆ†ï¼ˆæ‰¾å‡ºæœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„ç´¢å¼•ï¼Œç„¶åä» value å‘é‡ä¸­æå–è¯¥ä½ç½®çš„å€¼ä½œä¸º rewardã€‚ï¼‰

[å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†](https://www.big-yellow-j.top/posts/2025/04/22/OpenRLHF-1.html)
=========================================================================================

æœ¬æ–‡ä¸»è¦ä»‹ç» **å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†**

modelsæ¡†æ¶è®¾è®¡
----------

äº†è§£ä¸€ä¸‹ **OpenRLHF**çš„æ¨¡å‹æ¡†æ¶è®¾è®¡èŒƒå¼ï¼š

![](https://img2024.cnblogs.com/blog/3395559/202504/3395559-20250422223843015-1614471598.png)

> From:[https://arxiv.org/pdf/2405.11143](https://arxiv.org/pdf/2405.11143)

å¯ä»¥çŸ¥é“ä¸€ä¸ªå¤§æ¦‚çš„æµç¨‹ï¼šè¾“å…¥Pormpté€šè¿‡Actor modelè¾“å‡ºå›å¤ Responseï¼Œè€Œåå°†ä¸¤éƒ¨åˆ†è¿›è¡Œæ‹¼æ¥å†å»ç”±å…¶ä»–æ¨¡å‹è¿›è¡Œå¤„ç†

### 1ã€actor.py

> [https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/actor.py](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/actor.py)

è¿™éƒ¨åˆ†ä¸»è¦ä¸ºåŠ è½½æ‰€éœ€è¦çš„æ¨¡å‹

    class Actor(nn.Module):
        def __init__(...):
            if isinstance(pretrain_or_model, str):
                ...
                self.model = model_class.from_pretrained(
                    pretrain_or_model,
                    trust_remote_code=True,
                    attn_implementation=attn_implementation,
                    quantization_config=nf4_config,
                    torch_dtype=torch.bfloat16 if bf16 else "auto",
                    device_map=device_map,
                )
                if lora_rank > 0:
                    self.model.enable_input_require_grads()
                    lora_config = LoraConfig(
                        task_type=TaskType.CAUSAL_LM,
                        r=lora_rank,
                        lora_alpha=lora_alpha,
                        target_modules=target_modules,
                        lora_dropout=lora_dropout,
                        bias="none",
                    )
                    self.model = get_peft_model(self.model, lora_config)
                    ...
            else:
                self.model = pretrain_or_model
        @torch.no_grad()
        def generate(self, input_ids: torch.Tensor, **kwargs):
            ...
            sequences = self.model.generate(**generate_args)
            eos_token_id = generate_args["eos_token_id"]
            pad_token_id = generate_args["pad_token_id"]
            return self.process_sequences(sequences, input_ids.size(1), eos_token_id, pad_token_id)
        def forward(...):
            ...
            output["logits"] = output["logits"].to(torch.float32) # å¾—åˆ°æ¯ä¸€ä¸ªtokenæ¦‚ç‡
            ...
            log_probs = log_probs_from_logits(
                        output["logits"][:, :-1, :], sequences[:, 1:], temperature=self.temperature
                    )
            ...
            action_log_probs = log_probs[:, -num_actions:]
    
    

è¿™ä¸ªactoræ¯”è¾ƒç®€å•ï¼Œ**é¦–å…ˆ**ä»huggingfaceåŠ è½½éœ€è¦çš„æ¨¡å‹ï¼Œå¹¶ä¸”å¯¹æ¨¡å‹è¿›è¡Œéƒ¨åˆ†è®¾ç½®å¦‚ï¼šé‡åŒ–/loraå¾®è°ƒã€‚æˆ–è€…ç›´æ¥åŠ è½½è‡ªå·±é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚  
1ã€`generate`ï¼šæ¨¡å—åˆ™æ˜¯æ ¹æ®è¾“å…¥çš„å†…å®¹ï¼ˆæ¯”å¦‚è¯´è¢« tokenizerå¤„ç†å¥½çš„æ–‡æœ¬ï¼‰input\_idsé€šè¿‡æ¨¡å‹**è¾“å‡ºæ–°çš„å†…å®¹**ï¼ˆæ ¹æ® `**kwargs`è·å–ç”Ÿæˆæ–‡æœ¬å‚æ•°è®¾ç½®æ¯”å¦‚è¯´ï¼š`top_k`ç­‰ï¼‰  
2ã€`forward`ï¼š**æ ¹æ®è¾“å…¥çš„ token åºåˆ—ï¼ˆsequencesï¼‰ï¼Œè®¡ç®—æ¨¡å‹åœ¨ç”Ÿæˆæœ€åè‹¥å¹²ä¸ª tokenï¼ˆå³ "åŠ¨ä½œ"ï¼‰æ—¶çš„å¯¹æ•°æ¦‚ç‡ï¼ˆlog probsï¼‰**ï¼Œä¹‹æ‰€ä»¥è¦è¿™ä¹ˆå¤„ç†æ˜¯å› ä¸ºï¼Œåœ¨å¼ºåŒ–å­¦ä¹ æ¨¡å‹ä¸­ï¼ˆPPOã€DPOç­‰ï¼‰ä¸€èˆ¬è€Œè¨€æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªåºåˆ—ï¼Œä½†ä¼˜åŒ–ç›®æ ‡ä¸æ˜¯â€œèƒ½ä¸èƒ½ç”Ÿæˆè¿™ä¸ªåºåˆ—â€ï¼Œè€Œæ˜¯ï¼šè¿™ä¸ªåºåˆ—ä¸­ï¼Œå“ªäº› token æ˜¯â€œå¥½â€çš„ï¼Ÿæ¨¡å‹å¯¹è¿™äº› token çš„æ¦‚ç‡åº”è¯¥æ›´é«˜ï¼æ¯”å¦‚è¯´åœ¨ **DPO**ä¸­ï¼š

\\\[L(Î¸) = E\[ min(r(Î¸) \* A, clip(r(Î¸), 1-Îµ, 1+Îµ) \* A) \] \\\]

é‡Œé¢çš„

\\\[r(\\theta)=\\pi\_{\\theta}(a|s)/\\pi\_{old}(a|s) \\\]

å°±æ˜¯æ¦‚ç‡æ¯”å€¼ï¼Œä¸Šé¢ä»£ç ä¸­ï¼š

    log_probs_from_logits(output["logits"][:, :-1, :], sequences[:, 1:], temperature=self.temperature)
    

è®¡ç®—çš„å°±æ˜¯ï¼š\\(log(\\pi\_{\\theta}(a|s))\\)ï¼Œåœ¨å…·ä½“ä»£ç ä¸­ï¼š

    def log_probs_from_logits(logits: torch.Tensor, labels: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:
        if temperature != 1.0:
            logits.div_(temperature)
        if logits.dtype in [torch.float32, torch.float64]:
            batch_dim = logits.shape[:-1]
            last_dim = logits.shape[-1]
            try:
                from flash_attn.ops.triton.cross_entropy import cross_entropy_loss
    
                output = cross_entropy_loss(logits.reshape(-1, last_dim), labels.reshape(-1))
                log_probs_labels = -output[0].view(*batch_dim)
            except ImportError:
                logits_labels = torch.gather(logits, dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)
                logsumexp_values = _logsumexp_by_chunk(logits.reshape(-1, last_dim))
                logsumexp_values = logsumexp_values.view(*batch_dim)
                log_probs_labels = logits_labels - logsumexp_values  # log_softmax(x_i) = x_i - logsumexp(x)
        else:
            log_probs_labels = []
            for row_logits, row_labels in zip(logits, labels):  # loop to reduce peak mem consumption
                row_log_probs = F.log_softmax(row_logits, dim=-1)
                row_log_probs_labels = row_log_probs.gather(dim=-1, index=row_labels.unsqueeze(-1)).squeeze(-1)
                log_probs_labels.append(row_log_probs_labels)
            log_probs_labels = torch.stack(log_probs_labels)
        return log_probs_labels
    

> **è¡¥å……-1**ï¼š  
> åœ¨ä½¿ç”¨ `AutoModelForCausalLM.from_pretrained`ä½¿ç”¨å¾—åˆ° `model`ä¹‹åï¼Œå…¶æ”¯æŒè¾“å…¥å‚æ•°ä¸ºï¼š

    outputs = model(
        input_ids=None,            # è¾“å…¥çš„tokenï¼ˆbatch_size, seq_lengthï¼‰
        attention_mask=None,       # æŒ‡ç¤ºå“ªäº› token æ˜¯æœ‰æ•ˆçš„ï¼ˆé paddingï¼‰ï¼Œå½¢çŠ¶åŒ input_ids
        position_ids=None,         # ä½ç½®ç¼–ç 
        past_key_values=None,
        inputs_embeds=None,
        use_cache=None,            # æ˜¯å¦ä½¿ç”¨k-v cache
        labels=None,               # è¾“å…¥æ ‡ç­¾å°±ç›´æ¥è®¡ç®—loss
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    )
    

> **è¡¥å……-2**ï¼š  
> åœ¨LLMè®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°è¿‡çŸ­çš„è¯­å¥ä¸ºäº†èŠ‚çº¦æ˜¾å­˜ï¼ˆå¦‚æœéƒ½å°†å†…å®¹è¡¥å……åˆ°ç›¸åŒé•¿åº¦ï¼Œé‚£ä¹ˆå°±ä¼šæœ‰è¾ƒå¤šçš„paddingé€ æˆæµªè´¹ï¼‰ï¼Œå› æ­¤å¯ä»¥å°†å‡ ä¸ªçŸ­çš„æ‹¼æ¥èµ·æ¥ï¼Œä½†æ˜¯ä¸ºäº†åŒºåˆ†é‚£äº›æ˜¯ä¸€ä¸ªå¥å­é‚£äº›ä¸æ˜¯çš„ï¼Œåœ¨ **OpenRLHF**ä¸­é€šè¿‡å‚æ•°ï¼š`self.packing_samples`ã€‚å¦‚æœæ²¡æœ‰ `packing`é‚£ä¹ˆç›´æ¥æ ¹æ® `attention_mask`å°†ä½ç½®ç¼–ç åœ¨å¤„ç†ä¸€ä¸‹

    if not self.packing_samples:
        position_ids = attention_mask.long().cumsum(-1) - 1
        position_ids.masked_fill_(attention_mask == 0, 1)
    else:
        # convert attention_mask to position_ids
        if ring_attn_group is not None:
            labels = sequences
            sequences, attention_mask, position_ids = convert_ring_attn_params(
                sequences, attention_mask, packed_seq_lens, ring_attn_group
            )
        else:
            position_ids = reset_position_ids(attention_mask)
        # explicitly ignore attention_mask for packing_samples
        attention_mask = None
    

> å…¶ä¸­ `reset_position_ids`åšçš„å°±æ˜¯é‡æ–°åšä½ç½®ç¼–ç é‡æ–°å¤„ç†

### 2ã€model.py

> [https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/model.py](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/model.py)

![](https://img2024.cnblogs.com/blog/3395559/202504/3395559-20250422223842901-1120213596.png)

ä¸»è¦åŠŸèƒ½è¿”å›æ‰€éœ€è¦çš„æ¨¡å‹ï¼Œä¸»è¦è¿”å›2ä¸ªæ¨¡å‹ï¼š1ã€`CriticModel`ï¼›2ã€`RewardModel` å›é¡¾ä¸€ä¸‹è¿™å‡ ç±»æ¨¡å‹çš„ä½œç”¨ï¼šæ— è®ºæ˜¯åœ¨GRPOè¿˜æ˜¯DPOä¸­éƒ½ä¼šè¾“å‡ºtokenç„¶åéœ€è¦å»å¯¹tokenè¿›è¡Œè¯„åˆ†ï¼Œèµ·è¯„åˆ†ä½œç”¨çš„å°±æ˜¯ `reward model` å¯¹åº”ä¸Šé¢å›¾ä¸­ `reward model`ï¼Œé™¤æ­¤ä¹‹å¤–éƒ½ä¼šè®¡ç®— **ä¼˜åŠ¿å‡½æ•°**ï¼ˆ\\(Q(s,a)-V(s)\\)ï¼‰æ¥è¯„ä¼°ç­–ç•¥çš„å¥½åä¼˜åŠ¿å‡½æ•°é‡Œé¢è®¡ç®—å°±æ˜¯é€šè¿‡ `critic model`æ¥å¯¹æŸä¸€ä¸ªç­–ç•¥è¿›è¡Œè¯„ä¼°å¯¹åº”ä¸Šé¢å›¾åƒä¸­çš„ï¼š`value model`

    def _get_reward_model(base_pretrained_model, base_llm_model, value_head_prefix="score", packing_samples=False):
        class RewardModel(base_pretrained_model):
            def __init__(...):
                ...
                # åŠ è½½æ¨¡å‹
                setattr(self, self.base_model_prefix, base_llm_model(config))
                self.value_head_prefix = value_head_prefix
                setattr(self, value_head_prefix, nn.Linear(config.hidden_size, 1, bias=False) # è¾“å‡ºè¯„åˆ†
                ...
            def forward(self, input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, return_output=False, ring_attn_group=None,pad_sequence=False, packed_seq_lens=None,):
                ...# 1ã€å¤„ç†packing
                outputs = getattr(self, self.base_model_prefix)(
                    input_ids, attention_mask=attention_mask, position_ids=position_ids
                )
                last_hidden_states = outputs["last_hidden_state"]
                values = getattr(self, self.value_head_prefix)(last_hidden_states).squeeze(-1)
                ...# 1ã€å¤„ç†packing
                else:
                    # è¾“å‡ºæœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„è¯„åˆ†ä»£æ›¿æ•´ä¸ªå¥å­è¯„åˆ†
                    eos_indices = attention_mask.size(1) - 1 - attention_mask.long().fliplr().argmax(dim=1, keepdim=True)
                    reward = values.gather(dim=1, index=eos_indices).squeeze(1)
                if not self.training and self.normalize_reward:
                    reward = (reward - self.mean) / self.std
                return (reward, outputs) if return_output else reward
        return RewardModel
    
    def _get_critic_model(base_pretrained_model, base_llm_model, value_head_prefix="score", packing_samples=False):
        class CriticModel(base_pretrained_model):
            def __init__(...):
                ...
            def forward(...):
                ...# 1ã€å¤„ç†packing
                outputs = getattr(self, self.base_model_prefix)(
                    input_ids, attention_mask=attention_mask, position_ids=position_ids
                )
                last_hidden_states = outputs["last_hidden_state"]
                values = getattr(self, self.value_head_prefix)(last_hidden_states).squeeze(-1)
                ...
                if num_actions is None:
                    assert return_output
                    return outputs
                if not self.packing_samples:
                    action_values = values[:, -num_actions:]
                else:
                    assert isinstance(num_actions, list) and len(num_actions) == len(packed_seq_lens)
                    action_values = []
                    offset = 0
                    for num_action, seq_len in zip(num_actions, packed_seq_lens):
                        start, end = max(0, offset + seq_len - num_action - 1), offset + seq_len - 1
                        action_values.append(values[:, start:end])
                        offset += seq_len
                    action_values = torch.cat(action_values, dim=1)
    
                if return_output:
                    return (action_values, outputs)
                else:
                    return action_values
    
        return CriticModel
    

1ã€`reward model`: ä¼ å…¥ä¸€ä¸ª base\_pretrained\_modelï¼ˆæ¯”å¦‚ PreTrainedModelï¼‰ã€ä¸€ä¸ª base\_llm\_modelï¼ˆæ¯”å¦‚ AutoModelï¼‰ä»¥åŠä¸€äº›æ§åˆ¶å‚æ•°ã€‚å‡½æ•°å†…éƒ¨è¿”å›ä¸€ä¸ªå®šåˆ¶åŒ–çš„å¥–åŠ±æ¨¡å‹ç±» RewardModelï¼Œå®ƒå¯ä»¥åœ¨ç»™å®šè¾“å…¥å¥å­æ—¶ï¼Œ**è¾“å‡ºä¸€ä¸ªæ•°å€¼ï¼ˆreward åˆ†æ•°ï¼‰ï¼Œåæ˜ è¾“å‡ºæ–‡æœ¬çš„è´¨é‡**ã€‚åœ¨forwardè®¡ç®—ä¸­ï¼Œç›´æ¥å°†è¾“å…¥modelä½¿ç”¨çš„å‡ ä¸ªå‚æ•°ï¼ˆè§ä¸Šé¢çš„è¡¥å……æœ‰å…·ä½“è§£é‡Šï¼‰è®¡ç®—æœ€åå–æœ€åä¸€ä¸ªçŠ¶æ€çš„å€¼ï¼Œå¹¶ä¸”å°†è¿™ä¸ªå€¼å–è®¡ç®—è¯„åˆ†ã€‚ä¹Ÿå°±æ˜¯è¯´ reward modelï¼š**é¦–å…ˆè®¡ç®—ä¸‹ä¸€ä¸ªé¢„æµ‹çš„tokenè€Œåå¯¹è¿™äº›tokenè¿›è¡Œæ‰“åˆ†**  
2ã€`critic model`ï¼šå…·ä½“è¾“å…¥å‚æ•°å’Œ `reward model`ç›¸åŒã€‚å‚è€ƒä¹‹å‰[ä»‹ç»](https://www.big-yellow-j.top/posts/2025/03/23/DPO-PPO.html#:~:text=%E5%AF%B9%E4%BA%8E%E4%B8%8A%E8%BF%B0%E5%85%AC%E5%BC%8F,%E5%B9%B3%E5%9D%87%E7%B4%AF%E8%AE%A1%E6%9C%9F%E6%9C%9B)ï¼Œä¸Šé¢ä»£ç ä¸­ç›´æ¥è¿”å›`action_values = values[:, -num_actions:]`ï¼ˆ `num_actions`å­˜åœ¨æ¡ä»¶ä¸‹ï¼‰è¿™æ ·å°±ä¼šå¾—åˆ°ä¸åŒçš„Q(s, a1), Q(s, a2), ...

> **æ€»ç»“ä¸Šé¢ä¸¤ç»„æ¨¡å‹**ï¼Œåœ¨ LLM çš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ä¸‹ï¼ŒReward Model å’Œ Critic Model éƒ½ä» last\_hidden\_state å¾—åˆ° token-level è¡¨è¾¾ï¼Œå†ç”¨ Linear å±‚è¾“å‡ºæ¯ä¸ª token çš„ scoreã€‚
> 
> *   `Reward Model` æœ€åæå–çš„æ˜¯ EOS token çš„ scoreï¼Œè¡¨ç¤ºæ•´å¥è¯çš„å¥–åŠ±ã€‚
> *   `Critic Model` ä¼šè¿›ä¸€æ­¥æå–æœ€å num\_actions ä¸ª token çš„ valueï¼Œè¿™äº› token æ˜¯ Actor ç”Ÿæˆçš„åŠ¨ä½œï¼Œå¯¹åº”åˆ° PPO ä¸­çš„ï¼šğ´(ğ‘ ,ğ‘)=ğ‘„(ğ‘ ,ğ‘)âˆ’ğ‘‰(ğ‘ )ã€‚

ç†è§£ä¸Šé¢å†…å®¹ï¼Œå›é¡¾æœ€ä¸Šé¢çš„æ¡†æ¶è®¾è®¡ï¼Œç”¨ä¸‹é¢ä¾‹å­è¿›è¡Œè§£é‡Šã€‚  
Promptï¼š`"The capital of France is"`  
Actor modelï¼š`"Paris is beautiful"`ã€‚é‚£ä¹ˆåˆå¹¶å¾—åˆ°ï¼š`input_ids = ["The", "capital", "of", "France", "is", " Paris", "is", "beautiful"]`  
Reward modelï¼šå¯¹ä¸Šé¢æ¯ä¸ªå•è¯è¿›è¡Œè¯„åˆ†ï¼Œå‡è®¾ï¼š`values = [0.1, 0.2, 0.3, 0.2, 0.4, 0.7, 0.5, 0.8] # æ¯ä¸ª token çš„ score` è€Œåè¾“å‡ºå¥å­ä¸­æ•´ä½“è¯„åˆ† 0.8  
Critic modelï¼šåªå¯¹æœ€åå‡ ä¸ª token çš„ action è®¡ç®— lossï¼Œäºæ˜¯ï¼š`action_values = values[:, -3:] # å³å–å‡ºæœ€å 3 ä¸ªç”Ÿæˆ token çš„ Q å€¼`è¿™äº›å€¼ä¹Ÿå°±å¯¹åº”äº†æˆ‘ä»¬æ¨¡å‹çš„ç”Ÿæˆ

### 3ã€loss.py

> [https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/loss.py](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/loss.py)

> **è¡¥å……-1ï¼š**  
> è£å‰ªä½¿ç”¨çš„æ˜¯`torch.clamp`ï¼ˆ[https://pytorch.org/docs/stable/generated/torch.clamp.htmlï¼‰å¼ºåˆ¶å°†èŒƒå›´å¤–çš„æ•°å€¼å¤„ç†ä¸ºè¾¹ç•Œå€¼ï¼ŒèŒƒå›´å†…æ•°å­—ä¿æŒä¸å˜](https://pytorch.org/docs/stable/generated/torch.clamp.html%EF%BC%89%E5%BC%BA%E5%88%B6%E5%B0%86%E8%8C%83%E5%9B%B4%E5%A4%96%E7%9A%84%E6%95%B0%E5%80%BC%E5%A4%84%E7%90%86%E4%B8%BA%E8%BE%B9%E7%95%8C%E5%80%BC%EF%BC%8C%E8%8C%83%E5%9B%B4%E5%86%85%E6%95%B0%E5%AD%97%E4%BF%9D%E6%8C%81%E4%B8%8D%E5%8F%98)

1ã€`PolicyLoss`ï¼šPolicy Loss for PPO

\\\[\\begin{align\*} r\_t &= \\exp(\\log \\pi(a\_t \\mid s\_t) - \\log \\pi\_{\\text{old}}(a\_t \\mid s\_t)) \\\\ \\mathcal{L}\_{\\text{clip}}(t) &= \\min\\left(r\_t \\cdot A\_t,\\ \\text{clip}(r\_t,\\ 1 - \\epsilon,\\ 1 + \\epsilon) \\cdot A\_t\\right) \\\\ \\mathcal{L}\_{\\text{policy}} &= -\\mathbb{E}\_t \\left\[ \\mathcal{L}\_{\\text{clip}}(t) \\right\] \\end{align\*} \\\]

2ã€`ValueLoss`: Value Loss for PPO

\\\[\\mathcal{L}\_{\\text{value}} = \\frac{1}{2} \\cdot \\mathbb{E}\_{t \\sim \\text{mask}} \\left\[ \\max \\left( (V\_{\\text{clip}, t} - R\_t)^2, \\, (V\_t - R\_t)^2 \\right) \\right\]\\\\ \\text{å…¶ä¸­ï¼š}V\_{\\text{clip}} = V\_{\\text{old}} + \\text{clip}(V - V\_{\\text{old}}, -\\epsilon, \\epsilon) \\\]

ä»£ç æµ‹è¯•
----

ä¿®æ”¹äº†ä»£ç è§é“¾æ¥ï¼š[https://www.big-yellow-j.top/\_jupyter/OpenRLHF\_model.py](https://www.big-yellow-j.top/_jupyter/OpenRLHF_model.py.txt)

æ€»ç»“
--

æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åœ¨ **OpenRLHF**ä¸­æ¨¡å‹æ¡†æ¶è®¾è®¡ï¼Œä¸»è¦åˆ†ä¸º3ç±»æ¨¡å‹ï¼š1ã€`actor model`ï¼›2ã€`critic model`ï¼›3ã€`reward model`è¿™ä¸‰ç±»æ¨¡å‹ä¸­åˆ†åˆ«èµ·åˆ°ä½œç”¨ï¼š1ã€ç›´æ¥æ›´å…·promptè¾“å‡ºresponseï¼›2ã€è¾“å‡ºtokençš„è¯„åˆ†ï¼ˆ`action_values = values[:, -3:]`ï¼‰ï¼›3ã€è¿”å›æ•´å¥è¾“å‡ºè¯„åˆ†ï¼ˆæ‰¾å‡ºæœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„ç´¢å¼•ï¼Œç„¶åä» value å‘é‡ä¸­æå–è¯¥ä½ç½®çš„å€¼ä½œä¸º rewardã€‚ï¼‰