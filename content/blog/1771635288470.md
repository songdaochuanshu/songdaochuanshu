---
layout: post
title: '凸优化数学基础笔记（四）：Hessian 矩阵及 Taylor 展开'
date: "2026-02-21T00:54:48Z"
---
凸优化数学基础笔记（四）：Hessian 矩阵及 Taylor 展开
==================================

上一节说过，梯度 $\\nabla f(\\mathbf{X})$ 是$f(\\mathbf{X})$ 关于$\\mathbf{X}$ 的一阶导数，现在一个问题$f(\\mathbf{X})$ 关于 $\\mathbf{X}$ 的二阶导数是什么？Hessian 矩阵（海森矩阵）是一个多变量实值函数$f(\\mathbf{X})$的二阶导数构成得方阵，其几何意义是描述了一个多元函数或场函数的局部曲率。在机器学习、优化问题和物理反演问题中，Hessian矩阵扮演着重要的角色，尤其是在寻找函数的极值点（如损失函数或目标函数的最小值）和分析系统的稳定性。

1.Hessian矩阵定义及性质
----------------

​ 在上一节说过，梯度 \\(\\nabla f(\\mathbf{X})\\) 是\\(f(\\mathbf{X})\\) 关于\\(\\mathbf{X}\\) 的一阶导数，现在一个问题\\(f(\\mathbf{X})\\) 关于 \\(\\mathbf{X}\\) 的二阶导数是什么？Hessian 矩阵（海森矩阵）是一个多变量实值函数\\(f(\\mathbf{X})\\)的二阶导数构成得方阵，其几何意义是描述了一个多元函数或场函数的局部曲率。在机器学习、优化问题和物理反演问题中，Hessian矩阵扮演着重要的角色，尤其是在寻找函数的极值点（如损失函数或目标函数的最小值）和分析系统的稳定性。下面是关于Hessian矩阵的详细介绍：

​ **Definition 4.1.1** 设\\(f:\\mathbb{R}^n\\rightarrow{\\mathbb{R}}\\) , \\(\\mathbf{X}\\in{R}^n\\) ,如果\\(f\\)在点\\(\\mathbf{X}\_0\\) 处对于自变量\\(\\mathbf{X}\\) 的各个分量\\(x\_i,x\_j\\) 的二阶偏导

\\\[\\frac{\\part{f(\\mathbf{X}\_0)}}{\\part{x\_i}\\part{x\_j}} \\hspace{2em} (i,j=1,2,...,n) \\\]

都存在，则称函数\\(f\\) 在点\\(\\mathbf{X}\_0\\) 处二阶可导，并且称矩阵：

\\\[\\nabla^2f(\\mathbf{X}\_0)=\\left(\\begin{matrix}\\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_1^2}},&\\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_1}\\part{x\_2}},&\\cdots,&\\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_1}\\part{x\_n}}\\\\ \\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_1}\\part{x\_2}},&\\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_2^2}},&\\cdots,&\\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_2}\\part{x\_n}}\\\\ \\cdots &\\cdots &\\cdots &\\cdots \\\\ \\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_n}\\part{x\_1}},&\\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_n}\\part{x\_2}},&\\cdots,&\\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_n}^2} \\end{matrix}\\right) \\tag{2} \\\]

是\\(f\\)在点\\(\\mathbf{X}\_0\\) 处的**Hessian矩阵**，记为\\(\\mathbf{H}\_{ij}=\\frac{\\part^2{f}}{\\part{x\_i}\\part{x\_j}}\\)。

​ 在数学分析中已经知道，当\\(f\\) 在点\\(\\mathbf{X}\_0\\) 处的所有二阶偏导数为连续时有：

\\\[\\frac{\\part^2f(\\mathbf{X}\_0)}{\\part{x\_i}\\part{x\_j}}=\\frac{\\part^2{f(\\mathbf{X}\_0)}}{\\part{x\_j}\\part{x\_i}} \\tag{3} \\\]

因此，在这种情况下Hessian矩阵是**对称矩阵**。

**推论 1** 设\\(\\mathbf{a}\\in{\\mathbf{R}^n},\\mathbf{X}\\in{\\mathbf{R}^n},b\\in{R}\\)，求线性函数：

\\\[f(\\mathbf{X})=\\mathbf{a}^T\\mathbf{X}+b\\tag{4} \\\]

在任意点\\(\\mathbf{X}\\) 处的梯度向量和Hessian矩阵：

\\\[\\begin{aligned} &\\nabla{f(\\mathbf{X})}=\\mathbf{a}\\\\ &\\nabla^2{f(\\mathbf{X})}=\\mathbf{0} \\end{aligned} \\tag{5} \\\]

**证 明：** 设\\(\\mathbf{a}=\[a\_1,a\_2,\\cdots,a\_n\]^{T},\\mathbf{X}=\[x\_1,x\_2,\\cdots,x\_n\]^{T}\\) 则：

\\\[f(x\_1,x\_2,\\cdots,x\_n)=\\sum^{n}\_{i=1}a\_ix\_i+b \\tag{6} \\\]

根据梯度向量\\(\\nabla{f}\\)的定义：

\\\[\\nabla{f(\\mathbf{X})}=\\frac{\\part{f}}{\\part{x\_i}}=a\_i \\hspace{2em} (i=1,2,\\cdots,n) \\tag{7} \\\]

由式（7）可知，

\\\[\\nabla{f(\\mathbf{X})}=\[a\_1,a\_2,\\cdots,a\_n\]^T=\\mathbf{a} \\tag{8} \\\]

根据式（2）的\\(Hessian\\)矩阵的定义：

\\\[\\frac{\\part^2{f}}{\\part{x\_i}\\part{x\_j}}=0, \\hspace{2em} i,j=1,2,\\cdots,n \\tag{9} \\\]

所以：

\\\[\\nabla^2{f(\\mathbf{X})}=\\mathbf{0} \\tag{10} \\\]

**推论 2** 设\\(\\mathbf{A}\\in{R^{n\\times{n}}}\\) 是对称实方阵，\\(\\mathbf{b}\\in{\\mathbf{R}^n},c\\in\\mathbf{R}^1\\) ,其二次函数\\(f(\\mathbf{X})=\\frac{1}{2}\\mathbf{X}^T\\mathbf{A}\\mathbf{X}+\\mathbf{b}^T\\mathbf{X}+c\\) ,在任意点处的梯度向量\\(\\nabla{f(\\mathbf{X})}\\) 及Hessian矩阵 \\(\\nabla^2f(\\mathbf{X})\\) 为如下形式：

\\\[\\begin{aligned} &\\nabla{f}=\\mathbf{AX+b} \\\\ &\\nabla^2{f}=\\mathbf{A} \\end{aligned} \\tag{11} \\\]

**证 明：** 设\\(\\mathbf{A}=\[a\_{ij}\]\_{n\\times{n}}\\) ,\\(\\mathbf{X}=\[x\_1,x\_2,...,x\_n\]^T\\), \\(\\mathbf{b}=\[b\_1,b\_2,...,b\_n\]^T\\) ,二次函数\\(f(\\mathbf{X})\\) 可以写成如下形式：

\\\[f(x\_1,x\_2,\\cdots,x\_n)=\\frac{1}{2}\\sum\_{i=1}^{n}\\sum\_{j=1}^{n}a\_{ij}x\_ix\_j+\\sum\_{i=1}^{n}b\_ix\_i+c \\tag{12} \\\]

将它对各个变量 \\(x\_i\\) (\\(i=1,\\cdots,n\\)) 求偏导数，得到：

\\\[\\nabla{f(\\mathbf{X})}=\\left( \\begin{matrix} \\sum\_{j=1}^n a\_{1j}x\_j+b\_1\\\\ \\vdots \\\\\\sum\_{j=1}^{n}a\_{n\_j}x\_j+b\_n \\end{matrix} \\right)= \\left(\\begin{matrix}\\sum\_{j=1}^{n}a\_{1j}x\_j\\\\\\vdots\\\\\\sum\_{j=1}^na\_{nj}x\_j\\end{matrix}\\right)+\\left(\\begin{matrix}b\_1\\\\ \\vdots \\\\b\_n\\end{matrix}\\right) \\tag{13} \\\]

所以：

\\\[\\nabla{f(\\mathbf{X})}=\\mathbf{AX+b} \\tag{14} \\\]

再对它们的求偏导数是：

\\\[ \\frac{\\part{f(\\mathbf{X})}}{\\part{x\_i}\\part{x\_j}}=a\_{ij} \\tag{15} \\\]

所以：

\\\[\\nabla^2f(\\mathbf{X})=\\mathbf{A} \\tag{16} \\\]

以上两个例子说明，\\(n\\)元函数求导与一元函数求导在形式上一致的，即线性函数的一阶导数的求导为常向量，其二阶导数为零矩阵；而二次函数的一阶导数为线性向量函数， 其二阶导数为常矩阵。

​ 在此介绍在今后的计算中要用到的向量函数的导数。

​ **Definition 4.1.2** 设一个向量函数\\(h:R^{n}\\rightarrow{R^m}，\\mathbf{X}\_0\\in{R^n}\\)，记\\(h(\\mathbf{X})=\\{h\_1(\\mathbf{X}),h\_2(\\mathbf{X}),\\cdots,h\_m(\\mathbf{X})\\}^T\\)，如果\\(h\_i(i=1,2,\\cdots,m)\\) 在点\\(\\mathbf{X}\_0\\) 处对于自变量\\(\\mathbf{X}=\[x\_1,x\_2,x\_3,\\cdots,x\_n\]^T\\) 的各分量的偏导数\\(\\frac{\\part{h\_i(\\mathbf{X}\_0)}}{\\part{x\_j}}\\space(j=1,2,3,\\cdots,m)\\) 都存在，则称向量函数\\(h(\\mathbf{X})\\) 在点\\(\\mathbf{X}\_0\\) 处是一阶可导的，并且称矩阵：

\\\[\\nabla\_{m\\times{n}}h(\\mathbf{X}\_0)=\\left\[\\begin{matrix} \\frac{\\part{h\_1(\\mathbf{X}\_0)}}{\\part{x\_1}} &\\frac{\\part{h\_1(\\mathbf{X}\_0)}}{\\part{x\_2}} &\\cdots &\\frac{\\part{h\_1(\\mathbf{X}\_0)}}{\\part{x\_n}} \\\\ \\frac{\\part{h\_2(\\mathbf{X}\_0)}}{\\part{x\_1}} &\\frac{\\part{h\_2(\\mathbf{X}\_0)}}{\\part{x\_2}} &\\cdots &\\frac{\\part{h\_2(\\mathbf{X}\_0)}}{\\part{x\_n}}\\\\ \\cdots &\\cdots &\\cdots &\\cdots \\\\ \\frac{\\part{h\_m(\\mathbf{X}\_0)}}{\\part{x\_1}} &\\frac{\\part{h\_2(\\mathbf{X}\_0)}}{\\part{x\_2}} &\\cdots &\\frac{\\part{h\_2(\\mathbf{X}\_0)}}{\\part{x\_n}} \\end{matrix}\\right\] \\tag{17} \\\]

是\\(h\\)在点\\(X\_0\\) 处的一阶导数或Jacobi 矩阵，简记为

\\\[\\nabla{\\mathbf{h}(\\mathbf{X}\_0)}=\\nabla\_{m\\times{n}}\\mathbf{h}(\\mathbf{X}\_0) \\tag{18} \\\]

由于\\(n\\)元函数\\(f:\\mathbf{R}^n\\rightarrow \\mathbf{R}^1\\) 的梯度是向量函数:

\\\[\\nabla{f(\\mathbf{X})}=\\left\[\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_1}},...,\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_n}}\\right\]^T \\tag{19} \\\]

所以\\(\\nabla f(\\mathbf{X})\\) 的一阶导数或Jacobi 矩阵为：

\\\[\\begin{aligned} \\nabla\_{n\\times{n}}\\nabla{f(\\mathbf{X})}&=\\left\[\\begin{matrix}\\frac{\\part}{\\part{x\_1}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_1}}\\right) &\\frac{\\part}{\\part{x\_2}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_1}}\\right) &\\cdots &\\frac{\\part}{\\part{x\_n}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_1}}\\right)\\\\ \\frac{\\part}{\\part{x\_1}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_2}}\\right) &\\frac{\\part}{\\part{x\_2}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_2}}\\right) &\\cdots &\\frac{\\part}{\\part{x\_n}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_2}}\\right)\\\\ \\cdots &\\cdots &\\cdots &\\cdots \\\\ \\frac{\\part}{\\part{x\_1}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_n}}\\right) &\\frac{\\part}{\\part{x\_2}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_n}}\\right) &\\cdots &\\frac{\\part}{\\part{x\_n}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x\_n}}\\right) \\end{matrix}\\right\]\\\\ &=\\left\[\\begin{matrix}\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x\_1^2}} &\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x\_1x\_2}} &\\cdots &\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x\_1x\_n}}\\\\ \\frac{\\part^2{f(\\mathbf{X})}}{\\part{x\_1x\_2}} &\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x\_2^2}} &\\cdots &\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x\_2x\_n}}\\\\ \\cdots &\\cdots &\\cdots &\\cdots\\\\ \\frac{\\part^2{f(\\mathbf{X})}}{\\part{x\_1x\_n}} &\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x\_2x\_n}} &\\cdots &\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x\_n^2}} \\end{matrix}\\right\]\\\\ &=\\nabla^2{f(\\mathbf{X})} \\end{aligned} \\tag{20} \\\]

即：

\\\[\\nabla\_{n\\times{n}}\\nabla{f(\\mathbf{X})}=\\nabla^2f(\\mathbf{X}) \\tag{21} \\\]

据此，从上式的得知，函数梯度的Jacobi的矩阵为此函数\\(f(\\mathbf{X})\\) 的Hessian矩阵。

下面给出常用Jacobi矩阵的几个公式和推论：

1.  \\(\\nabla{\\mathbf{c}\_{n\\times1}}=\\mathbf{0}\_{n\\times{n}}\\) ，其中\\(\\mathbf{c}\\) 是分量全部为常量的\\(n\\)维向量，\\(\\mathbf{0}\\) 是\\(n\\times{n}\\) 阶零向量；
2.  \\(\\nabla{\\mathbf{X}}=\\mathbf{I}\\)，其中\\(\\mathbf{X}\\) 是\\(n\\)维向量，\\(I\\) 为\\(n\\times{n}\\) 阶单位矩阵；
3.  \\(\\nabla{\\mathbf{AX}}=\\mathbf{A}^T\\)，其中\\(\\mathbf{A}\\)是\\(m\\times{n}\\)阶矩阵；
4.  设 \\(\\phi{(t)}=f(\\mathbf{X}\_0+t\\mathbf{P})\\)，其中\\(f:\\mathbf{R}^n\\rightarrow\\mathbf{R}^{1}\\), \\(\\phi:\\mathbf{R}\\rightarrow\\mathbf{R}\\)，则\\(\\phi^{\\prime}(t)=\\nabla f(\\mathbf{X}\_0+t\\mathbf{P})^T\\mathbf{P}\\), \\(\\phi^{\\prime\\prime}=\\mathbf{P}^T\\nabla^2{f(\\mathbf{X}\_0+t\\mathbf{P})}\\mathbf{P}\\)

2.多元函数的Taylor展开
---------------

​ 多元函数的Taylor展开式在最优化方法中是十分重要的，许多方法及其收敛方法的证明是从其出发，这里给出Taylor展开定理及其证明。Taylor展开近似可以有效地将复杂函数化为二次函数。

​ **定 理 4.2.1** 设\\(f:\\mathbf{R}^{n}\\rightarrow\\mathbf{R}\\) 具有二阶连续偏导数，则

\\\[f(\\mathbf{X}+\\mathbf{P})=f(\\mathbf{X})+\\nabla{f(\\mathbf{X})}^T\\mathbf{P}+\\frac{1}{2}\\mathbf{P}^T\\nabla^2f(\\overline{\\mathbf{X}})\\mathbf{P} \\tag{22} \\\]

其中：\\(\\overline{\\mathbf{X}}=\\mathbf{X}+\\theta\\mathbf{P}(\\theta\\in{(0,1)})\\)。

**证 明**：设 \\(\\psi(t)=f(\\mathbf{X}+t\\mathbf{P})\\) 其中 \\(t\\in\[0,1\]\\):

\\\[\\psi(0)=f(\\mathbf{X}) ， \\psi(1)=f(\\mathbf{X}+\\mathbf{P}) \\tag{23} \\\]

对\\(\\psi(t)\\) 按照一元函数在\\(t=0\\)点展开，得到下式：

\\\[ \\psi(t)=\\psi(0)+\\psi^{\\prime}(0)t+\\frac{1}{2}\\psi^{\\prime\\prime}(\\theta t)t^2 \\tag{24} \\\]

其中 \\(\\theta\\in(0,1)\\)。令\\(t=1\\)，于是

\\\[\\psi(1)=\\psi(0)+\\psi^{\\prime}(0)+\\frac{1}{2}\\psi^{\\prime\\prime}(\\theta) \\tag{25} \\\]

又因为上节中 \\(\\psi^{\\prime}(\\theta)=\\nabla{f(\\mathbf{X})}^T\\mathbf{P},\\psi^{\\prime\\prime}(\\theta)=\\mathbf{P}^T\\nabla^2f(\\mathbf{X}+\\theta{\\mathbf{P}})\\mathbf{P}\\) ，代入式（25）中得到：

\\\[f(\\mathbf{X}+\\mathbf{P})=f(\\mathbf{X})+\\nabla{f(\\mathbf{X})}^T\\mathbf{P}+\\frac{1}{2}\\mathbf{P}^T\\nabla^2f(\\mathbf{X}+\\theta\\mathbf{P})\\mathbf{P} \\tag{26} \\\]

式（26）还可以写成：

\\\[f(\\mathbf{X}+\\mathbf{P})=f(\\mathbf{X})+\\nabla{f(\\mathbf{X})}^{T}\\mathbf{P}+\\frac{1}{2}\\mathbf{P}^T\\nabla^2f(\\mathbf{X})\\mathbf{P}+o(||\\mathbf{P}||^2) \\tag{27} \\\]