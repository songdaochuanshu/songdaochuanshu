---
layout: post
title: '数据不出内网：基于Ollama+OneAPI构建企业专属DeepSeek智能中台'
date: "2025-02-19T00:36:17Z"
---
数据不出内网：基于Ollama+OneAPI构建企业专属DeepSeek智能中台
========================================

前言
--

之前已经在Linux服务器上使用Ollama部署了DeepSeek

这次在没有外网（应该说是被限制比较多）的服务器上部署，遇到一些坑，记录一下

ollama
------

ollama 自然无法使用在线安装脚本了

根据 ollama 的文档

先在本地电脑根据服务器的系统和CPU架构下载安装包

    curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz
    

然后使用 scp 等工具上传到服务器

    scp ollama-linux-amd64.tgz 服务器地址:/temp
    

连接到服务器上后解压安装，跟着 ollama 文档来就行（见第一个参考资料）

    sudo tar -C /usr -xzf ollama-linux-amd64.tgz
    

这时候已经能执行 ollama 程序了

    ollama serve
    

然后再添加到服务，这也是 ollama 官方推荐的做法，方便管理

    sudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama
    sudo usermod -a -G ollama $(whoami)
    

在 /etc/systemd/system 下新建 ollama.service 文件

    [Unit]
    Description=Ollama Service
    After=network-online.target
    
    [Service]
    ExecStart=/usr/bin/ollama serve
    User=ollama
    Group=ollama
    Restart=always
    RestartSec=3
    Environment="PATH=$PATH"
    
    [Install]
    WantedBy=default.target
    

然后启用服务

    sudo systemctl daemon-reload
    sudo systemctl enable ollama
    

到这里 ollama 的安装就搞定了

模型部署
----

离线服务器是无法使用 ollama pull 拉取模型的

需要先在本地下载，可以在本地的电脑上执行 ollama pull 的操作

然后把模型文件找到并上传到服务器

大概思路就是这样，具体的接下来介绍

### 找到本地模型文件

如果没有特别配置，ollama 默认的模型文件都在 `~/.ollama/models/blobs` 里

先执行命令看看指定模型的路径，比如说要找 deepseek-r1:32b 模型

    ollama show deepseek-r1:32b --modelfile
    

执行命令后的输出（节选）

    FROM C:\Users\deali\.ollama\models\blobs\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49
    TEMPLATE """{{- if .System }}{{ .System }}{{ end }}
    {{- range $i, $_ := .Messages }}
    {{- $last := eq (len (slice $.Messages $i)) 1}}
    {{- if eq .Role "user" }}<｜User｜>{{ .Content }}
    {{- else if eq .Role "assistant" }}<｜Assistant｜>{{ .Content }}{{- if not $last }}<｜end▁of▁sentence｜>{{- end }}
    {{- end }}
    {{- if and $last (ne .Role "assistant") }}<｜Assistant｜>{{- end }}
    {{- end }}"""
    PARAMETER stop <｜begin▁of▁sentence｜>
    PARAMETER stop <｜end▁of▁sentence｜>
    PARAMETER stop <｜User｜>
    PARAMETER stop <｜Assistant｜>
    

可以看到这一行

    FROM C:\Users\deali\.ollama\models\blobs\sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49
    

就是 ollama 下载到本地的模型的路径

把这个文件上传到服务器

### 导出Modelfile

这个文件格式类似 Dockerfile

使用以下命令导出

    ollama show deepseek-r1:32b --modelfile > Modelfile
    

然后这个文件也要上传到服务器上

### 服务器上导入模型

模型文件和 Modelfile 上传之后，放在同一个目录下

先重命名一下，方便后续导入

    mv sha256-96c415656d377afbff962f6cdb2394ab092ccbcbaab4b82525bc4ca800fe8a49 deepseek-r1_32b.gguf
    

接着编辑一下 Modelfile 文件，把 FROM 这一行改成，也就是刚才修改之后的模型文件名称

    FROM ./deepseek-r1_32b.gguf
    

然后执行以下命令导入

    ollama create deepseek-r1:32b -f Modelfile
    

如无意外就导入成功了，可以执行 `ollama list` 来查看是否已导入。

one-api
-------

One API 是一款开源的 LLM（大语言模型）API 管理与分发系统，旨在通过标准的 OpenAI API 格式，统一访问多种大模型，开箱即用。 它支持多种主流大模型，包括 OpenAI ChatGPT 系列、Anthropic Claude 系列、Google PaLM2/Gemini 系列、Mistral 系列、字节跳动豆包大模型、百度文心一言系列模型、阿里通义千问系列模型、讯飞星火认知大模型、智谱 ChatGLM 系列模型、腾讯混元大模型等。

### docker部署

one-api是用go的gin框架开发的，部署很容易，我一般用docker部署，这块不再赘述

    services:
      db:
        image: mysql:8.1.0
        container_name: mysql
        restart: always
        environment:
          MYSQL_ROOT_PASSWORD: mysql-password
        volumes:
          - ./data:/var/lib/mysql
      one-api:
        image: justsong/one-api
        container_name: one-api
        restart: always
        ports:
          - "3000:3000"
        depends:
          - db
        environment:
          - SQL_DSN=root:mysql-password@tcp(db:3306)/one_api
          - TZ=Asia/Shanghai
          - TIKTOKEN_CACHE_DIR=/TIKTOKEN_CACHE_DIR
        volumes:
          - ./data:/data
          - ./TIKTOKEN_CACHE_DIR:/TIKTOKEN_CACHE_DIR
    
    networks:
      default:
        name: one-api
    

### 解决 tiktoken 问题

遇到的问题是它依赖了 tiktoken 这个库，tiktoken 需要联网下载 token encoder

解决方法是看错误日志，比如

    one-api  | [FATAL] 2025/02/17 - 10:47:21 | relay/adaptor/openai/token.go:26 [InitTokenEncoders] failed to get gpt-3.5-turbo token encoder: Get "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken": dial tcp 57.150.97.129:443: i/o timeout, if you are using in offline environment, please set TIKTOKEN_CACHE_DIR to use exsited files
    

这里需要从 [https://openaipublic.blob.core.windows.net/encodings/cl100k\_base.tiktoken](https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken) 下载

我们先在本地下载这个文件，然后上传到服务器

但这时还不行

tiktoken 只认 URL 的 SHA-1

生成 SHA-1

    TIKTOKEN_URL=https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken
    echo -n $TIKTOKEN_URL | sha1sum | head -c 40
    

也可以合成一行命令

    echo -n "https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken" | sha1sum | head -c 40
    

在这行命令中，`echo -n` 用于输出指定的 URL 字符串（其 `-n` 参数的作用是**禁止在输出的末尾添加换行符**），`sha1sum` 计算其 SHA-1 哈希值，`head -c 40` 截取前 40 个字符，即哈希值的前 40 位。

执行结果是

    9b5ad71b2ce5302211f9c61530b329a4922fc6a4
    

然后把 cl100k\_base.tiktoken 文件重命名为输出的 `9b5ad71b2ce5302211f9c61530b329a4922fc6a4`

在前面的 docker-compose.yaml 里，我们已经指定了 TIKTOKEN\_CACHE\_DIR 环境变量

然后把这个 9b5ad71b2ce5302211f9c61530b329a4922fc6a4 文件放在 TIKTOKEN\_CACHE\_DIR 目录里即可。

后续还有遇到类似报错，重复以上操作，直到没有报错为止。

我目前使用的版本只下载了两个 encoder

在OneApi中添加Ollama渠道
------------------

这里因为docker网络的问题会有些麻烦

有多种思路，一种是让OneApi的容器跑在 host 网络模式下

一种是使用 host.docker.internal 这个地址

当然前提都是 ollama 的 host 设置为 `0.0.0.0` ，这个配置可以参考我之前的这篇文章: [LLM探索：本地部署DeepSeek-R1模型](https://www.cnblogs.com/deali/p/18695132)

在添加渠道的时候，类型选择 Ollama

自定义模型部分填入我们部署的 deepseek-r1:32b

然后代理填写 `http://host.docker.internal:11434`

_注意：在 Linux 环境中，`host.docker.internal` 可能无法工作，但你可以直接使用宿主机的 IP 地址。例如，如果宿主机的 IP 地址是 `192.168.1.100`，可以在OneApi中使用 `http://192.168.1.100:11434` 来访问 Ollama 服务。_

参考资料
----

*   [https://github.com/ollama/ollama/blob/main/docs/linux.md](https://github.com/ollama/ollama/blob/main/docs/linux.md)
*   [https://stackoverflow.com/questions/76106366/how-to-use-tiktoken-in-offline-mode-computer](https://stackoverflow.com/questions/76106366/how-to-use-tiktoken-in-offline-mode-computer)
*   [https://www.cnblogs.com/cjdty/p/18659438](https://www.cnblogs.com/cjdty/p/18659438)
*   [https://zhuanlan.zhihu.com/p/20485169539](https://zhuanlan.zhihu.com/p/20485169539)

微信公众号：「程序设计实验室」 专注于互联网热门新技术探索与团队敏捷开发实践，包括架构设计、机器学习与数据分析算法、移动端开发、Linux、Web前后端开发等，欢迎一起探讨技术，分享学习实践经验。