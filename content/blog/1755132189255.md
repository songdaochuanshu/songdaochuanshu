---
layout: post
title: '为什么RAG技术可以缓解大模型知识固化和幻觉问题'
date: "2025-08-14T00:43:09Z"
---
为什么RAG技术可以缓解大模型知识固化和幻觉问题
========================

RAG技术缓解大模型知识固化和幻觉问题的原理

**1、大模型知识固化和幻觉问题**

要理解大模型的时效性问题，需首先明确其技术原理：大模型通过输入文本与已固化在神经网络中的知识进行匹配，预测并输出概率最大的文本内容作为答案。其固化知识的神经网络形成于前期训练阶段，训练输入源自人类现有知识数据（包括互联网及线下知识数据）。模型一旦训练完成，其知识范围便被固定，回答能力完全取决于训练时的数据内容。  
幻觉产生的原因是：无论匹配概率多低，模型总会生成输出，这种缺乏依据的输出如同人类空想，即形成幻觉。  
如图所示：若大模型A基于2024年12月31日前的数据训练，当询问"2025年发生了哪些地震"时，它无法提供真实信息，强行回答则会虚构内容，因其神经网络中并无2025年数据。  

因此，大语言模型面临两大核心问题：  
1.知识固化：模型仅能回答训练数据范围内的内容，对训练后发生的事件（如2025年新发生的世界信息）或未参与训练的私有数据（如公司内部信息），统称为"外部知识"的内容，均无法直接回答。  
2.幻觉与不可溯源：由于模型输出本质是对固化知识的重组，用户难以验证信息来源，导致不敢轻易相信。

**2、RAG技术出现之前的解决方案**  
在RAG技术出现前，解决知识固化的主流方式是通过补充外部知识对模型进行持续微调，即利用新数据训练生成新模型（如下图所示）。  
  
但该方案效率低，成本高，主要原因包括：  
1.需庞大训练数据支撑  
2.模型训练需要高昂GPU计算资源  
3.模型训练技术门槛要求高  
4.训练周期长，且效果不稳定  
5.模型更新繁琐：如上图，即使在2025年8月9日好不容易完成了大模型B、C的迭代，仍无法覆盖2025年8月9日后的新知识。

**3、RAG技术解决知识固化和幻觉问题的原理**  
RAG是英文（Retrieval-Augmented Generation，检索增强生成）的缩写，是由 Meta AI（原Facebook AI） 的研究团队于2020年首次提出，核心论文《Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》（Patrick Lewis等）发表于2021年4月，论文地址：[https://arxiv.org/pdf/2005.11401。](https://arxiv.org/pdf/2005.11401%E3%80%82)  
RAG的核心思想是不将新出现的知识内容用于大模型的训练，而是将最新的的问题相关的知识和要问的问题一起送给大模型，利用大模型语言组织能力，形成自然语言形式的答案。  
RAG的核心思想是：不将新知识纳入模型训练，而是将实时问题与相关外部知识同步一起输入给模型，利用大模型的语言组织能力生成答案。如下图所示：  
  
第①步：为实时信息/本地数据建立向量索引库。关于向量和向量数据库，请参考：[https://www.cnblogs.com/twosedar/p/18957931](https://www.cnblogs.com/twosedar/p/18957931)  
第②步：用户提问后，将问题向量化；  
第③步：通过向量匹配，在第①步中建立的索引库中检索出最相关的条目  
第④步：整合问题与检索结果生成提示词，输入给大模型  
例如：“2025年发生了哪些地震？请参考如下信息回答：①2025.1.7西藏定日地震，②2025.7.30堪察加地震，回答时需标注参考条目序号"”  
第⑤步：将大模型回复结果转述给用户  
例如：“2025年发生了两次地震，包括1月7日定日地震，7月30日勘察加地震。参考条目① ②”

RAG巧妙的通过本地信息搜索和大模型集合的流程解决了知识固化问题，同时又避免了模型训练的高成本。而且通过展示参考条目，用户可验证信息来源，有效缓解幻觉问题。

**4、那么问题来了，既然能够提前检索到信息，还用大模型做什么？**  
首先，传统检索只能返回相关段落或者片段，但是大模型却可以生成人性化的自然语言描述的答案。另外，大模型还能基于已有的基础知识进行信息的组织，甚至跨文档推理。如果没有大模型，信息检索就和传统搜索引擎的效果差不多了。