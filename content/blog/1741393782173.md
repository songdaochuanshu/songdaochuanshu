---
layout: post
title: 'DeepSeek在M芯片Mac上本地化部署'
date: "2025-03-08T00:29:42Z"
---
DeepSeek在M芯片Mac上本地化部署
=====================

在 Mac 上使用 Ollama 运行 DeepSeek-R1，并通过 Open-WebUI 提供 Web 端访问。

1\. 安装 Ollama
=============

Ollama官方：[https://ollama.com/](https://ollama.com/)  
Ollama 是一个轻量级的 AI 推理框架，支持本地运行 LLM（大型语言模型）。首先，下载并安装 Ollama。

mac原生工具brew安装ollama

    $ brew install --cask ollama
    Running `brew update --auto-update`...
    ==> Auto-updated Homebrew!
    Updated 2 taps (homebrew/core and homebrew/cask).
    ==> New Formulae
    bpmnlint          gersemi           i686-elf-grub     kirimase          org-formation     rattler-index     semver            todoist           x86_64-elf-grub
    cf-terraforming   globstar          immich-go         largetifftools    ov                rhai              sequoia-sqv       trdsql            yoke
    cloudfoundry-cli  gotz              kafkactl          lazyjournal       pivy              rpds-py           sttr              typioca           ytt
    fortitude         hishtory          kapp              mox               punktf            sdl3_ttf          tml               unciv
    ==> New Casks
    candy-crisis                              font-winky-sans                           opera-air                                 trae-cn
    consul                                    fuse-t                                    pairpods                                  ua-midi-control
    focu                                      macskk                                    pareto-security                           veracrypt-fuse-t
    font-sf-mono-nerd-font-ligaturized        nvidia-nsight-compute                     trae
    
    You have 13 outdated formulae and 1 outdated cask installed.
    
    ==> Downloading https://github.com/ollama/ollama/releases/download/v0.5.13/Ollama-darwin.zip
    ==> Downloading from https://objects.githubusercontent.com/github-production-release-asset-2e65be/658928958/2dc24c17-0bc0-487a-92d1-0265efd65a14?X-Amz-Algorithm=AWS4-
    ############################################################################################################################################################### 100.0%
    ==> Installing Cask ollama
    ==> Moving App 'Ollama.app' to '/Applications/Ollama.app'
    ==> Linking Binary 'ollama' to '/opt/homebrew/bin/ollama'
    🍺  ollama was successfully installed!
    

检查Ollama是否安装成功，成功会显示版本号，如：ollama version is 0.5.13

    $ ollama --version
    Warning: could not connect to a running Ollama instance
    Warning: client version is 0.5.13
    

2\. 下载模型
========

下载 DeepSeek-R1 模型  
模型下载地址：[https://ollama.ai/library/deepseek-r1](https://ollama.ai/library/deepseek-r1)

该命令会自动下载 DeepSeek-R1 1.5B 版本的模型，并存储在本地。

    $  ollama pull deepseek-r1:7b
    pulling manifest
    pulling 96c415656d37... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB
    pulling 369ca498f347... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████▏  387 B
    pulling 6e4c38e1172f... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB
    pulling f4d24e9138dd... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████▏  148 B
    pulling 40fb844194b2... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████▏  487 B
    verifying sha256 digest
    writing manifest
    success
    

3.本地运行 DeepSeek-R1
==================

执行后，终端会进入交互模式，可以直接输入文本与模型进行对话。

    # 查看已下载的模型
    $ ollama list
    NAME              ID              SIZE      MODIFIED
    deepseek-r1:7b    0a8c26691023    4.7 GB    24 seconds ago
    # 运行模型
    $ ollama run deepseek-r1:7b
    >>> Send a message (/? for help)
    

4.通过 Open-WebUI 运行
==================

使用 Web 界面交互模型，可以安装 Open-WebUI。该工具提供了一个用户友好的 Web 前端，使得 DeepSeek-R1 更加易用。

克隆 Open-WebUI 仓库

    $ git clone https://github.com/open-webui/open-webui.git
    Cloning into 'open-webui'...
    remote: Enumerating objects: 91391, done.
    remote: Counting objects: 100% (131/131), done.
    remote: Compressing objects: 100% (74/74), done.
    remote: Total 91391 (delta 70), reused 57 (delta 57), pack-reused 91260 (from 2)
    Receiving objects: 100% (91391/91391), 177.81 MiB | 3.98 MiB/s, done.
    Resolving deltas: 100% (60008/60008), done.
    Updating files: 100% (4575/4575), done.
    

启动 Open-WebUI 容器

mac安装docker,安装完成后应用程序中会有docker程序，点击即可启动

    brew install --cask --appdir=/Applications docker
    

启动docker

    docker run -d \
      -p 3000:8080 \
      --add-host=host.docker.internal:host-gateway \
      -v open-webui:/app/backend/data \
      --name open-webui \
      --restart always \
      ghcr.io/open-webui/open-webui:main
    Unable to find image 'ghcr.io/open-webui/open-webui:main' locally
    main: Pulling from open-webui/open-webui
    d51c377d94da: Pull complete
    987cac002684: Pull complete
    076b75118273: Pull complete
    157e623d2984: Pull complete
    40d5353a5918: Pull complete
    4f4fb700ef54: Pull complete
    aebeb0b4e5d0: Pull complete
    03f562834d64: Pull complete
    dc0f62a912f5: Pull complete
    93fdf9ebd111: Pull complete
    596be9ce6130: Pull complete
    07dc67f42781: Pull complete
    7c2ef53b15e7: Pull complete
    e5511c24fa69: Pull complete
    69de4f91fd38: Pull complete
    Digest: sha256:74fc3c741a5f3959c116dd5abc61e4b27d36d97dff83a247dbb4209ffde56372
    Status: Downloaded newer image for ghcr.io/open-webui/open-webui:main
    26b786db658d187c2b82256fcbf33102c8c10c25b1087393483272e53708908b
    

• -p 3000:8080：将容器的 8080 端口映射到本机 3000 端口；  
• --add-host=host.docker.internal:host-gateway：允许容器访问宿主机网络；  
• -v open-webui:/app/backend/data：挂载数据存储目录，保存容器的状态和数据。  
• --restart always：确保容器在重启后自动运行；  
• ghcr.io/open-webui/open-webui:main：拉取 Open-WebUI 的最新版本镜像。

运行容器后，访问 [http://localhost:3000](http://localhost:3000) 即可访问 Open-WebUI。

    #停止容器
    docker stop open-webui
    #删除容器
    docker rm open-webui
    #删除存储数据
    docker volume rm open-webui