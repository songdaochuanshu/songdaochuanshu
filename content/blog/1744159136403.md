---
layout: post
title: '解密prompt系列52. 闲聊大模型还有什么值得探索的领域'
date: "2025-04-09T00:38:56Z"
---
解密prompt系列52. 闲聊大模型还有什么值得探索的领域
==============================

![解密prompt系列52. 闲聊大模型还有什么值得探索的领域](https://img2024.cnblogs.com/blog/1326688/202504/1326688-20250409082117985-1585311272.png) 在DeepSeek-R1的开源狂欢之后，感觉不少朋友都陷入了\*\*技术舒适区\*\*，但其实当前的大模型技术只是跨进了应用阶段，可以探索的领域还有不少，所以这一章咱不聊论文了，偶尔不脚踏实地，单纯仰望天空，聊聊还有什么有趣值得探索的领域，哈哈有可能单纯是最近科幻小说看太多的产物~

在DeepSeek-R1的开源狂欢之后，感觉不少朋友都陷入了**技术舒适区**，但其实当前的大模型技术只是跨进了应用阶段，可以探索的领域还有不少，所以这一章咱不聊论文了，偶尔不脚踏实地，单纯仰望天空，聊聊还有什么有趣值得探索的领域，哈哈有可能单纯是最近科幻小说看太多的产物~

尚未攻克的持续学习
---------

当前的大模型训练还是阶段式的，OpenAI每隔几个月就会对模型进行重新训练，延长模型的世界知识截止时间。说白了就是全新、覆盖式的知识写入。就像西西弗斯推石头，每次全量训练都意味着对先前知识的系统性遗忘。而真正意义上的持续学习和试试学习，当前似乎还是个未解之谜。当然也有观点认为有机体的进化，本身就和无机体的进化存在完全不同的路径，所以大模型是否真的需要持续学习，不少人也是打问号的。

之前接触到的真正使用在线更新的主要在推荐领域，基于实时发生的用户长短行为序列进行持续的模型训练和迭代更新。但是这类模型本质只是行为表征和拟合，和当前的大模型还是有比较大的区别。而在NLP领域虽然之前有不少元学习，持续学习的论文发表，但是和R1的RL训练，ChatGPT的SFT指令训练一对比就会发现，它们可能还没找到正确的打开方式。其实从Word2Vec、Bert、CLIP、ChatGPT、R1不难看出，每个划时代的模型所使用的技术本身基本都符合大道至简的原理，匠气更少一些，Scaling曲线更长一些。

持续学习其实包含多个方面，比较重要的一个是单纯的增量世界知识的补充，也就是在模型上一次截止训练后至今世界上增量产生的知识和信息。之前训练模式在持续训练上最大的问题就是灾难遗忘，学了新的忘了旧的，捡了芝麻丢了西瓜。这里纯纯个人猜想的一个可能原因来自于当前Transformer模型结构中，模型习得的语言能力、世界知识、任务完成能力、思考推理能力，这些能力是纠缠在一起存储在Transformer参数中的。导致继续学习的过程中我们只学知识，就会遗忘任务完成能力；只补充任务完成能力，不更新知识就会增加模型幻觉（模型以为自己行了！其实并不行）。但如果有结构能把以上能力分层解耦，知识既客观事实的存储纯靠背诵，推理能力更多依赖模型基于反馈探索优化，而语言能力其实没有更新的必要。甚至模型可以在实现推理和语言能力不变的情况下，持续更新知识，或者对知识存储定期进行蒸馏压缩。之前一些知识编辑的论文其实就研究过大模型的知识存储，并发现在MLP层其实存在知识以Key-Value键值对形式存储。

持续学习的另一个方向是推理和任务完成能力，是基于大模型在使用工具完成任务的过程中，收到的环境给予的反馈，模型需要基于反馈优化行为路径和任务完成形式，这样才能在不断的练习中逐步提高任务完成的成功率。哈哈那借鉴《三体》中的文明进化机制，我们是否可以为模型构建虚拟生态圈，类似斯坦福小镇等"AI 沙盒"，大模型本身是Policy，由沙盒本身生成大模型的任务todo，并评估模型的完成效果，生成反馈信号。沙盒中也允许模型接入各类MCP接口去和环境进行交互，还可以在沙盒环境中动态加入各类约束和竞争条件，例如

*   动态奖励：基于任务完成度进行推理资源的动态分配，鼓励模型用更少的资源解决更复杂的问题
*   种群竞争：多智能体同一任务完成效果对比择优
*   环境突变模拟：随机修改MCP接口，让模型动态适应和环境的不同交互

内生化RAG是什么样子
-----------

除了模型本身能力的持续进化，另一个似乎进入技术共识的就是RAG检索增强技术。当前解决模型获取实时信息的方式还是比较传统上一代的搜索方案，构建知识库，Query改写，多路召回，粗排精排，虽然Knowledge Retriever的每一步都可以使用大模型进行能力增强但整个知识、实时信息获取的模块还是完全外挂在模型之外的，其实是上一代搜索技术和这一代大模型技术的拼接式实现方案。这种方案有什么问题呢？

一个就是模型上文的有限长度，虽然通过各种注意力机制改进和前期的长上文训练，模型的上文context已经从最早的1024一路狂飙到了几十K的长度，但依旧难以避免在更长文本上回答效果的衰减。而之所以上文的长度会更长更长，来自于通过**搜索召回、多轮对话的信息是线性平铺，没有经过压缩处理**。

R1之后我也在想问题的表现形式和问题的解决方案可能是不一致的，就像我们看到模型推理过程中存在反思，纠错，生成新的假设，就认为这可能是树形的思考结构。而R1证明线性思考链路+Attention注意力机制也可以实现，那是否有可能上面提到的这种压缩处理也是可以通过Attention直接实现的呢？但现在的我（哈哈未来的我不一定这么想）认为Attention并不足够，因为之前所有Attention的改良都在提高Aettention对各个位置、各个长度信息的高效定位和选择能力，但这只是信息选择，而非信息压缩，选择只是信息的拼接，而压缩能产生信息之外的智能和抽象概念。有些类似GraphRAG的节点和关系抽象，但是又不想Graph受到三元组形式的限制。

所以我就想那是否有可能在模型使用检索上文进行推理的同时，对这部分上文进行重新的压缩编码并存储到另一个独立的存储模块，之后每一次回答模型都会使用存储模块和外部检索一同回答。并随着模型不断回答问题，存储模块的内容范围会持续扩展，而每次对存储模块的更新，都是新一轮的知识压缩，知识消歧，从知识中反思形成新的思考，这样存储中知识的密度会越来越高而长度却不会发生线性增长。看到最近英伟达推出的[star Attention](https://arxiv.org/pdf/2411.17116)其实就有类似的context先编码再进行推理的思路，不过只涉及到一次信息压缩，没有更深层次多步的压缩和反思，类似于on-the-fly的推理信息压缩方式。还有有一个开源项目[Mem0](https://github.com/mem0ai/mem0)也有类似的思路，会通过工程设计不断对对话的上文历史进行总结抽象，冲突消解并形成长短期、不同类型的记忆存储。

另一个就是搜索能力和模型能力的不匹配，搜索引擎一次搜索返回的信息深度和广度都比较有限，前一年主要方案是利用大模型去进行query改写，从多个角度一起检索，但这种方案的弊端就是闭着眼睛撒网全凭运气，改写的好问题就能回答，改写的不好就完蛋。于是在大模型能力（反思能力为主）逐步提升的当前，又出现了以模型反思驱动的链式搜索推理模式，包括OpenAI的Deep Research，以及jina, Dify，Huggingface推出的更多开源版模式相似的Deep Search实现方案（对于Research和Search的边界其实非常模糊，请不要纠结这个问题，一切以效果和具体解决的问题为主）。概念很好懂，就是每轮都是有限搜索，然后让模型判断对于回答用户提问还需要补充哪些信息，然后生成新的搜索query，再去搜索，对信息进行补充更新，然后迭代下去，直到模型判断Okay。

这种方式我们测试后信息密度和信息丰富度，在使用O1，R1以上的模型后，会有显著的提升，但使用非思考类的模型，效果基本和我们人工调优后的多步RAG效果差别不大，但速度要显著慢很多。原因也简单多数问题还是能通过前期规划和2步以内的信息补充完成，而超过这个复杂程度的问题，对模型本身思考推理能力的要求也就变得很高。但这种方式的问题就在于整个流程的时间会变得不可控制，短则几分钟，长则几十分钟。当然和人工收集信息的速度相比是快的，但是似乎又和我们理想中的Javis有了比较大的差距。

所以想要加速信息收集的过程，我们跳出Deep Research的框架，是否有可能把模型被动获取信息，转换成模型主动获取并存储信息，类似把模型直接接入数据流，持续处理、筛选、整合、并进行压缩编码。这样搜索的过程就不再是调用搜索引擎去访问外部数据，而是直接借助Attention在编码的数据库中直接获取有效信息，信息提取效率，和信息获取的丰富度都是都会更好。最大的难点不在于构建实时世界的数据流，毕竟可以先做一个子领域，像金融资讯的场景流式数据很多，难点主要在于如何流式处理数据并压缩编码成和模型内生参数处于同一个高维空间的数据库。毕竟这份数据并不随模型一起训练，所以如何保证向量空间的一致是最大的问题，或者训练个Adapter类似多模态的桥接模型。最近看到谷歌出的[Titan](https://arxiv.org/abs/2501.00663)其实已经开始探索这些方向啦，下一章我们就围绕记忆展开聊聊。

哈哈哈这一章就聊这么多，也是最近代码看的多，论文读的少，确实没看到啥值得分享的，所以又水了一章真棒！

**想看更全的大模型论文·微调预训练数据·开源框架·AIGC应用 >>** [**DecryPrompt**](https://github.com/DSXiangLi/DecryptPrompt/)