---
layout: post
title: '大模型基础补全计划(一)---重温一些深度学习相关的数学知识'
date: "2025-02-16T00:39:28Z"
---
大模型基础补全计划(一)---重温一些深度学习相关的数学知识
------------------------------

PS：要转载请注明出处，本人版权所有。  
  
PS: 这个只是基于《我自己》的理解，  
  
如果和你的原则及想法相冲突，请谅解，勿喷。  

###### 环境说明

  无

### 前言

* * *

  遥记在2021年左右，我写了一系列关于深度学习视觉方向基础学习的文章，它们如下：

*   DL基础补全计划(一)---线性回归及示例（Pytorch，平方损失） [https://githubio.e-x.top/2021/07/04/blog\_idx\_105/](https://githubio.e-x.top/2021/07/04/blog_idx_105/)
*   DL基础补全计划(二)---Softmax回归及示例（Pytorch，交叉熵损失） [https://githubio.e-x.top/2021/07/11/blog\_idx\_106/](https://githubio.e-x.top/2021/07/11/blog_idx_106/)
*   DL基础补全计划(三)---模型选择、欠拟合、过拟合 [https://githubio.e-x.top/2021/07/18/blog\_idx\_107/](https://githubio.e-x.top/2021/07/18/blog_idx_107/)
*   DL基础补全计划(四)---对抗过拟合：权重衰减、Dropout [https://githubio.e-x.top/2021/08/01/blog\_idx\_108/](https://githubio.e-x.top/2021/08/01/blog_idx_108/)
*   DL基础补全计划(五)---数值稳定性及参数初始化（梯度消失、梯度爆炸） [https://githubio.e-x.top/2021/08/08/blog\_idx\_109/](https://githubio.e-x.top/2021/08/08/blog_idx_109/)
*   DL基础补全计划(六)---卷积和池化 [https://githubio.e-x.top/2021/08/15/blog\_idx\_110/](https://githubio.e-x.top/2021/08/15/blog_idx_110/)

  那时候的我，还一心沉醉在视觉算法模型落地到侧端的各个场景，虽然对NLP有所了解，但是当时还未料想到，在后面的几年，由大语言模型引爆的大模型领域是如此的火爆。到了2023左右，开始逐渐的接触大模型，逐渐的将其应用到自己的工作中，逐渐在工作中将大模型迁移到侧端。从2024年开始，意识到如果要在以后将大模型应用的更好，急需要补充一些大模型及NLP相关的知识才能更好的理解它。因此有了从本文开始的一系列文章。

  从本文开始，预计从数学知识开始，到transformer及LLM结束（挖坑），挑选一些内容来学习记录。

  
  
  
  

### 概率论、数理统计

* * *

  
  

##### 基本概念

  在统计学中，把需要调查或者研究的某一现象或者事物的全部数据称为统计总体（简称 总体population)，其所属的数据分布称为 总体分布 (population distribution)， 单个数据称为个体(individual)。我们从统计总体中抽取样本的过程称为抽样（sampling），一次抽样的结果称为一份样本(sample)，一份样本中包含的个体数据的数量称为本容量(sample size)。

  可以把分布（distribution）看作对事件的概率分配，P(X)表示为随机变量X上的分布（distribution）, 分布告诉我们X获得某一值的概率

  概率（probability）在给定的样本空间中，A事件的发生的可信度， 表示为P(A)

  推断统计学（或称统计推断，英语：statistical inference）， 指统计学中，研究如何根据样本(sample)数据去推断总体(population)特征（或者参数）的方法， 比如根据样本的平均值去估计总体的均值参数。 它是在对样本数据进行描述的基础上，对统计总体的未知数量特征做出以概率形式表述的推断。

  通常我们会假设总体分布服从某种已知的概率分布，但是分布的某些参数是不确定的， 比如全国身高数据服从正态分布，但是期望和方差不知道， 这时我们期望能通过样本推断（估计）出总体正态分布的期望和方差参数。

  概率（probability）和统计（statistics）看似两个相近的概念，其实研究的问题刚好相反。

*   概率研究的是，已经知道了模型和参数后，给出一个事件发生的概率。θ是已知确定的，x是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少（表示不同x出现的概率）。概率函数记作\\(P(X=x\_i|\\theta)\\)。
*   统计是根据给出的观测数据，利用这些数据进行建模和参数的预测。x是已知确定的，θ是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数θ，出现x这个样本点的概率是多少(表示不同θ下，x出现的概率）。此时的函数也记作\\(L(\\theta|X=x\_i)\\)。

  
  

##### 联合概率

  给定任意值a和b，联合概率可以回答：A=a和B=b同时满足的概率是多少，其表示为 \\(P(A=a,B=b)=P(A=a)\*P(B=b)\\)

  
  

##### 条件概率

  联合概率的不等式带给我们一个有趣的比率：\\(0<= \\frac{P(A=a,B=b)}{P(A=a)} <=1\\)，表示为\\(P(B=b|A=a)\\), 代表在A=a已发生的情况下，B=b的概率。也就是 $P(B=b|A=a) = \\frac{P(A=a,B=b)}{P(A=a)} $。

  
  

##### 贝叶斯定理

  因为条件概率公式：$P(B=b|A=a) = \\frac{P(A=a,B=b)}{P(A=a)} ，P(A=a|B=b) = \\frac{P(A=a,B=b)}{P(B=b)} \\(。然后我们观察两个条件概率表达式,可看到有公有的联合概率部分，改变方程即可得到：\\)P(A,B) = P(A|B) \* P(B) = P(B|A) \* P(A)$ ，这个就是贝叶斯公式。

此外此定理在深度学习中有一些特殊的解释：

*   P(A|B)：在事件 B 已经发生的情况下，事件 A 发生的后验概率 。
*   P(B|A)：在事件 A 已经发生的情况下，事件 B 发生的似然 。
*   P(A)：事件 A 的先验概率（在没有额外信息时对 A 的概率估计）。
*   P(B)：事件 B 的边缘概率（即 B 发生的总概率）。

首先我们定义一个深度学习的模型为：\\(P(\\theta,D) = \\frac{P(D|\\theta) \* P(\\theta)}{P(D)}\\), 其参数为 θ，训练数据为 D。我们的目标是根据数据 D 来更新对参数 θ 的估计,这里我们来看一个例子：

*   $P(\\theta | D) $：在观察到数据 D 后，参数 $\\theta $ 的后验分布 。
*   $P(\\theta) $：参数 $\\theta $ 的先验分布 （在没有看到数据之前对参数的假设）。
*   $P(D | \\theta) $：在给定参数 \\(\\theta\\) 的情况下，生成数据 D 的似然函数 。
*   $P(D) $：数据 D 的边缘概率 （归一化常数）

  
  

##### 边际化

  B的概率相当于计算A的所有可能选择，并将所有选择的联合概率聚合在一起：\\(P(B) = \\sum\\limits\_{i=1}^{n} P(A\_i,B) , P(A) = \\sum\\limits\_{i=1}^{n} P(B\_i,A)\\)

  
  

##### 期望、均值、方差、概率

  一个随机变量X的期望（expectation，或平均值（average））表示为: \\(E(X) = \\sum\\limits\_{x=1}^{n} x\_i \* P(X=x\_i)\\)

  均值是一个统计量(基于样本构造的函数)，更偏统计学的概念；而期望完全由随机变量的概率分布所确定（更偏概率学的概念），类似于在“上帝视角”下去计算均值，所谓上帝视角是指你拥有的是总体并且知道总体所有取值出现的概率

  希望衡量随机变量X与其期望值的偏置。这可以通过方差来量化：\\(V(X) = \\frac{\\sum\\limits\_{i=1}^{n}(x\_i-E(X\_i))^2}{n}\\)

  
  

##### 似然函数及最大似然估计

  似然估计函数,其可以解释为：假设我们有一个关于X的概率分布是\\(P(X=x|\\theta)\\)，\\(\\theta\\)是关于X的概率分布的参数。假如我们从关于X的概率分布是\\(P(X=x|\\theta)\\)中抽取n个样本（这个时候往往我们是不知道其参数\\(\\theta\\)的），这个时候我们想去估计\\(\\theta\\)，因此我们可以定义似然函数为： \\(L(\\theta|x\_i,i\\in{n}) = \\prod\\limits\_{i=1}^{n} P(x\_i|\\theta)\\)。

  注意，我们可以知道由于有n个样本，每个样本都有一个\\(P(x\_i|\\theta)\\)，因此n个样本的联合概率就是似然函数，根据联合概率的定义，似然函数就是描述在\\(x\_i\\)出现概率已知的情况下，出现\\(\\theta\\)的概率。

  最大似然估计（Maximum Likelihood Estimation，MLE），又叫极大似然估计，是统计学中应用最广泛的一种未知参数估计方法。 它可以在已知随机变量属于哪种概率分布的前提下， 利用随机变量的一些观测值估计出分布的一些参数值。

  最大似然估计函数 就是 对似然估计函数 取对数，以简化乘积的计算。那么其定义是：\\(L(\\theta|x\_i,i\\in{n}) = \\sum\\limits\_{i=1}^{n} \\ln{P(x\_i|\\theta)}\\)，对其求最大值就等于对其求 负最小值，其定义为：\\(-\\ln\_{}{L(\\theta|x\_i,i\\in{n})} = -\\sum\\limits\_{i=1}^{n} \\ln(P(x\_i|\\theta))\\)

  
  
  
  

### 信息论

* * *

  
  

##### 基本概念

  根据信息论中的定义：

*   信息量，事件发生概率越大，所携带的信息量越小。定义为：\\(I(x)=\\log\_{2}(\\frac{1}{P(x)})=-\\log\_{2}(P(x))=表示此事件的最少比特位数\\) ,其也蕴含了我们需要使用多个比特才能表示信息量。
*   信息熵，一个随机变量的熵是指该变量可能的结果所蕴含的不确定性的平均水平。可以类别期望的定义，这里得到信息量的均值：\\(H(X)=-\\sum\\limits\_{i=0}^{n-1}P(Xi)\\log\_{2}(P(Xi))\\)
*   KL差异：定义原概率分布为P(X),近似概率分布为Q(X)，假如X是离散随机变量，KL差异定义为：\\(D\_{KL}(P(X)||Q(X))=\\sum\\limits\_{i=0}^{n-1}P(Xi)\\log\_{2}(P(Xi)/Q(Xi))=\\sum\\limits\_{i=0}^{n-1}P(Xi)\[\\log\_{2}(P(Xi)) - \\log\_{2}(Q(Xi))\]\\)
*   交叉熵（cross-entropy），交叉熵定义为：\\(H(P,Q)=-\\sum\\limits\_{i=0}^{n-1}P(Xi)\\log\_{2}(Q(Xi))\\)，我们可以看到\\(H(P,Q)=H(P)+D\_{KL}(P||Q)\\)

  
  

##### 深度学习中的交叉熵

  假如: 数据集{X, Y}有n个样本，有特征向量\\(x\_i \\in X\\)，独热标签向量\\(y\_i \\in Y\\)，当前模型最终softmax输出的向量\\(\\hat{y}\_i\\)。首先，我们将\\(\\hat{y}\_i\\)当做给定特征向量\\(x\_i\\)的每个类别的条件概率。因此我们可以得到：\\(\\hat{y}\_i=P(y\_i|x\_i)\\)

  根据似然函数定义：\\(L(\\theta) = \\prod\_{i=1}^{n} P(y\_i|x\_i,\\theta)\\)，其代表给定\\(x\_i, \\theta\\)情况下，观察到\\(y\_i\\)的概率。

  根据似然函数，要使得其值为最大值，及 对其取负对数，此外根据我们之前的定义：我们将\\(\\hat{y}\_i\\)当做给定特征向量\\(x\_i\\)的每个类别的条件概率。因此我们得到 \\(-log{L(\\theta)} = -log{\\prod\\limits\_{i=1}^{n} P(y\_i|x\_i,\\theta)} = -\\sum\\limits\_{i=1}^{n} log{P(y\_i|x\_i,\\theta)}= -\\sum\\limits\_{i=1}^{n} log{\\hat{y\_i}}\\)

  我们通过信息论中的定义得到交叉熵损失函数定义：\\(l(y,\\hat{y}) = - \\sum\\limits\_{j=1}^{q} y\_j \* log{\\hat{y}\_j}\\) ，其描述的是标签\\(y\_j\\)和预测值\\(\\hat{y\_j}\\)两个分布之间的差异值。

  我们对其交叉损失函数进行多个样本的求和可以得到：\\(\\sum\\limits\_{i=1}^{n}{l(y,\\hat{y})} = - \\sum\\limits\_{i=1}^{n}{\\sum\\limits\_{j=1}^{q} y\_j \* log{\\hat{y}\_j}}\\)

  由于\\(y\_j\\)是标签的热独向量，只有对应类别概率为1，其他类别为0，因此\\(\\sum\\limits\_{j=1}^{q} y\_j \* log{\\hat{y}\_j} = log{\\hat{y}\_j}\\)，因此可以得到交叉熵损失和似然函数之间的关系：$\\sum\\limits\_{i=1}^{n}{l(y,\\hat{y})} = - \\sum\\limits\_{i=1}{n}{\\sum\\limits\_{j=1} y\_j \* log{\\hat{y}_j}} = - \\sum\\limits_^{n}{log{\\hat{y}\_j}} $

  从这里可以知道交叉熵损失函数与负对数似然函数的关系。

  
  

##### 困惑度（Perplexity）

  这里我们讨论如何度量语言模型的质量。这里根据参考文件中的建议，不要直接使用交叉熵来理解，要从另外一个角度来理解。

  如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。 一个更好的语言模型应该能让我们更准确地预测下一个词元。 因此，它应该允许我们在压缩序列时花费更少的比特。 所以我们可以通过一个序列中所有的个词元的交叉熵损失的平均值来衡量：  
\\(\\frac{1}{n}\\sum\\limits\_{t=1}^{n} -\\log P(x\_t|x\_{t-1}, ...., x\_1)\\)，我们看这个公式的含义就是每个预测的词元信息量求和，然后再求平均值，最后平均值越小，意味着我们整个模型蕴含的信息量越小，我们要压缩这个文本需要的比特最少。

  由于历史原因，自然语言处理的科学家更喜欢使用一个叫做困惑度（perplexity）的量。 简而言之，它是上面提到的信息量均值的指数：\\(\\exp(\\frac{1}{n}\\sum\\limits\_{t=1}^{n} -\\log P(x\_t|x\_{t-1}, ...., x\_1))\\)

  我们来看一下困惑度的特性（信息量取值是\[0,1\]）：

*   当最好情况下，因此当每个变量预测概率都是1，信息量为0，困惑度是\\(\\exp^0 = 1\\)
*   当最坏情况下，因此当每个变量预测概率都是0，信息量为无穷大，困惑度是\\(\\exp^{\\infty} = \\infty\\)

  
  
  
  

### 后记

* * *

  看了上面这些内容，有些是以前接触过的，有些是新的体验 总的来说，脑袋大了。

### 参考文献

*   [http://zh.gluon.ai/](http://zh.gluon.ai/)

  
  

* * *

打赏、订阅、收藏、丢香蕉、硬币，请关注公众号（攻城狮的搬砖之路）

![qrc_img](https://flyinskyin2013.github.io/ImageBed0/blogs/qrcode_for_wx_official_account.jpg)

PS: 请尊重原创，不喜勿喷。  
  
PS: 要转载请注明出处，本人版权所有。  
  
PS: 有问题请留言，看到后我会第一时间回复。  

posted on 2025-02-15 19:50  [SkyOnSky](https://www.cnblogs.com/Iflyinsky)  阅读(51)  评论(0)  [编辑](https://i.cnblogs.com/EditPosts.aspx?postid=18717317)  [收藏](javascript:void\(0\))  [举报](javascript:void\(0\))