---
layout: post
title: '【Agent】 ACE（Agentic Context Engineering）源码阅读笔记---（3）关键创新'
date: "2025-11-07T00:42:09Z"
---
【Agent】 ACE（Agentic Context Engineering）源码阅读笔记---（3）关键创新
========================================================

【Agent】 ACE（Agentic Context Engineering）源码阅读笔记---（3）关键创新
========================================================

目录

*   [【Agent】 ACE（Agentic Context Engineering）源码阅读笔记---（3）关键创新](#agent-aceagentic-context-engineering源码阅读笔记---3关键创新)
    *   [0x00 概要](#0x00-概要)
    *   [0x01 增量式 Delta 更新](#0x01-增量式-delta-更新)
        *   [1.1 原理](#11-原理)
        *   [1.2 源码解读](#12-源码解读)
    *   [0x02 Grow-and-Refine](#0x02-grow-and-refine)
        *   [2.1 原理](#21-原理)
        *   [2.2 源码解读](#22-源码解读)
            *   [2.2.1 Playbook的去重机制](#221-playbook的去重机制)
            *   [2.2.2 质量策略和剪枝](#222-质量策略和剪枝)
            *   [2.2.3 策略标记和淘汰机制](#223-策略标记和淘汰机制)
    *   [0x03 Reflector阶段](#0x03--reflector阶段)
        *   [3.1 原理](#31-原理)
        *   [3.2 源码解读](#32-源码解读)
            *   [3.2.1 辅助数据类与函数](#321-辅助数据类与函数)
            *   [3.2.2 Generator](#322-generator)
            *   [3.2.3 Reflector](#323-reflector)
            *   [3.2.4 Curator](#324-curator)
                *   [3.2.4.1 判断依据](#3241-判断依据)
                    *   [Reflector 的分析结果](#reflector-的分析结果)
                    *   [当前Playbook状态](#当前playbook状态)
                    *   [问题上下文信息](#问题上下文信息)
                *   [3.2.4.2 决策逻辑](#3242-决策逻辑)
                *   [3.2.4.3 生成策略](#3243-生成策略)
                *   [3.2.4.4 Playbook应用更新](#3244-playbook应用更新)
                *   [3.2.4.5 代码](#3245-代码)

0x00 概要
-------

ACE 引入了三项关键创新：

*   增量式 Delta 更新机制：以局部编辑替代整体重写，显著降低延迟与计算开销；
*   grow-and-refine 机制：在持续扩充的同时抑制冗余，实现上下文的稳态演化。
*   专职反思者模块：将评估与洞见提取与curation过程解耦，提高上下文质量与下游性能；

我们逐一依据代码来学习。

0x01 增量式 Delta 更新
-----------------

### 1.1 原理

想象一下，我们有一个智能助手（ACE），它不是用一个简单的提示来解决问题，而是用一系列的小提示，就像是一系列的备忘录或者清单。这些清单上的每个项目都包含了两部分信息：

1.  **基本信息**：比如一个独特的名字，以及这个项目是帮助解决问题还是带来了困扰的记录。
2.  **具体内容**：这可能包括一些有用的策略、专业知识或者常见的错误模式。

当我们的智能助手遇到新问题时，它会标记这些清单上的项目，告诉我们哪些是有用的，哪些可能会误导我们。这样，助手就可以知道哪些信息需要改进。

这种清单式的设计风格有三个主要优点：

*   **针对性强**：我们只更新那些真正需要改进的项目，而不是整个清单。
*   **精确查找**：助手可以快速找到最相关的信息，而不是在大量信息中盲目搜索。
*   **逐步改进**：在解决问题的过程中，我们可以有效地添加新信息、删除过时的信息或者避免重复。

智能助手不会重新编写整个清单，而是添加一些新的、精炼的项目。这些项目是经过助手深思熟虑后挑选出来的，可以帮助我们更好地解决问题。

这种方法避免了大规模重写清单所需的大量计算和时间，同时还能保留旧知识并不断吸收新见解。随着时间的推移，这种机制使得智能助手能够适应那些需要长时间或大量知识的复杂任务。

### 1.2 源码解读

DeltaOperation是ACE中对playbook进行单一变更操作的数据类：

*   封装了更新playbook所需的所有信息，如section（章节），content（内容），bullet\_id和metadata（元数据）。
*   是Curator输出的核心数据结构之一。
*   支持四种操作：ADD，UPDATE，TAG,REMOVE。
*   更新的粒度是bullet，是原子操作。

DeltaBatch 是包含多个DeltaOperation的集合，以及相关的推理信息，即决策理由：

*   封装Curator的一批更新操作。每个操作都有明确的类型和目标，便于调试和审计。
*   包括Curator执行这些操作的推理过程，增加了可解释性和可追溯性。
*   是Curator输出的数据结构。
*   运行原子性应用一组相关的playbook操作。

    # 定义操作类型的字面量类型，限定仅能取"ADD"（添加）、"UPDATE"（更新）、"TAG"（标记）、"REMOVE"（删除）这四个值
    OperationType = Literal["ADD", "UPDATE", "TAG", "REMOVE"]
    
    @dataclass
    class DeltaOperation:
        """Single mutation to apply to the playbook.
        用于对行动手册（playbook）执行的单个变更操作
        """
    
        # 操作类型，取值范围为OperationType定义的四种类型
        type: OperationType
        # 目标操作的章节名称，指定变更作用于行动手册的哪个部分
        section: str
        # 操作对应的内容，可选参数（如删除操作可能无需内容）
        content: Optional[str] = None
        # 项目标识ID，用于精准定位章节内的具体项目（如列表项），可选参数
        bullet_id: Optional[str] = None
        # 元数据字典，存储操作相关的附加信息（键为字符串，值为整数），默认初始化为空字典
        metadata: Dict[str, int] = field(default_factory=dict)
    
        @classmethod
        def from_json(cls, payload: Dict[str, Any]) -> "DeltaOperation":
            """从JSON格式数据构造DeltaOperation实例
            Args:
                payload: 包含操作信息的JSON字典
            Returns:
                构造完成的DeltaOperation对象
            """
            return cls(
                # 提取操作类型并转换为字符串
                type=str(payload["type"]),
                # 提取章节名称，若不存在则默认空字符串
                section=str(payload.get("section", "")),
                # 提取内容，若存在则转换为字符串，否则为None
                content=payload.get("content") and str(payload["content"]),
                # 提取项目标识ID，若存在则转换为字符串，否则为None
                bullet_id=payload.get("bullet_id")
                and str(payload.get("bullet_id")),  # 类型忽略注释：解决类型检查时的参数类型提示问题
                # 提取元数据，转换为键为字符串、值为整数的字典，默认空字典
                metadata={
                    str(k): int(v) for k, v in (payload.get("metadata") or {}).items()
                },
            )
    
        def to_json(self) -> Dict[str, Any]:
            """将DeltaOperation实例转换为JSON格式字典
            Returns:
                包含实例信息的JSON字典
            """
            # 初始化JSON字典，包含必选的操作类型和章节名称字段
            data: Dict[str, Any] = {"type": self.type, "section": self.section}
            # 若内容存在，添加content字段
            if self.content is not None:
                data["content"] = self.content
            # 若项目标识ID存在，添加bullet_id字段
            if self.bullet_id is not None:
                data["bullet_id"] = self.bullet_id
            # 若元数据非空，添加metadata字段
            if self.metadata:
                data["metadata"] = self.metadata
            return data
    
    
    @dataclass
    class DeltaBatch:
        """Bundle of curator reasoning and operations.
        包含管理者（curator）决策依据与操作集合的批量对象
        """
    
        # 决策依据说明，记录执行这些操作的原因或逻辑
        reasoning: str
        # 操作集合，存储多个DeltaOperation对象，默认初始化为空列表
        operations: List[DeltaOperation] = field(default_factory=list)
    
        @classmethod
        def from_json(cls, payload: Dict[str, Any]) -> "DeltaBatch":
            """从JSON格式数据构造DeltaBatch实例
            Args:
                payload: 包含批量操作信息的JSON字典
            Returns:
                构造完成的DeltaBatch对象
            """
            # 提取操作集合的JSON数据
            ops_payload = payload.get("operations")
            operations = []
            # 若操作数据是可迭代类型，遍历构造DeltaOperation实例
            if isinstance(ops_payload, Iterable):
                for item in ops_payload:
                    if isinstance(item, dict):
                        operations.append(DeltaOperation.from_json(item))
            # 构造并返回DeltaBatch实例
            return cls(reasoning=str(payload.get("reasoning", "")), operations=operations)
    
        def to_json(self) -> Dict[str, Any]:
            """将DeltaBatch实例转换为JSON格式字典
            Returns:
                包含实例信息的JSON字典
            """
            return {
                "reasoning": self.reasoning,
                # 将操作集合中的每个DeltaOperation实例转换为JSON字典
                "operations": [op.to_json() for op in self.operations],
            }
    

0x02 Grow-and-Refine
--------------------

### 2.1 原理

在持续增长的基础上，ACE 通过定期或延迟蒸馏来确保上下文保持紧凑与相关性。在 Grow-and-Refine 过程中，新条目会被追加到上下文中，而已有条目则通过元数据更新（如计数器递增）进行原地修订。去重步骤则通过语义嵌入比较条目相似度来消除冗余。该过程可在每次增量更新后主动执行，也可在上下文窗口超限时被动触发，具体取决于延迟与精度要求。

我们接下来做解读。想象一下，我们有一个智能的笔记本（ACE），它能够随着时间的推移不断更新内容，保持信息的新鲜和有用。这个笔记本的工作方式是这样的：

1.  **不断添加新知识**：每当我们学到新东西时，就把它作为新的一页添加到笔记本中。
2.  **更新旧知识**：对于笔记本中已有的内容，我们会根据新的经验来更新它们，比如增加一些注释或者修改一些信息。
3.  **去除重复内容**：笔记本还会检查新旧内容，如果发现有相似的信息，就会合并它们，避免重复。

这个过程可以在我们每次添加新内容后立即进行，也可以等到笔记本快要装满时再进行。这取决于我们希望笔记本更新得有多快，以及我们对信息准确性的要求。

通过这样的方式，我们的笔记本就能始终保持内容的丰富和相关性，同时不会变得过于臃肿。这样，无论何时我们翻开笔记本，都能快速找到我们需要的信息。

### 2.2 源码解读

实际上，Grow-and-Refine 主要涉及的是去重、剪枝和质量控制：

*   基本去重：通过唯一ID和集合跟踪防止重复处理
*   内容去重：在添加新策略前检查相似性，避免冗余
*   质量剪枝：定期评估策略效果，移除低效或有害策略
*   大小控制：限制策略库规模，优先保留高质量策略
*   策略标记：通过标记机制追踪策略表现，为剪枝提供依据

这些机制共同确保了playbook的质量和效率，防止策略库变得臃肿和低效。

我们来看看源码中如何处理。

#### 2.2.1 Playbook的去重机制

*   使用字典\_bullets 存储策略条目，其中键是唯一的bullet ID。
*   通过\_generate\_id()方法确保每个新添加的bullet都有唯一的ID。
*   在\_make\_playbook\_excerpt函数中也有去重逻辑，防止同一bullet被多次引用。

    def _make_playbook_excerpt(playbook: Playbook, bullet_ids: Sequence[str]) -> str:
        lines: List[str] = []
        seen = set() # 跟踪已经处理过的bullet ID
        for bullet_id in bullet_ids:
            if bullet_id in seen: # 见过就跳过
                continue
            bullet = playbook.get_bullet(bullet_id)
            if bullet:
                seen.add(bullet_id) # 标志为已见
                lines.append(f"[{bullet.id}] {bullet.content}")
        return "\n".join(lines)
    

#### 2.2.2 质量策略和剪枝

在CURATOR\_V2\_PROMPT定义了明确的质量控制机制。

    ## 去重协议
    执行新增操作前：
    - 检索现有条目，排查是否存在相似策略
    - 若相似度达 70%：选择更新（UPDATE）而非新增（ADD）
    - 若针对同一问题但方法不同：新增时需添加区分说明
    
    ## 行动手册规模管理
    若行动手册中的策略数量超过 50 条：
    - 优先选择更新（UPDATE）而非新增（ADD）
    - 合并相似策略
    - 移除表现最差的条目
    - 注重质量而非数量
    

#### 2.2.3 策略标记和淘汰机制

Reflector会对策略进行标记，这是一种评价策略价值的方式。应用此函数之后，低效或有害的策略可能会在Curator阶段被移除。比如Curator 可以发出 REMOVE 操作来删除无效策略。

    markdown
    ### REMOVE Operations - Use when:
    - Strategy consistently causes errors
    - Duplicate or contradictory strategies exist
    - Strategy is too vague to be useful
    

0x03 Reflector阶段
----------------

### 3.1 原理

ACE 在DC基础上加入了Reflector组件，具体流程如下：

*   **Generator**：演员上台演戏（推理），即生成答案。
*   **Reflector**：导演观看表演过程，进行分析总结。即 分析生成结果并标记策略有效性。
*   **Curator**：把分析总结插入到剧本。即根据反思生成 DeltaBatch（包含多个 DeltaOperation）。
*   **Playbook**：应用这些 DeltaOperation 来更新自身内容

三个组件协同构成 ACE 框架的 “生成 - 反思 - 优化” 闭环，实现策略的自主学习与持续改进。

加入了Reflector阶段的主要原因是：补全学习循环（强调从失败中学习），增强诊断能力、提高学习质量和预防性学习，利用执行环境提供的反馈信息，不仅仅依赖最终答案的正确性。

*   接收Generator的输出和环境反馈
*   构造一个详细的提示，要求LLM分析Generator的表现。
*   解析LLM的响应，提取关于策略有效性的分析结果。
*   将分析结果结构化为ReflectorOutput对象
*   支持多轮精炼和错误恢复机制以提高可靠性。Reflector通过分析Generator的推理过程和最终答案，识别错误并提供改进建议，同时对使用的策略条目进行标记，为后续的策略更新提供依据。

### 3.2 源码解读

#### 3.2.1 辅助数据类与函数

*   `GeneratorOutput`/`ReflectorOutput`/`CuratorOutput`：分别封装三个核心角色的输出数据，结构化整合推理过程、结果和原始响应，便于组件间数据传递。
*   `_make_playbook_excerpt`：提取行动手册中与当前生成过程相关的条目，精简反思分析的上下文，提升效率。

    @dataclass
    class GeneratorOutput:
        reasoning: str  # 生成答案过程中的推理过程
        final_answer: str  # 最终生成的答案
        bullet_ids: List[str]  # 生成过程中使用的行动手册条目标识ID列表
        raw: Dict[str, Any]  # 原始输出数据（未解析的完整响应）
            
    @dataclass
    class BulletTag:
        id: str  # 条目标识ID
        tag: str  # 标记类型（如"helpful"、"harmful"等）     
            
    @dataclass
    class ReflectorOutput:
        reasoning: str  # 反思过程的推理说明
        error_identification: str  # 错误识别结果（指出具体错误）
        root_cause_analysis: str  # 根本原因分析（错误产生的深层原因）
        correct_approach: str  # 正确的解决方法
        key_insight: str  # 关键洞察（从成功或失败中提取的经验）
        bullet_tags: List[BulletTag]  # 条目标记列表（标记策略有效性）
        raw: Dict[str, Any]  # 原始输出数据（未解析的完整响应）
            
    @dataclass
    class CuratorOutput:
        delta: DeltaBatch  # 批量增量操作（用于更新行动手册）
        raw: Dict[str, Any]  # 原始输出数据（未解析的完整响应）
    

#### 3.2.2 Generator

生成器是ACE框架的三大核心角色之一。它接收问题，并利用行动手册中积累的策略生成有推理过程的答案。

*   **作用**：作为 ACE 框架的核心角色之一，利用行动手册中的策略生成带推理过程的答案，同时记录生成过程中使用的策略条目 ID。
*   **特色**：通过提示词模板整合行动手册、问题上下文和历史反思，引导大语言模型生成结构化输出；支持 JSON 解析失败时的自动重试，增强鲁棒性。

Generator 的代码如下。

    class Generator:
        """
        Produces answers using the current playbook of strategies.
        利用当前行动手册中的策略生成答案。
    
        The Generator is one of three core ACE roles. It takes a question and
        uses the accumulated strategies in the playbook to produce reasoned answers.
    
        Args:
            llm: The LLM client to use for generation
                 用于生成答案的大语言模型客户端
            prompt_template: Custom prompt template (uses GENERATOR_PROMPT by default)
                 自定义提示词模板（默认使用GENERATOR_PROMPT）
            max_retries: Maximum attempts if JSON parsing fails (default: 3)
                 JSON解析失败时的最大重试次数（默认：3）
    
        Example:
            >>> from ace import Generator, LiteLLMClient, Playbook
            >>> client = LiteLLMClient(model="gpt-3.5-turbo")
            >>> generator = Generator(client)
            >>> playbook = Playbook()
            >>>
            >>> output = generator.generate(
            ...     question="What is the capital of France?",
            ...     context="Answer concisely",
            ...     playbook=playbook
            ... )
            >>> print(output.final_answer)
            Paris
    
        Custom Prompt Example:
            >>> custom_prompt = '''
            ... Use this playbook: {playbook}
            ... Question: {question}
            ... Context: {context}
            ... Reflection: {reflection}
            ... Return JSON with: reasoning, bullet_ids, final_answer
            ... '''
            >>> generator = Generator(client, prompt_template=custom_prompt)
        """
    
        def __init__(
            self,
            llm: LLMClient,
            prompt_template: str = GENERATOR_PROMPT,
            *,
            max_retries: int = 3,
        ) -> None:
            self.llm = llm  # 大语言模型客户端
            self.prompt_template = prompt_template  # 提示词模板
            self.max_retries = max_retries  # 最大重试次数
    
        def generate(
            self,
            *,
            question: str,
            context: Optional[str],
            playbook: Playbook,
            reflection: Optional[str] = None,
            **kwargs: Any,
        ) -> GeneratorOutput:
            """
            Generate an answer using the playbook strategies.
            利用行动手册中的策略生成答案。
    
            Args:
                question: The question to answer
                          待回答的问题
                context: Additional context or requirements
                          额外的上下文或要求
                playbook: The current playbook of strategies
                          当前的策略行动手册
                reflection: Optional reflection from previous attempts
                          可选的、来自之前尝试的反思信息
                **kwargs: Additional arguments passed to the LLM
                          传递给大语言模型的额外参数
    
            Returns:
                GeneratorOutput with reasoning, final_answer, and bullet_ids used
                包含推理过程、最终答案及所使用条目标识的GeneratorOutput对象
            """
            # 格式化基础提示词，填充行动手册、反思、问题和上下文信息
            base_prompt = self.prompt_template.format(
                playbook=playbook.as_prompt() or "(empty playbook)",
                reflection=_format_optional(reflection),
                question=question,
                context=_format_optional(context),
            )
            prompt = base_prompt
            last_error: Optional[Exception] = None
            # 尝试生成并解析结果，最多重试max_retries次
            for attempt in range(self.max_retries):
                response = self.llm.complete(prompt,** kwargs)
                try:
                    data = _safe_json_loads(response.text)
                    reasoning = str(data.get("reasoning", ""))
                    final_answer = str(data.get("final_answer", ""))
                    bullet_ids = [
                        str(item)
                        for item in data.get("bullet_ids", [])
                        if isinstance(item, (str, int))
                    ]
                    return GeneratorOutput(
                        reasoning=reasoning,
                        final_answer=final_answer,
                        bullet_ids=bullet_ids,
                        raw=data,
                    )
                except ValueError as err:
                    last_error = err
                    if attempt + 1 >= self.max_retries:
                        break
                    # 若解析失败，补充提示词要求输出有效JSON
                    prompt = (
                        base_prompt + "\n\n务必仅输出单个有效 JSON 对象，"
                        "请转义所有引号或改用单引号，避免输出额外文本。"
                    )
            # 多次重试失败后抛出异常
            raise RuntimeError("Generator failed to produce valid JSON.") from last_error
    

#### 3.2.3 Reflector

反思器是ACE框架的第二个核心角色。它分析生成器的输出和环境反馈，以理解成功或失败的原因，并将行动手册中的条目分类标记为有帮助、有害或中性。

*   **作用**：分析生成器的输出结果、环境反馈和真实答案，识别错误、诊断根源、提炼正确方法，并对行动手册中策略的有效性进行标记（有帮助 / 有害 / 中性）。
    *   分析生成器输出。
        *   分析 Generator 的输出结果和推理过程
        *   结合环境反馈来理解哪些策略有效，哪些无效。
    *   错误诊断与根因分析。
        *   识别生成器输出中的具体错误
        *   进行深层次的根本原因分析，而不仅仅是表面症状
        *   提供正确的解决方法
    *   策略评估与标记。对 Playbook 中使用的策略进行分类标记：
        *   helpful (有帮助的)：直接导致正确答案的策略
        *   harmful (有害的)：导致错误答案或产生混淆的策略
        *   neutral (中性的)：被引用但不是决定性的策略提取关键洞察
    *   提取关键洞察
        *   从成功或失败中提取可重复利用的学习要点
        *   为未来类似问题提供关键见解
        *   支持持续改进
*   **特色**：聚焦生成过程中实际使用的策略条目（通过生成器记录的 bullet\_ids），避免无关信息干扰；支持多轮优化反思结果，优先输出含可行动标记或关键洞察的内容。

通过 reflect 方法生成的分析结果会被传递给 Curator 角色，用于更新和优化 Playbook简单来说，Reflector 扮演着 "反思者" 的角色，通过对生成结果的深入分析来帮助系统学习和改进。

    class Reflector:
        """
        Analyzes generator outputs to extract lessons and improve strategies.
        分析生成器的输出以提取经验并改进策略。
    
        The Reflector is the second ACE role. It analyzes the Generator's output
        and environment feedback to understand what went right or wrong, classifying
        which playbook bullets were helpful, harmful, or neutral.
    
        Args:
            llm: The LLM client to use for reflection
                 用于反思分析的大语言模型客户端
            prompt_template: Custom prompt template (uses REFLECTOR_PROMPT by default)
                 自定义提示词模板（默认使用REFLECTOR_PROMPT）
    
        Example:
            >>> from ace import Reflector, LiteLLMClient
            >>> client = LiteLLMClient(model="gpt-3.5-turbo")
            >>> reflector = Reflector(client)
            >>>
            >>> reflection = reflector.reflect(
            ...     question="What is 2+2?",
            ...     context="Show your work",
            ...     generator_trajectory="Reasoning: 2+2 = 4",
            ...     final_answer="4",
            ...     execution_feedback="Correct!",
            ...     playbook=playbook
            ... )
            >>> print(reflection.diagnosis)
            Successfully solved the arithmetic problem
        """
    
        def __init__(
            self,
            llm: LLMClient,
            prompt_template: str = REFLECTOR_PROMPT,
            *,
            max_retries: int = 3,
        ) -> None:
            self.llm = llm  # 大语言模型客户端
            self.prompt_template = prompt_template  # 提示词模板
            self.max_retries = max_retries  # 最大重试次数
    
        def reflect(
            self,
            *,
            question: str,
            generator_output: GeneratorOutput,
            playbook: Playbook,
            ground_truth: Optional[str],
            feedback: Optional[str],
            max_refinement_rounds: int = 1,
            **kwargs: Any,
        ) -> ReflectorOutput:
            # 生成行动手册摘要（仅包含生成器使用过的条目）
            playbook_excerpt = _make_playbook_excerpt(playbook, generator_output.bullet_ids)
            # 格式化基础提示词，填充问题、生成结果、真实答案等信息
            base_prompt = self.prompt_template.format(
                question=question,
                reasoning=generator_output.reasoning,
                prediction=generator_output.final_answer,
                ground_truth=_format_optional(ground_truth),
                feedback=_format_optional(feedback),
                playbook_excerpt=playbook_excerpt or "(no bullets referenced)",
            )
            result: Optional[ReflectorOutput] = None
            prompt = base_prompt
            last_error: Optional[Exception] = None
            # 最多进行max_refinement_rounds轮优化
            for round_idx in range(max_refinement_rounds):
                prompt = base_prompt
                for attempt in range(self.max_retries):
                    response = self.llm.complete(
                        prompt, refinement_round=round_idx,** kwargs
                    )
                    try:
                        data = _safe_json_loads(response.text)
                        bullet_tags: List[BulletTag] = []
                        tags_payload = data.get("bullet_tags", [])
                        if isinstance(tags_payload, Sequence):
                            for item in tags_payload:
                                if (
                                    isinstance(item, dict)
                                    and "id" in item
                                    and "tag" in item
                                ):
                                    bullet_tags.append(
                                        BulletTag(
                                            id=str(item["id"]), tag=str(item["tag"]).lower()
                                        )
                                    )
                        candidate = ReflectorOutput(
                            reasoning=str(data.get("reasoning", "")),
                            error_identification=str(data.get("error_identification", "")),
                            root_cause_analysis=str(data.get("root_cause_analysis", "")),
                            correct_approach=str(data.get("correct_approach", "")),
                            key_insight=str(data.get("key_insight", "")),
                            bullet_tags=bullet_tags,
                            raw=data,
                        )
                        result = candidate
                        # 若已标记或关键洞察，提前退出
                        if bullet_tags or candidate.key_insight:
                            return candidate
                        break
                    except ValueError as err:
                        last_error = err
                        if attempt + 1 >= self.max_retries:
                            break
                        # 若解析失败，补充提示词要求输出有效JSON
                        prompt = (
                            base_prompt + "\n\n请严格输出有效 JSON，对双引号进行转义，"
                            "不要输出额外解释性文本。"
                        )
            if result is None:
                raise RuntimeError("Reflector failed to produce a result.") from last_error
            return result
    

#### 3.2.4 Curator

Curator是ACE框架的第三个核心角色。它分析反思器的输出，决定如何更新行动手册——包括添加新策略、更新现有策略或移除有害模式。

*   **作用**：基于反思器的分析结果，生成具体的行动手册更新操作（ADD/UPDATE/TAG/REMOVE），实现策略的迭代优化。
*   **特色**：严格遵循预设的更新协议，确保行动手册的结构化演进；通过批量增量操作（DeltaBatch）高效管理策略变更，兼顾灵活性与可追溯性。

整个决策过程是基于Reflector提供的反馈和当前Playbook状态，通过LLM分析来确定哪些更新能提高系统性能。

##### 3.2.4.1 判断依据

Curator 决定生成哪些 DeltaOperation 主要依据以下几个方面的信息。

###### Reflector 的分析结果

Curator 主要依据 ReflectorOutput 来决定生成哪些操作，包括：

*   reasoning：Reflector 对整个过程的分析
*   error\_identification：识别出的具体错误
*   root\_cause\_analysis：错误的根本原因分析
*   correct\_approach：正确的解决方法
*   key\_insight：关键洞察
*   bullet\_tags：对各个策略条目的标记（helpful/harmful/neutral）

###### 当前Playbook状态

Curator会分析当前的Playbook状态，包括：

*   现有策略条目及其内容
*   各条目的helpful/harmful/neutral计数
*   Playbook的整体统计数据（通过playbook.stats()获取）

###### 问题上下文信息

上下文信息包括：

*   question\_context：问题的领域或类型描述
*   progress：训练进度信息（如“5/10 problems solved correctly”）

##### 3.2.4.2 决策逻辑

根据CURATOR\_PROMPT中的指示，Curator按优先级顺序决策：

*   优先级 1：关键错误模式（CRITICAL\_ERROR\_PATTERN）

若反思发现影响多个问题的系统性错误：→ 新增高优先级纠正策略→ 为已存在的有害模式添加标记→ 更新相关策略以提升清晰度

*   优先级 2：缺失能力（MISSING\_CAPABILITY）

若反思发现存在必要但缺失的策略：→ 新增含清晰示例的策略→ 确保策略具备针对性与可执行性

*   优先级 3：策略优化（STRATEGY\_REFINEMENT）

若现有策略需要改进：→ 通过更优的解释或示例进行更新→ 保留有价值的核心内容，同时修正问题

*   优先级 4：矛盾解决（CONTRADICTION\_RESOLUTION）

若策略间存在冲突：→ 移除或更新冲突策略→ 必要时新增用于澄清的元策略

*   优先级 5：成功强化（SUCCESS\_REINFORCEMENT）

若某策略被证明效果显著：→ 标记为 “有帮助” 并提高权重→ 考虑为边缘场景创建该策略的变体

##### 3.2.4.3 生成策略

当Reflector分析完Generator的输出并生成反思后，Curator会根据反思结果生成一系列更新操作  
这些更新操作就是DeltaOperation对象，表示对playbook的修改。

具体操作类型如下：

*   ADD：添加新的策略条目
*   UPDATE：更新现有策略条目TAG：更新策略条目的标签计数（如 helpful，harmful，neutral）
*   REMOVE：删除策略条目

##### 3.2.4.4 Playbook应用更新

Playbook的apply\_delta方法会接收一个DeltaBatch，其中包含多个DeltaOperation。然后Playbook会遍历这些操作并应用到自身状态中

##### 3.2.4.5 代码

    class Curator:
        """
        Transforms reflections into actionable playbook updates.
        将反思结果转换为可执行的行动手册更新操作。
    
        The Curator is the third ACE role. It analyzes the Reflector's output
        and decides how to update the playbook - adding new strategies, updating
        existing ones, or removing harmful patterns.
    
        Args:
            llm: The LLM client to use for curation
                 用于管理操作的大语言模型客户端
            prompt_template: Custom prompt template (uses CURATOR_PROMPT by default)
                 自定义提示词模板（默认使用CURATOR_PROMPT）
            max_retries: Maximum attempts if JSON parsing fails (default: 3)
                 JSON解析失败时的最大重试次数（默认：3）
    
        Example:
            >>> from ace import Curator, LiteLLMClient
            >>> client = LiteLLMClient(model="gpt-4")
            >>> curator = Curator(client)
            >>>
            >>> # Process reflection to get delta updates
            >>> output = curator.curate(
            ...     reflection=reflection_output,
            ...     playbook=playbook,
            ...     question_context="Math problem solving",
            ...     progress="5/10 problems solved correctly"
            ... )
            >>> # Apply the delta to update playbook
            >>> playbook.apply_delta(output.delta)
    
        Custom Prompt Example:
            >>> custom_prompt = '''
            ... Progress: {progress}
            ... Stats: {stats}
            ... Reflection: {reflection}
            ... Playbook: {playbook}
            ... Context: {question_context}
            ... Decide what changes to make. Return JSON with delta operations.
            ... '''
            >>> curator = Curator(client, prompt_template=custom_prompt)
    
        The Curator emits DeltaOperations:
            - ADD: Add new strategy bullets
                   添加新的策略条目
            - UPDATE: Modify existing bullets
                   修改现有条目
            - TAG: Update helpful/harmful counts
                   更新有帮助/有害的计数
            - REMOVE: Delete unhelpful bullets
                   删除无用条目
        """
    
        def __init__(
            self,
            llm: LLMClient,
            prompt_template: str = CURATOR_PROMPT,
            *,
            max_retries: int = 3,
        ) -> None:
            self.llm = llm  # 大语言模型客户端
            self.prompt_template = prompt_template  # 提示词模板
            self.max_retries = max_retries  # 最大重试次数
    
        def curate(
            self,
            *,
            reflection: ReflectorOutput,
            playbook: Playbook,
            question_context: str,
            progress: str,
            **kwargs: Any,
        ) -> CuratorOutput:
            """
            Generate delta operations to update the playbook based on reflection.
            根据反思结果生成用于更新行动手册的增量操作。
    
            Args:
                reflection: The Reflector's analysis of what went right/wrong
                            反思器对成功/失败原因的分析结果
                playbook: Current playbook to potentially update
                            可能需要更新的当前行动手册
                question_context: Description of the task domain or question type
                            任务领域或问题类型的描述
                progress: Current progress summary (e.g., "5/10 correct")
                            当前进度摘要（如"5/10正确"）
                **kwargs: Additional arguments passed to the LLM
                            传递给大语言模型的额外参数
    
            Returns:
                CuratorOutput containing the delta operations to apply
                包含待应用的增量操作的CuratorOutput对象
    
            Raises:
                RuntimeError: If unable to produce valid JSON after max_retries
                             若达到最大重试次数仍无法生成有效JSON，则抛出异常
            """
            # 格式化基础提示词，填充进度、统计数据、反思结果等信息
            base_prompt = self.prompt_template.format(
                progress=progress,
                stats=json.dumps(playbook.stats()),
                reflection=json.dumps(reflection.raw, ensure_ascii=False, indent=2),
                playbook=playbook.as_prompt() or "(empty playbook)",
                question_context=question_context,
            )
            prompt = base_prompt
            last_error: Optional[Exception] = None
            # 尝试生成并解析结果，最多重试max_retries次
            for attempt in range(self.max_retries):
                response = self.llm.complete(prompt,** kwargs)
                try:
                    data = _safe_json_loads(response.text)
                    delta = DeltaBatch.from_json(data)
                    return CuratorOutput(delta=delta, raw=data)
                except ValueError as err:
                    last_error = err
                    if attempt + 1 >= self.max_retries:
                        break
                    # 若解析失败，补充提示词要求输出有效JSON
                    prompt = (
                        base_prompt
                        + "\n\n提醒：仅输出有效 JSON，所有字符串请转义双引号或改用单引号，"
                        "不要添加额外文本。"
                    )
            # 多次重试失败后抛出异常
            raise RuntimeError("Curator failed to produce valid JSON.") from last_error
    
    
    def _make_playbook_excerpt(playbook: Playbook, bullet_ids: Sequence[str]) -> str:
        """生成行动手册摘要（仅包含指定ID的条目）
        Args:
            playbook: 行动手册实例
            bullet_ids: 需要包含在摘要中的条目标识列表
        Returns:
            格式化的条目摘要字符串
        """
        lines: List[str] = []
        seen = set()  # 用于避免重复条目
        for bullet_id in bullet_ids:
            if bullet_id in seen:
                continue
            bullet = playbook.get_bullet(bullet_id)
            if bullet:
                seen.add(bullet_id)
                lines.append(f"[{bullet.id}] {bullet.content}")
        return "\n".join(lines)