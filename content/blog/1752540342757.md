---
layout: post
title: 'Skill Discovery | DoDont：使用 do + don't 示例视频，引导 agent 学习人类期望的 skill'
date: "2025-07-15T00:45:42Z"
---
Skill Discovery | DoDont：使用 do + don't 示例视频，引导 agent 学习人类期望的 skill
==================================================================

dodont 将好坏行为的分类器 p hat 融入了 metra 框架里，因此看起来很有直觉。

  

*   论文标题：Do's and Don'ts: Learning Desirable Skills with Instruction Videos
*   NeurIPS 2024 poster。
*   arxiv：[https://arxiv.org/abs/2406.00324](https://arxiv.org/abs/2406.00324)
*   pdf：[https://arxiv.org/pdf/2406.00324](https://arxiv.org/pdf/2406.00324)
*   html：[https://arxiv.org/html/2406.00324](https://arxiv.org/html/2406.00324)
*   website：[https://mynsng.github.io/dodont/](https://mynsng.github.io/dodont/)
*   open review：[https://openreview.net/forum?id=7X5zu6GIuW](https://openreview.net/forum?id=7X5zu6GIuW)

主要内容：

*   这篇文章关注 skill discovery，希望通过好视频和坏视频，引导 agent 学到我们希望它探索的 skill。提出了 DoDont 方法。
*   动机：纯粹无监督学习在现实场景不安全且低效。用视频表达人类意图：低成本（无需动作标签）、易扩展（8个视频即可）。
*   主要方法是 metra（[本站博客](https://www.cnblogs.com/moonout/p/18980763)）魔改。
*   具体的，4.1 节训了一个分类器 \\(\\hat p\_\\psi(s\_t, s\_{t+1})\\)，用来判别 transition \\((s\_t, s\_{t+1})\\) 是否是人类喜欢的，直接用 cross-entropy loss function。
*   然后，在 4.2 节，dodont 把 metra 的 temporal distance 直接魔改成 \\(\\|\\phi(s)-\\phi(s')\\| \\le \\hat p\_\\psi(s, s')\\) ，虽然这样看起来没什么道理，但可以借用 metra 的形式，得到这个形式：

\\\[\\sup\_{\\pi, \\phi} \\quad \\mathbb{E}\_{p(\\tau, z)} \\left\[ \\sum\_{t=0}^{T-1} \\hat p\_\\psi(s\_t, s\_{t+1})\[\\phi(s\_{t+1}) - \\phi(s\_t)\]^\\top z \\right\] \\quad \\text{s.t.} \\quad \\|\\phi(s) - \\phi(s')\\|\_2 \\leq 1 \\tag{1} \\\]

*   这个形式只需要魔改 metra，在原先的 reward 上乘一个 \\(\\hat p\_\\psi(s\_t, s\_{t+1})\\) ，实现起来非常方便。

* * *

目录

*   [1 dodont 故事（纯故事，method 很简单）](#1-dodont-故事纯故事method-很简单)
    *   [1.1 motivation：故事的核心 - 现有方法的 Gap 与解决方案](#11-motivation故事的核心---现有方法的-gap-与解决方案)
    *   [1.2 method 的合理性与 Intuition (方法包装)](#12-method-的合理性与-intuition-方法包装)
    *   [1.3 总结故事线](#13-总结故事线)
*   [2 dodont 的方法](#2-dodont-的方法)
*   [3 dodont 的实验](#3-dodont-的实验)
    *   [3.1 实验 setting](#31-实验-setting)
    *   [3.2 baseline](#32-baseline)
    *   [3.3 评价指标](#33-评价指标)
    *   [3.4 实验结果](#34-实验结果)
    *   [3.5 更多细节](#35-更多细节)

* * *

1 dodont 故事（纯故事，method 很简单）
---------------------------

这篇论文讲了一个关于让 AI 智能体 **更安全、更高效地自学技能** 的故事。核心是解决无监督技能发现（unsupervised skill discovery）中的两个大问题：学不会复杂技能、学会危险 / 无用技能。

### 1.1 motivation：故事的核心 - 现有方法的 Gap 与解决方案

*   Gap 1: 学不会复杂技能。现有的 skill discovery 方法，如最大化互信息或状态距离，在简单环境还能 work，但在复杂环境 如多关节四足机器人跑步中效率低下，可能根本学不会目标技能，如跑步。
*   Gap 2: 学会危险 / 无用技能。更严重的是，纯无监督探索没有约束，智能体很容易学会危险或无效的行为，比如：
    *   物理危险：摔倒、翻滚（可能损坏机器人）。
    *   导航危险：掉进坑里、进入禁区。
    *   无效行为：停留在原地做无意义动作。
*   如何解决，Intuition：人类学习不仅靠自我探索，即内部动机，也需要一些观察示范和避免错误，即外部动机。dodont 想将这种外部动机，具体的，该做什么（Do’s）和 不该做什么（Don’ts）的指导融入 skill discovery 。
*   关键创新点（Motivation 的落脚点）：与其费时费力地手工设计复杂的奖励函数，难以平衡多个目标且不通用，dodont 提出用 **低成本**、**无动作标签** 的 **教学视频** (Instruction Videos) 来传达人类的意图。只需要少量（<8个）展示好行为 do 和坏行为 don't 的视频。

### 1.2 method 的合理性与 Intuition (方法包装)

*   核心思想： 用视频训练一个“教学网络”（Instruction Network）来区分行为的好坏，然后将这个网络的输出 **融入** 到原有的 skill discovery 优化目标中，引导智能体学 do，避开 don't。
*   方法流程与合理性：
*   1 教学阶段 (Offline)：
    *   收集少量 Do’s 视频 期望行为，如跑步，和 Don’ts 视频 禁止行为，如翻滚、掉坑。
    *   训练一个二元分类网络 \\(\\hat p\_\\psi(s\_t, s\_{t+1})\\)，这个网络学会给“好”的状态转移 \\((s\_t, s\_{t+1})\\) 打高分（接近1），给“坏”的转移打低分（接近0）。根据 dodont 的故事，它本质上是一个 **基于人类意图的距离 / 价值函数**。
*   2 技能学习阶段 (Online)：
    *   首先，选择一个强大的 skill discovery 基础算法，比如距离最大化的 skill discovery（如 METRA [本站博客](https://www.cnblogs.com/moonout/p/18980763)）。这类方法的核心，是基于一个 embedding，最大化一个距离函数 \\(d(s, s')\\)，鼓励智能体产生不同的、能到达远处状态的行为轨迹。
    *   关键融合步骤：将训练好的教学网络 \\(\\hat p\_\\psi(s\_t, s\_{t+1})\\) 直接作为 skill discovery 算法中的距离函数 \\(d(s, s')\\)。
    *   Intuition 1 (合理性)：在距离最大化框架下，\\(d(s, s')\\) 越大，智能体越被鼓励执行从 s 到 s' 的转移。
        *   直观上，\\(\\hat p\_\\psi(s\_t, s\_{t+1})\\) 对“好”转移打高分，意味着智能体会被 强烈鼓励 去执行这些转移（相当于放大了好行为的“吸引力”）。
        *   反之，对“坏”转移打低分（甚至接近 0），意味着这些转移几乎没有“距离收益”，智能体自然没有动力去学习它们（相当于抑制了坏行为的“吸引力”）。
    *   Intuition 2 (有效性)：dodont 发现，对比加法奖励和直接优化 metra 距离乘 \\(\\hat p\_\\psi(s\_t, s\_{t+1})\\) 的形式，直接将 \\(\\hat p\_\\psi(s\_t, s\_{t+1})\\) 乘到 skill discovery 算法的内在奖励上效果最好。

### 1.3 总结故事线

1.  问题：纯无监督技能发现（skill discovery）在复杂环境中效率低，且会学危险 / 无用技能。
2.  intuition：模仿人类学习，引入“该做(Do’s)”和“不该做(Don’ts)”的外部指导。
3.  应用 intuition 的方案：用 **极少量无标签视频** 训练一个 **教学网络** 来量化行为好坏。
4.  与 metra 的融合：将教学网络 **无缝嵌入** 强大的 skill discovery 算法（METRA）中，**直接替换其核心的距离函数**，让算法在最大化距离/多样性的同时，**天然地** 被引导向好行为、远离坏行为。
5.  结果：实验证明，该方法能高效学习复杂技能（如跑步），同时有效避免危险行为，且行为多样性保持良好。个人感觉，核心在于分类器网络跟 metra 形式的融合良好，因此，视频信息直接、精准地重塑了 skill discovery 的探索方向。

个人思考，感觉这篇文章的故事 好在两个地方：1. 使用了 metra 的理论框架，理论支持看起来很强；2. 范式比较新，听起来有意义，感觉一个 novel 且有意义的范式是很重要的。

2 dodont 的方法
------------

首先，metra（[本站博客](https://www.cnblogs.com/moonout/p/18980763)）的目标函数能写成以下形式：

\\\[\\sup\_{\\pi, \\phi} \\quad \\mathbb{E}\_{p(\\tau, z)} \\left\[ \\sum\_{t=0}^{T-1} (\\phi(s\_{t+1}) - \\phi(s\_t))^\\top z \\right\] \\quad \\text{s.t.} \\quad \\|\\phi(s) - \\phi(s')\\|\_2 \\leq 1 \\tag{2} \\\]

从而，可以得到以下三个 loss function / policy 的 intrinsic reward：

*   训练 state embedding \\(\\phi(s)\\) ：\\(\\max \\mathbb{E}\_{(s,z,s') \\sim \\mathcal{D}} \[(\\phi(s') - \\phi(s))^\\top z + \\lambda \\cdot \\min(\\varepsilon, 1 - \\|\\phi(s) - \\phi(s')\\|\_2^2)\]\\) ；
*   更新 Lagrange 乘子 \\(\\lambda\\) ：\\(\\min \\mathbb{E}\_{(s,z,s') \\sim \\mathcal{D}} \[\\lambda \\cdot \\min(\\varepsilon, 1 - \\|\\phi(s) - \\phi(s')\\|\_2^2)\]\\) ；
*   更新 policy \\(\\pi(a|s,z)\\) ：reward \\(r(s, z, s') = (\\phi(s') - \\phi(s))^\\top z\\)，使用 SAC 算法。

dodont 直接把 (2) 式变成这样：

\\\[\\sup\_{\\pi, \\phi} \\quad \\mathbb{E}\_{p(\\tau, z)} \\left\[ \\sum\_{t=0}^{T-1} (\\phi(s\_{t+1}) - \\phi(s\_t))^\\top z \\right\] \\quad \\text{s.t.} \\quad \\|\\phi(s) - \\phi(s')\\|\_2 \\leq \\hat p\_\\psi(s,s') \\tag{3} \\\]

然后，通过假设新 \\(\\tilde\\phi(s\_t) = \\phi(s\_t) / \\hat p\_\\psi(s\_t, s\_{t+1})\\) ，发现 (3) 式可以变成这样：

\\\[\\sup\_{\\pi, \\phi} \\quad \\mathbb{E}\_{p(\\tau, z)} \\left\[ \\sum\_{t=0}^{T-1} \\hat p\_\\psi(s\_t, s\_{t+1})\[\\phi(s\_{t+1}) - \\phi(s\_t)\]^\\top z \\right\] \\quad \\text{s.t.} \\quad \\|\\phi(s) - \\phi(s')\\|\_2 \\leq 1 \\tag{1} \\\]

（或许，应该写成 \\(\\tilde\\phi(s\_t) = \\phi(s\_t) / \\mathbb E\_{s\_{t+1}\\sim \[\\pi,~p\_\\text{env}\]}~\\hat p\_\\psi(s\_t, s\_{t+1})\\) 的形式）

最后，metra 的三个 oss function / policy 的 intrinsic reward 目标，可以被改成这样：

*   （不变）训练 state embedding \\(\\phi(s)\\) ：\\(\\max \\mathbb{E}\_{(s,z,s') \\sim \\mathcal{D}} \[(\\phi(s') - \\phi(s))^\\top z + \\lambda \\cdot \\min(\\varepsilon, 1 - \\|\\phi(s) - \\phi(s')\\|\_2^2)\]\\) ；
*   （不变）更新 Lagrange 乘子 \\(\\lambda\\) ：\\(\\min \\mathbb{E}\_{(s,z,s') \\sim \\mathcal{D}} \[\\lambda \\cdot \\min(\\varepsilon, 1 - \\|\\phi(s) - \\phi(s')\\|\_2^2)\]\\) ；
*   更新 policy \\(\\pi(a|s,z)\\) ：reward \\(r(s, z, s') = \\hat p\_\\psi(s\_t, s\_{t+1})\\cdot(\\phi(s') - \\phi(s))^\\top z\\)，使用 SAC 算法。

只需得到 \\(\\hat p\_\\psi(s\_t, s\_{t+1})\\) 之后，简单改改 metra 代码即可。

3 dodont 的实验
------------

感觉整个实验设置，也是完全为故事服务的…

### 3.1 实验 setting

*   环境：
    *   Cheetah / Quadruped（DeepMind Control Suite）：学习复杂运动，如奔跑、避障 。
    *   Kitchen：学习精细操作，如开微波炉、开关柜门 。
*   输入：状态向量或像素图像；输出：连续动作控制机器人。
*   目标：无预设奖励下学习**多样技能**，并**避免危险行为**（如跌倒、进入危险区）。
*   dodont 视频：8 段视频（4×Do's + 4×Don'ts）
    *   对于 5.2.1 节，do 视频是往四个方向跑的视频，而 dont 视频是随机行为，这样，相当于鼓励 agent 要跑，而不要（比如说）原地转圈 或者抬左脚抬右脚之类，所以性能超过 metra 是可以理解的。
    *   对于 5.2.2 节，do 视频是往右跑的视频，而 dont 视频是往左跑的。从而，dodont 可以训到只往右跑的 agent。
    *   对于 kitchen，do 视频直接采用 d4rl 里的全部视频，而 dont 视频采用 10 个随机行为。

### 3.2 baseline

*   METRA：skill discovery baseline，仅最大化技能多样性。
*   METRA+：METRA 变体，**人工设计奖励函数** 作为距离度量，例如 安全区 +1，危险区 0。
*   SMERL / DGPO：混合任务奖励和多样性奖励的方法，用指令网络代替任务奖励。
*   仅鼓励正面行为（Only Do's）：消融实验，不惩罚负面行为。

### 3.3 评价指标

*   状态覆盖率：衡量技能探索范围，如 Quadruped 的 x-y 平面覆盖。
*   零样本任务奖励：预训练技能直接执行任务的性能，如奔跑速度。
*   安全状态覆盖率：安全区域探索比例，危险区得分 -1，安全区 +1。
*   任务完成数（Kitchen）：成功完成 6 类操作的数量 。

### 3.4 实验结果

*   效率提升：DoDont 学习奔跑技能比 METRA 快 30%，覆盖率提高 25%。**仅需 8 段视频**（4 正 4 负），即可有效引导.
*   安全性： 危险区域规避率 >90%，METRA 仅 50%。翻滚等危险行为减少 70%。
*   多任务能力：Kitchen 任务完成数达 4.2 / 6，超过 METRA 的 3.1。
*   baseline 的局限性：
    *   SMERL / DGPO 因互信息目标失效，技能多样性差。
    *   METRA+ 在复杂环境奖励设计困难，如奔跑与翻滚奖励冲突。

### 3.5 更多细节

*   附录提到，We use the interquartile mean (IQM) for overall aggregation to assess performance，使用四分位距均值（IQM）\[ 2\] 进行整体聚合以评估性能，不知道是什么 trick。
*   看实验，总体感觉 dodont 相比 metra 和 metra+ 的性能，cheetah 不如 quadruped 的好。
*   dodont 和 metra 都有神秘的 offline 版本。metra offline 版本：[https://github.com/seohongpark/HILP](https://github.com/seohongpark/HILP)
*   训 classifier 的超参数：使用四个卷积层作为骨干网络，并使用三个全连接层作为预测头。为了提高稳定性，使用 Tanh 函数对输出进行约束，此外，因为输入是 pixel ，所以还采用了简单的随机平移增强。使用 Adam 优化器，学习率为 1e-4，batch 大小为 1024。
*   dodont 声称，在 5.2.1（agent 要跑）、5.2.3（agent 不能摔倒翻滚）和 5.3（ablation）节中，由于包含了行为意图，他们使用基于状态的环境，并使用非彩色的默认像素作为人类意图。看起来，在 5.2.2 节（agent 要往右跑），dodont 使用的是跟 metra 一样的彩色地板 + pixel-based observation。至少，训 classifier 都是用视频训的。
*   训练效率：单卡 RTX 3090 训练 < 28 小时。
*   dodont 声称，如果只有 Do's 视频，就学不了这么好了，听起来像对比学习。