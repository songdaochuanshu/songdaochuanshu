---
layout: post
title: '[深度学习] 大模型学习1-大语言模型基础知识'
date: "2025-01-01T04:57:06Z"
---
\[深度学习\] 大模型学习1-大语言模型基础知识
=========================

大语言模型（Large Language Model，LLM）是一类基于Transformer架构的深度学习模型，主要用于处理与自然语言相关的各种任务。简单来说，当用户输入文本时，模型会生成相应的回复或结果。它能够完成许多任务，如文本续写、分类、摘要、改写、翻译等。常见的LLM包括GPT、LLaMA等。本文将重点介绍LLM的基本原理和应用。详细内容可参考[modelscope-classroom](https://github.com/modelscope/modelscope-classroom)进行深入学习。

![https://github.com/onejune2018/Awesome-LLM-Eval](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img0.gif)

目录

*   [1 LLM基础知识](#1-llm基础知识)
    *   [1.1 LLM介绍](#11-llm介绍)
    *   [1.2 LLM训练范式](#12-llm训练范式)
    *   [1.3 Transformer结构解析](#13-transformer结构解析)
    *   [1.4 LLM扩展应用](#14-llm扩展应用)
*   [2 LLM训练概览](#2-llm训练概览)
    *   [2.1 LLM推理过程](#21-llm推理过程)
    *   [2.2 LLM应用构建](#22-llm应用构建)
        *   [2.2.1 提示词工程](#221-提示词工程)
        *   [2.2.2 模型训练与微调](#222-模型训练与微调)
        *   [2.2.3 RAG](#223-rag)
    *   [2.3 LLM评估](#23-llm评估)
        *   [2.3.1 LLM自动评估](#231-llm自动评估)
        *   [2.3.2 LLM人工评估](#232-llm人工评估)
        *   [2.3.3 LLM评估工具](#233-llm评估工具)
    *   [2.4 LLM量化、部署、优化](#24-llm量化部署优化)
        *   [2.4.1 模型量化](#241-模型量化)
        *   [2.4.2 模型推理部署](#242-模型推理部署)
        *   [2.4.3 模型优化技术](#243-模型优化技术)
*   [3 总结](#3-总结)
*   [4 参考](#4-参考)

1 LLM基础知识
=========

1.1 LLM介绍
---------

**LLM发展历程**

2022年11月30日，OpenAI推出的ChatGPT在LLM技术领域取得了创新突破，迅速引起了全球业界的广泛关注，并在短短两个月内成功吸引了超过一亿用户。作为一款基于LLM的应用，ChatGPT以其强大的文本生成、对话交互和信息提取能力，成为人工智能领域的一个重要里程碑，推动了人机交互的边界。然而，由于OpenAI未公开其底层技术并封闭源代码，这引发了全球AI开发者对开源技术的强烈需求。

![https://www.appeconomyinsights.com/p/threads-fastest-growing-app-in-history](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img1.jpg)

随着LLM技术的飞速发展，Meta推出的LLaMA模型、Mistral AI发布的Mistral模型以及BigScience团队推出的BLOOM模型等多个开源LLM相继问世。这些模型在性能上已接近甚至媲美商业化LLM，进一步推动了LLM技术的广泛应用与创新。以下是几款代表性LLM系列的发展时间线，展现了这一领域的迅猛进步：

![https://arxiv.org/pdf/2306.13549](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img2.jpg)

到2024年底，在众多LLM中，闭源模型中表现最为出色的是GPT-4，而在开源模型中，LLama 3.3和LLama 3.2最为推荐。尽管LLama 3.2在各类基准测试中优于GPT-4，但在实际应用中，GPT-4的表现仍然更为卓越：

![https://onegen.ai/llama-3-2-models-comparison-use-cases-and-fine-tuning/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img3.jpg)

**LLM的“大”体现在哪些方面？**

1.  庞大的参数量：LLM的“大”首先体现在参数数量上。例如，OpenAI的GPT-3有1750亿个参数，GPT-4更为庞大。参数越多，模型的语言理解和任务处理能力越强。
2.  海量的训练数据：LLM依赖海量数据进行训练，包括书籍、新闻、网页内容和社交媒体等。这些多样化的数据帮助模型掌握丰富的语言模式，具备强大的理解和生成能力。
3.  广泛的任务适应性：模型在多种数据上训练，赋予其从自然语言理解到翻译、摘要、情感分析等多任务的处理能力，使其具备显著的通用性。
4.  巨大的计算资源需求：LLM的训练与推理依赖大量高性能计算资源，如GPU和专用加速器。随着模型规模的增加，计算需求呈指数级增长。

**LLM为什么要基于Transformer架构？**

在Transformer架构出现之前，自然语言模型主要依赖循环神经网络（RNN），但RNN的顺序处理方式限制了计算的并行性，且在处理长序列时，信息容易丢失或遗忘。

Transformer通过引入自注意力机制和位置编码，克服了传统模型在捕捉长距离依赖和并行计算方面的局限。自注意力机制允许模型同时关注输入序列中的所有词，捕捉更远距离的依赖关系，避免了RNN及其变体LSTM模型中存在的顺序处理瓶颈。因此，Transformer成为大规模预训练模型的基础架构，并在多个任务中展现了出色的性能。

1.2 LLM训练范式
-----------

**LLM训练阶段**

LLM的训练可分为以下四个关键阶段：

1.  预训练（Unsupervised Pretraining）：构建基座模型。
    
    *   数据来源：广泛采集的书籍、新闻、科研论文、社交媒体等多领域文本数据，作为模型训练的素材。
    *   学习目标：利用无监督学习技术，使模型能够根据上下文预测下一个词。
    *   训练过程：不依赖标注数据，通过不断优化模型预测与实际结果之间的差异，随着数据量的增加，逐步提升模型的性能。
2.  有监督微调（Supervised Fine-Tuning，SFT）：打造对话模型。
    
    *   数据来源：采用人工标注的对话数据，以提高模型在对话任务中的表现。
    *   学习目标：通过有针对性的训练，增强模型与用户互动的能力。
    *   训练过程：使用少量但高质量的对话数据进行微调，显著提高模型的对话能力。
3.  奖励模型训练（Reward Model Training）：培养能够评估回答的模型。
    
    *   数据来源：生成多个候选答案，并依据人工评分和排序进行评估。
    *   学习目标：培养奖励模型，利用评分数据来评估和优化模型生成的答案质量。
    *   训练过程：奖励模型根据人工评分提供反馈，引导模型生成更符合人类预期的答案，这个过程也常被称为人类对齐训练（alignment）。
4.  强化学习训练（Reinforcement Learning with Human Feedback，RLHF）：进一步提升对话模型的回答质量。
    
    *   数据来源：利用第三步训练好的奖励模型，通过强化学习进一步优化第二步训练好的对话模型。
    *   学习目标：依据奖励模型的反馈调整生成策略，提高模型回答的质量。
    *   训练过程：模型根据奖励评分调整其策略，并结合用户反馈，进一步改进生成答案的质量。

这一过程可以用[Llama 2](https://arxiv.org/abs/2307.09288)论文中的示例图片更为直观地展示：

![https://arxiv.org/abs/2307.09288](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img4.jpg)

**基座模型和对话模型**

在LLM的训练过程中，主要有两种模型类型：基座模型（Base模型）和对话模型（Chat模型）。两者的工作方式相似，都是通过预测文本的后续内容来进行训练。Base模型经过了预训练，可能进行了部分通用指令的微调；而Chat模型则在Base模型的基础上，进一步通过大量通用数据的微调和人类反馈的训练来优化性能。虽然“指令模型”（Instruction Model）这一术语有时也被提及，但它与Chat模型本质上是相同的，都是通过指令微调和强化学习对Base模型进行优化，以提高其理解和生成能力。

![https://journals.sagepub.com/doi/full/10.1177/17562848241227031](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img5.jpg)

开源LLM的研究机构通常会同时发布Base模型和Chat模型，以满足不同的需求。例如，Llama 2（Base模型）和Llama 2-Chat（Chat模型）。Base模型主要侧重于泛化能力，通常仅经过预训练，具备基本的文本补全功能，但缺乏对话上下文的理解，因此在与人类对话时，回答往往显得较为生硬、感觉在背书。而Chat模型则在Base模型的基础上进行优化和调整，增强了对话能力和自然语言理解，生成的回答更加自然。两者具体对比如下：

维度

Base模型

Chat模型

训练方式

预训练

预训练+监督微调（SFT）+强化学习（RLHF）

数据来源

大量未标注文本数据

标注好的对话数据集、人类对齐

模型特性

庞大的参数规模，具备广泛的语言特征

强大的对话生成和理解能力，能够生成连贯且有意义的回复

应用场景

适用于多种NLP任务，如文本生成、语义理解、翻译等

专门用于构建聊天机器人、虚拟助理等对话系统

优势

泛化能力强，适用于多种任务

对话能力强，能够生成符合人类偏好的回复

不足

可能需要进一步的微调才能适应特定任务

相对于Base模型，训练过程更复杂

1.3 Transformer结构解析
-------------------

LLM通过预测下一个词生成高质量文本，这得益于强大的Transformer架构。Transformer的基本结构包括输入嵌入、编码器、解码器和输出层。编码器和解码器通过多层堆叠的自注意力机制和前馈神经网络相互作用，进而实现复杂的序列处理任务。自注意力机制使得Transformer能够在全局范围内捕捉词语之间的依赖关系，这是其处理长序列和大量数据的关键优势。

![https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img6.jpg)

**Transformer编码器**

Transformer的编码器部分主要包括以下几个步骤：

1.  输入token化：将输入文本拆分为可以被计算机处理的token，每个token对应词汇表中的一个索引。
2.  嵌入层：将这些token转换为向量表示，捕捉词汇之间的语法和语义关系。
3.  位置编码：给嵌入向量添加位置信息，帮助模型理解词在句子中的顺序。
4.  自注意力机制：这是编码器的核心，通过计算每个词与其他词之间的相关性，确定注意力权重，捕捉输入序列中的重要信息。
5.  多头自注意力：使用多个“头”来分别关注输入数据的不同方面，增强模型的表达能力。
6.  前馈神经网络：对多头自注意力的输出进行处理，帮助模型识别更复杂的模式。
7.  堆叠编码器：将多个编码器层叠加在一起，让模型能够在不同层次上理解输入数据，从而逐步提炼信息。

**Transformer解码器**

Transformer解码器的工作流程可以简化为以下几个步骤：

1.  输入处理：解码器接收上一个时间步的输出和编码器的输出。初始步骤时，输入通常是一个特殊的起始标记（如 `<sos>`）或先前生成的结果。
2.  嵌入和位置编码：解码器将每个输入词转化为向量，并加入位置编码，使模型知道词在序列中的位置。
3.  自注意力机制（带掩码）：解码器的自注意力机制只关注当前词之前的词，通过掩码确保模型生成下一个词时只依赖已生成的部分，从而实现自回归生成。
4.  编码器-解码器注意力：解码器通过这个机制关注编码器的输出，帮助理解输入序列，从而生成更合适的输出。
5.  前馈神经网络和解码器堆叠：解码器使用前馈神经网络对注意力输出进行处理，并通过多层堆叠提升对输入输出关系的理解，生成更复杂的结果。
6.  线性层和Softmax：解码器将输出转化为词汇表大小的向量，通过Softmax层生成词的概率分布。
7.  选择输出：模型选择概率最高的词作为下一个输出，直到生成结束标记（如 `<eos>`），完成整个序列的生成。

解码器中的多头自注意力机制与编码器类似，但采用遮蔽处理，以防模型关注未来词语。这样，模型在预测位置\\(i\\)的输出时，仅依赖于位置\\(i\\)之前的已知信息。如下图所示，解码器逐步生成每个单词：

![https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img7.jpg)

**LLM中的Transformer**

Transformer架构是一种非常灵活且强大的神经网络结构，已经被广泛应用于各种自然语言处理任务。根据不同的任务需求，Transformer的架构可以分为几种不同的类型，每种类型都有其特定的优点和用途。以下是这几种主要结构的特点：

1.  仅编码器模型（自编码器模型）
    
    *   主要用途： 仅编码器模型通常用于从输入数据中提取有用的特征信息，进行理解或表示学习。这些模型不需要生成输出，而是侧重于学习输入的上下文和表示。
    *   工作方式： 编码器的作用是将输入的序列通过多层的自注意力机制和前馈神经网络处理，最终将其转化为一个固定长度的向量或一组向量。
    *   应用： 这类模型在需要提取深度特征或做文本分类、情感分析等任务时非常有效。它们不涉及生成过程，而是通过理解和表示输入数据来完成任务。
    *   代表模型：
        *   BERT（双向编码器表示模型）： BERT是一种双向编码器，它通过“遮蔽”输入中的某些单词来训练模型，让模型预测这些被遮蔽的单词，从而获得输入文本的深层次理解。BERT的预训练后可以通过微调应用于多种任务，如文本分类、命名实体识别（NER）、问答等。
2.  仅解码器模型（自回归模型）
    
    *   主要用途： 解码器模型通常用于生成任务，尤其是序列生成任务，如文本生成、对话生成等。这类模型的目标是从给定的输入或上下文中生成连贯的输出。
    *   工作方式： 解码器采用自回归的方式生成序列，它会基于已生成的词或标记不断预测下一个词，直到生成完整的输出。
    *   应用： 解码器模型广泛应用于需要生成连续文本的任务，比如机器翻译、文本生成、代码生成等。
    *   代表模型：
        *   GPT：基于Transformer的解码器结构，采用自回归方式生成文本。在训练过程中，它通过大量的文本数据学习语言模式，并通过不断预测下一个词生成连贯的文章。GPT系列（如GPT-3、GPT-4）已经成为文本生成任务中的重要模型。
3.  编码器-解码器模型（序列到序列模型）
    
    *   主要用途： 编码器-解码器模型适用于需要将一个输入序列映射到一个输出序列的任务，例如机器翻译、文本摘要、图像描述等。这种结构通常包含两个部分：编码器负责理解输入序列，解码器负责生成输出序列。
    *   工作方式： 编码器首先处理输入序列，将其转化为一个中间的表示（通常是一个上下文向量），然后解码器基于该表示生成输出序列。解码器可以使用自回归方式来逐步生成输出。
    *   应用： 这类模型适用于任何需要将一个序列转换为另一个序列的任务，常见的应用场景包括机器翻译、摘要生成、对话生成等。
    *   代表模型：
        *   T5：将所有任务统一转换为文本到文本的任务，即输入和输出都是文本形式。它结合了编码器和解码器的结构，可以用于机器翻译、文本摘要、问答等多种任务。
        *   BART：一种结合了BERT和GPT优点的模型，使用编码器-解码器架构，既能够进行双向的理解，又能进行自回归的生成。它特别适用于文本生成、序列到序列的转换等任务。

![https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img8.jpg)

然而随着技术发展，Transformer架构不断优化，尤其是LLM（GPT系列）取得突破后，解码器模型变得更加流行，而编码器-解码器模型逐渐不再主流，原因如下：

1.  简洁高效：解码器模型结构简单，训练和推理时更加高效。
2.  更佳上下文理解：解码器专注于生成任务，能更好地理解上下文，生成连贯、符合语境的文本。

1.4 LLM扩展应用
-----------

为了应对日益复杂的任务需求，一些新型的大模型应运而生，它们对单一LLM的能力进行了扩展和补充。这些模型主要包括多模态大语言模型、LLM智能体（Agent）、垂直领域LLM等。以下是对这些模型的简要介绍：

1.  多模态大语言模型 (Multimodal Large Language Models)
    
    多模态大语言模型通过融合文本、图像、视频和音频等多种信息，能够同时处理不同类型的输入，生成更丰富的语义理解。与传统模型不同，它在多元数据训练下显著提升了对各类数据的理解能力，展现出更强的任务适应性和通用性。例如，在图像描述任务中，模型结合图像和文本生成精准自然的语言；在音频处理任务中，通过融合音频和文本信息，提高语音识别和语义理解的准确性。
    
    ![https://arxiv.org/abs/2306.09093](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img9.jpg)
    
2.  LLM智能体（Agent）
    
    LLM智能体（Agent）是基于LLM的人工智能系统，它能够理解、生成和处理语言，以执行各种任务。与传统的程序或工具不同，智能体不仅能够提供信息和答案，还能根据上下文进行自主推理、决策和行动。通过与用户的互动，它可以处理复杂的问题，提供个性化建议，并完成诸如对话、文本生成、翻译、问答等多种任务。智能体的核心是成熟的LLM，它通过大量的语料库学习语言的结构和含义，不断优化其理解和生成能力。
    
    ![https://blog.abacus.ai/blog/2023/08/31/supercharge-productivity-accomplish-10x-more-with-ai-agents/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img10.jpg)
    
3.  垂直领域LLM
    
    垂直领域LLM是指专门针对某一特定行业或应用场景进行训练和优化的LLM。这类模型与通用LLM不同，它们在处理特定领域的问题时表现出更强的专业性和精确性。垂直领域LLM通常会利用行业专有的数据和知识进行训练，从而提高在该领域内的表现。  
    例如，在代码生成上，它能根据需求自动生成高质量代码，减少开发时间与错误；在医学领域，它协助医生分析病历、解读检查结果，并提出诊断与治疗建议，提升诊疗效率；在法律咨询中，模型帮助解析法律条文、提供法律意见，让用户更清晰地理解法律问题和风险；在金融分析中，它能分析市场趋势、预测股市走向，为投资者提供数据支持；在客户服务方面，它能自动处理咨询、快速回应问题，提高服务质量；在技术支持中，它能识别技术问题并提供解决方案，保障用户体验。
    

2 LLM训练概览
=========

2.1 LLM推理过程
-----------

LLM推理是指在训练完成后，利用训练好的模型对新数据进行预测或生成的过程。通过LLM解决实际问题，如回答问题、生成文案等，便是在进行模型推理。可以认为，模型推理是LLM应用的核心环节。  
在推理阶段，LLM根据输入的提示（prompt）以及先前生成的内容，逐步预测下一个词或标记（token）。这一过程持续进行，直到生成完整的句子或多个句子。因而这类模型被称为自回归模型（autoregressive Model）。

![https://www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img11.jpg)

在推理阶段，单个词生成的过程如下所示：

1.  提示（Prompt）作为输入文本，用来引导模型生成特定类型的回应或输出。首先，分词器会将提示文本拆分成一系列的标记（tokens）。根据模型的词汇表，一些单词可能会被拆成多个标记，每个标记都会对应一个唯一的数字。接下来，这些数字会被数字转化成固定长度的向量嵌入（embedding）。所有的向量合起来形成一个embedding矩阵，作为模型的输入。
2.  Embedding矩阵首先通过Transformer层进行处理，Transformer由多个堆叠的子层组成，每一层的输出作为下一层的输入。每个子层包含自注意力机制，能够在处理当前输入时综合考虑序列中其他位置的信息，从而捕捉全局依赖关系。与此同时，经过每一层处理后，输入矩阵的维度保持不变。
3.  在Transformer层处理后，模型生成logits，这些logits表示每个可能token的预测分数。然后通过softmax函数将logits 转换为概率分布，并使用多种采样技术（如贪心解码、top-k采样或top-p采样等）之一从概率分布中选择下一个token。
4.  上一步所选的token会被附加到当前生成的token序列中。然后，新的token序列作作为输入，模型会再次执行步骤1至步骤3的过程生成后续token。生成后续token。此过程会持续进行，直到模型生成特殊的结束标记（EOS），或达到预设的token数量。

2.2 LLM应用构建
-----------

构建LLM应用时，选择合适的方法整合专有和领域数据是关键一步，常用的方式包括提示词工程（Prompt Engineering）、模型训练与微调、以及检索增强生成（RAG，Retrieval-Augmented Generation）等。

### 2.2.1 提示词工程

提示词工程通是用户和LLM互动中最常用的方式。提示词工程通过设计和优化输入给模型的提示词（如问题或指令），帮助模型生成更加准确、符合需求的回答。相当于在告诉模型应该关注哪些信息或如何理解问题。可以把它理解为学会提出正确的问题，以获得最佳的答案。不过，能从中获得的帮助是有限的，因为模型的回答只能基于它已经学到的信息。

![https://myscale.com/blog/prompt-engineering-vs-finetuning-vs-rag/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img12.jpg)

提示词工程的优缺点以及适用环境如下所示：

优点：

1.  易用：操作简单，无需专业技术，任何用户都能轻松上手。
2.  成本低：使用预训练模型，计算开销小，比微调方式更具成本效益。
3.  灵活：可以快速调整提示内容，探索不同的结果，无需重新训练模型。

缺点：

1.  一致性差：模型的响应质量可能因提示的措辞不同而产生较大的差异。
2.  定制化受限：回答的个性化程度取决于提示的设计和技巧，定制能力有限。
3.  知识局限：输出依赖于模型训练时获得的知识，无法处理过于专业或最新的信息。

适用环境：

1.  模型能力视情况而定：提示词工程的效果依赖于所用模型的能力。在使用前，用户应评估模型是否具备处理业务问题的基础能力。若模型基础能力较强，提示词工程可作为快速、高效的解决方案。
2.  适合通用任务和简单场景：与其他深度学习技术相比，提示词工程的开发门槛较低，适用于常识问答、文本生成、情感分析、角色扮演等通用任务和简单应用。

对于提示词工程的应用，Zero-Shot Learning（零样本学习）和Few-Shot Learning（少样本学习）是两种常见的提示词策略，它们在帮助用户获取所需内容时各有特点。以下是对这两种学习模式及其应用的进一步扩展说明：

**Zero-Shot Learning**

在Zero-Shot Learning模式下，用户提出任务时并不提供任何示例或样本，模型依赖于其预先学习的知识和语言能力直接生成答案。这样的模式适用于当用户需要一种快速、直接的生成内容方式，且不想提供或不需要提供额外的训练数据或背景信息。

举例来说，如果用户想了解宠物狗的基本情况，可以在LLM应用中通过一个简洁、明确的提示词来引导模型生成所需的内容：

    请写一段介绍宠物狗的文字，描述一下它的外观、性格特点以及和人类相处的情况。
    

此时，模型基于其在大量文本数据中学习到的语言规律和知识，能够从各类资料中提取信息，生成一段介绍宠物狗的文字，涵盖它的外观、性格以及与人类的互动方式。例如，模型可能会生成如下内容：

    宠物狗通常体型适中，拥有毛茸茸的皮毛，种类繁多，常见的有短毛、长毛、卷毛等。它们的耳朵形状、眼睛颜色和尾巴的长短各不相同。性格上，狗狗一般忠诚、活泼，喜爱与人类互动，是非常好的家庭伴侣。它们常常热情迎接主人，喜欢与家人一起散步、玩耍。狗狗特别喜欢获得主人的关注和奖励，在日常生活中会表现出对主人的深厚感情。
    

这种生成方式快速高效，适合快速获取一些普遍性信息。

**Few-Shot Learning**

与Zero-Shot Learning不同，Few-Shot Learning是通过提供少量示例或样本来帮助模型更好地理解任务要求。通过这些示例，模型能够更准确地理解输入和输出之间的关系，从而提高生成内容的质量。此方式在某些情况下会更加精确，尤其是当用户有特定需求时，示例可以帮助引导模型产生更符合预期的回答。

例如，以下是一个典型的Few-Shot Learning的提示词示例：

    以下是几个描述动物的示例，请你按照这样的格式写一段介绍宠物狗的文字，描述它的外观、性格特点以及和人类相处的情况。
    
    示例 1：
       动物：猫咪
       外观：猫咪体型较小，毛茸茸的身体，眼睛大而明亮，耳朵尖尖，尾巴长而灵活，走路时轻盈优雅。
       性格特点：猫咪性格独立，喜欢自己待着，但也会对主人撒娇，开心时会发出呼噜声。
       和人类相处：它喜欢依偎在主人身边，陪伴主人休息或看电视，偶尔会捣乱，但总是给家庭带来温馨感。
    
    示例 2：
       动物：兔子
       外观：兔子身体小巧，耳朵长而竖立，眼睛圆润，嘴巴三瓣，后腿强壮，蹦跳时非常可爱。
       性格特点：兔子性格温顺胆小，喜欢啃食干草和胡萝卜，平时安静乖巧。
       和人类相处：兔子是孩子们的好伙伴，可以被抱在怀里抚摸，吃东西时很治愈。
    
    请写一段介绍宠物狗的文字：
    

在这个示例中，通过提供两个不同动物的描述，LLM能够识别出描述结构和内容的相似性，从而在生成宠物狗的描述时保持一致的格式和逻辑结构。在LLM应用中使用该提示词，生成的内容可能是：

    动物：宠物狗
    外观：宠物狗体型从小巧到中等大小不等，毛发种类繁多，有短毛、长毛或卷毛。它们的眼睛明亮，耳朵多为立耳或垂耳，尾巴通常是卷曲或直的，走动时充满活力。
    性格特点：宠物狗通常友好、忠诚，喜欢与人类互动，性格开朗，适合做家庭宠物。它们非常聪明，能够学会一些基本的指令，喜欢参与户外活动。
    和人类相处：狗狗是人类的忠实伙伴，喜欢陪伴主人，无论是散步、玩耍还是安静地待在主人身边，都会表现出对主人的深厚情感。狗狗还会通过摇尾巴、撒娇等方式表达情感，是非常合适的家庭宠物。
    

### 2.2.2 模型训练与微调

LLM的训练通过海量数据学习语言规律和知识，而微调则是在特定任务或领域上对已经训练好的模型进行优化，以便更好地满足具体需求。训练LLM通常需要巨大的显存和长时间的训练周期，受限于成本。微调常常通过在已有的LLM模型上附加额外结构、冻结原模型的参数，仅训练新结构，从而降低计算开销。在推理阶段，额外结构可以与原模型合并（但并非所有结构都支持合并）。目前，LoRA（低秩适配）是最流行的微调方法，它具有简单的结构和较低的训练成本，并在部分任务上能够接近全量微调的效果。

微调使模型能够学习新知识或调整生成结果。通过向已训练好的LLM提供额外数据，微调会改变模型的权重，从而更好地控制生成内容。与全量训练相比，微调通常是一个相对轻量的过程，且往往只需少量数据便能在特定任务上显著提升性能。然而，微调也可能带来意想不到的副作用，甚至可能削弱模型的通用能力，因此需要谨慎评估其效果。

对于大多数公司来说，微调模型是一种更加经济、高效和灵活的选择，尤其是在LLM已经预训练的情况下。公司无需承担从零开始训练模型的高昂成本和技术难题，只需关注特定领域的定制化需求。

![https://clive-gomes.medium.com/pre-training-large-language-models-at-scale-d2b133d5e219](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img13.jpg)

下面是微调的优缺点以及适用场景的总结：

**优点**

1.  高定制化：微调能根据特定任务调整模型，提升在特定领域或任务中的表现，如金融、法律、医学等领域的专业应用。
2.  一致性高：微调能减少因提示词不同导致的输出差异，使模型在特定任务中的表现更加稳定和一致。
3.  长效性与可扩展性：微调后的模型能在当前任务中表现更好，且可以通过进一步微调适应新任务，具有较强的长期适应性。
4.  应对复杂场景：微调能处理比提示词工程更复杂的场景，尤其是需要高精度和深度的任务，如技术文档生成、编程辅助等。

**缺点**

1.  数据质量要求高：微调通常需要大量特定领域的高质量标注数据，这增加了数据准备的成本。
2.  计算资源需求高：微调涉及重新训练部分模型参数，计算开销较大，需要较强的计算资源。
3.  开发难度较大：微调需要一定的深度学习背景，开发者需要理解如何设计数据、调整参数和评估效果，门槛较高。
4.  过拟合风险：如果微调的数据集过于单一或有限，模型可能在特定任务上过拟合，导致在其他场景中表现差。

**适用环境**

1.  需要高定制化的任务：如医疗、法律、金融等领域，微调能帮助模型精确处理专业术语和规则。
2.  拥有大量标注数据的任务：当有足够标注数据时，微调能有效提升模型性能。
3.  要求高度一致性的任务：如自动文案生成、客户支持等，微调能确保模型输出的稳定性和一致性。
4.  复杂或多任务场景：在多任务学习或复杂问题求解中，微调能够提升模型在不同任务中的适应性和处理能力。

除了模型训练和微调，开发LLM的一个关键环节是确保模型能够理解并回答符合人类需求的问题，这一过程被称为人类对齐。经过人类对齐的模型（Chat模型）通常能够更好地进行通用的问答任务，还可以在此基础上通过少量数据进行微调，以适应特定领域的应用场景。需要注意的是，人类对齐的技术难度通常高于微调，一般由LLM开发方负责实施。

此外，如果想要获取公开的LLM数据集以进行模型微调，可以参考以下开源仓库：[data-prep-kit](https://github.com/IBM/data-prep-kit)和[LLMDataHub](https://github.com/Zjh-819/LLMDataHub)。

### 2.2.3 RAG

**检索增强生成（RAG）简介**

检索增强生成（RAG, Retrieval-Augmented Generation）是一种融合LLM与外部知识库的技术。其核心理念是在LLM为用户提供答案时，首先通过从知识库中检索相关信息，并基于这些信息生成精确的回答，类似于对信息库进行快速查询以获得最佳解答。RAG的工作流程如下：

1.  问题转化与检索：当用户提出问题时，RAG系统会将问题转换为向量表示，并利用向量数据库进行高效检索，找到与问题相关的文档或信息。
2.  信息召回：系统从知识库中召回与问题相关的文档或信息，确保模型能够基于最新且相关的内容生成答案。
3.  答案生成：通过将检索到的信息与LLM的生成能力相结合，生成最终的回答。此过程可以是直接返回检索结果，也可以将召回的资料作为上下文输入LLM，由模型进一步加工整理，生成更为详细和精准的答案。

![https://gradientflow.substack.com/p/best-practices-in-retrieval-augmented](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img14.jpg)

RAG的优缺点及适用场景总结：

**优点**

1.  提高准确性与覆盖面：结合外部信息，RAG能够生成更准确、详细的答案，尤其适用于处理大量实时信息的任务，如问答和文本生成。
2.  灵活性强：动态检索相关文档，避免依赖固定训练数据，适应多领域及变化任务。
3.  生成与检索结合：融合生成模型的语言能力与检索模型的知识，使答案既自然流畅又准确及时。
4.  减少训练难度：相比微调模型，RAG通过检索系统的外部知识提升表现，减少对大规模标注数据的依赖。

**缺点**

1.  依赖外部数据源：若检索源质量差或不准，生成结果会受到影响。
2.  计算资源需求高：双重过程增加计算开销，尤其在实时检索时需更多资源。
3.  模型复杂度高：联合训练与推理增加系统复杂性，需更多调试与优化。
4.  上下文窗口限制：信息量受限于上下文窗口，超大规模知识无法完全纳入生成过程。

**适用场景**

1.  实时更新任务：如新闻生成、知识问答，能够整合最新外部数据，生成高质量内容。
2.  处理大量知识或冷门任务：适合医学、法律等领域，能检索并生成准确答案。
3.  跨领域应用：支持跨学科研究与复杂决策，利用多领域信息进行综合分析。
4.  增强通用LLM：提升通用LLM的表现，尤其在缺乏实时信息或知识库时。

2.3 LLM评估
---------

LLM表现出卓越的泛化能力和适应性，在以前未见过的任务和不同领域中展现出强大的迁移能力。然而，其强大的能力也为评估带来了新的挑战。由于其输出具有高度生成性和开放性，标准化指标通常不足以进行全面评估。

这是因为尽管LLM具有巨大的潜力和显著的优势，但它也面临着一些关键挑战。例如，LLM的评估结果往往受到提示模板的影响，这可能导致评估有偏见或不一致。考虑到 LLM是在大量文本语料库上训练的，它们还可能继承各种隐性偏见，影响其评估的公平性和可靠性。

目前，常见的评估方法主要分为两种：自动评估和人工评估。其分类标准基于评估是否可以自动计算：若评估标准能够自动计算，则归类为自动评估；否则，归类为人工评估。关于LLM评估的详细指南可以查看huggingface提供评估工具：[evaluation-guidebook](https://github.com/huggingface/evaluation-guidebook)。

![https://arxiv.org/pdf/2310.19736](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img15.jpg)

### 2.3.1 LLM自动评估

自动评估是最常见且广泛应用的评估方法之一，通常利用标准化的指标和工具来衡量模型性能。与人工评估相比，自动评估不依赖大量人工参与，能够节省时间并减少人为主观偏差，从而使评估过程更加规范化。自动评估通常从多个维度对模型进行评价，包括准确率、校准性、公平性和鲁棒性等：

1.  准确率：衡量模型在特定任务上正确预测的程度。准确率的定义因任务和问题的不同而有所差异，通常使用多种指标进行衡量，如精确匹配、F1 分数和 ROUGE 分数。
    
    *   精确匹配（Exact Match）：用于评估模型在文本生成任务中的输出是否与参考答案完全一致。在问答任务中，如果模型生成的答案与人工提供的答案完全一致，则精确匹配为1，否则为0。
    *   F1分数：综合考虑模型的精度和召回率。
    *   ROUGE分数：主要用于评估文本摘要和机器翻译任务，衡量生成文本与参考文本之间的重叠和匹配程度。
2.  校准性：衡量模型输出的置信度与实际预测精度之间的一致性。
    
    *   期望校准误差（Expected Calibration Error）：评估模型校准性能的常用方法。该方法通过将预测概率划分为多个区间，计算每个区间内的预测误差，并对这些误差加权平均，得出整体的校准误差。较低的值表示模型在不同置信度水平下具有较好的校准性。
    *   选择性准确率和覆盖率的曲线下面积（AUC）：选择性准确率表示在特定置信度阈值下，模型正确预测的比例，而覆盖率则是该置信度阈值下，模型预测结果中有效预测的比例。通过绘制选择性准确率与覆盖率之间的关系曲线，可以计算出其曲线下面积AUC。AUC值较大的模型通常表明高置信度的预测更为准确。
3.  公平性：评估模型对不同群体表现的一致性，即模型在不同群体中的表现是否公平。群体差异可能涉及性别、种族、年龄等因素：
    
    *   人口平衡差异（Demographic Parity Difference）：衡量模型的预测是否在不同人群之间均匀分布。如果不同群体的预测结果差异较大，说明模型可能存在对某些群体的偏见。
    *   平等机会差异（Equalized Odds Difference）：旨在确保模型在不同群体中具有相等的错误率，即模型在各群体中的预测错误概率应相似。
4.  鲁棒性：评估模型在面对各种挑战性输入时的表现，包括对抗性攻击、数据分布变化和噪声等因素的影响：
    
    *   攻击成功率（Attack Success Rate）：用于评估LLM在面对对抗性攻击时的鲁棒性。
    *   性能下降率（Performance Drop Rate）：评估LLM在面对不同提示词时的鲁棒性，衡量模型性能在这些情况下的下降程度。

![https://www.airtrain.ai/blog/the-comprehensive-guide-to-llm-evaluation](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img16.jpg)

### 2.3.2 LLM人工评估

随着LLM能力的不断增强，它们已经超越了传统的通用自然语言任务评估标准。虽然一些生成任务可以采用自动评估方法，但在人类评估的情况下，生成结果往往能超越标准答案，因此备受青睐。

人工评估是一种通过人类参与来评估模型生成结果质量和准确性的方法。与自动评估相比，人工评估更贴近实际应用场景，能够提供更全面、准确的反馈。在LLM的人工评估中，通常会邀请评估者（如专家、研究人员或普通用户）对模型生成的结果进行评价。然而，即使是人工评估也可能存在较大的波动和不稳定性，这可能与文化差异和个体差异有关。在实际应用中，通常会综合考虑这两种评估方法，并根据具体情况进行权衡。

探索LLM的人工评估方法需要特别关注多个关键因素，以确保评估结果的可靠性和准确性。评估者的数量是一个至关重要的因素，它与评估的代表性和统计意义密切相关。合理选择评估者数量有助于对LLM进行更细致、全面的评估，从而更可靠地将评估结果推广到更广泛的场景中。

评估标准是人工评估过程中的核心内容。这些标准能够全面分析LLM在语法、语义和上下文等方面的表现，从而更深入地评估生成文本的质量。常见的评估标准包括：

1.  准确性：准确性是评估标准中最为关键的一项，关注生成文本的正确性与精确性。该标准要求检查模型生成的信息是否与事实一致，避免出现错误或不准确的内容。
2.  相关性：相关性评估生成内容的适切性和重要性，考察文本是否与给定的上下文或问题紧密相关，确保提供的信息不仅直接相关，还具有实际应用价值。
3.  流畅性：流畅性评估模型生成内容的连贯性和一致性。一个流畅的文本不仅语法正确，还需确保可读性和用户体验，避免出现生硬的表达或突兀的语言转换。
4.  透明性：透明性关注模型决策过程的清晰度与可解释性，评估模型是否能够清晰地传达其思维过程，使用户理解为何以及如何生成某些回答。透明的模型能够提供对其内部工作原理的深入洞察。
5.  安全性：安全性是评估标准中的一个关键项，关注生成文本可能带来的潜在风险或不良后果。该标准评估模型在避免生成不适当、冒犯性或有害内容方面的能力，确保用户的安全，并防止虚假信息的传播。
6.  人类对齐性：人类对齐性评估生成内容是否符合人类的价值观、偏好和期望。该标准关注生成内容的伦理影响，确保模型生成的文本符合社会规范与用户期望，促进积极的用户互动。

### 2.3.3 LLM评估工具

随着LLM在各行各业及应用场景中的广泛应用，评估流程在自然语言处理、内容生成、客户服务自动化等任务中的重要性日益凸显。与此同时，随着LLM技术的不断进步，涌现出一系列自动化评估工具、评估数据集和评估基准，以更好地满足日益增长的实际应用需求。开源仓库[Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)和[Awesome LLM Eval](https://github.com/onejune2018/Awesome-LLM-Eval)提供了各类LLM评测工具、基准/数据集、排行榜等内容的精选资源。

如果想了解LLM的评估情况，尤其是当前开源LLM的表现，可以参考[Open LLM](https:////huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/)排行榜，这是由Hugging Face 设立的一个专注于评测开源LLM的公开榜单。该排行榜汇集了多个模型的评测结果，展示了各种模型在不同任务中的性能，包括推理速度、生成质量、理解能力等多个维度。

![https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img17.jpg)

2.4 LLM量化、部署、优化
---------------

### 2.4.1 模型量化

随着LLM技术的迅猛发展，其模型复杂度呈现出指数级增长，导致参数数量的显著增加。例如，2018年发布的首个GPT模型拥有约1.1亿个参数，而至2019年底，GPT-2的参数量已扩展至15亿，2020年底发布的GPT-3更是突破了1750亿个参数。目前，GPT-4的参数量已预计突破1万亿。

这一增长伴随着一系列挑战：随着模型规模的扩大，内存需求急剧上升。这种日益增长的内存需求不仅限制了推理模型的训练与部署，也制约了基于LLM的解决方案在实际应用中的普及。因此，如何在保证模型性能的前提下缩减模型规模，成为亟待解决的核心问题。量化技术作为一种有效的解决手段，在尽可能保持性能的同时，显著降低内存占用。

量化旨在将LLM中的权重和激活值从高精度表示转化为低精度表示，即将数据从能够存储更多信息的类型转换为存储信息较少的类型。例如，将32位浮点数（float32）转换为8位整数（int8）。尽管量化技术在LLM中得到广泛应用，但它很早便在深度学习领域中得到应用，尤其在图像处理等任务中。通过减少每个权重或激活值所需的位数，量化显著减少了模型的总体大小。因此，量化不仅能够减小LLM在内存中的占用，还能降低存储需求并提升能效。然而，在提升计算效率的同时，量化技术需要在性能和精度之间寻找到合理的平衡点。

量化的基本原理是基于每个张量（tensor）的浮点型最大值与最小值，将其映射为一个固定范围内的整数集合，例如\[-127, 127\]。其公式可表示为：

\\\[q\_{\\text{weight}} = \\text{round} \\left( \\frac{\\text{weight}}{\\text{scale}} \\right) \\\]

其中\\(q\_{\\text{weight}}\\) 为量化后的权重，\\(\\text{weight}\\) 为量化前的权重，\\(\\text{scale}\\) 为缩放因子。可以看出，在进行量化时，通过缩放和取整操作，浮点数会丢失小数部分。在后续的计算或反量化过程中，由于无法完全恢复原浮点值，因此会出现一定的精度损失。

量化过程可以类比于图像压缩。高分辨率图像通常需要进行压缩，以加快网页加载速度。压缩通过去除部分数据或信息位，减小图像文件的大小。尽管压缩会在一定程度上影响图像质量，但它显著减少了文件体积，同时仍能提供较为清晰的视觉效果。

![https://medium.com/@abonia/llm-series-quantization-overview-1b37c560946b](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img18.jpg)

根据量化发生的阶段，量化方法可以分为两种：一种是训练后量化（Post-Training Quantization，PTQ），也称为离线量化，它在模型训练完成后对参数进行量化，不修改训练过程。在推理阶段，模型参数的动态范围将被重新计算；另一种是训练感知量化（Quantization-Aware Training，QAT），也称为在线量化，它通过修改训练过程，模拟量化对模型的影响，从而增强模型对量化误差的鲁棒性，并提高最终的准确性。在QAT过程中，训练的中间状态同时保存量化后的权重和原始未量化的权重（两者存储在内存中），推理时使用量化版本的模型，而在反向传播阶段则使用未量化的权重进行更新。QAT相较PTQ更加复杂且训练时间较长，但通常能够带来更高的准确性。

更多关于LLM量化的详细介绍可参考：[Quantization Overview](https://medium.com/@abonia/llm-series-quantization-overview-1b37c560946b)。

### 2.4.2 模型推理部署

**LLM推理**

LLM的推理部署有多种方式：

*   直接使用PyTorch代码： 对于熟悉PyTorch的用户来说，可以直接利用其提供的接口进行推理。
*   使用专用框架：
    *   VLLM: 专为高效处理LLM而设计。
    *   XInference: 提供便捷的部署流程，可以快速将模型部署到不同的硬件平台。
    *   FastChat: 专注于对话模型的推理和优化。
*   使用C++推理框架：
    *   llama.cpp/chatglm.cpp/qwen.cpp: 提供了高性能的C++实现，适合对性能有较高要求的场景。

**文本生成策略**

在文本生成过程中，LLM通常采用自回归生成方式，即基于已生成的部分预测下一个部分。不同的采样策略会影响模型生成下一个词的方式，常见的采样策略包括：

*   贪婪搜索（Greedy Search）：模型根据前文内容，从词表中选择生成概率最高的下一个token。该方法简单高效，但由于始终选择概率最大值，生成的文本可能显得单调和重复。
*   束搜索（Beam Search）：与贪婪搜索不同，束搜索会同时考虑生成概率最高的k个token。在每一步生成下一个token时，模型会根据当前已生成的tokens，生成k个候选token，从而形成k²个可能的组合，并从中选择概率最高的k个序列继续生成，最终输出条件概率最优的序列。该方法能够在生成质量与多样性之间取得平衡，但可能导致生成的文本显得较为保守且不够自然。
*   随机采样（Sampling）：根据每个token的生成概率，从词表中随机选择下一个token。此方法能够增加生成文本的多样性，是目前主流的文本生成方式，但也可能导致生成的文本缺乏连贯性或逻辑性。

![https://heidloff.net/article/greedy-beam-sampling/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img19.jpg)

可以看出，以上方法各自存在一定的局限性，因此LLM中引入了temperature温度参数，用于调节生成文本的随机性与确定性，并结合top-k采样和top-p采样这两种介于贪心解码和完全随机采样之间的策略，以优化文本生成过程。具体解释如下：

*   temperature：该参数控制生成文本的随机性与确定性之间的平衡。该值越高，生成的文本越具有多样性；而值越低，生成的文本则趋向于更加确定性和一致性。
*   top-k：top-k采样是对传统贪心解码策略的一种优化，它从概率排名前k的token中进行采样，允许其他高概率的token也有机会被选中。这种方式通过引入一定的随机性，有助于提升生成文本的质量。
*   top-p：top-p采样在每一步生成时，只从累积概率超过预设阈值p的最小token集合中进行随机采样，忽略低概率的token。该方法聚焦于概率分布的核心部分，避免了过多依赖概率较低的尾部元素。

top-p采样在实际应用中通常被认为比top-k采样更自然和灵活，因为它允许模型考虑更多的候选词，通过累积概率的方式来选择下一个词，从而生成的文本更加连贯和多样化，而不是像 top-k采样那样只从概率最高的几个词中选择，可能导致生成的文本过于单一和重复。

对于温度参数（temperature），首先需要了解Softmax函数的基本概念。Softmax函数广泛应用于神经网络的输出层。它将一个向量或一组实数映射到另一个向量，使得输出值位于0到1之间，并且所有输出值的总和为1。因此，Softmax函数的输出可以被解释为概率分布。

具体而言，对于输入向量\\(z\\)中的第\\(i\\)个元素，Softmax函数的定义为：

\\\[\\text{Softmax}(z\_i) = \\frac{e^{z\_i}}{\\sum\_{j} e^{z\_j}} \\\]

其中，\\(e^{z\_i}\\)表示\\(z\_i\\)的指数，而分母是所有输入元素的指数和。这样的设计确保了较大值的输入元素对应的概率较高，而较小值的输入元素对应的概率较低，但不会完全为零。

为了调整输出的概率分布，在Softmax函数中引入了温度参数（temperature parameter）\\(T\\)。温度参数\\(T\\)可以通过修改指数部分来控制输出的平滑度。具体形式为：

\\\[\\text{Softmax}(z\_i, T) = \\frac{e^{z\_i / T}}{\\sum\_{j} e^{z\_j / T}} \\\]

当\\(T\\)值较大时，输出的概率分布趋于平坦，即所有输出的概率接近均匀分布；而当\\(T\\)趋向于0时，概率分布则变得更加集中，几乎所有的概率都会集中在最大的元素上。这种调整可以控制模型输出的确定性或多样性。

关于文本生成策略更详细的介绍见：![https://newsletter.theaiedge.io/p/how-llms-generate-text](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img20.jpg)

### 2.4.3 模型优化技术

随着LLM的发展，各种优化技术从不同角度不断涌现，具体包括常见以下几种：

1.  Static kv-cache and torch.compile
    
    *   Static kv-cache（键值缓存）是一种优化技术，旨在存储LLM在解码过程中的键值对，以避免重复计算，从而提高效率。与`torch.compile`结合使用时，通过预先分配kv-cache的大小，可以实现静态分配，进一步优化性能，可能带来最多4倍的速度提升。
2.  Speculative decoding
    
    *   推测性解码是一种加速自回归模型采样的技术。它使用一个较小且更快的辅助模型生成候选token，然后由较大的LLM在单次前向传播中验证这些token。如果验证正确，LLM可以“免费”获得这些token，而不需要自己生成，且不会损失准确性。
3.  Prompt lookup decoding
    
    *   Prompt lookup decoding适用于输入和输出之间存在重叠词汇的任务（如摘要生成）。它通过在输入提示中进行字符串匹配，生成候选token序列，从而替代传统推测性解码中的草稿模型。
4.  Attention optimizations
    
    *   FlashAttention-2：这是一种优化算法，通过将注意力计算分解为更小的块，减少GPU内存的中间读写操作，从而加速推理过程。
    *   Fine-Tuning with torch.compile and Padding-Free Data Collation：通过使用`torch.compile`进行微调，结合无填充数据整理技术，可以进一步提高模型运行效率。
    *   PyTorch scaled dot product attention：通过硬件加速和优化的计算图来加速模型的训练和推理。
5.  VLLM
    
    *   VLLM是一个快速且易于使用的LLM推理与服务库，支持分布式和容器化部署，且内置LLM服务，用户可通过命令直接启动。
6.  FastChat
    
    *   FastChat是一个开源平台，旨在支持训练和分布式部署。它提供了先进模型的训练评估代码和分布式多模型服务系统，包含Web界面以及与OpenAI兼容的RESTful API。

关于这些技术的详细介绍见：[llm inference optimization](https://huggingface.co/docs/transformers/main/en/llm_optims#llm-inference-optimization)。

3 总结
====

涉及LLM的项目可能是一项艰巨的任务。它需要适当的协调和技能才能成功执行任务。一个LLM项目从最初的构思到最终部署的整个过程。可以将这个过程分为如下主要阶段：

1.  定义项目的范围：
    
    *   明确目标：首先要明确这个LLM项目的目标是什么？是用于生成文本、对话助手、翻译语言、还是执行其他任务？
    *   确定范围：接下来要确定项目的范围，即这个LLM模型需要处理的数据量、任务复杂度以及所需的性能。
2.  数据预处理和相关考虑：
    
    *   数据收集：收集与项目相关的大量高质量数据。
    *   数据清洗：对数据进行清洗，去除噪声、错误和不一致的数据。
    *   数据标注：为数据添加标签，以便模型能够学习到正确的关联。
3.  选择一个基座的模型：
    
    *   模型选择：根据项目的需求选择一个合适的基座模型，以及是使用Base模型还是Chat模型。不同的模型在处理不同任务时表现会有所不同。
    *   考虑因素：在选择模型时，需要考虑模型的规模、参数数量、训练数据量以及计算资源等因素。
4.  模型训练：
    
    *   模型训练：利用准备好的数据集和适当的训练方法对模型进行训练，特别要注重选择合适的训练策略。
    *   训练稳定性：LLM的训练相比其他深度学习模型更容易遇到意外问题，如不收敛、训练不稳定、突发中断等，要提前做好应对方案。
5.  强化学习：
    
    *   交互学习：通过与环境的交互，让模型不断学习和改进。
    *   奖励机制：设计合理的奖励机制，引导模型朝着目标方向发展。
6.  评估模型：
    
    *   性能评估：使用测试数据对模型的性能进行评估，以确定模型是否达到预期的效果。
    *   指标选择：选择合适的评价指标。
7.  模型优化和部署：
    
    *   模型优化：对模型进行量化等，以减小模型的尺寸和提高推理速度。
    *   部署：将训练好的模型部署到实际应用中，提供服务。
8.  模型监控和构建LLM应用：
    
    *   模型监控：持续监控模型的性能，及时发现问题并进行调整。
    *   应用开发：基于训练好的模型开发各种LLM应用，如聊天机器人、文本生成工具等。

![https://datasciencedojo.com/blog/llm-project-lifecycle/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/img/img21.jpg)

总之，LLM项目的整个生命周期是一个持续迭代的过程，涵盖了目标定义、模型训练、模型部署、监控以及应用开发等各个环节，且每个阶段都紧密相连、相互依赖。训练LLM的难点，不仅仅在于单一技术的挑战，更在于系统性复杂性的应对。这一过程需要数据、工程师、框架和硬件等多个方面的紧密协作与配合，才能推动工作的顺利进行。

4 参考
====

*   [modelscope-classroom](https://github.com/modelscope/modelscope-classroom)
*   [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
*   [Understanding Encoder And Decoder LLMs](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)
*   [Prompt Engineering vs Fine-tuning vs RAG](https://myscale.com/blog/prompt-engineering-vs-finetuning-vs-rag/)
*   [data-prep-kit](https://github.com/IBM/data-prep-kit)
*   [LLMDataHub](https://github.com/Zjh-819/LLMDataHub)
*   [evaluation-guidebook](https://github.com/huggingface/evaluation-guidebook)
*   [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)
*   [Awesome LLM Eval](https://github.com/onejune2018/Awesome-LLM-Eval)
*   [Quantization Overview](https://medium.com/@abonia/llm-series-quantization-overview-1b37c560946b)
*   [How LLMs Generate Text](https://newsletter.theaiedge.io/p/how-llms-generate-text)
*   [llm inference optimization](https://huggingface.co/docs/transformers/main/en/llm_optims#llm-inference-optimization)

本文来自博客园，作者：[落痕的寒假](https://www.cnblogs.com/luohenyueji/)，转载请注明原文链接：[https://www.cnblogs.com/luohenyueji/p/18644847](https://www.cnblogs.com/luohenyueji/p/18644847)

![](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/wechat/content/%E5%8A%A0%E6%B2%B9%E9%B8%AD.gif)