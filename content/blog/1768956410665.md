---
layout: post
title: '吴恩达深度学习课程五：自然语言处理  第二周：词嵌入（三）Word2Vec'
date: "2026-01-21T00:46:50Z"
---
吴恩达深度学习课程五：自然语言处理 第二周：词嵌入（三）Word2Vec
====================================

此分类用于记录吴恩达深度学习课程的学习笔记。  
课程相关信息链接如下：

1.  原课程视频链接：[\[双语字幕\]吴恩达深度学习deeplearning.ai](https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&from_spmid=playlist.playlist-detail.0.0&is_story_h5=false&mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1713085655&unique_k=DfBgvFW&up_id=8654113&vd_source=e035e9878d32f414b4354b839a4c31a4)
2.  github课程资料，含课件与笔记:[吴恩达深度学习教学资料](https://github.com/robbertliu/deeplearning.ai-andrewNG)
3.  课程配套练习（中英）与答案：[吴恩达深度学习课后习题与答案](https://blog.csdn.net/u013733326/article/details/79827273)

本篇为第五课的第二周内容，[2.6](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=168)的内容以及一些相关知识的补充。

* * *

本周为第五课的第二周内容，与 CV 相对应的，这一课所有内容的中心只有一个：**自然语言处理（Natural Language Processing，NLP）**。  
应用在深度学习里，它是专门用来进行**文本与序列信息建模**的模型和技术，本质上是在全连接网络与统计语言模型基础上的一次“结构化特化”，也是人工智能中**最贴近人类思维表达方式**的重要研究方向之一。  
**这一整节课同样涉及大量需要反复消化的内容，横跨机器学习、概率统计、线性代数以及语言学直觉。**  
语言不像图像那样“直观可见”，更多是抽象符号与上下文关系的组合，因此**理解门槛反而更高**。  
因此，我同样会尽量补足必要的背景知识，尽可能用比喻和实例降低理解难度。  
本周的内容关于词嵌入，是一种**相对于独热编码，更能保留语义信息的文本编码方式**。通过词嵌入，模型不再只是“记住”词本身，而是能够**基于语义关系进行泛化**，在一定程度上实现类似“**举一反三**”的效果。词嵌入是 NLP 领域中最重要的基础技术之一。

本篇的内容关于**Word2Vec**，这是词嵌入领域中里程碑式的内容。

1\. Word2Vec
============

在[上一篇](https://www.cnblogs.com/Goblinscholar/p/19503620)里我们提到了，早期的词嵌入模型虽然实现了通过神经网络学习词向量，但在计算成本与实际性能上仍然存在明显的优化空间。  
因此，正如 CNN 是在传统卷积操作基础上的系统化建模，RNN 是对一般循环结构的模型化抽象 一样，在词嵌入这一思想被提出之后，围绕其训练效率、表示质量与可扩展性的一系列新模型与新技术便不断出现。  
而这些改进在解决旧问题的同时，又会引入新的结构性挑战，推动研究不断向前演进。正是在这种“问题—改进—新问题”的循环中，计算机科学得以持续发展。

在词嵌入领域同样如此，当使用神经网络学习连续词向量被证明可行之后，新的问题就是：**如何在保证表示质量的同时，提高实际部署价值，让词向量能够更高效、稳定地训练出来？**

正是在这样的背景下，Mikolov 等人于 2013 年在论文[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)中提出了 **Word2Vec**。  
简单说明一下，Word2Vec 的命名来源于“Word to Vec”，其中的“2”是工程写法，相当于 **to**，直观表达了“从词汇到向量”的映射思想。

在论文中，作者并未引入复杂的深层网络结构，而是通过极其简洁的模型设计与目标函数重构，大幅提升了词向量学习的效率，使其**首次具备在超大规模语料上训练的可行性**。  
同年，在论文 [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf) 中，作者进一步引入了 **负采样（Negative Sampling）** 等近似训练策略，在不显著牺牲表示质量的前提下，将训练复杂度从依赖词表规模的 Softmax 形式降为常数级，从而使词嵌入模型真正成为 NLP 系统中的基础组件。  
尽管如今有更先进的上下文相关嵌入模型，Word2Vec 的思想仍然具有重要价值：它奠定了现代词嵌入方法的理论基础，并在轻量级、资源受限的场景中仍可提供高效、可解释的词向量。

需要强调的是，**Word2Vec 并不是某一个具体的神经网络结构名称**，而是对一类词向量学习方法的统称，主要包括两种基本模型：

1.  **CBOW（Continuous Bag-of-Words）**：根据上下文词预测中心词。
2.  **Skip-gram**：根据中心词预测上下文词。

还是简单过了过历史，下面就来详细展开这部分内容。

1.1 连续词袋模型 (Continuous Bag-of-Words，CBOW)
-----------------------------------------

CBOW 的思想并不复杂，相对于上一篇中[早期词嵌入模型](https://www.cnblogs.com/Goblinscholar/p/19503620)的模型设计，CBOW 最大的改变其实是**对上下文信息的选择和词向量处理**部分。  
我们分点来进行展开：

### （1）数据准备

首先，一句话概括一下 CBOW 的核心思想 ：**用上下文词来预测中心词**。  
所谓“上下文词”，就是目标词前后一定范围内的词，这个范围同样还是由**窗口大小 \\(t\\)** 决定。  
只是不同于之前的只使用目标词**前 \\(t\\) 个词**，CBOW 中的窗口大小表示**中心词左右各最多 \\(t\\) 个词**。

这里可能混淆，我们展开一下：对于中心词，模型会把**左右 \\(1\\sim t\\) 个词**作为上下文集合，如果左右不足所选窗口大小个词（如在句首或句尾），就取实际存在的词。  
举个例子，对于句子：

     I want to go to the ____ to get a few potato chips
    

假如窗口大小为 2，训练时可能存在以下两种选择：

1.  **左右各选 1 个词：**
    *   左边 1 个词：`the`
    *   右边 1 个词：`to`
    *   上下文集合：\\(\\{\\text{the}, \\text{to}\\}\\)
2.  **左右各选 2 个词：**
    *   左边 2 个词：`to`, `the`
    *   右边 2 个词：`to`, `get`
    *   上下文集合：\\(\\{\\text{to}, \\text{the}, \\text{to}, \\text{get}\\}\\)

![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260120065631726-2047059396.png)

你会发现，在 CBOW 训练时，**窗口大小 \\(t\\) 只规定了一个最大值**，左右各选 1~t 个上下文词是随机的，这样模型可以在不同组合下学习更鲁棒的词向量。

但同时，你可能也会发现一个问题：**如果还按照之前拼接词向量的逻辑，那么这种不固定长度的选词模式就会带来模型输入维度的不统一。**

这个问题在 CBOW 中是怎么解决的？我们继续：

### （2）模型传播

实际上，在模型结构方面，CBOW 进行的改动不多，但为了**处理可变长度上下文和提升训练效率**，它做了几个关键优化。我们先整体看一看它的传播过程：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260120065303686-89437418.png)  
这里展开几个更改的内容：

1.  **对词向量的处理从拼接改为取平均或求和**：这一步极大缩小了模型输入的维度，从而**减少了计算成本**。但这不是没有代价，这样的操作**忽略了信息的顺序**： 没有顺序的信息就像被一股脑地装进了一个袋子里交给模型，这也是 CBOW 命名的由来。
2.  **隐藏层没有使用激活函数引入非线性**：这一步通过简化模型结构来**提高训练效率**，但同样，这也让模型**失去了非线性能力**。

看到这里，你会发现：**这些优化好像有利有弊，并没有我们想象的那么“完美”**。  
实际上，CBOW 的优化体现了一个**权衡**：通过求平均和去掉非线性，它在**训练效率和可扩展性**上获得了显著优势，但同时牺牲了**顺序信息和非线性表达能力**，顺序信息完全丢失，模型无法区分左右词序。  
最终，CBOW 更像是在训练**词汇的统计共现规律**，适合在大规模语料上快速学习基础词向量，但对复杂上下文或语义关系的建模能力有限。  
相应地，它在速度、资源消耗和向量可解释性方面具有明显优势。

而Word2Vec中的另一类模型：Skip-gram 便与之不同，我们继续：

1.2 Skip-gram
-------------

Skip-gram 的核心思想与 CBOW 相反： **用中心词去预测它的上下文词**

也就是说，CBOW 是“上下文 → 中心词”，而 Skip-gram 是“中心词 → 上下文词”。这种方向的转换，使 Skip-gram 在捕捉**稀有词语义**时表现更优，我们按同样的格式来展开：

### （1）数据准备

Skip-gram 也依赖**窗口大小 \\(t\\)** 来确定上下文范围，对于句子：

    I want to go to the pier to get a few potato chips
    

假如中心词选择 `"pier"`，窗口大小 \\(t=2\\)，它的上下文集合就是中心词左右各**最多** 2 个词：

*   左侧上下文：`to`, `the`
*   右侧上下文：`to`, `get`

到这部分的逻辑，它和 CBOW 是相同的，而不同之处在这里：  
**Skip-gram 会将每一个上下文词与中心词组成一个训练样本，生成多条训练对**，就像这样：

中心词

上下文词

pier

the

pier

to

pier

to

pier

get

而 Skip-gram 的传播过程也和 CBOW 有所不同，我们在下面来详细展开：

### （2）模型传播

首先要明确的核心点是：**在 Skip-gram 中，只有中心词的词向量被输入模型。**  
它的具体传播过程如下：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260120065446602-1058700542.png)  
同样强调一些细节：

1.  **同一中心词形成的多个训练对每个都作为独立样本输入模型，重复的训练对不会去重。**
2.  **上下文词即为该样本的标签，模型以此计算损失并训练中心词的词向量。**

同时，你会发现，Skip-gram 同样忽略了上下文词的顺序，以换取在大规模语料上的训练效率。它的本质也是在捕捉**词汇的统计共现规律**，但相比 CBOW，每个上下文词都单独训练，因此不容易被高频词“淹没”，能学习更细粒度的词向量。  
在实践中，多个训练对通常组成 batch，一次性向量化计算，来提高训练效率。

1.3 对比和小结
---------

Word2Vec 的两类基本模型 CBOW 和 Skip-gram 在训练目标、数据处理和适用场景上各有特点，但都体现了 Word2Vec 的核心价值：**通过简单、高效的神经网络结构，将词汇映射为向量，从而捕捉词汇的统计共现规律和语义特征。**  
我们简单对比总结如下：

特性

CBOW

Skip-gram

**训练目标**

上下文 → 中心词

中心词 → 上下文

**输入**

上下文词向量求平均或求和

中心词向量

**输出**

中心词预测

上下文词预测

**样本生成**

每个中心词 1 个训练样本

每个中心词 × 上下文词数 个训练样本

**稀有词建模**

不擅长

更稳健

**顺序信息**

丢失

丢失

**计算效率**

高

相对低（可用负采样优化）

**适用场景**

高频词快速学习、大规模语料

低频词学习、捕捉长尾语义

**向量质量**

平滑、基础语义关系

更细粒度、长尾词语义表现好

最后，Word2Vec 提供了**效率、可扩展性与语义捕捉能力的平衡方案**：CBOW 注重训练速度和高频词表示，Skip-gram 注重低频词和长尾词的语义细节。  
**但二者同样具有局限性**，主要在于在于顺序信息丢失和标准 Softmax 在大词表下计算开销高，这也为我们之后要介绍的**负采样和分层 Softmax**等优化策略提供了研究和应用空间。

2.总结
====

概念

原理

比喻

**Word2Vec**

通过浅层神经网络将词汇映射为稠密向量，学习词汇的统计共现规律

就像给每个词配一个“身份卡片”，卡片上的信息可以反映它与其他词的关系

**CBOW**

以上下文词预测中心词，输入为上下文向量的平均或求和，输出为中心词概率

像根据周围邻居的描述来猜某个人是谁，信息平均处理，顺序不重要

**Skip-gram**

以中心词预测每个上下文词，输入为中心词向量，输出为上下文词概率

像一个人介绍自己时，把自己的特征逐一告诉周围人，每条信息单独训练

**训练样本生成**

CBOW：每个中心词生成 1 个训练样本；Skip-gram：每个中心词 × 上下文词数生成多条样本

CBOW 是一次性“汇总邻居描述”，Skip-gram 是逐条“对每个邻居单独说明”

**局限性**

顺序信息丢失；标准 Softmax 在大词表下计算开销高

就像在聚餐中只知道菜里有哪些食材，但不清楚顺序；如果菜太多，要统计全部组合很费力