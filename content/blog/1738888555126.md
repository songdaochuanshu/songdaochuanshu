---
layout: post
title: 'Spring AI + DeepSeek：提升业务流程的智能推理利器'
date: "2025-02-07T00:35:55Z"
---
Spring AI + DeepSeek：提升业务流程的智能推理利器
==================================

今天，我们将深入探讨如何利用DeepSeek来真正解决我们当前面临的一些问题。具体来说，今天我们仍然会将DeepSeek接入到Spring AI中进行利用。需要注意的是，虽然DeepSeek目前主要作为推理型助手存在，并不完全适合作为智能体的首选，但它仍然能够有效地融入并提升你的业务流程。因此，你可以将其集成到自己的业务系统中，借此发挥它在推理和数据分析上的强大能力。

接下来，我们将继续使用之前在腾讯云HAI服务器上部署的DeepSeek大模型进行开发，利用它的强大功能来满足我们当前的需求。

Spring AI
=========

首先，我们已经知道HAI服务器上部署的实际上也是Ollama，而在Spring文档中也提供了相应的Ollama接口，便于开发者进行集成。当然，如果你觉得使用Ollama有些麻烦，完全可以选择接入OpenAI接口，因为目前Ollama和OpenAI是兼容的，二者可以相互替代，因此你可以根据实际需求来选择适合的接口。具体情况如图所示：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250206114335674-1579677020.png)

ollama配置
--------

在此之前，我们首先需要通过 Ollama 接口进行对接。为此，我们需要引入 Ollama 的相关依赖，具体操作如下：

    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-ollama-spring-boot-starter</artifactId>
    </dependency>
    

紧接着，再添加一些必要的配置信息如调用地址以及调用模型，如下所示：

> spring.ai.ollama.base-url=http://ip:6399  
> spring.ai.ollama.chat.options.model=deepseek-r1:7b

在这里，只需将 IP 信息修改为 HAI 所暴露的对应外网 IP 地址即可。值得注意的是，HAI 目前提供了两种模型版本：1.5b 和 7b。根据性能需求，推荐使用 7b 版本，它在处理能力和效果方面相较 1.5b 更为优秀。接下来，我们将简要编写一个接口 API，以供业务系统进行调用。具体代码如下：

    @PostMapping("/ai")
    ChatDataPO generationByText(@RequestParam("userInput")  String userInput) {
        String content = this.chatClient.prompt()
                .user(userInput).advisors(new MessageChatMemoryAdvisor(chatMemory))
                .call().content();
        log.info("content: {}", content);
        ChatDataPO chatDataPO = ChatDataPO.builder().code("text").data(ChildData.builder().text(content).build()).build();
        return chatDataPO;
    }
    

大概需要等一会时间才可以有结果，因为这是阻塞式响应，你可以接入流式服务就会实时查看响应结果。效果如图所示：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250206114342471-665634229.png)

当然，如果您觉得 7b 模型的回答效果不尽如人意，您完全可以选择加载更强大的模型版本，例如 70b。然而，需要注意的是，选择更大规模的模型会大幅提高算力需求。此外，70b 模型的存储需求也更为苛刻，可能需要额外的磁盘空间来进行部署和运行。

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250206114346424-758751635.png)

目前，基础性机器的存储容量仅为 200GB，因此无法下载完整版本的模型，只能支持下载最大为 70b 参数规模的模型。由于更大规模的模型需要更多的存储空间，因此对于存储容量有限的环境来说，选择合适的模型版本至关重要。如果需要了解各个模型的存储需求，建议访问 Ollama 官方网站，在那里可以查看详细的模型大小信息。以下是相关模型大小的展示，供参考：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250206114350855-134813729.png)

然后我们可以直接去HAI服务器的终端上使用命令进行拉取下载。

> ollama run deepseek-r1:70b

虽然存储容量已经足够，但显存的级别仍然不足，导致加载速度依然较慢。因此，为了确保系统的流畅运行，建议将显存扩展至32GB及以上，这样才能实现更加高效的性能和更好的用户体验。

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250206114357640-1033871976.png)

总结
==

总结来说，DeepSeek作为推理型助手在提升业务流程中具有巨大的潜力，尽管它不是智能体的首选，但它在推理和数据分析上的强大能力依然可以为您的系统增添显著价值。通过集成到Spring AI与Ollama接口，开发者能够灵活选择适合的模型版本来满足不同的性能需求。随着技术不断发展和硬件条件的提升，DeepSeek将进一步展现其在实际应用中的优势，帮助企业更加高效地实现智能化转型。

* * *

我是努力的小雨，一个正经的 Java 东北服务端开发，整天琢磨着 AI 技术这块儿的奥秘。特爱跟人交流技术，喜欢把自己的心得和大家分享。还当上了腾讯云创作之星，阿里云专家博主，华为云云享专家，掘金优秀作者。各种征文、开源比赛的牌子也拿了。

💡 想把我在技术路上走过的弯路和经验全都分享出来，给你们的学习和成长带来点启发，帮一把。

🌟 欢迎关注努力的小雨，咱一块儿进步！🌟