---
layout: post
title: 'Phi小模型开发教程：用C#开发本地部署AI聊天工具，只需CPU，不需要GPU，3G内存就可以运行，不输GPT-3.5'
date: "2025-01-21T00:34:51Z"
---
Phi小模型开发教程：用C#开发本地部署AI聊天工具，只需CPU，不需要GPU，3G内存就可以运行，不输GPT-3.5
===========================================================

**大家好，我是编程乐趣。**

行业诸多大佬一直在说：“‌**2025年将是AI应用元年‌**”，虽然说大佬的说法不一定对，但AI趋势肯定没错的。

对于我们程序员来说，储备AI应用开发技能，不管对找工作、接项目、创业肯定是不错的选择。

从今天开始，我将会学习和研究Phi小模型，并基于此模型开发一些小Demo，也作为我的学习笔记，欢迎大家关注收藏！

**下面先用C#开发一个调用本地模型的示例，一起来感受下Phi的魅力。**

**什么是Phi？**

Phi模型是微软推出的一系列小型语言开源模型，刚刚发布了最新版本：Phi-4。

在GPQA研究生水平、MATH数学基准测试中，超过了OpenAI的GPT-4o，也超过了同类顶级开源模型Qwen 2.5 -14B和Llama-3.3-70B。

在美国数学竞赛AMC的测试中phi-4更是达到了91.8分，超过了Gemini Pro 1.5、GPT-4o、Claude 3.5 Sonnet、Qwen 2.5等知名开闭源模型，甚至整体性能可以与4050亿参数的Llama-3.1媲美。

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239040-1296965260.png)

**模型下载地址**

微软在HuggingFace开源这款超强的小参数模型，并且支持MIT许可证下商业用途。

**当前最新版本开源地址：**

**[https://huggingface.co/microsoft/phi-4](https://huggingface.co/microsoft/phi-4)**

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239071-270506812.png)

**C#源码开发示例**

**1、下载ONNX**

ONNX（Open Neural Network Exchange）是由微软和Facebook等科技巨头于2017年联合推出的一种开放格式。

ONNX 已经对接了多种深度学习框架和多种推理引擎。因此，ONNX 被当成了深度学习框架到推理引擎的桥梁。

ONNX Runtime提供了简单易用的API，支持Python、C++、C#和Java等多种编程语言，方便开发者将其集成到现有应用中。

**微软针对Phi-3版本，已经为我们提供了onnx文件，我们这里下载的是Phi-3版本的，因为Phi-4还没有onnx文件。**

**下载地址：**

[https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239097-1769965489.png)

**onnx提供了CPU、GPU版本，我这边使用的是CPU版本，把以下文件下载到本地。**

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239071-1362313505.png)

**下载后的本地文件如下：**

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239061-539739467.png)

**2、创建控制台应用**

创建控制台应用，我这边使用的是.Net 9。

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239033-2140271934.png)

安装依赖库：

**Microsoft.ML.OnnxRuntimeGenAI**

官方为我们提供多个套件，不同套件针对不同的硬件加速需求和环境进行优化，后面在详细介绍，这边我们使用的CPU模型，安装**Microsoft.ML.OnnxRuntimeGenAI就行。**

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239098-648992098.png)

**3、代码示例**

    using Microsoft.ML.OnnxRuntimeGenAI;
    
    // 指定模型路径
    var modelPath = @"F:\Model";
    // 创建Model对象，加载模型
    var model = new Model(modelPath);
    // 创建Tokenizer对象，用于文本的编码和解码
    var tokenizer = new Tokenizer(model);
    
    // 设置系统提示，定义AI助手的行为风格
    //“您是一个帮助人们查找信息的AI助手。请使用直接的风格回答问题。不要分享用户未请求的额外信息。”
    var systemPrompt = "You are an AI assistant that helps people find information. Answer questions using a direct style. Do not share more information that the requested by the users.";
    
    // 提示用户输入问题，空字符串退出
    Console.WriteLine(@"Ask your question. Type an empty string to Exit.");
    
    // 循环等待用户输入问题
    while (true)
    {
        Console.WriteLine();
        Console.Write(@"Q: ");
        var userQ = Console.ReadLine();
        // 如果用户输入为空字符串，则退出循环
        if (string.IsNullOrEmpty(userQ))
        {
            break;
        }
    
        // 显示AI助手的回答前缀
        Console.Write("Phi3: ");
        // 构建完整的提示文本，包括系统提示、用户问题和AI助手的开始标记
        var fullPrompt = $"<|system|>{systemPrompt}<|end|><|user|>{userQ}<|end|><|assistant|>";
        // 使用Tokenizer将文本编码为tokens
        var tokens = tokenizer.Encode(fullPrompt);
    
        // 创建GeneratorParams对象，设置生成参数
        var generatorParams = new GeneratorParams(model);
        // 设置最大生成长度
        generatorParams.SetSearchOption("max_length", 2048);
        // 设置past和present是否共享缓冲区，这里设置为false
        generatorParams.SetSearchOption("past_present_share_buffer", false);
        // 设置输入序列
        generatorParams.SetInputSequences(tokens);
    
        // 创建Generator对象，用于生成文本
        var generator = new Generator(model, generatorParams);
        // 循环生成文本，直到生成完成
        while (!generator.IsDone())
        {
            // 计算logits
            generator.ComputeLogits();
            // 生成下一个token
            generator.GenerateNextToken();
            // 获取当前生成的序列
            var outputTokens = generator.GetSequence(0);
            // 获取新生成的token
            var newToken = outputTokens.Slice(outputTokens.Length - 1, 1);
            // 解码新生成的token为文本
            var output = tokenizer.Decode(newToken);
            // 输出生成的文本
            Console.Write(output);
        }
        // 换行，准备下一轮输入
        Console.WriteLine();
    }
    

**4、运行效果如下**

**初始化界面：**

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239059-478543132.png)

**输入问题：**

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239062-2005940807.png)

**回答结果：**

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239060-305788687.png)

Phi是使用英文作为训练材料的，所以用英文提问效果会比较好点。

**5、占用资源如下**

**测试环境：Intel i7处理器。**

![图片](https://img2024.cnblogs.com/blog/93789/202501/93789-20250120112239085-524573949.png)

这样就完成一个小Demo了。

**好了，今天就分享到这边了，此系列会持续更新，欢迎关注我！**

**以上相关模型、源码示例，我也打包好了**，[https://pan.quark.cn/s/53f3e932e9bf](https://pan.quark.cn/s/53f3e932e9bf)

\- End -

**更多开源项目：** [https://github.com/bianchenglequ/NetCodeTop](https://github.com/bianchenglequ/NetCodeTop)