---
layout: post
title: 'AI一周资讯 250918-250925'
date: "2025-09-26T00:38:22Z"
---
AI一周资讯 250918-250925
====================

原文: [https://mp.weixin.qq.com/s/6\_sSbUDYOujOjeF-n1rnGA](https://mp.weixin.qq.com/s/6_sSbUDYOujOjeF-n1rnGA)

行业首个“高刷”视频理解多模态模型！MiniCPM-V 4.5凭三大技术成30B以下最优开源
==============================================

本周，由清华大学自然语言处理实验室和面壁智能联合开发的MiniCPM-V 4.5亮相，作为行业首个具备“高刷”视频理解能力的多模态模型，它通过三大关键技术突破多模态效率瓶颈：①采用统一3D-Resampler架构，时空联合压缩视频帧（6帧448×448压缩至64Token，实现96倍视觉压缩率），统一图像与视频编码及知识迁移；②创新面向文档的统一OCR和知识学习范式，通过不同程度损坏文字区域，将OCR、综合推理、知识学习融合于“重建原文”目标，摆脱外部解析器依赖；③提出可控混合快速/深度思考的多模态强化学习，混合优化节省30%采样开销，推理耗时为同规格深度思考模型的42.9%-68.2%，结合RLPR（概率奖励信号解决开放式回答奖励问题）、RLAIF-V（抑制幻觉）提升性能可靠性。性能评测方面，8B参数规模的MiniCPM-V 4.5在OpenCompass综合评测获77.0平均分，超越GPT-4o-latest、Qwen2.5-VL-72B等模型，成为30B以下最优开源多模态模型；在Video-MME上时间开销仅为同级模型1/10；HuggingFace、ModelScope合计下载超22万，登HuggingFace Trending TOP2。其系列模型下载超1300万次，GitHub星标超2万，获多项学术产业认可。

*   技术报告：[https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/MiniCPM\_V\_4\_5\_Technical\_Report.pdf](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/MiniCPM_V_4_5_Technical_Report.pdf)
*   Github：[https://github.com/OpenBMB/MiniCPM-o](https://github.com/OpenBMB/MiniCPM-o)
*   HuggingFace：[https://huggingface.co/openbmb/MiniCPM-V-4\_5](https://huggingface.co/openbmb/MiniCPM-V-4_5)
*   ModelScope：[https://www.modelscope.cn/models/OpenBMB/MiniCPM-V-4\_5](https://www.modelscope.cn/models/OpenBMB/MiniCPM-V-4_5)

无问芯穹推出“基础设施智能体蜂群”，用Agentic Infra范式让智能体开发更丝滑
===========================================

2025年9月23日，无问芯穹推出基于长期AI-Native基础设施建设成果的“基础设施智能体蜂群”，针对传统智能体基建依赖“胶水代码”拼合、智算资源闲置、突发故障影响高价值训练任务、运维团队疲于排障等痛点，封装SOTA模型筛选、基础设施平台管家、资源运营、答疑排障、智算集群运维五大智能体模块，构建高度自治、动态协作体系，实现基础设施全生命周期智能感知与闭环执行；同时落地Agentic Infra（智能体化基础设施）范式，改变传统IaaS→PaaS→MaaS→Agent应用层层堆叠模式，将异构算力、云原生组件等统一纳入智能体调用范围，实现从算力适配、模型选型到部署的全链路智能化生产（“一句话，一个Agent”）。落地案例中，超百万月活的二次元创作平台“捏TA”通过端到端自动化调度减少30%通用组件重构与流程维护资源消耗，迭代速度提升5倍；年轻人社交平台Soul App压缩创新周期、降低试错成本，让搁置想法快速落地。无问芯穹具备从大模型优化、分布式训练到GPU集群运维的全链路技术栈，核心主张“AI Native基础设施无感化”，目标让有领域知识的中小团队以更低门槛、更高效率构建高质量智能体应用，将重复劳动交给机器，解放开发者创造力。

阿里通义大模型团队开源三大多模态模型：全模态Qwen3-Omni、超SOTA TTS与增强型图像编辑齐发
====================================================

2025年9月23日，阿里通义大模型团队开源三款多模态相关模型。其中，**Qwen3-Omni**为原生全模态大模型，支持文本、图像、音频、视频输入及实时流式文本与自然语音输出；在36项音频及音视频基准测试中获32项开源SOTA、22项总体SOTA，超越Gemini-2.5-Pro、Seed-ASR、GPT-4o-Transcribe等闭源模型，图像和文本性能同尺寸模型达SOTA；支持119种文本语言交互、19种语音理解语言、10种语音生成语言，纯模型端到端音频对话延迟211ms、视频对话延迟507ms，支持30分钟音频理解；采用Thinker-Talker架构（Thinker负责文本生成、Talker专注流式语音Token生成）及2000万小时音频训练的AuT音频编码器，均采用MoE架构，支持function call，开源Qwen3-Omni-30B-A3B-Instruct（指令跟随）、Qwen3-Omni-30B-A3B-Thinking（推理）、Qwen3-Omni-30B-A3B-Captioner（通用音频字幕器）三个版本。**Qwen3-TTS-Flash**文本转语音模型，中英稳定性在seed-tts-eval test set达SOTA，超越SeedTTS、MiniMax、GPT-4o-Audio-Preview；多语言稳定性和音色相似度在MiniMax TTS multilingual test set中，中文、英文、意大利语、法语WER达SOTA，英文、意大利语、法语说话人相似度超MiniMax、ElevenLabs、GPT-4o-Audio-Preview；支持17种音色、10种语言、9种方言（普通话、闽南语、吴语等），高表现力、高鲁棒性，首包延迟97ms。**Qwen-Image-Edit-2509**图像编辑模型，支持多图编辑（人物+人物、人物+商品等）；单图一致性增强（人物ID/商品ID保持、文字内容/字体/色彩/材质编辑）；原生支持ControlNet（深度图、边缘图、关键点图等）。

*   GitHub：[https://github.com/QwenLM/Qwen3-Omni](https://github.com/QwenLM/Qwen3-Omni)
*   Hugging Face：[https://huggingface.co/Qwen](https://huggingface.co/Qwen)

小米开源首个原生端到端语音模型MiMo-Audio：70亿参数SOTA，首次实现语音续写与少样本泛化
==================================================

小米正式开源首个原生端到端语音模型Xiaomi-MiMo-Audio，参数规模70亿，预训练数据超1亿小时，在语音智能和音频理解基准测试中实现SOTA，多项测试超越同参数量开源模型、谷歌Gemini-2.5-Flash及OpenAI GPT-4o-Audio-Preview。该模型具备音频字幕、音频推理、长时间音频理解能力，首次在语音领域实现基于ICL的少样本泛化，预训练中“涌现”语音转换、风格迁移、语音编辑等训练数据缺失任务的应对能力，是开源领域首个有语音续写能力的语音模型。技术创新包括：首次证明语音无损压缩预训练Scaling至1亿小时可“涌现”跨任务泛化性；首个明确语音生成式预训练目标和定义并开源完整方案（含无损压缩Tokenizer、全新模型结构、训练方法和评测体系）；首个将思考同时引入语音理解和生成过程（支持混合思考）。模型支持对话、口语陪练、网络热梗学习、哲学探讨等场景，MiMo-Audio-7B-Instruct可通过提示词切换非思考/思考模式，作为语音强化学习和Agentic训练的基座模型。

*   huggingface: [https://huggingface.co/XiaomiMiMo](https://huggingface.co/XiaomiMiMo)
*   论文：[https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf](https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf)

创作者福音！ElevenLabs Studio 3.0上线：一站式AI音频视频编辑，一条时间轴搞定专业作品
=====================================================

ElevenLabs正式发布Studio 3.0一站式AI音频视频编辑器，可将旁白、音乐、音效、字幕、视频编辑整合至一条时间线，帮助创作者通过一个工具完成专业级作品。核心亮点包括从10000+声音中选择AI旁白（修改脚本即可更改音频）、Eleven Music自动生成多风格配乐、输入提示生成AI音效、语音校正（改文本替换错误录音）、语音隔离（清理噪音混响）、一键生成多语言自定义字幕、声音变声、支持MP4/MOV视频剪辑增强、多人协作（分享链接+时间戳评论）、覆盖32+语言；适用人群含视频创作者、播客主、有声书作者、AI电影人，所有功能支持API大规模集成。

*   官网介绍：[https://elevenlabs.io/studio](https://elevenlabs.io/studio)
*   体验地址：[https://elevenlabs.io/app/studio](https://elevenlabs.io/app/studio)
*   官方推文：[https://x.com/elevenlabsio/status/1968344592740434188](https://x.com/elevenlabsio/status/1968344592740434188)

通义万相Wan2.2-Animate动作生成模型开源：支持人物/动漫/动物驱动，性能超StableAnimator等
==========================================================

2025年9月19日，通义万相全新动作生成模型Wan2.2-Animate正式开源，支持驱动人物、动漫形象和动物照片，可应用于短视频创作、舞蹈模板生成、动漫制作等领域。该模型基于此前开源的Animate Anyone全面升级，支持“角色模仿”（输入角色图片和参考视频，将视频角色的动作、表情迁移至图片角色）和“角色扮演”（保留原始视频的动作、表情及环境，替换视频角色为图片角色）两种模式；训练上构建了涵盖说话、面部表情和身体动作的大规模人物视频数据集，基于通义万相图生视频模型进行后训练，通过统一表示格式实现单一模型兼容两种推理模式，针对身体运动用骨骼信号、脸部表情用隐式特征配合动作重定向模块精准复刻动作表情，替换模式中设计独立光照融合LoRA保证光照效果。性能上，在视频生成质量、主体一致性和感知损失等关键指标超越StableAnimator、LivePortrait等开源模型，人类主观评测中超越Runway Act-two代表的闭源模型，是目前性能最强的动作生成模型之一。用户可在Github、HuggingFace和魔搭社区下载模型和代码，也可在阿里云百炼平台调用API，或通义万相官网直接体验。

*   Github：[https://github.com/Wan-Video/Wan2.2](https://github.com/Wan-Video/Wan2.2)
*   huggingface：[https://huggingface.co/Wan-AI/Wan2.2-Animate-14B](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B)
*   下载地址：[https://modelscope.cn/models/Wan-AI/Wan2.2-Animate-14B](https://modelscope.cn/models/Wan-AI/Wan2.2-Animate-14B)

Notion 3.0正式上线：时隔7年的重大更新，核心引入“世界首个知识工作Agent”
============================================

2025年9月19日，Notion创始人Ivan Zhao宣布Notion 3.0正式上线，为时隔7年的重大更新（上一次2.0版本为2018年发布），核心引入**Agent功能**，定位为“内置在Notion中的AI队友”“世界上第一个知识工作Agent”。Notion 1.0为文档和知识管理的协作画布；2.0新增数据库与集成功能以管理各类工作；3.0让Notion AI学会运用基础功能，集成“上下文理解、协作、行动执行”三大能力，实现Agent完成实际工作。Agent核心能力包括：1.自主执行复杂工作流：可自主运行长达20分钟，处理几百个页面，能跨工具（Slack、邮件、Notion等）检索信息、整合结论、创建结构化内容（如数据库、报告），示例场景包括汇总客户反馈提炼可执行见解、会议纪要转化为提案/任务表/跟进消息、更新知识库、生成个性化新员工入职计划等；2.个性化记忆库：通过“指令页面”自定义Agent行为（如任务分类、回复格式、参考信息），支持编辑优化，可下载模板添加专业技能，且使用次数越多越个性化；3.即将推出自定义Agent功能，可按时间/触发条件自动运行，共享给团队（如每日用户反馈汇总、每周项目更新、IT需求分类等）。

*   示例库：[https://www.notion.com/product/ai/use-cases](https://www.notion.com/product/ai/use-cases)
*   视频合集：[https://www.youtube.com/watch?v=1NgA3YB6qAk&list=PLzaYMdbJMZW3s2mkxPpQcve380K-ipKe9&index=3](https://www.youtube.com/watch?v=1NgA3YB6qAk&list=PLzaYMdbJMZW3s2mkxPpQcve380K-ipKe9&index=3)

Chrome 17年最大变革：集成Gemini AI变智能伙伴，跨页总结、自动干活样样行
============================================

Chrome迎来2008年发布以来最大升级，集成Gemini AI转型为能主动“干活”的智能伙伴，功能涵盖Gemini助理、AI搜索模式、智能体及安全便捷工具。关键功能包括跨标签页AI总结（整合多网页/YouTube视频信息，解决行业调研等信息过载问题）、未来数月推出的智能体（结合Google生态自动完成订沙拉等重复任务）、地址栏升级（Omnibox根据页面推荐问题，处理复杂需求）、安全功能（Gemini Nano拦截诈骗网站、一键退订通知、泄漏密码一键修改）。适用范围：目前美国、英语首选的Mac/Windows用户可用，Android、iOS即将推出；此前Gemini仅Pro/Ultra用户可享，现全用户开放。

*   官方使用指南：[https://support.google.com/gemini/answer/16283624](https://support.google.com/gemini/answer/16283624)
*   谷歌博客：
    *   [https://blog.google/products/chrome/new-ai-features-for-chrome/](https://blog.google/products/chrome/new-ai-features-for-chrome/)
    *   [https://blog.google/products/chrome/chrome-reimagined-with-ai/](https://blog.google/products/chrome/chrome-reimagined-with-ai/)

百度开源Qianfan-VL企业级视觉大模型！3B/8B/70B多尺寸，支持思维链与全场景OCR
================================================

2025年9月22日，百度智能云千帆推出并全面开源面向企业级多模态应用的视觉理解大模型**Qianfan-VL**，包含3B、8B、70B三个尺寸版本。模型基于开源模型开发，在百度自研昆仑芯P800上完成全流程计算（支持单任务5000卡并行），核心特点包括多尺寸覆盖不同规模企业需求、8B/70B模型支持特殊token激活思维链能力（覆盖复杂图表理解、视觉推理、数学解题等场景）、OCR与文档理解增强（全场景OCR识别及复杂版面文档理解）。性能上，通用能力随参数提升显著（ScienceQA、RefCOCO等测试表现突出），OCR与文档理解优于主流模型（OCRBench测试），8B/70B模型数学推理成绩优异（MathVista-mini、MathVision等测试）。技术创新涵盖四阶段训练策略、高精度数据合成管线、昆仑芯P800驱动的超大规模分布式计算系统，应用场景覆盖OCR识别、数学推理、文档理解等。即日起至10月10日可在百度智能云千帆平台免费体验8B/70B模型。

*   Github：[https://github.com/baidubce/Qianfan-VL](https://github.com/baidubce/Qianfan-VL)
*   Hugging Face：[https://huggingface.co/baidu/Qianfan-VL-70B、https://huggingface.co/baidu/Qianfan-VL-8B、https://huggingface.co/baidu/Qianfan-VL-3B](https://huggingface.co/baidu/Qianfan-VL-70B%E3%80%81https://huggingface.co/baidu/Qianfan-VL-8B%E3%80%81https://huggingface.co/baidu/Qianfan-VL-3B)
*   模型Blog：[https://baidubce.github.io/Qianfan-VL/](https://baidubce.github.io/Qianfan-VL/)
*   技术报告：[https://github.com/baidubce/Qianfan-VL/blob/main/docs/qianfan\_vl\_report\_comp.pdf](https://github.com/baidubce/Qianfan-VL/blob/main/docs/qianfan_vl_report_comp.pdf)
*   体验地址：[https://console.bce.baidu.com/qianfan/ais/console/onlineTest/multimodal/Qianfan-VL-8B](https://console.bce.baidu.com/qianfan/ais/console/onlineTest/multimodal/Qianfan-VL-8B)

美团发布国内首个「深度思考+形式化推理」大模型LongCat-Flash-Thinking，开源且多项能力超闭源模型
==========================================================

美团LongCat团队正式发布全新高效推理模型LongCat-Flash-Thinking，该模型在保持LongCat-Flash-Chat极致速度的同时，成为国内首个融合「深度思考+工具调用」与「非形式化+形式化」推理能力的大语言模型，尤其在数学、代码、智能体等超高复杂度任务处理上优势显著。关键技术方面，采用领域并行强化学习训练方法解耦多领域任务优化并融合，实现能力均衡提升至帕累托最优；异步弹性共卡系统（DORA）较同步RL训练框架提速三倍，支持万卡集群稳定运行及高效KV缓存复用；智能体双路径推理框架在AIME25实测中90%准确率下Tokens从19653降至6965，节省64.5%；形式化推理框架基于集成Lean4服务器的专家迭代框架生成严格验证证明，系统性提升形式化推理能力。评测表现上，通用推理ARC-AGI获50.3分，超越OpenAI o3、Gemini2.5 Pro等闭源模型；数学能力超OpenAI o3，与Qwen3-235B-A22B-Thinking相当；代码能力LiveCodeBench达79.4分（开源SOTA，与GPT-5相当），OJBench得40.7分接近Gemini2.5-Pro；智能体τ2-Bench获74.0分刷新开源SOTA；形式化推理MiniF2F-test pass@1达67.6分大幅领先其他模型。目前该模型已在HuggingFace、Github全面开源，并提供体验地址。

*   Github：[https://github.com/meituan-longcat/LongCat-Flash-Thinking](https://github.com/meituan-longcat/LongCat-Flash-Thinking)
*   huggingface：[https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking](https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking)
*   体验地址：[https://longcat.ai/](https://longcat.ai/)

谷歌推出Deep Research新范式：用Diffusion迭代逻辑重构AI Agent研究流程，性能超越OpenAI DeepResearch
=========================================================================

谷歌推出新Deep Research范式，重新定义AI Agent研究流程，打破传统“规划-检索-生成”模式，采用类似Diffusion的迭代修正逻辑——先基于需求生成初步研究计划与粗糙草稿，再循环执行“以草稿指导检索（思考需补充信息并生成检索问题）、用检索结果优化草稿”的流程直至报告质量达标；框架包含两大优化技巧：一是每环节生成多版本输出，通过LLM打分、提修改意见并融合精华确保输出质量，二是报告级降噪，将检索新知识完美融入现有逻辑框架甚至重塑结构，该框架在GAIA等需多步推理与深度检索的任务上全面优于OpenAI DeepResearch。

*   官网介绍：[https://research.google/blog/deep-researcher-with-test-time-diffusion](https://research.google/blog/deep-researcher-with-test-time-diffusion)
*   体验地址：[https://cloud.google.com/agentspace/docs/research-assistant](https://cloud.google.com/agentspace/docs/research-assistant)

DeepSeek发布V3.1-Terminus模型：3系列架构终极版本，修复Bug优化性能并开源
================================================

2025年9月22日，DeepSeek发布DeepSeek-V3.1-Terminus模型（Terminus拉丁语意为“终点、界限”，象征该系列架构终极版本），同步更新至官方App、网页端、小程序及API并宣布开源；此次更新修复了DeepSeek-V3.1存在的语言一致性、偶发异常字符（随机输出“极”“極”“extreme”等）、多语言混用（中、英、俄等混杂）等Bug，优化了编程和搜索智能体表现；基准测试中，非Agent类较V3.1提升0.2%-36.5%（HLE人类终极测试提升最明显），Agent测评中网页浏览、简单问答、多项编程测试表现小幅提升；体验上“极”字Bug用“写Go语言”等高危提示词未复现，翻译小语种无语言混杂，编程任务（如小球弹跳动画）模拟重力自然，搜索智能体推荐阳台安全盆栽时事实无误且含风险提示；未来计划今年年底推出Agent模型。

*   huggingface: [https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus)
*   modelscope: [https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1-Terminus](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1-Terminus)
*   体验地址：[https://chat.deepseek.com/](https://chat.deepseek.com/)