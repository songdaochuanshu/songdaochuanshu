---
layout: post
title: '为什么残差结构拯救了深度神经网络？'
date: "2025-05-14T00:41:14Z"
---
为什么残差结构拯救了深度神经网络？
=================

  这段时间细细读了Yolov12的论文，被里面backbone部分吸引了，从这里展开看了很多这几年大家在backbone上的研究，觉得很有意思，在这里跟大家分享一下自己的心得。

  之前学习深度学习的时候，最早就是从图像分类入手的，当时比较流行的模型架构就是很简单的几层卷积，包括后面的VGG系列，当时还没有学习到resnet，觉得模型的结构很简单易懂，就是一层一层的堆叠。基于对卷积和池化本身的理解，我认为这样直接的堆叠确实是很有效的方案，但是后来学习到了resnet，看到了残差结构。我一度不理解为什么要这样做？为什么这解决了梯度消失和梯度爆炸的问题？后来通过一段时间的学习和实验，自己也慢慢理解了，没有一个shortcut用于特征的直连，梯度很容易在一层层的累计中，发生爆炸和消失，大致的意思如图所示。

![梯度消失和梯度爆炸](https://img2024.cnblogs.com/blog/3188981/202505/3188981-20250513132315384-783127485.jpg)

  虽然在浅层的某个语义的表达只是偏离的均值的一点点，但是随着网络的加深，这一点不同将会无限放大或者缩小，而shortcut通过前层和深层的相连，强制将越来浅层较平滑的表达传递给了深层。这是我个人，也是之前一直对残差的理解。

  但是在了解VoVNet、PRN、CSP、ELAN一系列工作之后，我发现他们使用了梯度多样性的视角去看待shortcut这个功能，如何让模型在方向传播中通过更多样的梯度信息，是提升模型学习能力，加快拟合的关键。以VGG为例，在反向传播的梯度计算中，计算图只有bottom to top的一条路线，梯度复杂度则为1，而加入残差结构后，计算图就多了一条捷径，接收的梯度复杂度则变成了2。以此类推，复杂的网络结构带来了更多样的梯度信息，这也是为什么后续的网络各种千奇百怪的结构。

  在这里，我分享这篇论文给大家，可以帮助理解backbone不同结构背后的意义。[https://openaccess.thecvf.com/content\_ICCVW\_2019/papers/LPCV/Wang\_Enriching\_Variety\_of\_Layer-Wise\_Learning\_Information\_by\_Gradient\_Combination\_ICCVW\_2019\_paper.pdf](https://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Wang_Enriching_Variety_of_Layer-Wise_Learning_Information_by_Gradient_Combination_ICCVW_2019_paper.pdf)

  那么除此之外，后续发展的shortcut结构也降低了模型的参数量和计算量，CSP结构的出现很大程度的降低计算成本，这种将特征空间通道一分为二的思想，虽然粗暴但是有效。我认为，在工业应用上，小参数模型可以针对性地解决单点问题，就可以灵活地在原模型架构上去应用shortcut和CSP的结构。

  以上都是个人见解，如有指点，欢迎评论区讨论。