---
layout: post
title: 'åŸºäº nano-vLLM å­¦ä¹ å¤§æ¨¡å‹æ¨ç†å…³é”®åŠŸèƒ½'
date: "2026-01-13T00:41:44Z"
---
åŸºäº nano-vLLM å­¦ä¹ å¤§æ¨¡å‹æ¨ç†å…³é”®åŠŸèƒ½
========================

> æ³¨ï¼šæœ¬æ–‡å·²äº2025.12.31 å‘è¡¨äºçŸ¥ä¹å’Œå…¬ä¼—å·

1\. èƒŒæ™¯
======

å¦‚æœè¦å‘ä¸€ä½å®Œå…¨ä¸äº†è§£å¤§æ¨¡å‹æ¨ç†æŠ€æœ¯çš„å¼€å‘è€…ä»‹ç»è¿™ä¸ªé¢†åŸŸï¼Œæˆ‘åº”è¯¥ä»å“ªé‡Œè®²èµ·ï¼Ÿ

å¤§æ¨¡å‹æ¨ç†çš„æœ€ç®€æµç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šè¾“å…¥ä¸€ä¸²æ–‡æœ¬ â†’ æ–‡æœ¬é€šè¿‡è¯å…¸æ˜ å°„è¡¨è½¬æ¢æˆä¸€ä¸²æ•°å­—åºå· â†’ åºå·å†ç»è¿‡ embedding å±‚çš„è®¡ç®—ï¼Œå˜æˆä¸€ç»„èƒ½ä»£è¡¨è¯­ä¹‰çš„æµ®ç‚¹æ•°å‘é‡ â†’ è¿™ç»„å‘é‡é€å…¥æ¨ç†ç³»ç»Ÿï¼Œç»è¿‡å±‚å±‚çš„çŸ©é˜µä¹˜æ³•ã€åŠ æ³•å’Œå„ç±»ä¸“ç”¨å‡½æ•°çš„è¿ç®—ï¼Œå¾—åˆ°æ–°çš„è¾“å‡ºå‘é‡ â†’ å¯¹è¾“å‡ºå‘é‡åšæ¦‚ç‡ç­›é€‰ï¼Œé€‰å‡ºæ¦‚ç‡æœ€é«˜çš„é‚£ä¸ªæ•°å€¼å¯¹åº”çš„åºå· â†’ æœ€åå†é€šè¿‡è¯å…¸æ˜ å°„è¡¨ â€œç¿»è¯‘â€ å›æ–‡å­—ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºçš„ä¸€ä¸ªè¯ã€‚

![](https://pic1.zhimg.com/80/v2-52738e1009b99b73843b74399250f825_1440w.png?source=ccfced1a)

å›¾ 1Â 

è¿™æ˜¯å¯¹å¤§æ¨¡å‹æ¨ç†æœ€æœ´ç´ çš„ç†è§£ï¼Œä¸Šè¿°æµç¨‹çœ‹ä¼¼ç®€å•ï¼Œä½†èƒŒåçš„æ¨ç†è®¡ç®—ç¯èŠ‚å¯¹æ™®é€šå¼€å‘è€…è€Œè¨€ä»æ˜¯ä¸€ä¸ª â€œé»‘ç›’â€ã€‚å¦‚æœæƒ³æ›´è¿›ä¸€æ­¥æ‹†è§£æ¨ç†å¼•æ“çš„åº•å±‚åŠ é€ŸåŸç†ï¼Œnano-vllm ä¼šæ˜¯ä¸€ä¸ªæä½³çš„å…¥é—¨åˆ‡å…¥ç‚¹ã€‚

2\. ç®€ä»‹
======

[nano-vLLM](https://github.com/GeeeekExplorer/nano-vllm) ä»£ç é‡ä»…çº¦ 1200 è¡Œï¼Œå´å®ç°äº†ç”Ÿäº§çº§æ¨ç†æ¡†æ¶çš„æ ¸å¿ƒæŠ€æœ¯åŸå‹ï¼Œå…·ä½“åŒ…æ‹¬ï¼š

*   è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰
    
*   KV ç¼“å­˜ï¼ˆPrefix KV Cache / Paged KV Cacheï¼‰
    
*   é«˜æ€§èƒ½ç¼–è¯‘ä¸æ‰§è¡Œä¼˜åŒ–ï¼ˆTorch Compilationã€Tritonã€CUDA Graphï¼‰
    
*   å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰
    

è¯¥æ¡†æ¶æå…·å…¥é—¨å­¦ä¹ ä»·å€¼ï¼Œæœ¬æ–‡å°†å…ˆä»‹ç» nano-vLLM çš„åŸºæœ¬ç»„æˆæ¶æ„ï¼Œå†å¯¹éƒ¨åˆ†æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹å±•å¼€æ·±å…¥è§£æã€‚

3\. ç³»ç»Ÿæ¶æ„
========

nano-vLLM çš„æ¶æ„éå¸¸æœ‰å±‚æ¬¡æ„Ÿã€‚

3.1. æ•´ä½“æ¶æ„æ¦‚è§ˆ
-----------

![](https://picx.zhimg.com/80/v2-41cd766bd06373d580df6302b2ba3203_1440w.png?source=ccfced1a)

å›¾ 2, æ¥è‡ªï¼šhttps://deepwiki.com/GeeeekExplorer/nano-vllm

ä¸‰å±‚ç»“æ„

*   æ¥å£å±‚ï¼šUser Interface Layer
    
*   æ¨ç†å¼•æ“ä¸­æ§å±‚ï¼šInference Engine Layer
    
*   æ˜¾å­˜ç®¡ç†å’Œæ¨¡å‹æ‰§è¡Œå±‚ï¼šMemory Management & Model Execution Layer
    

3.2. ç±»å±‚é¢æ¶æ„
----------

ä»ç±»è®¾è®¡å±‚é¢è§‚å¯Ÿ nano-vLLM çš„æ¶æ„ã€‚

![](https://pic1.zhimg.com/80/v2-8612f2dcf6e20d6dd1fccb4ebb494bd2_1440w.png?source=ccfced1a)

Â å›¾ 3

ä¸Šå›¾ä¸­å››ç§é¢œè‰²ä»£è¡¨ç³»ç»Ÿçš„å››ä¸ªç»„æˆéƒ¨åˆ†

*   æµ…è“è‰²ï¼Œå…¥å£å’Œæ¨ç†å¼•æ“ä¸­æ§å±‚
    
*   æµ…ç»¿è‰²ï¼Œæ¨¡å‹æ¨ç†
    
*   æµ…çº¢è‰²ï¼ŒKV Cache ç®¡ç†
    
*   æµ…ç´«è‰²ï¼Œæƒé‡åŠ è½½å’ŒçŸ©é˜µè®¡ç®—çš„å°è£…
    

3.3. æºç å±‚é¢åˆ’åˆ†
-----------

æºç è§„åˆ’ä¸Šä¹Ÿè¾ƒä¸ºç®€æ´ã€‚ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

nanovllm/
â”œâ”€â”€ engine
â”œâ”€â”€ layers
â”œâ”€â”€ models
â””â”€â”€ utils

*   engineï¼Œå¼•æ“çš„å…¥å£ã€ä¸­æ§ï¼ŒåŒæ—¶ KV Cache æ¯”è¾ƒç®€å•ï¼Œä»£ç ä¹Ÿæ”¾åœ¨è¿™ä¸ªç›®å½•ä¸‹ã€‚
    
*   layersï¼Œæ¨¡å‹æ¨ç†çš„é€šç”¨ç»„ä»¶ï¼Œå†…éƒ¨åŒ…æ‹¬ï¼šlinearã€layernormã€rotary\_embeddingã€attentionã€activation ç­‰åŸºç¡€åŠŸèƒ½çš„å°è£…ï¼Œå¯ä»¥è¢«ä¸åŒæ¨¡å‹ä½¿ç”¨ã€‚
    
*   modelsï¼Œæ¨¡å‹çš„å®ç°ï¼Œä¾èµ– layers çš„ç»„ä»¶å®ç°ä¸åŒæ¨¡å‹çš„æ¨ç†ã€‚
    
*   utilsï¼Œä¸åŒå±‚éƒ½å¯èƒ½ä¼šç”¨åˆ°çš„å·¥å…·å‡½æ•°ã€‚
    

4\. è¿ç»­æ‰¹å¤„ç†
=========

4.1. æ¦‚å¿µç†è§£
---------

ï¼ˆ1ï¼‰å®šä¹‰

è¿ç»­æ‰¹å¤„ç† (Continuous Batching)ï¼šæ˜¯ä¸€ç§è¿­ä»£çº§ï¼ˆIteration-levelï¼‰çš„è°ƒåº¦ç­–ç•¥ã€‚å®ƒä»¥â€œToken ç”Ÿæˆæ­¥éª¤â€ä¸ºè°ƒåº¦ç²’åº¦ã€‚é€šè¿‡åŠ¨æ€åœ°åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­æ›¿æ¢å·²å®Œæˆçš„ä»»åŠ¡ï¼Œæ¶ˆé™¤äº†ç”±äºç”Ÿæˆé•¿åº¦ä¸ä¸€å¯¼è‡´çš„ GPU è®¡ç®—æ°”æ³¡ï¼Œæå¤§åœ°æå‡äº†ç³»ç»Ÿçš„ååé‡ã€‚

ï¼ˆ2ï¼‰æœ´ç´ ç†è§£

ä¸€ä¸ªè¯·æ±‚éœ€è¦æ‰§è¡Œå¤šè½®ï¼Œä¸åŒè¯·æ±‚éœ€è¦æ‰§è¡Œçš„è½®æ•°ä¸åŒï¼Œç³»ç»Ÿä¸€è½®æœ€å¤šåªèƒ½åŒæ—¶æ‰§è¡Œä¸€æ‰¹ N ä¸ªè¯·æ±‚ï¼Œå½“ä¸€ä¸ªæ‰¹æ¬¡é‡Œçš„è¯·æ±‚å‚å·®ä¸é½çš„å®Œæˆæ—¶ï¼Œæ¯å®Œæˆä¸€ä¸ªè¯·æ±‚å°±å°†å…¶ç”¨æ–°è¯·æ±‚æ›¿ä»£æ‰ã€‚

å¯¹æ¯”ä¼ ç»Ÿæ‰¹å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†ï¼š

*   ä¼ ç»Ÿæ‰¹å¤„ç† (Static Batching)ï¼šå¿…é¡»ç­‰å¾… Batch ä¸­ç”Ÿæˆåºåˆ—æœ€é•¿çš„é‚£ä¸ªè¯·æ±‚å®Œæˆï¼Œæ•´ä¸ª Batch æ‰ä¼šé‡Šæ”¾ã€‚åœ¨æ­¤æœŸé—´ï¼Œç”Ÿæˆåºåˆ—çŸ­è¯·æ±‚å®Œæˆåæ§½ä½ä¼šç©ºè½¬ã€‚
    
*   è¿ç»­æ‰¹å¤„ç† (Continuous Batching)ï¼šè¯·æ±‚å®Œæˆå³é€€å‡ºï¼Œæ–°è¯·æ±‚ç«‹å³è¡¥ä½ï¼Œæ§½ä½å§‹ç»ˆæ»¡è½½ã€‚
    

4.2. æœ€åŸºç¡€çš„è¿ç»­æ‰¹å¤„ç†
--------------

æœ€ç®€å•çš„è¿ç»­æ‰¹å¤„ç†ï¼Œä¸è€ƒè™‘ prefill å’Œ decode çš„å·®å¼‚ï¼Œç¤ºä¾‹ä»£ç ï¼š

import time
import threading
import queue
import random

# 1. åˆå§‹åŒ–çº¿ç¨‹å®‰å…¨çš„ç­‰å¾…é˜Ÿåˆ—
waiting\_queue = queue.Queue()
MAX\_BATCH\_SIZE \= 3

# --- æ¨¡æ‹Ÿç”¨æˆ·è¯·æ±‚çº¿ç¨‹ (ç”Ÿäº§è€…) ---
def user\_request\_producer():
    request\_id \= 1
    while True:
        # æ¨¡æ‹Ÿç”¨æˆ·éšæœºåˆ°è¾¾ï¼šæ¯ 1~2 ç§’æ¥ä¸€ä¸ªæ–°è¯·æ±‚
        time.sleep(random.uniform(1, 2))
        
        # æ¯ä¸ªè¯·æ±‚éœ€è¦çš„ Token é•¿åº¦éšæœºï¼ˆ3åˆ°8ä¹‹é—´ï¼‰
        req = {"id": f"REQ-{request\_id}", "remain": random.randint(3, 8)}
        waiting\_queue.put(req)
        
        print(f"\\n\[ç”¨æˆ·ç«¯\] é€å…¥æ–°è¯·æ±‚: {req\['id'\]} (é¢„è®¡é•¿åº¦: {req\['remain'\]})")
        request\_id += 1
        if request\_id > 5:
          break

# --- æ ¸å¿ƒæ¨ç†å¾ªç¯ (æ¶ˆè´¹è€…/æ‰§è¡Œå™¨) ---
def inference\_loop():
    running\_batch \= \[\]
    
    print("\--- æ¨ç†å¼•æ“å·²å¯åŠ¨ ---")
    iteration \= 0
    while True:
        # A. è¡¥ä½é€»è¾‘ï¼šåªè¦ Batch æ²¡æ»¡ä¸”é˜Ÿåˆ—é‡Œæœ‰è´§ï¼Œå°±æ‹‰è¿›æ¥
        while len(running\_batch) < MAX\_BATCH\_SIZE:
            try:
                # ä½¿ç”¨ block=Falseï¼Œå¦‚æœé˜Ÿåˆ—ç©ºäº†ç›´æ¥æŠ¥é”™è¿› exceptï¼Œä¸é˜»å¡æ¨ç†é€»è¾‘
                new\_req = waiting\_queue.get(block=False)
                running\_batch.append(new\_req)
                print(f"  >>> \[è°ƒåº¦\] {new\_req\['id'\]} è¿›å…¥ Batch")
            except queue.Empty:
                break

        # B. æ¨ç†é€»è¾‘ï¼šå¦‚æœå½“å‰ Batch æœ‰ä»»åŠ¡ï¼Œå°±æ‰§è¡Œä¸€æ¬¡ Step
        if running\_batch:
            iteration += 1
            print("\="\*20 + f"{iteration=}" + "\="\*20)
            # æ¨¡æ‹Ÿ GPU æ¨ç†è€—æ—¶ (Step è€—æ—¶)
            time.sleep(1.2) 
            
            # å½“å‰ Batch çŠ¶æ€å±•ç¤º
            active\_ids = \[f"{r\['id'\]}(å‰©{r\['remain'\]-1})" for r in running\_batch\]
            print(f"\[GPUæ¨ç†\] å¤„ç†ä¸­: {active\_ids}")
            
            # æ¯ä¸€ä¸ªè¯·æ±‚çš„å‰©ä½™é•¿åº¦å‡ 1
            finished\_this\_step = \[\]
            for req in running\_batch:
                req\["remain"\] -= 1
                if req\["remain"\] <= 0:
                    finished\_this\_step.append(req)
            
            # C. å‰”é™¤é€»è¾‘ï¼šåšå®Œçš„ç«‹åˆ»è¸¢å‡ºï¼Œä¸‹ä¸€è½®å¾ªç¯å¼€å¤´å°±ä¼šæœ‰æ–°è¯·æ±‚è¡¥è¿›æ¥
            for req in finished\_this\_step:
                print(f"  <<< \[å®Œæˆ\] {req\['id'\]} ç”Ÿæˆå®Œæ¯•ï¼Œé‡Šæ”¾ä½ç½®")
                running\_batch.remove(req)
        else:
            # å¦‚æœ Batch å’Œ é˜Ÿåˆ—éƒ½ç©ºäº†ï¼Œç¨å¾®æ­‡ä¼šï¼Œé¿å… CPU ç©ºè½¬
            time.sleep(0.5)

# --- å¯åŠ¨ç¨‹åº ---
if \_\_name\_\_ == "\_\_main\_\_":
    # å¯åŠ¨ç”¨æˆ·è¯·æ±‚çº¿ç¨‹
    t = threading.Thread(target=user\_request\_producer, daemon=True)
    t.start()

    # ä¸»çº¿ç¨‹æ‰§è¡Œæ¨ç†å¾ªç¯
    try:
        inference\_loop()
    except KeyboardInterrupt:
        print("\\næœåŠ¡å·²åœæ­¢")

æ ¸å¿ƒé€»è¾‘ï¼š

*   å­˜å‚¨ç»“æ„ï¼šä»£ç çš„æ ¸å¿ƒæœ‰ä¸¤ä¸ªé˜Ÿåˆ—ï¼Œwaiting\_queue è´Ÿè´£å­˜å‚¨è¯·æ±‚çº¿ç¨‹ä¸æ–­æ¥æ”¶åˆ°çš„æ–°è¯·æ±‚ï¼Œrunning\_queue è´Ÿè´£å­˜å‚¨å·²ç»è¿è¡Œä½†è¿˜æ²¡æœ‰ç»“æŸçš„è¯·æ±‚ã€‚
    
*   è¿­ä»£å¾ªç¯ï¼šç”Ÿäº§è€…æŒç»­å¾€ waiting\_queue å†™å…¥æ–°è¯·æ±‚ï¼Œè¿­ä»£å¾ªç¯æŒç»­ä» waiting\_queue è·å–æ–°è¯·æ±‚åŠ å…¥åˆ° running\_queueï¼ŒåŒæ—¶æ¸…ç† running\_queue é‡Œå·²ç»å®Œæˆçš„è¯·æ±‚ã€‚
    

4.3. prefill ä¼˜å…ˆçš„è¿ç»­æ‰¹å¤„ç†
---------------------

prefill ä¼˜å…ˆçš„æ‰¹å¤„ç†ï¼Œéœ€è¦åŒºåˆ† prefll å’Œ decodeï¼Œä¼˜å…ˆå¤„ç†æ–°è¯·æ±‚ï¼Œç¤ºä¾‹ä»£ç ï¼š

import time
import queue
import random
import threading

# æ ¸å¿ƒé˜Ÿåˆ—
waiting\_queue = queue.Queue()  
running\_queue \= \[\]             

MAX\_BATCH\_SIZE \= 4

def user\_request\_producer():
    """
    ä¿®æ”¹ç‚¹ï¼šæ¨¡æ‹Ÿçˆ†å‘å¼è¯·æ±‚åˆ°è¾¾ï¼Œä»¥è§¦å‘å¤šè¯·æ±‚ Prefill
    """
    # ç¬¬ä¸€æ³¢ï¼šçˆ†å‘å¼åˆ°è¾¾ (3ä¸ªè¯·æ±‚åŒæ—¶è¿›å…¥é˜Ÿåˆ—)
    print("\\n\[ç”¨æˆ·\] --- çˆ†å‘å¼è¯·æ±‚åˆ°è¾¾ (3ä¸ªè¯·æ±‚) ---")
    for i in range(1, 4):
        req \= {"id": f"REQ-{i}", "remain": random.randint(2, 5)}
        waiting\_queue.put(req)
        print(f"\[ç”¨æˆ·\] è¯·æ±‚ {req\['id'\]} è¿›å…¥ç­‰å¾…é˜Ÿåˆ—")
    
    # å»¶è¿Ÿä¸€ä¼šå„¿ï¼Œå†æ¥ç¬¬äºŒæ³¢å•ç‚¹è¯·æ±‚
    time.sleep(5)
    print("\\n\[ç”¨æˆ·\] --- å»¶è¿Ÿè¯·æ±‚åˆ°è¾¾ (1ä¸ªè¯·æ±‚) ---")
    req \= {"id": "REQ-4", "remain": 3}
    waiting\_queue.put(req)
    print(f"\[ç”¨æˆ·\] è¯·æ±‚ {req\['id'\]} è¿›å…¥ç­‰å¾…é˜Ÿåˆ—")

def inference\_loop():
    print("\--- è¿ç»­æ‰¹å¤„ç†å¼•æ“ï¼šå¤šè¯·æ±‚ Prefill æ¨¡å¼ ---")
    iteration \= 0
    
    while True:
        current\_batch \= \[\]
        is\_prefill\_stage \= False
        
        # 1. è°ƒåº¦ï¼šæ„å»ºå½“å‰æ‰¹æ¬¡
        # åªè¦ waiting\_queue éç©ºï¼Œå°±å°½å¯èƒ½å¡«æ»¡ MAX\_BATCH\_SIZE
        if not waiting\_queue.empty():
            is\_prefill\_stage \= True
            while not waiting\_queue.empty() and len(current\_batch) < MAX\_BATCH\_SIZE:
                req \= waiting\_queue.get()
                current\_batch.append(req)
        elif running\_queue:
            is\_prefill\_stage \= False
            current\_batch \= list(running\_queue)
        
        if not current\_batch:
            time.sleep(0.5)
            continue

        # 2. æ‰§è¡Œï¼šæ¨¡æ‹Ÿæ¨ç†
        iteration += 1
        print(f"\\n{'='\*15} Iteration {iteration} {'='\*15}")
        
        if is\_prefill\_stage:
            print(f"\[PREFILL\] æ‰¹é‡ç”Ÿæˆä¸­: {\[r\['id'\] for r in current\_batch\]}")
            time.sleep(1.5) 
        else:
            print(f"\[DECODE \] æ‰¹é‡ç”Ÿæˆä¸­: {\[f'{r\['id'\]}(å‰©{r\['remain'\]})' for r in current\_batch\]}")
            time.sleep(0.4) 

        # 3. ç»Ÿä¸€çŠ¶æ€æ›´æ–°
        for req in current\_batch:
            req\['remain'\] -= 1

        # 4. ç»Ÿä¸€åˆ¤æ–­ç”Ÿå‘½å‘¨æœŸ
        # æ³¨æ„ï¼šä¸ºäº†é¿å…åœ¨éå†åˆ—è¡¨æ—¶åˆ é™¤å…ƒç´ ï¼Œæˆ‘ä»¬å…ˆæ”¶é›†è¦åˆ é™¤çš„å¯¹è±¡
        to\_remove\_from\_running = \[\]
        
        for req in current\_batch:
            if req\['remain'\] <= 0:
                print(f"  <<< \[å®Œæˆ\] {req\['id'\]} é€€å‡ºç³»ç»Ÿ")
                if req in running\_queue:
                    to\_remove\_from\_running.append(req)
            else:
                if is\_prefill\_stage:
                    running\_queue.append(req)
                    print(f"  -> {req\['id'\]} Prefill å®Œæˆï¼Œè½¬å…¥ running\_queue")
                else:
                    pass
        
        # çœŸæ­£çš„ä» running\_queue ç§»é™¤
        for req in to\_remove\_from\_running:
            running\_queue.remove(req)

if \_\_name\_\_ == "\_\_main\_\_":
    t \= threading.Thread(target=user\_request\_producer, daemon=True)
    t.start()
    try:
        inference\_loop()
    except KeyboardInterrupt:
        pass

å åŠ ä¸Š prefill ä¼˜å…ˆä¹‹åçš„è¿ç»­æ‰¹å¤„ç†ä»£ç ä¹Ÿè¾ƒä¸ºç®€å•ï¼Œä¸»è¦æ˜¯ç»´æŠ¤ä¸‰ä¸ªå˜é‡ï¼šwaiting\_queueã€running\_queueã€current\_batchã€‚

5\. KV Cache
============

5.1. æ¦‚å¿µç†è§£
---------

### 5.1.1. KV Cache çš„ç”¨é€”

KV Cache æœ‰ä¸¤å±‚ç”¨é€”ã€‚ä¸€æ˜¯ç”¨åœ¨åŒä¸€ä¸ªè¯·æ±‚çš„ Decode é˜¶æ®µï¼Œå¤ç”¨ä¹‹å‰å·²ç»è®¡ç®—è¿‡çš„ KV ç»“æœä»¥é¿å…é‡å¤è®¡ç®—ï¼›äºŒæ˜¯ç”¨åœ¨ä¸åŒè¯·æ±‚ä¹‹é—´ï¼Œä½¿å…·æœ‰ç›¸åŒå‰ç¼€çš„è¯·æ±‚å¯ä»¥å…±äº«ä¸€éƒ¨åˆ† KV æ•°æ®ï¼Œè¿™å°±æ˜¯ Prefix KV Cacheã€‚

### 5.1.2. PagedAttention æŠ€æœ¯

åœ¨ Cache çš„å­˜å‚¨å±‚é¢ï¼ŒPagedAttention å®ç°äº†æ˜¾å­˜çš„æŒ‰éœ€ç”³è¯·ã€‚ç”±äº KV Cache ç©ºé—´ä¸å†ä¸€æ¬¡æ€§é¢„åˆ†é…ï¼Œè¯·æ±‚åºåˆ—å¯¹åº”çš„ç‰©ç†åœ°å€æ˜¯ç¦»æ•£çš„ã€‚PagedAttention çš„æ ¸å¿ƒåœ¨äºï¼Œå®ƒèƒ½å¤Ÿç›´æ¥è¯»å–è¿™äº›ç‰©ç†ç¦»æ•£çš„å—æ¥å®Œæˆæ³¨æ„åŠ›è®¡ç®—ï¼Œè¿™èƒŒåå®ç°äº†ä¸€å±‚ä»â€œé€»è¾‘è¿ç»­åœ°å€â€åˆ°â€œç‰©ç†ç¦»æ•£åœ°å€â€çš„æ˜ å°„ã€‚

å¯¹äºæ²¡æœ‰æ¥è§¦è¿‡é PagedAttention å®ç°çš„è¯»è€…æ¥è¯´ï¼Œè¿™ç§è®¾è®¡ä¼¼ä¹ç†æ‰€å½“ç„¶ï¼šæŒ‰éœ€ç”³è¯·ã€åˆ†é¡µç®¡ç†ã€åœ°å€æ˜ å°„ã€å±€éƒ¨æ€§åŸç†â€”â€”è¿™äº›éƒ½æ˜¯è®¡ç®—æœºç§‘å­¦ä¸­éå¸¸å¸¸è§„çš„æ€ç»´ï¼Œç”šè‡³å¾ˆéš¾æƒ³åˆ°ä¸è¿™ä¹ˆå†™çš„ç†ç”±ã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆ PagedAttention ä¼šè¢«è®¤ä¸ºæ˜¯ä¸€é¡¹é‡Œç¨‹ç¢‘å¼çš„å…ˆè¿›æŠ€æœ¯å‘¢ï¼Ÿ

é¦–å…ˆï¼Œåœ¨ PagedAttention å‡ºç°ä¹‹å‰ï¼Œä¸šç•Œæ™®éè®¤ä¸º KV Cache åœ¨æ˜¾å­˜ä¸­å¿…é¡»ç‰©ç†è¿ç»­ï¼Œå¦åˆ™ä¼šå› è®¿å­˜ä¸è¿ç»­å¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚å…¶æ¬¡ï¼Œå½“æ—¶çš„æ³¨æ„åŠ›ç®—å­ï¼ˆå¦‚æ ‡å‡†çš„ FlashAttentionï¼‰å¹¶ä¸æ”¯æŒäºŒæ¬¡å¯»å€æ˜ å°„ã€‚PagedAttention è¯æ˜äº†å³ä¾¿ç‰©ç†å­˜å‚¨ä¸è¿ç»­ï¼Œæ€§èƒ½ä¾ç„¶å¯ä»¥ä¿æŒæé«˜ã€‚å…¶ä»£ç å®ç°æœ€å…³é”®çš„ç‚¹åœ¨äºé‡æ„äº† CUDA å†…æ ¸ï¼Œä½¿å…¶åŸç”Ÿæ”¯æŒ KV Cache äºŒæ¬¡å¯»å€ã€‚ä¸€ä¸ªåºåˆ—çš„ KV Cache ä¸éœ€è¦ç‰©ç†è¿ç»­ï¼Œä¹Ÿæ­£æ˜¯ä¸åŒåºåˆ—é—´èƒ½å¤Ÿçµæ´»å¤ç”¨ Prefix KV Cache çš„æŠ€æœ¯å‰æã€‚

æ€»çš„æ¥è¯´ï¼Œè™½ç„¶åˆ†é¡µè™šæ‹Ÿå†…å­˜åœ¨ CPU é¢†åŸŸæ˜¯å¸¸è¯†ï¼Œä½†åœ¨ GPU ç®—å­é¢†åŸŸå…¶å‘å±•ç›¸å¯¹ç¼“æ…¢ã€‚å®ç°ä¸€å¥—æ—¢èƒ½åˆ†é¡µç®¡ç†ã€åˆä¸æŸå¤±ç®—åŠ›åˆ©ç”¨ç‡çš„ Attention Kernel æ˜¯ PagedAttention çš„æ ¸å¿ƒæ‰€åœ¨ã€‚

5.2. Prefix KV Cache çš„å®ç°
------------------------

Cache çš„ç®¡ç†è¾ƒä¸ºç®€å•ï¼Œåªæœ‰ BlockManager ç±»ï¼Œè´Ÿè´£ç»´æŠ¤æ˜¾å­˜æ± å„ä¸ª block çš„çš„çŠ¶æ€ã€‚

### 5.2.1. åŠŸèƒ½ç»†èŠ‚

*   ä½¿ç”¨ hash æ¥è¯†åˆ«æ˜¯å¦æœ‰å¯å¤ç”¨å‰ç¼€ï¼Œä»¥ block ä¸ºåŸºæœ¬å•å…ƒ
    
*   é“¾å¼ hashï¼Œæ¯ä¸ª block çš„ hash è®¡ç®—è¾“å…¥ä¸ºå‰åº block çš„ hash å€¼åŠ ä¸Šæœ¬ block çš„ token id
    
*   æ¯ä¸€ä¸ª block æœ‰å¯¹åº”çš„ meta ä¿¡æ¯å¯¹è±¡ï¼Œè®°å½• block è¢«å¤ç”¨çš„å¼•ç”¨è®¡æ•°ï¼Œç¡®ä¿å¤ç”¨æ—¶ä¸ä¼šè¢«é‡Šæ”¾
    
*   ä¸ºé¿å… hash ç¢°æ’å‡ºç°é”™è¯¯ï¼Œblock meta ä¿¡æ¯è¿˜éœ€è¦è®°å½•åŸå§‹çš„ token id
    
*   åœ¨è·å– KV Cache ç©ºé—´æ—¶éœ€è¦è€ƒè™‘æ˜¯å¦è·¨ block
    

### 5.2.2. å†…å­˜æ± 

åœ¨è¿›ç¨‹å¯åŠ¨æ—¶ï¼Œä¸€æ¬¡æ€§ç”³è¯·å†…å­˜æ± çš„ç©ºé—´ï¼š

kv\_cache = torch.empty(
    2,                          # K å’Œ V
    num\_layers,                 # å±‚æ•°
    num\_blocks,                 # æ€»å—æ•°
    block\_size,                 # æ¯å— token æ•°
    num\_kv\_heads // tp\_size,    # KV head æ•°ï¼ˆè€ƒè™‘å¼ é‡å¹¶è¡Œï¼‰
    head\_dim                    # head ç»´åº¦
)

ä¸Šè¿°ç”³è¯·æ˜¾å­˜ä»£ç ä¸­çš„ num\_blocks æ˜¯æ ¹æ®å¯ç”¨äº KV Cache çš„æ˜¾å­˜ç®—å‡ºæ¥çš„ï¼š

block\_bytes = 2 \* hf\_config.num\_hidden\_layers \* self.block\_size \* num\_kv\_heads \* head\_dim \* hf\_config.torch\_dtype.itemsize
config.num\_kvcache\_blocks \= int(total \* config.gpu\_memory\_utilization - used - peak + current) // block\_bytes

ä¸Šè¿°ä»£ç ä¸­çš„ block\_bytes å¹¶ä¸æ˜¯æŒ‡ block çš„å¤§å°ï¼Œè€Œæ˜¯æŠŠè®¡ç®— blocks æ•°çš„æ‰€æœ‰é™¤æ•°ä¹˜åˆ°äº†ä¸€èµ·ï¼Œé™¤æ•°åŒ…æ‹¬ï¼šblock å¤§å°ã€k å’Œ vã€æ¨¡å‹å±‚æ•°ã€‚

total \* config.gpu\_memory\_utilization - used - peak + current è¿™éƒ¨åˆ†åˆ™æ˜¯æ ¹æ®æœ€é«˜çš„æ˜¾å­˜åˆ©ç”¨ç‡ç®—å‡ºæ¥å¯ç”¨æ˜¾å­˜ï¼Œå‡å»å½“å‰æ¨¡å‹åŠ è½½å®Œåä½¿ç”¨äº†çš„éƒ¨åˆ†ï¼Œå†å‡å»æ¨¡å‹é¢„çƒ­æ—¶ä½¿ç”¨çš„æ¿€æ´»æ˜¾å­˜ï¼špeak - currentã€‚

ç”³è¯·åˆ°å†…å­˜æ± åï¼ŒæŒ‰å±‚å…±äº«è§†å›¾ç»™å„ä¸ªå±‚çš„ Attention å¯¹è±¡ï¼Œä»£ç çœ‹èµ·æ¥æ¯”è¾ƒ trickyï¼Œä½†åœ¨ python é‡Œå€’æ¯”è¾ƒå¸¸è§ï¼š

for module in self.model.modules():
    if hasattr(module, "k\_cache") and hasattr(module, "v\_cache"):
        module.k\_cache \= self.kv\_cache\[0, layer\_id\]
        module.v\_cache \= self.kv\_cache\[1, layer\_id\]
        layer\_id += 1

éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰ nn.Module å­æ¨¡å—ï¼Œé€šè¿‡æ£€æŸ¥æ˜¯å¦å­˜åœ¨ k\_cache å’Œ v\_cache å±æ€§æ¥è¯†åˆ« Attention å±‚ã€‚å¯¹äºæ¯ä¸ª Attention å±‚ï¼Œå°†å…¶ k\_cache å’Œ v\_cache å±æ€§æ›¿æ¢ä¸ºæŒ‡å‘å…¨å±€ KV Cache æ˜¾å­˜æ± çš„å¼ é‡è§†å›¾ï¼Œè¿™æ ·æ‰€æœ‰å±‚å…±äº«åŒä¸€å—è¿ç»­çš„æ˜¾å­˜ç©ºé—´ï¼Œä½†æ¯å±‚åªèƒ½è®¿é—®è‡ªå·±å¯¹åº”çš„åˆ‡ç‰‡ã€‚

### 5.2.3. KV Cache å†™å…¥

åœ¨ attention å­å±‚çš„ forward å‰åš KV Cache çš„å†™å…¥ï¼Œä½¿ç”¨çš„ store\_kvcache\_kernel å‡½æ•°æ˜¯ triton.jit å®ç°çš„ï¼Œä»£ç ä¹Ÿæ¯”è¾ƒç®€æ´ï¼š

@triton.jit
def store\_kvcache\_kernel(
    key\_ptr,
    key\_stride,
    value\_ptr,
    value\_stride,
    k\_cache\_ptr,
    v\_cache\_ptr,
    slot\_mapping\_ptr,
    D: tl.constexpr,
):
    idx \= tl.program\_id(0)
    slot \= tl.load(slot\_mapping\_ptr + idx)
    if slot == -1: return
    key\_offsets \= idx \* key\_stride + tl.arange(0, D)
    value\_offsets \= idx \* value\_stride + tl.arange(0, D)
    key \= tl.load(key\_ptr + key\_offsets)
    value \= tl.load(value\_ptr + value\_offsets)
    cache\_offsets \= slot \* D + tl.arange(0, D)
    tl.store(k\_cache\_ptr + cache\_offsets, key)
    tl.store(v\_cache\_ptr + cache\_offsets, value)

def store\_kvcache(key: torch.Tensor, value: torch.Tensor, k\_cache: torch.Tensor, v\_cache: torch.Tensor, slot\_mapping: torch.Tensor):
    N, num\_heads, head\_dim \= key.shape
    D \= num\_heads \* head\_dim
    assert key.stride(-1) == 1 and value.stride(-1) == 1
    assert key.stride(1) == head\_dim and value.stride(1) == head\_dim
    assert k\_cache.stride(1) == D and v\_cache.stride(1) == D
    assert slot\_mapping.numel() == N
    store\_kvcache\_kernel\[(N,)\](key, key.stride(0), value, value.stride(0), k\_cache, v\_cache, slot\_mapping, D)

ä½¿ç”¨ stride å‡½æ•°æ¥ç¡®è®¤æ˜¾å­˜æ˜¯å¦è¿ç»­ï¼Œå› ä¸ºåœ¨ store\_kvcache\_kernel çš„å®ç°é‡Œä¼šæŒ‰ç…§æ˜¾å­˜è¿ç»­æ¥è¯»å–æŒ‡å®šä½ç½®çš„å€¼ã€‚

æ˜¾å­˜è¿ç»­å’Œä¸è¿ç»­çš„ä¾‹å­ï¼š

import torch

key \= torch.randn(2, 3, 4)
print(key.stride())  
key\_t \= key.transpose(1, 2)
print("è½¬ç½®åï¼ˆéè¿ç»­ï¼‰:", key\_t.stride())  
key\_t \= key\_t.contiguous()  # é‡æ–°åˆ†é…ï¼Œä½¿å…¶è¿ç»­
print("contiguous å:", key\_t.stride())

# è¾“å‡ºï¼š
# (12, 4, 1)
# è½¬ç½®åï¼ˆéè¿ç»­ï¼‰: (12, 1, 4)
# contiguouså: (12, 3, 1)

6\. cuda graph
==============

6.1. æ¦‚å¿µç†è§£
---------

CUDA Graph æ˜¯ä¸€ç§å°†ä¸€ç³»åˆ— CUDA æ“ä½œå½•åˆ¶æˆå›¾çš„æŠ€æœ¯ï¼Œåœ¨é‡å¤æ‰§è¡Œçš„å›ºå®šæ“ä½œåºåˆ—åœºæ™¯ä¸‹å¯ä»¥æ˜¾è‘—æå‡æ¨ç†æ€§èƒ½ï¼Œä¸»è¦åŸºäºè¿™å‡ æ–¹é¢ï¼š

*   å‡å°‘ CPU ä¸ GPU ä¹‹é—´çš„é¢‘ç¹åŒæ­¥å’ŒæŒ‡ä»¤ä¸‹å‘å¼€é”€ï¼Œé™ä½ä¼ ç»Ÿç‹¬ç«‹æ“ä½œå¸¦æ¥çš„æ§åˆ¶æµäº¤äº’æŸè€—ï¼›
    
*   å‡å°‘æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ CPU å¹²é¢„ï¼ŒGPU è‡ªä¸»æ‰¹é‡æ‰§è¡Œå›¾å†…æ“ä½œï¼Œæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡ï¼Œé™ä½å»¶è¿Ÿã€æå‡ååã€‚
    
*   è§„é¿å¤šæ¬¡ç‹¬ç«‹ CUDA Kernel çš„å¯åŠ¨å›ºå®šå¼€é”€ï¼Œå¤šä¸ª Kernel æ‰“åŒ…åä»…éœ€ä¸€æ¬¡è°ƒåº¦è§¦å‘ï¼Œå¤§å¹…æå‡å° Kernel å¯†é›†åœºæ™¯çš„æ‰§è¡Œæ•ˆç‡ï¼›
    
*   å¯é…åˆæ˜¾å­˜æ± å®ç°æ˜¾å­˜èµ„æºå¤ç”¨ï¼Œå‡å°‘ â€œå°‘é‡å¤šæ¬¡â€ æ˜¾å­˜ç”³è¯· / é‡Šæ”¾çš„å¼€é”€ï¼ŒåŒæ—¶é©±åŠ¨ä¼šåŸºäºå›¾å†…æ˜¾å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–å¸¦å®½åˆ©ç”¨ç‡ï¼›
    
*   CUDA é©±åŠ¨å¯è·å–æ“ä½œåºåˆ—çš„å…¨å±€è§†å›¾ï¼ŒåŸºäºå®Œæ•´çš„ä¾èµ–å…³ç³»è¿›è¡Œå…¨å±€ä¼˜åŒ–ï¼ˆå¦‚ Kernel é¡ºåºè°ƒæ•´ã€èµ„æºåˆå¹¶ç­‰ï¼‰ï¼›
    

6.2. åŠŸèƒ½ç»†èŠ‚
---------

*   å½•åˆ¶æ—¶ä½¿ç”¨çš„å¼ é‡å†…å­˜åœ°å€ï¼Œåœ¨é‡æ”¾æ—¶å¿…é¡»ä¿æŒä¸å˜ï¼Œä¹Ÿå°±æ˜¯åé¢å¤šæ¬¡ replay éƒ½ä¼šä½¿ç”¨æ•è·æ—¶ç”³è¯·çš„å˜é‡ç©ºé—´
    
*   æ•è·åçš„ graph å¯¹è±¡è®°å½•åœ¨æˆå‘˜å˜é‡é‡Œï¼Œä¾›ä¸‹æ¬¡æ¨ç†æ—¶é€‰æ‹©
    
*   é‡æ”¾æ—¶é€‰æ‹©æ¯”è¯·æ±‚ batch size å¤§çš„æœ€å° graph batch size
    
*   æ•è·æ—¶ä¸åŒçš„ batch size å…±äº«ç›¸åŒçš„é™æ€æ˜¾å­˜ç©ºé—´ï¼Œå¹¶è®©å¤šä¸ªæ‰¹æ¬¡å…±äº«æ˜¾å­˜æ± ï¼Œä½¿å¾—è™½ç„¶æœ‰å¤šä¸ª batch sizeï¼Œä½†åªä¼šä½¿ç”¨ Max Batch Size çš„æ˜¾å­˜ç©ºé—´
    

6.3. ç¤ºä¾‹ä»£ç 
---------

import torch
import torch.nn as nn

# 1. åŸºç¡€é…ç½®
device = "cuda"
D \= 512                 # ç»´åº¦
graph\_bs = \[1, 8, 32\]   # é¢„å®šä¹‰çš„æ¡¶ï¼ˆåˆ†æ¡¶å°ºå¯¸ï¼‰
NUM\_LAYERS = 100        # ææ·±æ¨¡å‹ï¼Œå¢åŠ  Kernel æ•°é‡ä»¥æ”¾å¤§ Graph ä¼˜åŠ¿
iters = 50              # æ€§èƒ½æµ‹è¯•è¿­ä»£æ¬¡æ•°
max\_bs = max(graph\_bs)

# 2. å®šä¹‰æ·±å±‚æ¨¡å‹ (äº§ç”Ÿçº¦ 400 ä¸ª Kernel)
class UltraDeepModel(nn.Module):
    def \_\_init\_\_(self):
        super().\_\_init\_\_()
        self.blocks \= nn.ModuleList()
        for \_ in range(NUM\_LAYERS):
            block \= nn.ModuleDict({
                'ln': nn.LayerNorm(D).to(device),
                'linear': nn.Linear(D, D).to(device),
                'act': nn.ReLU()
            })
            self.blocks.append(block)

    def forward(self, x):
        for block in self.blocks:
            identity \= x
            x \= block\['ln'\](x)
            x \= block\['linear'\](x)
            x \= block\['act'\](x)
            x \= x + identity 
        return x

model \= UltraDeepModel().eval()

# 3. é™æ€ç¼“å†²åŒºå‡†å¤‡
static\_input = torch.empty(max\_bs, D, device=device)
static\_output \= torch.empty(max\_bs, D, device=device)

graphs \= {}
graph\_pool \= None

# 4. å½•åˆ¶é˜¶æ®µ (ä»å¤§åˆ°å°ï¼Œå…±äº«å†…å­˜æ± )
print(f"\--- å¼€å§‹å½•åˆ¶åˆ†æ¡¶ CUDA Graphs ---")
for bs in reversed(sorted(graph\_bs)):
    current\_input \= static\_input\[:bs\]
    # Warmup
    for \_ in range(5):
        \_ \= model(current\_input)
    
    g \= torch.cuda.CUDAGraph()
    with torch.cuda.graph(g, pool\=graph\_pool):
        static\_output\[:bs\] \= model(current\_input)
    
    if graph\_pool is None:
        graph\_pool \= g.pool()
    graphs\[bs\] \= g
    print(f"âœ… å·²å½•åˆ¶æ¡¶ BS={bs}")

# 5. è¾…åŠ©å‡½æ•°ï¼šæ ¹æ®å®é™… BS åŒ¹é…æœ€è¿‘çš„æ¡¶
def get\_bucket\_bs(actual\_bs):
    for b in sorted(graph\_bs):
        if actual\_bs <= b:
            return b
    return None

# 6. æ€§èƒ½å¯¹æ¯”æµ‹è¯• (åŒ…å« Padding é€»è¾‘)
def benchmark(actual\_test\_bs=7):
    print(f"\\n--- æ€§èƒ½æµ‹è¯•å¼€å§‹: å®é™…è¯·æ±‚ BS={actual\_test\_bs} ---")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test\_data = torch.randn(actual\_test\_bs, D, device=device)
    
    start\_event \= torch.cuda.Event(enable\_timing=True)
    end\_event \= torch.cuda.Event(enable\_timing=True)

    # --- æ®µè½ A: Standard Eager Mode (ç›´æ¥è·‘ 7 ä¸ª) ---
    torch.cuda.nvtx.range\_push("Eager\_Mode")
    start\_event.record()
    for \_ in range(iters):
        \_ \= model(test\_data)
    end\_event.record()
    torch.cuda.synchronize()
    eager\_time \= start\_event.elapsed\_time(end\_event)
    torch.cuda.nvtx.range\_pop()

    # --- æ®µè½ B: CUDA Graph Mode (Padding å¯¹é½åˆ° 8) ---
    # 1. è·¯ç”±é€»è¾‘
    bucket\_bs = get\_bucket\_bs(actual\_test\_bs)
    if bucket\_bs is None:
        print(f"âŒ é”™è¯¯: å®é™… BS={actual\_test\_bs} è¶…è¿‡äº†æœ€å¤§åˆ†æ¡¶ {max\_bs}")
        return

    torch.cuda.nvtx.range\_push(f"Graph\_Mode\_Bucket\_{bucket\_bs}")
    
    # 2. æ•°æ®å¯¹é½ (Padding): å°† 7 æ¡æ•°æ®æ‹·å…¥ 8 çš„é™æ€åŒºåŸŸ
    # static\_input çš„å‰ 7 è¡Œè¢«è¦†ç›–ï¼Œç¬¬ 8 è¡Œä¿æŒä¸å˜ï¼ˆå³ Padding ä½ï¼‰
    static\_input\[:actual\_test\_bs\].copy\_(test\_data)
    
    start\_event.record()
    for \_ in range(iters):
        # 3. é‡æ”¾åˆ†æ¡¶ 8 çš„å›¾
        graphs\[bucket\_bs\].replay()
    end\_event.record()
    torch.cuda.synchronize()
    graph\_time \= start\_event.elapsed\_time(end\_event)
    
    # 4. ç»“æœæˆªæ–­ (Slicing): ä»é™æ€åŒºæ‹¿å›å‰ 7 æ¡
    final\_res = static\_output\[:actual\_test\_bs\]
    torch.cuda.nvtx.range\_pop()

    # æ‰“å°ç»“æœ
    print(f"åŒ¹é…åˆ°çš„æ¡¶: {bucket\_bs} (Padding æµªè´¹ç‡: {(bucket\_bs-actual\_test\_bs)/bucket\_bs\*100:.1f}%)")
    print(f"{'Mode':<20} | {'Avg Time (ms)':<15}")
    print("\-" \* 40)
    print(f"{'Eager Mode':<20} | {eager\_time/iters:>15.4f}")
    print(f"{'Graph Mode':<20} | {graph\_time/iters:>15.4f}")
    print("\-" \* 40)
    print(f"ğŸš€ åŠ é€Ÿæ¯”: {eager\_time/graph\_time:.2f}x")
    print(f"æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {final\_res.shape}")

if \_\_name\_\_ == "\_\_main\_\_":
    # æµ‹è¯•ä¸åŒçš„è¾“å…¥ BS
    benchmark(actual\_test\_bs=7)  # è§¦å‘å¯¹é½åˆ° 8
    benchmark(actual\_test\_bs=1)  # ç²¾ç¡®åŒ¹é…åˆ° 1

æ‰§è¡Œï¼šnsys profile --trace=cuda,osrt,nvtx python3 cu2.py

è¾“å‡ºï¼š

Collecting data...
\--- å¼€å§‹å½•åˆ¶åˆ†æ¡¶ CUDA Graphs ---
âœ… å·²å½•åˆ¶æ¡¶ BS\=32
âœ… å·²å½•åˆ¶æ¡¶ BS\=8
âœ… å·²å½•åˆ¶æ¡¶ BS\=1

--- æ€§èƒ½æµ‹è¯•å¼€å§‹: å®é™…è¯·æ±‚ BS=7 ---
åŒ¹é…åˆ°çš„æ¡¶: 8 (Padding æµªè´¹ç‡: 12.5%)
Mode                 | Avg Time (ms)  
\----------------------------------------
Eager Mode           |          7.9331
Graph Mode           |          1.0136
----------------------------------------
ğŸš€ åŠ é€Ÿæ¯”: 7.83x
æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: torch.Size(\[7, 512\])

\--- æ€§èƒ½æµ‹è¯•å¼€å§‹: å®é™…è¯·æ±‚ BS=1 ---
åŒ¹é…åˆ°çš„æ¡¶: 1 (Padding æµªè´¹ç‡: 0.0%)
Mode                 | Avg Time (ms)  
\----------------------------------------
Eager Mode           |          8.1803
Graph Mode           |          0.8011
----------------------------------------
ğŸš€ åŠ é€Ÿæ¯”: 10.21x
æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: torch.Size(\[1, 512\])

æŸ¥çœ‹ nsysï¼š

![](https://picx.zhimg.com/80/v2-fa4eefbebda0472fa1f21d8bd01e4c17_1440w.png?source=ccfced1a)

å›¾ 4

å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ cuda graph çš„æ—¶å€™ï¼ŒSM ä½¿ç”¨æ›´å……åˆ†ã€‚

6.4. Q&A
--------

ï¼ˆ1ï¼‰ä¸ºä»€ä¹ˆæ¨ç†æ—¶ cuda graph çš„é€‰æ‹©è¦é‡‡ç”¨å‘ä¸Šå¯¹é½çš„åˆ†æ¡¶ç­–ç•¥ï¼Œå³ï¼šæ‰¹æ¬¡ç›¸ç­‰æˆ–ç¨å¤§çš„ï¼Œè€Œä¸æ˜¯é€‰æ‹©æ‰¹æ¬¡æœ€å¤§çš„ï¼Ÿ

è™½ç„¶åœ¨å»ºå›¾ï¼ˆCaptureï¼‰é˜¶æ®µï¼Œç³»ç»Ÿä¼šæŒ‰ç…§æœ€å¤§æ‰¹æ¬¡ï¼ˆMax Batch Sizeï¼‰é¢„å…ˆç”³è¯·å¹¶é”å®šé™æ€æ˜¾å­˜ç©ºé—´ï¼Œæ­¤æ—¶å³ä¾¿é€‰æ‹©æœ€å¤§æ‰¹æ¬¡æ‰§è¡Œä¹Ÿä¸ä¼šäº§ç”Ÿé¢å¤–çš„æ˜¾å­˜å®¹é‡æµªè´¹ï¼Œä½†ä¼šå¼•å…¥ä»¥ä¸‹ä¸¤ä¸ªç»´åº¦çš„æ€§èƒ½æŸè€—ï¼š

æ˜¾å­˜å¸¦å®½çš„æ— æ•ˆå ç”¨ï¼Œå¤§æ¨¡å‹æ¨ç†ï¼ˆå°¤å…¶æ˜¯ Decoding é˜¶æ®µï¼‰å±äºå…¸å‹çš„è®¿å­˜å¯†é›†å‹ä»»åŠ¡ï¼Œå…¶ç“¶é¢ˆåœ¨äºæ¨¡å‹æƒé‡ä»æ˜¾å­˜åˆ°è®¡ç®—å•å…ƒçš„æ¬è¿é€Ÿåº¦ã€‚å³ä¾¿å¤§éƒ¨åˆ†æ‰¹æ¬¡ä½ç½®æ˜¯ Paddingï¼ˆç©ºæ•°æ®ï¼‰ï¼ŒCUDA Graph ä¾ç„¶ä¼šä¸¥æ ¼æ‰§è¡Œå½•åˆ¶æ—¶çš„å†…å­˜å¯»å€å®šä¹‰ï¼Œæ¬è¿å®Œæ•´æ‰¹æ¬¡çš„æ•°æ®ã€‚ä½¿ç”¨è¿‡å¤§çš„æ‰¹æ¬¡ä¼šå¯¼è‡´ GPU æµªè´¹æå…¶å®è´µçš„å¸¦å®½å»æ¬è¿â€œæ— æ•ˆæ•°æ®â€ï¼Œä»è€Œå¢åŠ å•æ¬¡æ¨ç†çš„è€—æ—¶ï¼Œæ¨é«˜æ¨ç†å»¶è¿Ÿï¼ˆLatencyï¼‰ã€‚

è®¡ç®—èµ„æºçš„æ— æ•ˆå ç”¨ï¼ŒGPU è°ƒåº¦å™¨ä¼šæ ¹æ®å›¾å®šä¹‰çš„è§„æ¨¡é¢„åˆ†é…ç¡¬ä»¶èµ„æºï¼ˆå¦‚ SM æ ¸å¿ƒã€å¯„å­˜å™¨ã€å…±äº«æ˜¾å­˜ç­‰ï¼‰ã€‚è™½ç„¶ Padding éƒ¨åˆ†çš„è®¡ç®—é€»è¾‘æå¿«ï¼Œä½†è¿™äº›èµ„æºåœ¨æ•´ä¸ª CUDA Graph æ‰§è¡Œå®Œæˆå‰æ— æ³•è¢«é‡Šæ”¾ã€‚è¿™ä¼šå¯¼è‡´ GPU ç¡¬ä»¶å¤„äºâ€œè™šå‡ç¹å¿™â€çŠ¶æ€ï¼Œé˜»å¡äº†å…¶ä»–æ½œåœ¨ä»»åŠ¡ï¼ˆå¦‚å¤šæµå¹¶è¡Œç­‰ï¼‰è·å–ç¡¬ä»¶èµ„æºï¼Œå‰Šå¼±äº†ç³»ç»Ÿæ•´ä½“çš„å¹¶å‘ååèƒ½åŠ›ï¼ˆThroughputï¼‰ã€‚

7\. Torch Compilation
=====================

7.1. æ¦‚å¿µç†è§£
---------

torch.compile èƒ½å¤Ÿå°† PyTorch å¼ é‡è®¡ç®—ç›¸å…³çš„ Python é€»è¾‘ï¼Œè½¬åŒ–ä¸ºæ›´é«˜æ•ˆçš„ä¸­é—´è¡¨ç¤ºï¼ˆåœ¨ CUDA è®¾å¤‡ä¸Šï¼Œé€šå¸¸æ˜¯ Triton å†…æ ¸ä»£ç ï¼Œä¹Ÿæ”¯æŒåŸç”Ÿ CUDA å†…æ ¸ï¼‰ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„å³æ—¶æ‰§è¡Œï¼ˆEager Modeï¼‰ï¼Œè¿™ç§æ–¹å¼é€šè¿‡ä¼˜åŒ–è®¡ç®—å†…æ ¸æœ¬èº«å¸¦æ¥æ˜¾è‘—çš„è¿è¡Œæ•ˆç‡æå‡ï¼›æ­¤å¤–ï¼Œå½“è¾“å…¥å¼ é‡å½¢çŠ¶ã€æ•°æ®ç±»å‹å›ºå®šæ—¶ï¼Œtorch.compile è¿˜ä¼šè‡ªåŠ¨å¯ç”¨ CUDA Graph ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æ”¾å¤§æ€§èƒ½æ”¶ç›Šã€‚

åœ¨ torch.compile é—®ä¸–ä¹‹å‰ï¼ŒPyTorch å¼€å‘è€…è‹¥æƒ³è¿½æ±‚é«˜æ€§èƒ½ï¼Œä»…æœ‰ä¸¤ç§æ ¸å¿ƒé€‰æ‹©ï¼šä¸€æ˜¯ä½¿ç”¨ Eager Mode æ¥å—å…¶åŸç”Ÿæ€§èƒ½ä¸Šé™ï¼ŒäºŒæ˜¯æ‰‹åŠ¨ç¼–å†™ Triton æˆ– CUDA åº•å±‚å†…æ ¸ä»£ç ï¼ˆè¯¥æ–¹å¼å¼€å‘é—¨æ§›é«˜ã€å‘¨æœŸé•¿ã€ç»´æŠ¤æˆæœ¬é«˜ï¼‰ã€‚è€Œæœ‰äº† torch.compile åï¼Œå¼€å‘è€…åªéœ€ç¼–å†™ç®€æ´æ˜“æ‡‚çš„ PyTorch Python ä¸šåŠ¡é€»è¾‘ï¼Œæ— éœ€å…³æ³¨åº•å±‚ç¡¬ä»¶é€‚é…ä¸å†…æ ¸å®ç°ï¼Œå³å¯è·å¾—æ¥è¿‘æ‰‹å†™ Triton/CUDA çš„ä¼˜å¼‚æ€§èƒ½ï¼Œå¤§å¹…å¹³è¡¡äº†å¼€å‘æ•ˆç‡ä¸è¿è¡Œæ€§èƒ½ã€‚

7.2. ä½¿ç”¨æ–¹æ³•
---------

åº”ç”¨ torch.compile éå¸¸ç®€å•ï¼Œæ ¸å¿ƒæœ‰ä¸¤ç±»ä½¿ç”¨æ–¹å¼ï¼š

*   è£…é¥°å™¨æ–¹å¼ï¼šåœ¨ PyTorch å‡½æ•°ä¸Šç›´æ¥æ·»åŠ  @torch.compile è£…é¥°å™¨ï¼Œå®šä¹‰æ—¶å³å®Œæˆç¼–è¯‘å£°æ˜ï¼›
    
*   æ˜¾å¼è°ƒç”¨æ–¹å¼ï¼šé€šè¿‡ compiled\_obj = torch.compile(target) æ˜¾å¼ç¼–è¯‘ç›®æ ‡å¯¹è±¡ï¼Œåç»­è°ƒç”¨ compiled\_obj å³å¯ä½¿ç”¨ä¼˜åŒ–åçš„é€»è¾‘ï¼›
    

å¦å¤–ï¼Œå¯ä»¥å¯¹æ¨¡å‹å®ä¾‹çš„ç›´æ¥ç¼–è¯‘ï¼šå¯¹äº PyTorch æ¨¡å‹ï¼ˆnn.Module å­ç±»å®ä¾‹ï¼‰ï¼Œå¯ç›´æ¥ä¼ å…¥ torch.compile å®Œæˆæ•´ä½“ç¼–è¯‘ï¼Œæ— éœ€å•ç‹¬ä¿®é¥° forward æ–¹æ³•ã€‚

ç¤ºä¾‹ä»£ç ï¼š

æ–¹å¼ 1ï¼šè£…é¥°å™¨æ–¹å¼ï¼ˆé€‚ç”¨äºå‡½æ•° / æ¨¡å‹æ–¹æ³•ï¼‰

import torch
import torch.nn as nn

# å¯¹æ™®é€šPyTorchå‡½æ•°ä½¿ç”¨è£…é¥°å™¨
@torch.compile
def my\_tensor\_func(x, y):
    return torch.matmul(x, y) + torch.relu(y)

# å¯¹æ¨¡å‹çš„forwardæ–¹æ³•ä½¿ç”¨è£…é¥°å™¨
class MyModel(nn.Module):
    @torch.compile  # ä¿®é¥°forwardæ–¹æ³•ï¼Œè‡ªåŠ¨ç¼–è¯‘æ¨¡å‹æ¨ç†é€»è¾‘
    def forward(self, x):
        return nn.Linear(10, 20)(x)

æ–¹å¼ 2ï¼šæ˜¾å¼è°ƒç”¨æ–¹å¼ï¼ˆé€‚ç”¨äºå‡½æ•° / æ¨¡å‹ï¼Œçµæ´»æ€§æ›´é«˜ï¼‰

import torch
import torch.nn as nn

# æ˜¾å¼ç¼–è¯‘æ™®é€šå‡½æ•°
def my\_tensor\_func(x, y):
    return torch.matmul(x, y) + torch.relu(y)
compiled\_func \= torch.compile(my\_tensor\_func)  # ç”Ÿæˆç¼–è¯‘åçš„å‡½æ•°
output = compiled\_func(torch.randn(32, 10), torch.randn(10, 20))  # è°ƒç”¨ç¼–è¯‘åçš„å‡½æ•°

# æ˜¾å¼ç¼–è¯‘æ¨¡å‹ï¼ˆä¸æ–¹å¼3æœ¬è´¨ä¸€è‡´ï¼Œæ›´å¼ºè°ƒâ€œå…ˆç¼–è¯‘åä½¿ç”¨â€çš„æ˜¾å¼æµç¨‹ï¼‰
class MyModel(nn.Module):
    def forward(self, x):
        return nn.Linear(10, 20)(x)
model \= MyModel()
compiled\_model \= torch.compile(model)  # ç›´æ¥ç¼–è¯‘æ•´ä¸ªæ¨¡å‹å®ä¾‹
output = compiled\_model(torch.randn(32, 10))  # è°ƒç”¨ç¼–è¯‘åçš„æ¨¡å‹

æ–¹å¼ 3ï¼šç›´æ¥ç¼–è¯‘æ¨¡å‹å®ä¾‹ï¼ˆæ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨ï¼Œç®€åŒ–å†™æ³•ï¼‰

import torch
import torch.nn as nn

class MyModel(nn.Module):
    def forward(self, x):
        return nn.Linear(10, 20)(x)

# ç›´æ¥ç¼–è¯‘æ¨¡å‹å®ä¾‹ï¼Œä¸€æ­¥åˆ°ä½ï¼ˆæ— éœ€è£…é¥°å™¨ï¼Œæœ€ç®€æ´å¸¸ç”¨ï¼‰
model = torch.compile(MyModel())
output \= model(torch.randn(32, 10))

7.3. æ€§èƒ½å¯¹æ¯”
---------

ä¸‹é¢ä»¥ä¸€ä¸ªç®€å•çš„ä¾‹å­å¯¹æ¯” Eager Mode å’Œ Compiled Modeï¼š

import torch
import time

# ç¡®ä¿ä½¿ç”¨çš„æ˜¯ GPU
device = "cuda" if torch.cuda.is\_available() else "cpu"
if device == "cpu":
    print("è­¦å‘Šï¼šCUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU è¿è¡Œï¼ˆtorch.compile çš„ä¼˜åŠ¿åœ¨ GPU ä¸Šæœ€æ˜æ˜¾ï¼‰ã€‚")

def complex\_operation\_eager(x, y):
    z \= x \* y
    z \= z + x
    z \= torch.relu(z)
    return z.sum()

@torch.compile
def complex\_operation\_graph(x, y):
    z \= x \* y
    z \= z + x
    z \= torch.relu(z)
    return z.sum()

# 1. å‡†å¤‡æ•°æ®
x = torch.randn(10000, 10000, device=device)
y \= torch.randn(10000, 10000, device=device)

# 2. çƒ­èº« (Warm up)
print("æ­£åœ¨ç¼–è¯‘å¹¶è¿›è¡Œå¤šæ¬¡çƒ­èº«ä»¥ç¨³å®š GPU çŠ¶æ€...")
# å¢åŠ çƒ­èº«å¾ªç¯
for i in range(3): 
    complex\_operation\_graph(x, y)
    if i == 0:
        print("\-> é¦–æ¬¡ç¼–è¯‘å®Œæˆï¼Œæ­£åœ¨è¿›è¡Œåç»­é¢„çƒ­...")

torch.cuda.synchronize()
print("é¢„çƒ­å®Œæ¯•ï¼Œå¼€å§‹æ­£å¼æµ‹è¯•ã€‚")

def benchmark(func, x, y, label, iterations=100):
    start\_event \= torch.cuda.Event(enable\_timing=True)
    end\_event \= torch.cuda.Event(enable\_timing=True)
    
    start\_event.record()
    for \_ in range(iterations):
        func(x, y)
    end\_event.record()
    
    torch.cuda.synchronize()
    
    elapsed\_time\_ms \= start\_event.elapsed\_time(end\_event)
    avg\_time\_s \= (elapsed\_time\_ms / 1000) / iterations
    print(f"{label} å¹³å‡è€—æ—¶: {avg\_time\_s:.6f} ç§’")
    return avg\_time\_s

# 3. æ‰§è¡Œæµ‹è¯•å¹¶è®¡ç®—åŠ é€Ÿæ¯”
with torch.no\_grad():
    print("\-" \* 30)
    eager\_time \= benchmark(complex\_operation\_eager, x, y, "Eager Mode   ")
    compile\_time \= benchmark(complex\_operation\_graph, x, y, "Compiled Mode")
    print("\-" \* 30)
    
    # è®¡ç®—åŠ é€Ÿæ¯”é€»è¾‘
    speedup = eager\_time / compile\_time
    improvement \= (speedup - 1) \* 100
    
    print(f"æ€§èƒ½æå‡ç»“æœ:")
    print(f"åŠ é€Ÿæ¯” (Speedup): {speedup:.2f}x")
    print(f"è¿è¡Œé€Ÿåº¦æå‡äº†: {improvement:.1f}%")

è¿è¡Œè¾“å‡ºï¼š

æ­£åœ¨ç¼–è¯‘å¹¶è¿›è¡Œå¤šæ¬¡çƒ­èº«ä»¥ç¨³å®š GPU çŠ¶æ€...
\-> é¦–æ¬¡ç¼–è¯‘å®Œæˆï¼Œæ­£åœ¨è¿›è¡Œåç»­é¢„çƒ­...
é¢„çƒ­å®Œæ¯•ï¼Œå¼€å§‹æ­£å¼æµ‹è¯•ã€‚
\------------------------------
Eager Mode    å¹³å‡è€—æ—¶: 0.005690 ç§’
Compiled Mode å¹³å‡è€—æ—¶: 0.001528 ç§’
\------------------------------
æ€§èƒ½æå‡ç»“æœ:
åŠ é€Ÿæ¯” (Speedup): 3.72x
è¿è¡Œé€Ÿåº¦æå‡äº†: 272.3%

æ€§èƒ½æœ‰æ•°å€çš„æå‡ã€‚

7.4. Q&A
--------

ï¼ˆ1ï¼‰æ—¢ç„¶ torch.compile æœ‰è¿™ä¹ˆå¤§çš„å¥½å¤„ï¼Œä¸ºä»€ä¹ˆä¸èƒ½ç»™æ‰€æœ‰çš„å¼ é‡æ“ä½œå‡½æ•°éƒ½åŠ ä¸Š @torch.compileï¼Ÿ

æœ‰å‡ æ–¹é¢çš„åŸå› ï¼šé¦–å…ˆï¼Œå­˜åœ¨ç¼–è¯‘å¼€é”€ï¼Œä¼šå¯¼è‡´é¦–æ¬¡è¿è¡Œæ˜¾è‘—å˜æ…¢ï¼›ç„¶åï¼Œç¼–è¯‘å™¨ç”Ÿæˆçš„ Triton å†…æ ¸ï¼ˆæˆ–å…¶ä»–åç«¯ä»£ç ï¼‰é€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šå¼ é‡å½¢çŠ¶ã€æ•°æ®ç±»å‹å’Œè®¾å¤‡é…ç½®ä¼˜åŒ–çš„ï¼Œå¦‚æœè¾“å…¥å¼ é‡çš„è¿™äº›å±æ€§é¢‘ç¹å˜åŒ–ï¼Œä¼šåå¤è§¦å‘é‡æ–°ç¼–è¯‘ï¼ˆå³ â€œç¼–è¯‘ç¼“å­˜å¤±æ•ˆâ€ï¼‰ï¼Œåè€ŒæŠµæ¶ˆæ€§èƒ½æ”¶ç›Šï¼›ç¬¬ä¸‰ï¼Œtorch.compile è‡ªèº«ä¼šå¸¦æ¥é¢å¤–çš„æ˜¾å­˜å¼€é”€ï¼ˆç”¨äºå­˜å‚¨ç¼–è¯‘åçš„ä¸­é—´è¡¨ç¤ºã€å†…æ ¸ç¼“å­˜ç­‰ï¼‰ï¼Œè¿‡å¤šæ— å·®åˆ«ä½¿ç”¨å¯èƒ½å¯¼è‡´æ˜¾å­˜ä¸è¶³ï¼ˆOOMï¼‰ï¼›æœ€åï¼Œå¹¶éæ‰€æœ‰ä»£ç éƒ½èƒ½è¢«æˆåŠŸå›¾åŒ–ä¼˜åŒ–ï¼Œå¦‚æœå¼ é‡æ“ä½œä¸­è°ƒç”¨äº†é PyTorch åŸç”Ÿçš„ç¬¬ä¸‰æ–¹åº“ï¼ˆæˆ–çº¯ Python åŸç”Ÿé€»è¾‘ï¼‰ï¼Œä¼šå¯¼è‡´è®¡ç®—å›¾ä¸­æ–­ï¼Œæ­¤æ—¶ç¼–è¯‘å™¨æ— æ³•ç»§ç»­ä¼˜åŒ–åç»­é€»è¾‘ï¼Œè¿˜éœ€è¦å°†æ§åˆ¶æƒäº¤å›ç»™ Python è§£é‡Šå™¨ï¼Œäº§ç”Ÿä¸å¿…è¦çš„ä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€ï¼Œå¯èƒ½å¯¼è‡´è´Ÿä¼˜åŒ–ã€‚

ï¼ˆ2ï¼‰torch.compile æ”¯æŒç”ŸæˆåŸç”Ÿ CUDA å†…æ ¸ä»£ç ï¼Œä½†é€šå¸¸æ¥è¯´ï¼Œç¼–è¯‘å™¨è‡ªåŠ¨ç”Ÿæˆçš„é€šç”¨åŸç”Ÿ CUDA ä»£ç ä¼˜åŒ–ç²’åº¦ä¸å¤Ÿç²¾ç»†ï¼›è€Œ Triton å†…ç½®äº†æå¼ºçš„ Autotuningï¼ˆè‡ªåŠ¨è°ƒä¼˜ï¼‰èƒ½åŠ›ï¼Œé’ˆå¯¹æ·±åº¦å­¦ä¹ å¼ é‡è®¡ç®—åœºæ™¯åšäº†æ·±åº¦é€‚é…ï¼Œå› æ­¤åœ¨ç»å¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒTriton å†…æ ¸çš„æ€§èƒ½é€šå¸¸ä¼˜äºè‡ªåŠ¨ç”Ÿæˆçš„åŸç”Ÿ CUDA å†…æ ¸ã€‚

8\. Torch Compilationã€Tritionã€CUDA Graph ä¸‰è€…çš„åŒºåˆ«å’Œè”ç³»
=================================================

8.1. æ ¸å¿ƒåŒºåˆ«
---------

ï¼ˆ1ï¼‰Torch Compilationï¼šPyTorch é«˜å±‚ä¸€ç«™å¼æ€§èƒ½ä¼˜åŒ–å…¥å£ï¼ˆç”¨æˆ·æ€æŠ½è±¡æ¥å£ï¼‰

ä½œä¸ºé¢å‘å¼€å‘è€…çš„é¡¶çº§ä¼˜åŒ–å°è£…ï¼Œtorch.compile æ— éœ€å¼€å‘è€…å…³æ³¨åº•å±‚ç¡¬ä»¶ç»†èŠ‚ä¸ä¼˜åŒ–å®ç°ï¼Œå…¶æ ¸å¿ƒå®šä½æ˜¯å¯¹ PyTorch å¼ é‡è®¡ç®—é€»è¾‘ï¼ˆå‡½æ•° /nn.Module æ¨¡å‹ï¼‰è¿›è¡Œç«¯åˆ°ç«¯è‡ªåŠ¨ä¼˜åŒ–ï¼Œå±è”½äº†åº•å±‚å†…æ ¸ç”Ÿæˆä¸æ‰§è¡Œä¼˜åŒ–çš„å¤æ‚æ€§ï¼Œæ˜¯ç»å¤§å¤šæ•° PyTorch å¼€å‘è€…çš„é¦–é€‰æ€§èƒ½ä¼˜åŒ–å·¥å…·ã€‚

ï¼ˆ2ï¼‰Tritonï¼šé«˜æ€§èƒ½ GPU å†…æ ¸ä¸“ç”¨ DSL

Triton æ—¢æ˜¯å¼€å‘è€…æ‰‹åŠ¨ç¼–å†™é«˜æ€§èƒ½å†…æ ¸çš„é¢†åŸŸä¸“ç”¨è¯­è¨€ï¼ˆDSLï¼‰ï¼Œä¹Ÿæ˜¯ torch.compile è‡ªåŠ¨åŒ–ç”Ÿæˆä»£ç çš„æ ¸å¿ƒç›®æ ‡åç«¯ã€‚å…¶ä¸­ï¼Œtriton.jit æ˜¯ Triton æ¡†æ¶æä¾›çš„å³æ—¶ç¼–è¯‘è£…é¥°å™¨ï¼Œå®šä½ä¸ºé«˜æ€§èƒ½è·¨å¹³å° GPU å†…æ ¸çš„æ‰‹åŠ¨å¼€å‘å…¥å£ï¼ŒæŠ½è±¡å±‚çº§ä½äº torch.compileã€é«˜äºåŸç”Ÿ CUDA C++ã€‚å®ƒå…è®¸å¼€å‘è€…ä»¥ Python é£æ ¼è¯­æ³•ç¼–å†™ GPU å†…æ ¸é€»è¾‘ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†çº¿ç¨‹è°ƒåº¦ã€å¯„å­˜å™¨åˆ†é…ç­‰åº•å±‚ç»†èŠ‚ï¼Œæœ€ç»ˆç¼–è¯‘ä¸ºé«˜æ•ˆ GPU æœºå™¨ç ï¼Œç”¨äºæ»¡è¶³å®šåˆ¶åŒ–ç®—å­çš„é«˜æ€§èƒ½éœ€æ±‚ã€‚

ï¼ˆ3ï¼‰CUDA Graphï¼šGPU åº•å±‚é™æ€ä»»åŠ¡æµæ‰§è¡Œä¼˜åŒ–æŠ€æœ¯

CUDA Graph æ˜¯ä¸€ç§é™æ€ä»»åŠ¡æµè°ƒåº¦æŠ€æœ¯ï¼Œæ—¨åœ¨æ¶ˆé™¤ä¸»æœºç«¯ï¼ˆHostï¼‰ä¸è®¾å¤‡ç«¯ï¼ˆDeviceï¼‰ä¹‹é—´çš„äº¤äº’å»¶è¿Ÿã€‚å®ƒå¹¶é â€œå†…æ ¸ç”Ÿæˆå·¥å…·â€ï¼Œä¹Ÿé â€œç”¨æˆ·æ€ç¼–ç¨‹æ¥å£â€ï¼Œè€Œæ˜¯é’ˆå¯¹ CPU-GPU äº¤äº’ç“¶é¢ˆçš„åº•å±‚æ‰§è¡Œä¼˜åŒ–æŠ€æœ¯ï¼ŒæŠ½è±¡å±‚çº§æœ€ä½ã€‚å…¶æ ¸å¿ƒä½œç”¨æ˜¯å›ºåŒ–è¿ç»­çš„ CUDA å†…æ ¸è°ƒç”¨åºåˆ—ä¸å†…å­˜é…ç½®ï¼Œé€šè¿‡ â€œå½•åˆ¶ - é‡æ”¾â€ æ¨¡å¼æ¶ˆé™¤é‡å¤å†…æ ¸å¯åŠ¨ã€CPU-GPU é¢‘ç¹é€šä¿¡çš„å¼€é”€ï¼Œä»…ä¼˜åŒ–æ‰§è¡Œæµç¨‹ï¼Œä¸æ”¹å˜å†…æ ¸æœ¬èº«çš„è®¡ç®—æ€§èƒ½ã€‚

8.2. æ ¸å¿ƒè”ç³»
---------

ï¼ˆ1ï¼‰torch.compile ä¾èµ– triton.jit å®ç°é«˜æ€§èƒ½å†…æ ¸ç”Ÿæˆ

torch.compile çš„é»˜è®¤åº•å±‚ç¼–è¯‘å™¨ï¼ˆInductorï¼‰åœ¨ CUDA è®¾å¤‡ä¸Šï¼Œä¼šè‡ªåŠ¨å°† PyTorch è®¡ç®—é€»è¾‘è½¬åŒ–ä¸º Triton å†…æ ¸ä»£ç ï¼Œå¹¶éšå¼è°ƒç”¨ triton.jit å®Œæˆç¼–è¯‘ï¼Œç”Ÿæˆé«˜æ€§èƒ½ GPU å†…æ ¸ï¼ˆå¼€å‘è€…æ— éœ€æ‰‹åŠ¨ç¼–å†™ Triton ä»£ç ï¼Œä¹Ÿæ— éœ€æ„ŸçŸ¥ triton.jit çš„å­˜åœ¨ï¼‰ã€‚æ­¤å¤–ï¼Œtorch.compile ä¹Ÿæ”¯æŒç”ŸæˆåŸç”Ÿ CUDA å†…æ ¸ï¼Œä½œä¸º Triton å†…æ ¸çš„å¯é€‰è¡¥å……æ–¹æ¡ˆã€‚

ï¼ˆ2ï¼‰torch.compile é›†æˆ CUDA Graph å®ç°æ‰§è¡Œå±‚äºŒæ¬¡ä¼˜åŒ–

å½“è¾“å…¥å¼ é‡çš„å½¢çŠ¶ã€æ•°æ®ç±»å‹ç­‰å±æ€§å›ºå®šæ—¶ï¼Œtorch.compile ä¼šè‡ªåŠ¨å¯ç”¨ CUDA Graph ä¼˜åŒ–ï¼Œå°†ç¼–è¯‘ç”Ÿæˆçš„ Triton/CUDA å†…æ ¸è°ƒç”¨åºåˆ—å½•åˆ¶ä¸º CUDA å›¾ã€‚åç»­é‡å¤æ‰§è¡Œè¯¥é€»è¾‘æ—¶ï¼Œç›´æ¥åœ¨ GPU ä¸Šé‡æ”¾è¯¥å›¾ï¼Œè¿›ä¸€æ­¥æ”¾å¤§æ€§èƒ½æ”¶ç›Šï¼Œå®ç° â€œå†…æ ¸è®¡ç®—ä¼˜åŒ–â€ ä¸ â€œæ‰§è¡Œæµç¨‹ä¼˜åŒ–â€ çš„ååŒå¢æ•ˆã€‚

ï¼ˆ3ï¼‰triton.jit è‡ªå®šä¹‰å†…æ ¸å¯ä¸ CUDA Graph æ‰‹åŠ¨ååŒ

å¼€å‘è€…æ‰‹åŠ¨é€šè¿‡ triton.jit ç¼–å†™å¹¶ç¼–è¯‘çš„è‡ªå®šä¹‰å†…æ ¸ï¼Œåœ¨æ‰¹é‡é‡å¤æ‰§è¡Œï¼ˆè¾“å…¥å½¢çŠ¶å›ºå®šï¼‰çš„åœºæ™¯ä¸‹ï¼Œå¯æ‰‹åŠ¨é›†æˆ CUDA Graph å®Œæˆ â€œå½•åˆ¶ - é‡æ”¾â€ æµç¨‹ï¼Œæ¶ˆé™¤ CPU å¯¹ GPU çš„è°ƒåº¦å¼€é”€ï¼Œå®ç°å†…æ ¸è®¡ç®—æ€§èƒ½ä¸æ‰§è¡Œæ•ˆç‡çš„åŒé‡æè‡´ä¼˜åŒ–ã€‚

ï¼ˆ4ï¼‰ä¸‰è€…ååŒæ„å»ºæè‡´æ€§èƒ½è®¡ç®—é“¾è·¯

å…¸å‹æè‡´æ€§èƒ½é“¾è·¯ï¼šæ‰‹åŠ¨ç¼–å†™ triton.jit å®šåˆ¶å†…æ ¸ â†’ åµŒå…¥ PyTorch æ¨¡å‹ / å‡½æ•° â†’ é€šè¿‡ torch.compile è¿›è¡Œä¸Šå±‚è®¡ç®—å›¾ä¼˜åŒ–ï¼ˆç®—å­èåˆã€å†…å­˜å¤ç”¨ç­‰ï¼‰ â†’ torch.compile è‡ªåŠ¨å¯ç”¨ CUDA Graph ä¼˜åŒ–æ‰§è¡Œæµç¨‹ â†’ å®ç° GPU è®¡ç®—æ€§èƒ½æœ€å¤§åŒ–ã€‚

9\. TP æ¨¡å¼
=========

TP æ¨¡å¼å°†çŸ©é˜µè®¡ç®—æŒ‰è¡Œã€åˆ—æ‹†åˆ†åˆ°å¤šé¢— GPU ä¸Šæ‰§è¡Œï¼Œæ¶‰åŠä¸¤ä¸ªå…³é”®ç‚¹ï¼šæƒé‡å‚æ•°æ€ä¹ˆåŠ è½½ã€å¤šæ ¸è®¡ç®—ä¹‹é—´å¦‚ä½•ååŒï¼Œä¸‹é¢åšä»‹ç»ã€‚

9.1. åŠ è½½æƒé‡å‚æ•°
-----------

æƒé‡å‚æ•°ä¸çŸ©é˜µè®¡ç®—å¼ºç›¸å…³ï¼Œå› æ­¤æƒé‡å‚æ•°çš„åŠ è½½é€»è¾‘é€šå¸¸ä¸çŸ©é˜µè®¡ç®—é€»è¾‘ä¸€åŒå°è£…åœ¨åŒä¸€ä¸ªç±»ä¸­ï¼Œå®ç°åŠŸèƒ½çš„å†…èšæ€§ã€‚

### 9.1.1. å…³é”®æŠ€æœ¯ç‚¹

ï¼ˆ1ï¼‰å‚æ•°æ–‡ä»¶ä¸­ï¼Œæƒé‡çŸ©é˜µä»¥ Key-Value é”®å€¼å¯¹å½¢å¼å­˜å‚¨ï¼Œè¯»å–æ—¶åŒæ ·é‡‡ç”¨ Key-Value æ–¹å¼è§£æã€‚å…¶ä¸­ key å¯¹åº”æƒé‡çŸ©é˜µåœ¨æ¨¡å‹ä¸­çš„å½’å±ä½ç½®ï¼Œä¾‹å¦‚ï¼šæ¨¡å‹ç¬¬ 0 å±‚ MLP å­å±‚çš„ down proj æƒé‡å¯¹åº”çš„ key ä¸º model.layers.0.mlp.down\_proj.weightã€‚

ï¼ˆ2ï¼‰å‚æ•°æ–‡ä»¶ç”±è®­ç»ƒæµç¨‹å†™å…¥ã€æ¨ç†æµç¨‹è¯»å–ï¼Œè®­ç»ƒä¸æ¨ç†ä¸¤ä¾§å¿…é¡»ä¸¥æ ¼å¯¹é½ key çš„å‘½åè§„åˆ™ã€‚æ¨¡å‹å‚æ•°åŠ è½½æ—¶ï¼Œä¼šæ ¹æ®å‚æ•°æ–‡ä»¶ä¸­çš„ key åç§°ï¼Œåœ¨ nn.Module å¯¹è±¡ä¸­åŒ¹é…å¹¶è°ƒç”¨å¯¹åº”çš„ weight\_loader æ–¹æ³•å®ŒæˆåŠ è½½ã€‚

ï¼ˆ3ï¼‰æ¨¡å‹ç»“æ„åŒ…å«å¤šä¸ªå±‚çº§ï¼Œæ¯ä¸€å±‚å†…éƒ¨åˆåŒ…å«å¤šä¸ªå­æ¨¡å—ï¼Œä¸åŒå­æ¨¡å—å¯¹åº”å„è‡ªä¸“å±çš„å‚æ•°åŠ è½½æ–¹æ³•ã€‚PyTorch çš„ nn.Module é€šè¿‡ç‰¹æ®Šæ–¹æ³• \_\_setattr\_\_ï¼Œå°†æ¨¡å‹ç»“æ„ä¸­çš„å„ä¸ªå­æ¨¡å—æ„å»ºä¸ºæ ‘å½¢ç»“æ„ï¼›æ ‘å½¢ç»“æ„ä¸­æ¯ä¸ªå¶å­èŠ‚ç‚¹çš„è·¯å¾„ï¼Œä¸å‚æ•°æ–‡ä»¶ä¸­çš„ key ä¸€ä¸€æ˜ å°„ï¼Œé€šè¿‡è¯¥è·¯å¾„æ‰¾åˆ°å¶å­èŠ‚ç‚¹åï¼Œå³å¯è·å–å¯¹åº”çš„å‚æ•°å¯¹è±¡ nn.Parameterï¼Œè€Œè¯¥å‚æ•°å¯¹è±¡ç»‘å®šäº†å…¶æ‰€å±å­æ¨¡å—çš„ weight\_loader æ–¹æ³•ã€‚

ï¼ˆ4ï¼‰çŸ©é˜µä¹˜æ³• A \* B éµå¾ªã€ŒA çš„è¡Œ Ã— B çš„åˆ—ã€è®¡ç®—è§„åˆ™ï¼Œåœ¨æ¨¡å‹æ¨ç†ä¸­ï¼ŒB ä¸ºæƒé‡çŸ©é˜µï¼Œå®é™…è®¿é—®æ—¶ä»¥åˆ—ç»´åº¦ä¸ºä¸»ã€‚ä¸ºæå‡è¯»å–æ•ˆç‡ã€é¿å…ç¼“å­˜ï¼ˆCacheï¼‰é¢‘ç¹å¤±æ•ˆï¼Œæƒé‡çŸ©é˜µ B é€šå¸¸ä»¥è½¬ç½®å½¢å¼å­˜å‚¨ã€‚TPï¼ˆTensor Parallelï¼‰worker åŠ è½½æƒé‡æ—¶ï¼Œéœ€é€‚é…è¯¥è½¬ç½®å­˜å‚¨ç‰¹æ€§ â€”â€” å³æƒé‡çŸ©é˜µç¬¬ 0 ç»´å¯¹åº”åŸå§‹çŸ©é˜µçš„åˆ—æ•°æ®ï¼Œç¬¬ 1 ç»´å¯¹åº”åŸå§‹çŸ©é˜µçš„è¡Œæ•°æ®ã€‚

ä»ä¸Šè¿°æŠ€æœ¯ç‚¹å¯å¾—å‡ºæ ¸å¿ƒå¯¹åº”å…³ç³»ï¼šå‚æ•°æ–‡ä»¶ä¸­çš„æ¨¡å‹ç»“æ„ä»¥ä¸€ä¸ªä¸ª key è¡¨ç¤ºï¼Œè¿™äº› key æŒ‰å±‚çº§å…³ç³»å¯æ„å»ºä¸ºä¸€æ£µè·¯å¾„æ ‘ï¼›ä»£ç ä¸­çš„æ¨¡å‹ç»“æ„ä»¥æœ‰åŒ…å«å…³ç³»çš„ç±»å¯¹è±¡è¡¨ç¤ºï¼Œè¿™äº›ç±»å¯¹è±¡åŒæ ·æ„æˆä¸€æ£µä¸å‚æ•°æ–‡ä»¶è·¯å¾„æ ‘å®Œå…¨å¯¹åº”çš„æ ‘ã€‚

### 9.1.2. å®æ“ä¸¾ä¾‹ï¼ˆFFN å±‚ up proj æƒé‡åŠ è½½ï¼‰

ï¼ˆ1ï¼‰å‡è®¾ TP size=2ï¼Œup proj æƒé‡çŸ©é˜µçš„åŸå§‹å½¢çŠ¶ä¸º \[1024, 3072\]ï¼Œä¸‹é¢ä»‹ç»ä¸€ä¸ª TP worker å¦‚ä½•åŠ è½½æƒé‡ã€‚

ï¼ˆ2ï¼‰é¦–å…ˆï¼Œæ„é€ æ¨¡å‹å¯¹è±¡æ—¶ï¼Œä¼šåˆå§‹åŒ– ColumnParallelLinear å¯¹è±¡ï¼Œå¹¶è®¾å®šæ ¸å¿ƒå‚æ•°ï¼šinput\_size=1024ï¼Œoutput\_size=3072/2=1536ï¼ˆæŒ‰ TP å°ºå¯¸åšå‡åˆ†ï¼‰ã€‚è¿™ä¸¤ä¸ªå‚æ•°æœ€ç»ˆç”¨äºåˆå§‹åŒ– nn.Parameter å¯¹è±¡ï¼Œå¯¹åº”ä»£ç ä¸º self.weight = nn.Parameter(torch.empty(output\_size, input\_size))ï¼Œéœ€æ³¨æ„æ­¤å¤„åˆå§‹åŒ–çš„å¼ é‡ä»¥ output\_size ä¸ºè¡Œç»´åº¦ã€input\_size ä¸ºåˆ—ç»´åº¦ã€‚

ï¼ˆ3ï¼‰éšåï¼Œå¯åŠ¨æ¨¡å‹æƒé‡åŠ è½½æµç¨‹ï¼šå…ˆä»å‚æ•°æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰ key-value é”®å€¼å¯¹ï¼Œå†é€šè¿‡ key åœ¨ nn.Module æ ‘å½¢ç»“æ„ä¸­æŸ¥æ‰¾å¯¹åº”çš„ nn.Parameter å¯¹è±¡ï¼ŒåŒ¹é…åˆ°åè°ƒç”¨å…¶ç»‘å®šçš„ weight\_loader å‡½æ•°ï¼Œæ‰§è¡Œå…·ä½“çš„å‚æ•°åŠ è½½æ“ä½œã€‚

ï¼ˆ4ï¼‰å‚æ•°åŠ è½½é˜¶æ®µï¼Œé’ˆå¯¹åˆ—å¹¶è¡Œæ¨¡å¼ï¼Œéœ€è¦å¯¹æƒé‡å¼ é‡çš„ç¬¬ 0 ç»´åº¦è¿›è¡Œæ‹†åˆ†ï¼Œå†æ ¹æ®å½“å‰è¿›ç¨‹çš„ tp\_rankï¼ˆTP è¿›ç¨‹ç¼–å·ï¼‰ï¼Œç¡®å®šæœ¬è¿›ç¨‹éœ€è¦åŠ è½½çš„æƒé‡åŒºé—´ï¼Œå®Œæˆåˆ†ç‰‡æƒé‡çš„åŠ è½½ã€‚

æ³¨ï¼šä»£ç å®ç°ä¸­ï¼Œä¼šå°† gate çŸ©é˜µä¸ up çŸ©é˜µè¿›è¡Œåˆå¹¶åŠ è½½åˆ°æ˜¾å­˜ä¸­ï¼Œå› æ­¤å®é™…åŠ è½½æµç¨‹ä¼šåœ¨æ­¤åŸºç¡€ä¸Šå¢åŠ å‡ æ­¥é¢å¤–æ­¥éª¤ã€‚

### 9.1.3. æ„é€ æ ‘å½¢ç»“æ„ç¤ºä¾‹ä»£ç 

ä¸‹é¢ demo ä»£ç å±•ç¤ºå¤šä¸ªæœ‰å±‚çº§çš„å¯¹è±¡å¦‚ä½•é€šè¿‡ç‰¹æ®Šæ–¹æ³• \_\_setattr\_\_ æ„é€ æ ‘å½¢ç»“æ„.

class MiniModule:
    def \_\_init\_\_(self, name="root"):
        self.\_name \= name
        self.\_modules \= {}
        self.\_parameters \= {}

    def \_\_setattr\_\_(self, name, value):
        if isinstance(value, MiniModule):
            self.\_modules\[name\] \= value
        elif name.endswith("\_weight\_loader"):
            self.\_parameters\[name\] \= value

        super().\_\_setattr\_\_(name, value)

    def get\_all\_paths(self, prefix=""):
        """é€’å½’éå†å¹¶æ”¶é›†æ‰€æœ‰å‚æ•°çš„å®Œæ•´è·¯å¾„"""
        paths \= \[\]

        # 1. å…ˆæ”¶é›†å½“å‰å±‚çº§çš„å‚æ•°è·¯å¾„
        for p\_name in self.\_parameters:
            full\_path \= f"{prefix}.{p\_name}" if prefix else p\_name
            paths.append(full\_path)

        # 2. é€’å½’è¿›å…¥å­æ¨¡å—ï¼Œä¼ é€’æ›´æ–°åçš„å‰ç¼€
        for m\_name, m\_obj in self.\_modules.items():
            new\_prefix \= f"{prefix}.{m\_name}" if prefix else m\_name
            paths.extend(m\_obj.get\_all\_paths(new\_prefix))

        return paths

def q\_weight\_loader():
    print(f"this is q\_weight\_loader")

def down\_weight\_loader():
    print(f"this is down\_weight\_loader")

# --- æ„é€ æ ‘å½¢ç»“æ„ ---
model = MiniModule("Qwen3")
model.layers \= MiniModule("Layers")
model.layers.attention \= MiniModule("Attention")
model.layers.attention.q\_weight\_loader \= q\_weight\_loader
model.layers.mlp \= MiniModule("MLP")
model.layers.mlp.down\_weight\_loader \= down\_weight\_loader

# --- æ‰“å°æ‰€æœ‰è·¯å¾„ ---
print("éå†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°è·¯å¾„ï¼š")
all\_paths \= model.get\_all\_paths()
for path in all\_paths:
    print(f"è·¯å¾„: {path}")

# --- æ¨¡æ‹Ÿ nano-vllm çš„è®¿é—®é€»è¾‘ ---
def mock\_get\_parameter(root, path):
    parts \= path.split(".")
    curr \= root
    for part in parts\[:-1\]:
        curr \= curr.\_modules\[part\]
    return curr.\_parameters\[parts\[-1\]\]

target \= "layers.attention.q\_weight\_loader"
print(f"\\næ¨¡æ‹ŸæŸ¥æ‰¾è·¯å¾„ '{target}':")
loader \= mock\_get\_parameter(model, target)
loader()

è¾“å‡ºç»“æœï¼š

éå†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°è·¯å¾„ï¼š
è·¯å¾„: layers.attention.q\_weight\_loader
è·¯å¾„: layers.mlp.down\_weight\_loader

æ¨¡æ‹ŸæŸ¥æ‰¾è·¯å¾„ 'layers.attention.q\_weight\_loader':
this is q\_weight\_loader

9.2. å¤š GPU ä¹‹é—´çš„è®¡ç®—ååŒ
------------------

### 9.2.1. åŠŸèƒ½ç»†èŠ‚

nano-vllm åªè€ƒè™‘å•æœºå†…çš„å¤š GPU ååŒï¼ŒååŒè¿‡ç¨‹å¦‚ä¸‹ï¼š

![](https://pic1.zhimg.com/80/v2-7f054d91f4badab37cab087059b847da_1440w.png?source=ccfced1a)

å›¾ 5

ï¼ˆ1ï¼‰è¿›ç¨‹éš”ç¦»ä¸ç‹¬ç«‹åŠ è½½ï¼š é‡‡ç”¨å¤šè¿›ç¨‹æ¨¡å¼ï¼Œä¸€ä¸ª GPU å¯¹åº”ä¸€ä¸ªç‹¬ç«‹è¿›ç¨‹ã€‚å„è¿›ç¨‹å¹¶å‘è¯»å–æƒé‡æ–‡ä»¶ï¼Œå¹¶æ ¹æ®è‡ªå·±çš„ tp\_rank æŒ‰ç…§é¢„è®¾çš„åˆ‡åˆ†ç­–ç•¥ï¼ˆå¦‚ ColumnParallel çš„è¡Œåˆ‡åˆ†æˆ– RowParallel çš„åˆ—åˆ‡åˆ†ï¼‰ï¼Œå°†å±äºè‡ªå·±çš„é‚£éƒ¨åˆ†æ•°æ®ä» safetensors åŠ è½½åˆ°æ˜¾å­˜ä¸­ã€‚

ï¼ˆ2ï¼‰æ§åˆ¶é¢ååŒï¼ˆControl Planeï¼‰ï¼š Rank 0 è´Ÿè´£å…¨å±€è°ƒåº¦ï¼Œé€šè¿‡å…±äº«å†…å­˜å°†æ¨ç†è¯·æ±‚ï¼ˆTokensã€Sampling Params ç­‰ï¼‰åŒæ­¥ç»™å…¶ä»– Rankã€‚

ï¼ˆ3ï¼‰æ•°æ®é¢é€šä¿¡ï¼ˆData Planeï¼‰ï¼š åœ¨çŸ©é˜µè®¡ç®—çš„å…³é”®èŠ‚ç‚¹ï¼Œåˆ©ç”¨é€šä¿¡åŸè¯­ï¼ˆå¦‚ all-reduce å¤„ç†åˆ†å—æ±‚å’Œã€all-gather å¤„ç†åºåˆ—æ‹¼æ¥ï¼‰å®Œæˆå¼ é‡å¹¶è¡Œçš„ç»“æœæ±‡æ€»ï¼Œä½¿åˆ†å¸ƒåœ¨ä¸åŒæ˜¾å¡ä¸Šçš„è®¡ç®—ç»“æœåœ¨æ•°å­¦ä¸Šç­‰ä»·äºå•å¡è®¡ç®—ã€‚

### 9.2.2. ç¤ºä¾‹ä»£ç 

ä¸‹é¢å†™ä¸€ä¸ªç®€å•çš„æ•°æ®é€šä¿¡ä¾‹å­ï¼š

import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import time

def setup(rank, world\_size):
    dist.init\_process\_group(
        "nccl", "tcp://127.0.0.1:2333", world\_size=world\_size, rank=rank
    )
    torch.cuda.set\_device(rank)

def cleanup():
    dist.destroy\_process\_group()

def test\_all\_reduce(rank, world\_size):
    """
    All-Reduce å­å‡½æ•°ï¼šå°†æ‰€æœ‰ GPU çš„æ•°æ®è¿›è¡Œå½’çº¦æ“ä½œï¼ˆæ±‚å’Œï¼‰ï¼Œç»“æœåŒæ­¥åˆ°æ‰€æœ‰ GPU

    ä½¿ç”¨åœºæ™¯ï¼š
    \- RowParallelLinear çš„è¾“å‡ºèšåˆ
    \- éœ€è¦æ‰€æœ‰ GPU éƒ½å¾—åˆ°ç›¸åŒçš„èšåˆç»“æœ
    """
    print(f"\[Rank {rank}\] ===== All-Reduce ç¤ºä¾‹ =====")

    if rank == 0:
        data \= torch.tensor(\[1.0, 2.0\], device=f"cuda:{rank}")
    else:
        data \= torch.tensor(\[3.0, 4.0\], device=f"cuda:{rank}")

    print(f"\[Rank {rank}\] All-Reduce before: {data}")
    dist.all\_reduce(data, op\=dist.ReduceOp.SUM)
    print(f"\[Rank {rank}\] All-Reduce after:  {data}")

def test\_all\_gather(rank, world\_size):
    """
    All-Gather å­å‡½æ•°ï¼šæ”¶é›†æ‰€æœ‰ GPU çš„æ•°æ®åˆ°æ¯ä¸ª GPU ä¸Š

    ä½¿ç”¨åœºæ™¯ï¼š
    \- VocabParallelEmbedding çš„è¾“å‡ºæ”¶é›†
    \- ParallelLMHead çš„ logits æ”¶é›†
    \- éœ€è¦æ¯ä¸ª GPU éƒ½è·å¾—æ‰€æœ‰ GPU çš„å®Œæ•´æ•°æ®
    """
    print(f"\[Rank {rank}\] ===== All-Gather ç¤ºä¾‹ =====")

    # Rank 0 äº§ç”Ÿ \[10, 20\], Rank 1 äº§ç”Ÿ \[30, 40\]
    local\_data \= torch.tensor(
        \[10.0 + rank \* 20, 20.0 + rank \* 20\], device=f"cuda:{rank}"
    )
    print(f"\[Rank {rank}\] All-Gather local data: {local\_data}")

    gathered\_list \= \[torch.zeros\_like(local\_data) for \_ in range(world\_size)\]
    dist.all\_gather(gathered\_list, local\_data)

    print(f"\[Rank {rank}\] All-Gather result: {gathered\_list}")

    gathered\_tensor \= torch.cat(gathered\_list, dim=0)
    print(f"\[Rank {rank}\] All-Gather concatenated: {gathered\_tensor}")

def tp\_demo(rank, world\_size):
    setup(rank, world\_size)

    test\_all\_reduce(rank, world\_size)
    time.sleep(5)
    test\_all\_gather(rank, world\_size)

    cleanup()

def run\_demo():
    world\_size \= 2
    mp.spawn(tp\_demo, args\=(world\_size,), nprocs=world\_size, join\=True)

if \_\_name\_\_ == "\_\_main\_\_":
    if torch.cuda.device\_count() >= 2:
        run\_demo()
    else:
        print(f"éœ€è¦è‡³å°‘ 2 å¼  GPU æ¥è¿è¡Œæ­¤ TP ç¤ºä¾‹")

10\. å…¶ä»–
=======

æœ¬æ–‡åŸºäº nano-vllm é¡¹ç›®ï¼Œèšç„¦è®²è§£å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿé¢†åŸŸä¸­æœ€åŸºç¡€çš„è‹¥å¹²æ ¸å¿ƒæŠ€æœ¯ç‚¹ã€‚éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå¤§æ¨¡å‹æ¨ç†åŠ é€Ÿçš„æŠ€æœ¯ä½“ç³»ååˆ†ä¸°å¯Œï¼Œæœ¬é¡¹ç›®å¹¶æœªè¦†ç›–å…¨éƒ¨å†…å®¹ï¼Œä¾‹å¦‚ï¼šè®¡ç®—é€šä¿¡é‡å ï¼ˆOverlapï¼‰ã€å¤š token é¢„æµ‹ï¼ˆMTPï¼ŒMultiâ€‘Token Predictionï¼‰ã€å¤šæµã€å¤šè¿›ç¨‹æœåŠ¡ï¼ˆMPSï¼ŒMulti-Process Serviceï¼‰ã€æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ã€æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€ä¸Šä¸‹æ–‡å¹¶è¡Œï¼ˆCPï¼‰ã€ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰ä»¥åŠ PD åˆ†ç¦»ç­‰è¿›é˜¶æŠ€æœ¯æ–¹å‘ï¼Œå¯ä½œä¸ºåç»­æ·±å…¥å­¦ä¹ çš„æ‹“å±•å†…å®¹ã€‚

æ³¨1ï¼šåœ¨æœ¬æ–‡çš„æ€»ç»“è¿‡ç¨‹ä¸­ï¼Œé™¤äº†æŸ¥çœ‹æºä»£ç ï¼Œä¹Ÿå€ŸåŠ© deepwiki ä»¥åŠå…¶ä»– AI å·¥å…·è¾…åŠ©ã€‚Â 

æœ¬æ–‡:Â [https://www.cnblogs.com/cswuyg/p/19471225](https://www.cnblogs.com/cswuyg/p/19471225)

çŸ¥ä¹:Â [https://zhuanlan.zhihu.com/p/1989806890381746916](https://zhuanlan.zhihu.com/p/1989806890381746916)

å…¬ä¼—å·:Â [https://mp.weixin.qq.com/s/6mAZ49iP1SCKt5ZdWf6ErQ](https://mp.weixin.qq.com/s/6mAZ49iP1SCKt5ZdWf6ErQ)