---
layout: post
title: 'æ‰‹æŠŠæ‰‹æ•™ä½ å®ç°PyTorchç‰ˆViTï¼šå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„Transformerå®æˆ˜'
date: "2025-06-14T00:40:52Z"
---
æ‰‹æŠŠæ‰‹æ•™ä½ å®ç°PyTorchç‰ˆViTï¼šå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„Transformerå®æˆ˜
========================================

> ä½œè€…ï¼šSkyXZ
> 
> CSDNï¼š[SkyXZï½-CSDNåšå®¢](https://blog.csdn.net/xiongqi123123?spm=1000.2115.3001.5343)
> 
> åšå®¢å›­ï¼š[SkyXZ - åšå®¢å›­](https://www.cnblogs.com/SkyXZ)

*   ViTè®ºæ–‡Arxivåœ°å€ï¼š[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929)

Â Â Â Â Â Â Â Â æœ€è¿‘å…·èº«æ™ºèƒ½è¶³å¤Ÿç«çƒ­ï¼ŒVLMã€VLAã€VLNå±‚å‡ºä¸ç©·å‘å±•è¿…é€Ÿï¼Œè€ŒTransformerä½œä¸ºè¿™äº›æ¶æ„æœ€é‡è¦çš„åº•åº§ä¹‹ä¸€ï¼Œå¾—ç›Šäºå…¶å¼ºå¤§çš„å»ºæ¨¡èƒ½åŠ›ã€è‰¯å¥½çš„å¯æ‰©å±•æ€§ä¸ç»Ÿä¸€çš„ç»“æ„è®¾è®¡ï¼ŒTransformer å·²ç»æˆä¸ºæ„å»ºå¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿçš„äº‹å®æ ‡å‡†ã€‚ä»æœ€åˆçš„ BERTã€GPT åœ¨ NLP ä¸­çš„æˆåŠŸï¼Œåˆ° ViTã€CLIPã€RT-1 ç­‰æ¨¡å‹åœ¨è§†è§‰å’Œæ§åˆ¶é¢†åŸŸçš„å»¶ä¼¸ï¼ŒTransformer æ„ç­‘èµ·äº†ç»Ÿä¸€è¯­è¨€ã€è§†è§‰ä¹ƒè‡³åŠ¨ä½œç©ºé—´çš„æ¡¥æ¢ã€‚

Â Â Â Â Â Â Â Â **æ—¢ç„¶ Transformer æˆä¸ºäº†å…·èº«æ™ºèƒ½çš„åŸºç¡€è®¾æ–½ï¼Œé‚£ä½œä¸ºä¸€åæƒ³èµ°è¿›æœºå™¨äººã€èµ°è¿›æœªæ¥çš„å·¥ç¨‹å¸ˆï¼Œæˆ‘å½“ç„¶ä¹Ÿè¦å­¦ä¼šå®ƒã€‚**äºæ˜¯æˆ‘å†³å®šä»æœ€ç»å…¸ã€æœ€åŸºç¡€çš„ Vision Transformerï¼ˆViTï¼‰å…¥æ‰‹ï¼Œä¸€æ­¥æ­¥ä»åŸç†å‡ºå‘ï¼Œäº²æ‰‹ç”¨ PyTorch å¤ç°ï¼Œå¹¶æ•´ç†ä¸‹è¿™ä¸€è·¯çš„å­¦ä¹ è¿‡ç¨‹ä¸æ€è€ƒï¼Œä½œä¸ºè¿™ç¯‡åšå®¢çš„åˆ†äº«å†…å®¹ã€‚å¦‚æœä½ ä¹Ÿå¯¹ Transformer åœ¨è§†è§‰é¢†åŸŸçš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œæˆ–è€…æ­£åœ¨å…¥é—¨å…·èº«æ™ºèƒ½ç›¸å…³æ–¹å‘ï¼Œå¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼

PSï¼šğŸ’» é¡¹ç›®å®Œæ•´ä»£ç å·²ä¸Šä¼ è‡³Githubï¼š[ViT\_PyTorch](https://github.com/xiongqi123123/ViT_PyTorch.git)ï¼Œå¦‚æœä½ åœ¨é˜…è¯»ä¸­æœ‰ä»»ä½•é—®é¢˜ã€å»ºè®®æˆ–é”™è¯¯æŒ‡å‡ºï¼Œä¹Ÿæ¬¢è¿åœ¨è¯„è®ºåŒºä¸æˆ‘è®¨è®ºï¼Œæˆ‘ä»¬å…±åŒè¿›æ­¥ï¼

ä¸€ã€ViTï¼šä»è®ºæ–‡å‡ºå‘ç†è§£æ¶æ„è®¾è®¡
-----------------

Â Â Â Â Â Â Â Â åœ¨æ­£å¼åŠ¨æ‰‹å¤ç°ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»æºå¤´å‡ºå‘ï¼Œæ¥è¯»ä¸€è¯» Vision Transformer çš„åŸå§‹è®ºæ–‡ï¼šã€Š**An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**ã€‹\[[arXiv:2010.11929](https://arxiv.org/abs/2010.11929)\]ã€‚è¿™æ˜¯ç”± Google Research äº 2020 å¹´æå‡ºçš„ä¸€ç¯‡å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„è®ºæ–‡ï¼Œå®ƒé¦–æ¬¡å±•ç¤ºäº† **çº¯ Transformer æ¶æ„åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šå¯ä»¥ä¸ä¾èµ–ä»»ä½•å·ç§¯æ¨¡å—ï¼Œä¾ç„¶å–å¾—ä¼˜ç§€æ€§èƒ½**ã€‚

![image-20250613213212649](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613213218339-753981665.png)

Â Â Â Â Â Â Â Â Transformer åŸæœ¬æ˜¯ä¸ºäº†è§£å†³è¯­è¨€æ–‡å­—å¤„ç†ä»»åŠ¡è€Œæå‡ºçš„æ¨¡å‹ï¼Œå…¶è®¾è®¡åˆè¡·æ˜¯ç”¨äºå»ºæ¨¡åºåˆ—æ•°æ®ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚åœ¨ NLP é¢†åŸŸä¸­ï¼ŒTransformer èƒ½å¤Ÿé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶çµæ´»åœ°æ•æ‰å•è¯ä¹‹é—´çš„å…¨å±€å…³ç³»ï¼Œæå¤§æå‡äº†è¯­è¨€ç†è§£ä¸ç”Ÿæˆçš„èƒ½åŠ›ã€‚è€Œè°·æ­Œçš„ç ”ç©¶å›¢é˜Ÿæå‡ºäº†éå¸¸å¤§èƒ†ä¹Ÿéå¸¸ä¼˜é›…çš„ä¸€ä¸ªæ€æƒ³ï¼šå¦‚æœæˆ‘ä»¬èƒ½æŠŠå›¾åƒåˆ‡å‰²æˆå°å—ï¼ˆPatchï¼‰ï¼Œå†æŠŠæ¯ä¸ª Patch å½“ä½œä¸€ä¸ªâ€œè¯â€ï¼Œæ˜¯å¦ä¹Ÿèƒ½å°†å›¾åƒè½¬åŒ–ä¸ºåºåˆ—ï¼Œä»è€Œè®© Transformer ä¹Ÿèƒ½å¤„ç†è§†è§‰ä¿¡æ¯ï¼Ÿè€Œå…¶æå‡ºçš„ViT å°±æ˜¯è¿™æ ·åšçš„ï¼šå®ƒå°†ä¸€å¼ å›¾åƒåˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„ Patchï¼ˆå¦‚ 16Ã—16ï¼‰ï¼Œå°†æ¯ä¸ª Patch å±•å¹³æˆå‘é‡ï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§æŠ•å½±å±‚å°†å…¶æ˜ å°„åˆ°ç»Ÿä¸€çš„ç»´åº¦ç©ºé—´ï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ª token åºåˆ—ã€‚éšåï¼ŒViT åœ¨è¿™ä¸ª token åºåˆ—å‰åŠ ä¸Šä¸€ä¸ªå¯å­¦ä¹ çš„ `[CLS]` tokenï¼Œå¹¶å åŠ ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼Œä»¥ä¿ç•™å›¾åƒä¸­çš„ç©ºé—´ä½ç½®ä¿¡æ¯ã€‚æ•´ä¸ªåºåˆ—å°±åƒä¸€æ®µæ–‡æœ¬ï¼Œé€å…¥å¤šå±‚æ ‡å‡†çš„ Transformer ç¼–ç å™¨ç»“æ„è¿›è¡Œå¤„ç†ï¼Œæœ€åé€šè¿‡ `CLS` token çš„è¾“å‡ºï¼Œå®Œæˆæ•´å¼ å›¾åƒçš„åˆ†ç±»ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•ä¸ä¾èµ–ä»»ä½•å·ç§¯æ“ä½œï¼Œå®Œå…¨åŸºäºåºåˆ—å»ºæ¨¡ï¼Œå±•ç°äº† Transformer åœ¨å›¾åƒå»ºæ¨¡ä¸Šçš„å·¨å¤§æ½œåŠ›ã€‚

![image-20250613215651876](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613215655365-764269196.png)

Â Â Â Â Â Â Â Â ViTçš„æ¶æ„å¦‚ä¸Šå›¾ï¼Œä¸å¯»å¸¸çš„åˆ†ç±»ç½‘ç»œç±»ä¼¼ï¼Œæ•´ä¸ªVision Transformerå¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ˜¯ç‰¹å¾æå–éƒ¨åˆ†ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯åˆ†ç±»éƒ¨åˆ†ï¼Œ**ç‰¹å¾æå–éƒ¨åˆ†**æ˜¯å…¶æœ€æ ¸å¿ƒçš„ç»„æˆï¼Œå®ƒåŒ…æ‹¬äº†Patch Embeddingã€Positional Encodingä»¥åŠTransformer Encoderï¼Œ**åˆ†ç±»éƒ¨åˆ†** åˆ™æ˜¯ç´§æ¥åœ¨ç‰¹å¾æå–ä¹‹åï¼Œé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„ `[CLS]` token æ¥ä»£è¡¨æ•´å¼ å›¾åƒçš„å…¨å±€è¯­ä¹‰ã€‚è¿™ä¸ª token ä¼šéšç€å…¶ä»– token ä¸€èµ·å‚ä¸ Transformer ç¼–ç è¿‡ç¨‹ï¼Œæœ€ç»ˆè¢«é€å…¥ä¸€ä¸ªç®€å•çš„ **MLP åˆ†ç±»å¤´** è¿›è¡Œç±»åˆ«é¢„æµ‹ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬æŒ‰ç…§å¦‚ä¸‹çš„åˆ’åˆ†æ¥é€ä¸ªè®²è§£ViTç½‘ç»œæ¶æ„

*   **å›¾åƒåˆ†å—ä¸çº¿æ€§åµŒå…¥æ¨¡å—ï¼ˆPatch Embeddingï¼‰**

![image-20250613220222669](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613220226013-189121227.png)

Â Â Â Â Â Â Â Â ViT çš„ç¬¬ä¸€æ­¥æ“ä½œï¼Œå°±æ˜¯å°†è¾“å…¥å›¾åƒè½¬åŒ–ä¸ºä¸€ç³»åˆ—çš„ **è§†è§‰ token**ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º **Patch Embedding**ï¼ŒPatch æŒ‡çš„å°±æ˜¯åˆ†å‰²åçš„ä¸€å°å—å›¾åƒåŒºåŸŸï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³éå¸¸ç›´æ¥ï¼š

> å°†ä¸€å¼ äºŒç»´å›¾åƒæŒ‰ç…§å›ºå®šå¤§å°ï¼ˆå¦‚ 16Ã—16ï¼‰åˆ’åˆ†æˆè‹¥å¹²ä¸ªå°å—ï¼ˆPatchï¼‰ï¼Œç„¶åå°†æ¯ä¸ª Patch å±•å¹³æˆä¸€ä¸ªå‘é‡ï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚å°†å…¶æ˜ å°„åˆ°æŒ‡å®šçš„ç»´åº¦ç©ºé—´ï¼ˆä¾‹å¦‚ 768ç»´ï¼‰ï¼Œä»è€Œå¾—åˆ°ä¸€ç»„è¾“å…¥ tokenï¼Œä¾› Transformer ä½¿ç”¨ã€‚

Â Â Â Â Â Â Â Â è¿™ä¸ªå¤„ç†æ–¹å¼æœ¬è´¨ä¸Šå°±æ˜¯åœ¨æ¨¡æ‹Ÿ NLP ä¸­â€œå°†æ¯ä¸ªå•è¯ç¼–ç ä¸ºå‘é‡â€çš„è¿‡ç¨‹â€”â€”åªä¸è¿‡è¿™é‡Œçš„â€œå•è¯â€æ˜¯å›¾åƒå— patchï¼Œè€Œä¸æ˜¯æ–‡å­—ï¼Œæˆ‘ä»¬å‡è®¾å‡è®¾è¾“å…¥å›¾åƒå¤§å°ä¸º `224Ã—224Ã—3`ï¼ŒPatch å¤§å°ä¸º `16Ã—16`ï¼Œåˆ™ä¸€å¼ å›¾åƒå°†è¢«åˆ’åˆ†ä¸º$ (224/16)^2=14Ã—14=196$ ä¸ª patchï¼Œè€Œæ¯ä¸ª Patch å°†è¢«å±•å¹³æˆä¸€ä¸ª \\(16 Ã— 16 Ã— 3 = 768\\) ç»´çš„å‘é‡ï¼Œå°†å…¶å±•å¹³æˆå‘é‡åï¼Œå†é€šè¿‡ä¸€ä¸ª `Linear` å±‚æ˜ å°„åˆ°æ¨¡å‹çš„ embedding ç©ºé—´ï¼ˆæ‰‹åŠ¨è®¾ç½®ï¼ŒViT-Baseä¸º 768 ç»´ï¼ŒViT-Largeä¸º1024ï¼ŒViT-Hugeä¸º1280ï¼Œé€šå¸¸ä½¿ç”¨768ï¼‰ï¼Œæœ€ç»ˆæˆ‘ä»¬å°±èƒ½å¾—åˆ°ä¸€ä¸ªå½¢çŠ¶ä¸ºï¼š`[batch_size, 196, embed_dim]`çš„patch token åºåˆ—ï¼Œè€Œæˆ‘ä»¬è¯¥å¦‚ä½•å¯¹å›¾åƒè¿›è¡Œåˆ†å‰²å®ç°Patch Embeddingå‘¢ï¼Ÿè¿™æ—¶å€™æˆ‘ä»¬ä¾¿å¯ä»¥æƒ³åˆ°æˆ‘ä»¬çš„å·ç§¯ï¼Œç”±äºå·ç§¯ä½¿ç”¨çš„æ˜¯æ»‘åŠ¨çª—å£çš„æ€æƒ³ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦å°†å·ç§¯æ ¸ä»¥åŠæ­¥é•¿è®¾ç½®æˆä¸Patch-Sizeç›¸ç­‰ä¾¿å¯ï¼Œè¿™æ—¶ä¸¤ä¸ªå›¾ç‰‡åŒºåŸŸçš„ç‰¹å¾æå–è¿‡ç¨‹å°±ä¸ä¼šæœ‰é‡å ï¼Œå½“æˆ‘ä»¬è¾“å…¥çš„å›¾ç‰‡æ˜¯`[224, 224, 3]`çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—ä¸€ä¸ª`[14, 14, 768]`çš„ç‰¹å¾å±‚ã€‚

![å·ç§¯](https://i-blog.csdnimg.cn/blog_migrate/2c1957df057acb9c81aa653920479cb5.gif#pic_center)

Â Â Â Â Â Â Â Â è€Œè·å¾—äº†ç‰¹å¾ä¿¡æ¯ä¹‹åæˆ‘ä»¬éœ€è¦å°†å¾—åˆ°çš„ç‰¹å¾ä¿¡æ¯ç»„åˆæˆåºåˆ—ï¼Œç»„åˆçš„æ–¹å¼å¾ˆç®€å•ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹è¿™ä¸ªç‰¹å¾å›¾è¿›è¡Œå±•å¹³ï¼ˆFlattenï¼‰å¹¶è½¬ç½®ä¸ºæ ‡å‡†åºåˆ—æ ¼å¼ï¼Œä¾¿å¯ä»¥å¾—åˆ°æœ€ç»ˆçš„ Patch Token åºåˆ—ï¼Œç”¨äºè¾“å…¥ Transformerï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢å¯¹å›¾åƒè¿›è¡Œåˆ†å‰²åå¾—åˆ°äº†ä¸€ä¸ª`[14, 14, 768]`çš„ç‰¹å¾å±‚ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªç‰¹å¾å›¾çš„**é«˜å®½ç»´åº¦è¿›è¡Œå¹³é“º**åå³å¯å¾—åˆ°**ä¸€ä¸ª`[196, 768]`çš„ç‰¹å¾å±‚**ï¼Œè‡³æ­¤Patch Embeddingä¾¿å®Œæˆå•¦ï¼

*   **åˆ†ç±»æ ‡è®°ä¸ä½ç½®ç¼–ç æ¨¡å—ï¼ˆcls\_token + Position Embeddingï¼‰**

![image-20250613220236402](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613220239619-291879325.png)

Â Â Â Â Â Â Â Â åœ¨å®Œæˆ Patch Embedding å¾—åˆ°å½¢å¦‚ `[batch_size, 196, 768]` çš„ Patch Token åºåˆ—åï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¦åšä¸¤ä»¶å…³é”®çš„äº‹æƒ…ï¼š

1.  æ·»åŠ  `[CLS] Token` â€”â€” å›¾åƒçš„â€œå…¨å±€æ‘˜è¦â€å…¥å£
    
    Â Â Â Â Â Â Â Â Transformer æœ€åˆåœ¨å¤„ç†æ–‡æœ¬ä»»åŠ¡æ—¶ï¼Œä¼šåœ¨åºåˆ—çš„æœ€å‰é¢æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„ `[CLS]` Tokenï¼Œç”¨äºèšåˆæ•´ä¸ªå¥å­çš„è¯­ä¹‰ä¿¡æ¯ã€‚åŒç†ï¼Œåœ¨ ViT ä¸­ä¹Ÿå¼•å…¥äº† `[CLS] Token`ï¼Œå®ƒå¹¶ä¸ä»£è¡¨æŸä¸ªå…·ä½“çš„ Patchï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªå…¨å±€çš„ä»£è¡¨ Tokenï¼Œåœ¨ Transformer ä¸­â€œå‚ä¸â€æ¯ä¸€å±‚çš„ä¿¡æ¯äº¤äº’ï¼Œæœ€ç»ˆç”¨äºæå–æ•´ä¸ªå›¾åƒçš„å…¨å±€ç‰¹å¾ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç¼–å·ä¸º `0*` çš„é‚£ä¸ªä½ç½®å³è¡¨ç¤º `[CLS] Token`ï¼Œå…¶åˆå§‹å€¼æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°å‘é‡ï¼Œç»´åº¦ä¸ Patch Token ç›¸åŒï¼ˆä¾‹å¦‚ 768ï¼‰ï¼Œç»è¿‡ Transformer ç¼–ç åï¼ŒViT ä¼š**ä½¿ç”¨è¿™ä¸ª `[CLS] Token` çš„è¾“å‡ºå‘é‡ä½œä¸ºå›¾åƒçš„åˆ†ç±»ç»“æœè¾“å…¥**åˆ° MLP Head ä¸­ï¼Œå®Œæˆæœ€ç»ˆåˆ†ç±»ã€‚
    
    Â Â Â Â Â Â Â Â æ·»åŠ äº† `[CLS] Token` ä¹‹åï¼ŒåŸæœ¬çš„ `196` ä¸ª Patch Token åºåˆ—å°±å˜æˆäº† `197` ä¸ª Tokenï¼Œå½¢çŠ¶å˜ä¸ºäº†å½¢å¦‚ï¼š`[batch_size, 196 + 1, 768]`
    
2.  æ·»åŠ ä½ç½®ç¼–ç ï¼ˆPositional Embeddingï¼‰â€”â€” å¸®åŠ©æ¨¡å‹ç†è§£â€œå›¾åƒä¸­çš„ä½ç½®â€
    
    Â Â Â Â Â Â Â Â ç”±äº Transformer æ˜¯å®Œå…¨åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶æ„å»ºçš„ï¼Œå®ƒå¹¶ä¸å…·å¤‡å·ç§¯ç½‘ç»œä¸­å¤©ç„¶çš„**ä½ç½®ä¿¡æ¯å»ºæ¨¡èƒ½åŠ›**ã€‚æ‰€ä»¥æˆ‘ä»¬è¿˜éœ€è¦ç»™æ¯ä¸ª Token æ·»åŠ ä¸€ä¸ª**ä½ç½®ç¼–ç **ï¼Œç”¨äºå‘Šè¯‰æ¨¡å‹è¿™ä¸ª Token æ¥è‡ªäºå›¾åƒçš„å“ªä¸€å—åŒºåŸŸã€‚ViT é‡‡ç”¨çš„æ˜¯ä¸€ç§ **å¯å­¦ä¹ çš„ç»å¯¹ä½ç½®ç¼–ç **ï¼Œä¹Ÿå°±æ˜¯ä¸ºæ¯ä¸€ä¸ª Token çš„ä½ç½®ï¼ˆåŒ…æ‹¬ `[CLS]` Tokenï¼‰éƒ½åˆå§‹åŒ–ä¸€ä¸ªå¯å­¦ä¹ çš„ä½ç½®å‘é‡ï¼Œå¹¶ä¸åŸå§‹ Token ç›¸åŠ ï¼Œè¿™æ ·ï¼Œæ¨¡å‹å°±èƒ½åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è‡ªå·±æŒæ¡ç©ºé—´é¡ºåºå’Œè¯­ä¹‰ä¹‹é—´çš„å…³ç³»ã€‚
    
    Â Â Â Â Â Â Â Â ä½ç½®ç¼–ç çš„å½¢çŠ¶ä¸è¾“å…¥åºåˆ—ä¸€è‡´ï¼Œä¹Ÿæ˜¯`[1, 196 + 1, 768]`ï¼Œä¸”ä½ç½®ç¼–ç çš„åŠ å…¥æ–¹å¼éå¸¸ç®€å•å³ï¼š`tokens = tokens + pos_embed # [B, 197, 768]`,ç»è¿‡è¿™ä¸¤ä¸ªæ­¥éª¤ä¹‹åï¼ŒViT çš„è¾“å…¥æ‰çœŸæ­£å‡†å¤‡å¥½ï¼Œå¯ä»¥é€å…¥ Transformer ç¼–ç å™¨ä¸­è¿›è¡Œå¤šå±‚ç‰¹å¾äº¤äº’ä¸å»ºæ¨¡ï¼Œè‡³æ­¤cls\_token + Position Embeddingä¾¿å®Œæˆå•¦ï¼
    

*   **æ ‡å‡† Transformer ç¼–ç å™¨ï¼ˆMulti-head Attention + LayerNorm + MLP + æ®‹å·®è¿æ¥ï¼‰**

![image-20250613220454690](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613220458003-7279660.png)

Â Â Â Â Â Â Â Â å½“æˆ‘ä»¬å¾—åˆ°äº†å¸¦æœ‰ `[CLS] Token` å’Œä½ç½®ç¼–ç çš„å®Œæ•´ Patch åºåˆ—ï¼ˆå½¢çŠ¶ä¸º `[B, 197, 768]`ï¼‰ä¹‹åï¼ŒViT ä¼šå°†å…¶é€å…¥ä¸€ç³»åˆ—æ ‡å‡†çš„ **Transformer Encoder Block** ä¸­è¿›è¡Œæ·±åº¦å»ºæ¨¡ã€‚æ¯ä¸€ä¸ª Block çš„è®¾è®¡ä¸åŸå§‹çš„ NLP Transformer ä¸­çš„ Encoder ä¿æŒä¸€è‡´ï¼Œç»“æ„éå¸¸ç»å…¸ï¼Œç”±ä¸¤ä¸ªå­æ¨¡å—ç»„æˆï¼š

1.  **LayerNorm + å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self Attentionï¼‰**
    
    Â Â Â Â Â Â Â Â åœ¨è¿™ä¸ªå­æ¨¡å—ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œ LayerNorm å½’ä¸€åŒ–ï¼Œå†é€å…¥ Multi-Head Self-Attention æ¨¡å—ï¼Œè¿™é‡Œçš„è‡ªæ³¨æ„åŠ›çš„ä½œç”¨æ˜¯å»ºç«‹æ‰€æœ‰ Token ä¹‹é—´çš„**å…¨å±€å…³ç³»**ï¼Œä½¿æ¯ä¸ª Token éƒ½èƒ½è·å–å…¶ä»–åŒºåŸŸçš„ä¿¡æ¯ï¼Œè¿™ä¹Ÿæ˜¯Transformerçš„çµé­‚éƒ¨åˆ†ï¼Œå…¶å…·ä½“å®ç°åˆ™æ˜¯è®©æ¯ä¸ª Token é€šè¿‡æŸ¥è¯¢ï¼ˆQueryï¼‰ä¸æ‰€æœ‰å…¶ä»– Token çš„é”®ï¼ˆKeyï¼‰è¿›è¡ŒåŒ¹é…ï¼Œè®¡ç®—å…¶å¯¹å…¶ä»–ä½ç½®çš„å…³æ³¨æƒé‡ï¼Œä»è€Œæå–å¯¹å½“å‰ä»»åŠ¡æœ€æœ‰ç”¨çš„ä¿¡æ¯ã€‚ç”¨å…¬å¼å’Œå›¾ç‰‡è¡¨ç¤ºå¦‚ä¸‹ï¼š
    

\\\[\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^\\top}{\\sqrt{d\_k}} \\right) V \\\]

![kqv](https://i-blog.csdnimg.cn/blog_migrate/b0dfd9f9109a979f94a1f8aa4e6663e3.gif#pic_center)

Â Â Â Â Â Â Â Â å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™éƒ¨åˆ†å†…å®¹çœ‹èµ·æ¥å¯èƒ½ä¼šæ¯”è¾ƒæŠ½è±¡ï¼Œä½†æ˜¯æˆ‘ä»¬å¦‚æœå°†å®ƒæ‹†è§£ä¸€æ­¥ä¸€æ­¥æ¥çœ‹ï¼Œå…¶å®éå¸¸ç›´è§‚ã€‚åœ¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæ¯ä¸€ä¸ªè¾“å…¥çš„ **Token**ï¼ˆå›¾åƒçš„ Patchï¼‰éƒ½ä¼šè¢«åˆ†åˆ«æ˜ å°„å‡ºä¸‰ä¸ªå‘é‡ï¼Œåˆ†åˆ«æ˜¯ï¼š**Queryï¼ˆæŸ¥è¯¢å‘é‡ï¼‰ã€Keyï¼ˆé”®å‘é‡ï¼‰ã€Valueï¼ˆå€¼å‘é‡ï¼‰**ï¼Œç”¨ä¸€ä¸ªç®€å•æ˜äº†çš„æ¯”å–»æ¥ç†è§£ï¼šå‡è®¾ä½ åœ¨å‚åŠ ä¸€æ¬¡ä¼šè®®ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼Œä½ æ˜¯ Queryï¼Œè€Œä¼šè®®å®¤é‡Œæ¯ä¸€ä¸ªä¸ä¼šè€…ï¼ˆåŒ…æ‹¬ä½ è‡ªå·±ï¼‰éƒ½æ˜¯ä¸€ä¸ª Keyï¼ŒåŒæ—¶ä»–ä»¬æ‰‹é‡Œéƒ½æ‹¿ç€ä¸€ä»½èµ„æ–™ï¼ˆValueï¼‰ï¼Œä½ ä¼šæ ¹æ®è‡ªå·±å’Œå…¶ä»–äºº Key çš„â€œç›¸ä¼¼ç¨‹åº¦â€å†³å®šä½ è¦å¤šå¤§ç¨‹åº¦å‚è€ƒä»–ä»¬çš„èµ„æ–™ï¼ˆValueï¼‰â€”â€”è¿™å°±æ˜¯æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—ã€‚è®¾å½“å‰è¾“å…¥åºåˆ—ä¸ºçŸ©é˜µ \\(X \\in \\mathbb{R}^{n \\times d}\\)ï¼Œå…¶ä¸­ \\(n\\) æ˜¯åºåˆ—é•¿åº¦ï¼ˆä¾‹å¦‚ ViT ä¸­æ˜¯ 197 ä¸ª Tokenï¼‰ï¼Œ\\(d\\) æ˜¯æ¯ä¸ª Token çš„ç»´åº¦ï¼ˆä¾‹å¦‚ 768ï¼‰ã€‚æˆ‘ä»¬ç”¨ä¸‰ç»„å¯å­¦ä¹ çš„å‚æ•°çŸ©é˜µå°†å…¶å˜æ¢ä¸ºï¼š

\\\[\[ Q = XW^Q,\\quad K = XW^K,\\quad V = XW^V \] \\\]

Â Â Â Â Â Â Â Â ç„¶åè®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼ˆScoreï¼‰ï¼š

\\\[\[ \\text{Score} = \\frac{QK^\\top}{\\sqrt{d\_k}} \] \\\]

Â Â Â Â Â Â Â Â æ¥ç€ä½¿ç”¨ Softmax å¯¹å¾—åˆ†è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ \\(\\alpha\\)ï¼š

\\\[\[ \\alpha = \\text{Softmax}\\left( \\frac{QK^\\top}{\\sqrt{d\_k}} \\right) \] \\\]

Â Â Â Â Â Â Â Â æœ€ååŠ æƒç»„åˆæ‰€æœ‰ Value å‘é‡ï¼Œå¾—åˆ°æ–°çš„è¾“å‡ºè¡¨ç¤ºï¼š

\\\[\[ \\text{Attention}(Q, K, V) = \\alpha V \] \\\]

Â Â Â Â Â Â Â Â **è¿™å¥—è®¡ç®—æµç¨‹ç”¨äººè¯è¯´å°±æ˜¯ï¼Œ**æˆ‘ä»¬è¿™å¥—æ³¨æ„åŠ›ç³»ç»Ÿå‡è®¾æœ‰ä¸‰ä¸ªè¾“å…¥åˆ†åˆ«æ˜¯`input-1`ã€`input-2`ã€`input-3`ä»¥åŠä¸‰ä¸ªå¯¹åº”çš„è¾“å‡º`output-1`ã€`output-2`ã€`output-3`ï¼Œæ¯ä¸ªè¾“å…¥éƒ½æœ‰ä»–ä»¬è‡ªå·±çš„QKVå‘é‡

![image-20250614010033394](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614010037294-166599101.png)

Â Â Â Â Â Â Â Â å¦‚æœæˆ‘ä»¬è¦æ±‚`output-1`ï¼Œé‚£æˆ‘ä»¬é¦–å…ˆå…ˆå°†`input-1`çš„QæŸ¥è¯¢å‘é‡ä¸ä¸‰ä¸ªè¾“å…¥çš„Ké”®å‘é‡åˆ†åˆ«ç›¸ä¹˜å¾—åˆ°å¯¹åº”çš„åˆ†æ•°ï¼Œè¿™ä¸ªåˆ†æ•°ä»£è¡¨çš„ä¾¿æ˜¯ `input-1` å¯¹å…¶å®ƒä¸‰ä¸ªè¾“å…¥çš„â€œæ³¨æ„åŠ›ç¨‹åº¦â€ï¼›æ¥ä¸‹æ¥æˆ‘ä»¬å°†è¿™ä¸‰ä¸ªåˆ†æ•°åˆ†åˆ«æ±‚ä¸€æ¬¡`softmax`ä½¿å®ƒä»¬å˜æˆ 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡å€¼ï¼Œå¹¶ä¸”åŠ èµ·æ¥ä¸º 1ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯ä»¥ç†è§£ä¸ºï¼š**åˆ†é…å…³æ³¨åº¦**ï¼Œå‘Šè¯‰æˆ‘ä»¬è¯¥â€œå…³æ³¨è°ã€å…³æ³¨å¤šå°‘â€ï¼›æœ€åï¼Œç”¨åˆšæ‰å¾—åˆ°çš„è¿™ä¸‰ä¸ªæ³¨æ„åŠ›æƒé‡ï¼Œå»åˆ†åˆ«åŠ æƒå¯¹åº”çš„ **å€¼å‘é‡ V**ï¼Œå†æŠŠå®ƒä»¬åŠ åœ¨ä¸€èµ·ï¼Œå¾—åˆ°çš„å°±æ˜¯æœ€ç»ˆçš„ `output-1`

![image-20250614010355459](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614025205817-1784798237.png)

Â Â Â Â Â Â Â Â ä¹Ÿå°±æ˜¯è¯´ï¼šè¾“å‡º = æ‰€æœ‰å…³æ³¨å¯¹è±¡çš„â€œå€¼â€ Ã— â€œå…³æ³¨å®ƒçš„ç¨‹åº¦â€çš„åŠ æƒå’Œã€‚æ¯ä¸ª Query ä¼šæ ¹æ®ä¸æ‰€æœ‰ Key çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œå¯¹å¯¹åº”çš„ Value è¿›è¡ŒåŠ æƒæ±‚å’Œï¼›è¿™æ ·çš„è¯æ‰€æœ‰ Token ä¹‹é—´éƒ½èƒ½è¿›è¡Œä¿¡æ¯äº¤æ¢ï¼Œä»è€Œæ•æ‰ **å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–**ï¼›å› æ­¤æœ€åçš„è¾“å‡ºçš„è™½ç„¶ä»ç„¶æ˜¯ä¸€ä¸ªä¸åŸå§‹ Token æ•°é‡ç›¸åŒçš„æ–°åºåˆ—ï¼Œä½†æ¯ä¸ª Token çš„è¡¨ç¤ºå·²ç»èåˆäº†å…¨å±€ä¿¡æ¯ã€‚

2.  **LayerNorm + MLP å‰é¦ˆç¥ç»ç½‘ç»œ**
    
    Â Â Â Â Â Â Â Â åœ¨æ¯ä¸ª Transformer Block ä¸­ï¼Œé™¤äº†æ³¨æ„åŠ›æœºåˆ¶ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„éƒ¨åˆ†ï¼Œé‚£å°±æ˜¯ **å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed Forward Network, FFNï¼‰**ï¼Œä¹Ÿå¸¸è¢«ç§°ä¸º **MLP å­æ¨¡å—**ã€‚è¿™ä¸ªå­æ¨¡å—çš„ç»“æ„å…¶å®éå¸¸ç®€å•ï¼Œå°±æ˜¯ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼ˆLinearï¼‰ï¼Œä¸­é—´å†åŠ ä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ GELUï¼‰ï¼š
    
    \\\[FFN(x)=Linear 2 â€‹ (GELU(Linear 1 â€‹ (x))) \\\]
    
    Â Â Â Â Â Â Â Â è¿™é‡Œçš„ Linear å±‚ä¹Ÿå°±æ˜¯æˆ‘ä»¬ç†Ÿæ‚‰çš„å…¨è¿æ¥å±‚ï¼Œç»´åº¦çš„å˜åŒ–ä¸€èˆ¬æ˜¯è¿™æ ·çš„ï¼šé¦–å…ˆç¬¬ä¸€ä¸ª Linear å±‚ä¼šæŠŠè¾“å…¥çš„ç»´åº¦ä» `d_model`ï¼ˆæ¯”å¦‚ 768ï¼‰æå‡åˆ°ä¸€ä¸ªæ›´é«˜çš„ç»´åº¦ï¼ˆæ¯”å¦‚ 3072ï¼‰ï¼Œæ¥ç€é€šè¿‡ GELU æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œæœ€åå†ç”¨ä¸€ä¸ª Linear å±‚å°†ç»´åº¦é™å›åŸæ¥çš„ `d_model`è¿™ä¸ª FFN çš„ç»“æ„å¯ä»¥ç†è§£ä¸ºå¯¹æ¯ä¸ª Token ç‹¬ç«‹åœ°è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„ç‰¹å¾å˜æ¢ã€‚ä¸åŒäºå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶é‚£ç§è·¨ Token çš„ä¿¡æ¯äº¤äº’ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œçš„å¤„ç†æ˜¯**é€ Token çš„ç‚¹å¯¹ç‚¹éçº¿æ€§å˜æ¢**ï¼Œä¸»è¦ç”¨äºå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è€Œæ®‹å·®è¿æ¥çš„å¼•å…¥å¯ä»¥åœ¨æ¯ä¸ª Transformer Block å†…å½¢æˆä¸€ç§**çŸ­è·¯è·¯å¾„ï¼ˆShortcut Pathï¼‰**ï¼Œå®ƒèƒ½æœ‰æ•ˆç¼“è§£æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼šä¸å…¶ç›´æ¥å­¦ä¹ ä¸€ä¸ªæ˜ å°„å‡½æ•° \\(F(x)\\)ï¼Œä¸å¦‚è®©ç½‘ç»œå­¦ä¹  \\(F(x) = H(x) - x\\)ï¼Œå³è®©æ¨¡å‹å…³æ³¨â€œè¾“å…¥ä¸è¾“å‡ºçš„å·®å€¼â€ï¼Œè¿™æ ·åè€Œæ›´å®¹æ˜“ä¼˜åŒ–ã€‚å› æ­¤å®Œæ•´çš„è®¡ç®—æµç¨‹å¦‚ä¸‹ï¼š
    
    \\\[y=x+FFN(LayerNorm(x)) \\\]
    

Â Â Â Â Â Â Â Â è‡³æ­¤ï¼ŒViT çš„æ ¸å¿ƒç»“æ„ä¹Ÿå°±å®Œæ•´æ‹¼è£…å®Œæˆäº†ã€‚ä» Patch Embedding åˆ° `[CLS] Token` ä¸ä½ç½®ç¼–ç ï¼Œå†åˆ°æ·±åº¦çš„å¤šå±‚ Transformer ç¼–ç å™¨ï¼ŒViT å®Œæ•´åœ°å°†è¯­è¨€æ¨¡å‹çš„ç»“æ„ç§»æ¤åˆ°äº†è§†è§‰é¢†åŸŸï¼Œå¹¶å–å¾—äº†çªç ´æ€§çš„è¡¨ç°ã€‚Transformer Block æ˜¯ ViT çš„â€œå»ºæ¨¡å¤§è„‘â€ï¼Œä¹Ÿæ˜¯å…¶é€šç”¨æ€§ä¸å¼ºå¤§æ€§èƒ½çš„æ ¹åŸºã€‚

*   **åˆ†ç±»å¤´ï¼ˆClassification Headï¼‰**

![image-20250613220508587](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250613220512171-1855280189.png)

Â Â Â Â Â Â Â Â ç»è¿‡å¤šä¸ª Transformer Block çš„æ·±åº¦ç‰¹å¾æå–ä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„åºåˆ—è¡¨ç¤ºï¼Œå…¶å½¢çŠ¶ä¸º `[B, 197, 768]`ï¼ˆå‡è®¾æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ ViT-Base æ¨¡å‹ï¼‰ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªä½ç½®çš„ Token ä»ç„¶æ˜¯æˆ‘ä»¬åœ¨æœ€å¼€å§‹åŠ å…¥çš„ `[CLS] Token`ã€‚è¿™ä¸ª `[CLS] Token` å¯ä»¥çœ‹ä½œæ˜¯æ•´ä¸ªå›¾åƒçš„å…¨å±€è¯­ä¹‰è¡¨ç¤ºï¼Œå› ä¸ºåœ¨å¤šè½®æ³¨æ„åŠ›äº¤äº’ä¸­ï¼Œå®ƒå·²ç»â€œèåˆâ€äº†æ‰€æœ‰ Patch çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªéœ€è¦ä»åºåˆ—ä¸­å–å‡ºè¿™ä¸€ä½ç½®çš„å‘é‡ï¼ˆå³ç¬¬ä¸€ä¸ª Tokenï¼‰ï¼Œç„¶åé€å…¥ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆLinearï¼‰å°±å¯ä»¥å®Œæˆåˆ†ç±»ä»»åŠ¡äº†ã€‚

äºŒã€å®æˆ˜å¤ç°PyTorchç‰ˆViTç½‘ç»œæ¶æ„
---------------------

### ï¼ˆä¸€ï¼‰æ¨¡å—1ï¼šPatchEmbeddingç±»

Â Â Â Â Â Â Â Â `PatchEmbedding` æ˜¯ ViT ä¸­æœ€å…³é”®çš„ä¸€æ­¥ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨å·ç§¯æ“ä½œå°†è¾“å…¥å›¾åƒåˆ’åˆ†ä¸ºè‹¥å¹²ä¸é‡å çš„å°å—ï¼ˆPatchï¼‰ï¼Œæ¯ä¸ª Patch è¢«ç¼–ç ä¸ºä¸€ä¸ªå‘é‡ï¼Œæˆ‘ä»¬é‡‡ç”¨ç­‰æ­¥é•¿å·ç§¯çš„æ–¹å¼å®ç°åˆ’åˆ†ï¼Œå¹¶åœ¨å±•å¹³åå°†å…¶é€å…¥ Transformer æ¨¡å—è¿›è¡Œåç»­å¤„ç†ã€‚

    class VisionPatchEmbedding(nn.Module):
        def __init__(self, image_size, patch_size, in_channels, embed_dim, flatter=True):
            super().__init__()
            self.proj = nn.Conv2d(in_channels, embed_dim, patch_size, patch_size)
            self.norm = nn.LayerNorm(embed_dim)
            self.flatter = flatter
    
        def forward(self, x):
            x = self.proj(x)
            if self.flatter:
                x = x.flatten(2).transpose(1, 2)  # [B, C, H, W] -> [B, N, C]
            x = self.norm(x)
            return x
    

### ï¼ˆäºŒï¼‰æ¨¡å—2ï¼šPositionEmbedding

Â Â Â Â Â Â Â Â ç”±äºViT ä¸åƒCNNï¼Œå…¶æ²¡æœ‰å·ç§¯æ„Ÿå—é‡ï¼Œå› æ­¤éœ€è¦åŠ å…¥ä½ç½®ç¼–ç ï¼ˆ`pos_embed`ï¼‰æ¥ä¿ç•™ä½ç½®ä¿¡æ¯æ¯ä¸€ä¸ªPatchçš„ä½ç½®ä¿¡æ¯

    self.cls_token = nn.Parameter(torch.zeros(1, 1, num_features))
    self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, num_features))
    

Â Â Â Â Â Â Â Â ç”±ViTViTåœ¨**é¢„è®­ç»ƒæ—¶**é€šå¸¸ä½¿ç”¨å›ºå®šåˆ†è¾¨ç‡ï¼ˆå¦‚ `224x224`ï¼‰ï¼Œå›¾åƒè¢«åˆ†å‰²ä¸º `14x14` ä¸ª Patchï¼ˆ`patch_size=16`ï¼‰ï¼Œä½ç½®ç¼–ç  `pos_embed` çš„å½¢çŠ¶ä¸º `[1, 197, 768]`ï¼ˆ`197 = 1(cls_token) + 14x14`ï¼‰ï¼Œä½†æ˜¯åŒæ ·ç”±äºViTæ²¡æœ‰æ„Ÿå—é‡ä¸€è¯´ï¼Œå› æ­¤**å®é™…åº”ç”¨**å½“è¾“å…¥åˆ†è¾¨ç‡ä¸åŒï¼ˆå¦‚ `256x256`ï¼‰æ—¶ï¼ŒPatch æ•°é‡å˜ä¸º `16x16 = 256`ï¼ˆ`+1 cls_token = 257`ï¼‰ï¼ŒåŸæ¥çš„ä½ç½®ç¼–ç ï¼ˆ`197`ï¼‰ä¾¿æ— æ³•ç›´æ¥ä½¿ç”¨ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦é€šè¿‡åŒä¸‰æ¬¡æ’å€¼ï¼ˆ`bicubic`ï¼‰å°† `14x14` çš„ä½ç½®ç¼–ç è°ƒæ•´åˆ°æ–°åˆ†è¾¨ç‡å¯¹åº”çš„ç½‘æ ¼å°ºå¯¸ï¼Œå…·ä½“å®ç°å¦‚ä¸‹ï¼š

    img_token_pos_embed = F.interpolate(
        img_token_pos_embed, size=self.features_shape, mode='bicubic', align_corners=False
    )
    pos_embed = torch.cat((cls_token_pos_embed, img_token_pos_embed), dim=1)
    x = self.pos_drop(x + pos_embed)
    

### ï¼ˆä¸‰ï¼‰æ¨¡å—3ï¼šMulti-head Attentionä¸MLP

Â Â Â Â Â Â Â Â æ¥ç€æˆ‘ä»¬å®ç°Transformerä¸­æœ€å…³é”®çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç±»`SelfAttention` æ¥å®ç°äº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è¿™ä¸ªæ¨¡å—é‡Œæˆ‘ä»¬å…ˆç”¨ä¸€ä¸ªçº¿æ€§å±‚åŒæ—¶ç”ŸæˆæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰ï¼Œå¹¶æŒ‰å¤´æ•°æ‹†åˆ†ç»´åº¦ï¼Œç„¶åè®¡ç®— Q å’Œ K çš„ç‚¹ç§¯å¹¶ç¼©æ”¾ï¼Œé€šè¿‡ `softmax`å¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼Œåˆ©ç”¨æƒé‡åŠ æƒå€¼ï¼ˆVï¼‰ï¼Œæœ€åå°†å¤šå¤´ç»“æœæ‹¼æ¥åé€šè¿‡çº¿æ€§å˜æ¢å’Œ `dropout`ï¼Œä½¿è¾“å‡ºå…·æœ‰ä¸è¾“å…¥ç»´åº¦ç›¸åŒçš„ç‰¹å¾ï¼Œå®Œæˆä¿¡æ¯çš„åŠ¨æ€èåˆä¸è¡¨è¾¾å¢å¼ºã€‚

    class SelfAttention(nn.Module):
        def __init__(self, dim, num_heads, qkv_bias=False, attn_drop_rate=0.0, proj_drop_rate=0.0):
            super().__init__()
            self.num_heads = num_heads
            self.head_dim = dim // num_heads
            self.scale = self.head_dim ** -0.5
    
            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
            self.attn_drop = nn.Dropout(attn_drop_rate)
            self.proj = nn.Linear(dim, dim)
            self.proj_drop = nn.Dropout(proj_drop_rate)
    
        def forward(self, x):
            B, N, C = x.shape
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2,0,3,1,4)
            q, k, v = qkv[0], qkv[1], qkv[2]
    
            attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale
            attn = attn .softmax(dim=-1)
            attn = self.attn_drop(attn)
    
            x = torch.matmul(attn, v).transpose(1,2).reshape(B,N,C)
            x = self.proj(x)
            x = self.proj_drop(x)
    
            return x
    

Â Â Â Â Â Â Â Â åœ¨è¿™ä¸ª `MLP` æ¨¡å—ä¸­ï¼Œæˆ‘è®¾è®¡äº†ä¸€ä¸ªä¸¤å±‚çš„å…¨è¿æ¥ç½‘ç»œï¼Œé¦–å…ˆé€šè¿‡ `fc1` å°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ°éšè—ç»´åº¦ï¼Œç„¶åç»è¿‡æ¿€æ´»å‡½æ•°éçº¿æ€§å˜æ¢ï¼Œæ¥ç€ç”¨ dropout åšæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæ¥ç€å†é€šè¿‡ `fc2` æ˜ å°„åˆ°è¾“å‡ºç»´åº¦ï¼Œæœ€åå†ç”¨ä¸€æ¬¡ dropoutï¼Œè¿™ä¸ªè¿‡ç¨‹ç”¨æ¥å¸®åŠ©æ¨¡å‹æ•æ‰æ›´ä¸°å¯Œçš„éçº¿æ€§ç‰¹å¾ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›ã€‚

    class MLP(nn.Module):
        def __init__(self, in_features, hidden_features, out_features, act_layer, drop_rate):
            super().__init__()
            out_features = out_features or in_features
            hidden_features = hidden_features or in_features
            drop_probs = (drop_rate, drop_rate)
    
            self.fc1 = nn.Linear(in_features, hidden_features)
            self.act = act_layer()
            self.drop1 = nn.Dropout(drop_probs[0])
            self.fc2 = nn.Linear(hidden_features, out_features)
            self.drop2 = nn.Dropout(drop_probs[1])
    
        def forward(self, x):
            x = self.fc1(x)
            x = self.act(x)
            x = self.drop1(x)
            x = self.fc2(x)
            x = self.drop2(x)
            return x
    

### ï¼ˆå››ï¼‰æ¨¡å—4ï¼šEncoderå±‚å †å 

Â Â Â Â Â Â Â Â ç”±äºPyTorchä¸­æ²¡æœ‰ç°æˆçš„`DropPath`å‡½æ•°å¯ä»¥ä½¿ç”¨ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è‡ªå·±å®ç°è¿™ä¸€ç”¨æ³•ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨DropPathæ¥éšæœºä¸¢å¼ƒå®Œæ•´è·¯å¾„å®ç°æ·±åº¦ç½‘ç»œçš„æ­£åˆ™åŒ–ï¼Œå¹¶åœ¨è®­ç»ƒæ—¶ä»¥æ¦‚ç‡drop\_pathè·³è¿‡å½“å‰æ¨¡å—å¹¶ç¼©æ”¾å‰©ä½™è·¯å¾„ä»¥ä¿æŒæœŸæœ›å€¼ï¼›åœ¨Blockç±»ä¸­æˆ‘åˆ™å°è£…äº†å®Œæ•´çš„Transformerå±‚ç»“æ„ï¼ŒåŒ…å«LayerNormå½’ä¸€åŒ–ã€å¤šå¤´æ³¨æ„åŠ›ã€MLPå‰é¦ˆç½‘ç»œå’Œæ®‹å·®è¿æ¥ï¼Œå…¶ä¸­æ³¨æ„åŠ›éƒ¨åˆ†ä½¿ç”¨æˆ‘è‡ªå®šä¹‰çš„SelfAttentionæ¨¡å—ï¼ŒMLPé‡‡ç”¨å…ˆæ‰©å±•åå‹ç¼©çš„ç»“æ„è®¾è®¡ï¼Œä¸¤è€…éƒ½é›†æˆäº†DropPathæœºåˆ¶

Â Â Â Â Â Â Â Â äºæ˜¯ä¸€ä¸ªå®Œæ•´çš„Transformer Blockè®¡ç®—æµç¨‹å¦‚ä¸‹ï¼š

graph TD A\[è¾“å…¥ x\] A --> B\[LayerNorm\] B --> C\[Multi-Head Self-Attention\] C --> D\["Residual Add: x + Attention"\] D --> E\[LayerNorm\] E --> F\[FeedForward MLP\] F --> G\["Residual Add: D + MLP"\] G --> H\[è¾“å‡º y\]

    class DropPath(nn.Module):
        def __init__(self, drop_prob=None):
            super(DropPath, self).__init__()
            self.drop_prob = drop_prob
    
        def drop_path(self, x, drop_prob, training):
            if drop_prob == 0. or not training:
                return x
            keep_prob       = 1 - drop_prob
            shape           = (x.shape[0],) + (1,) * (x.ndim - 1)
            random_tensor   = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
            random_tensor.floor_() 
            output          = x.div(keep_prob) * random_tensor
            return output
    
        def forward(self, x):
            return self.drop_path(x, self.drop_prob, self.training)
        
    class Block(nn.Module):
        def __init__(self, dim, num_heads, mlp_radio, qkv_bias, drop, attn_drop, drop_path, act_layer, norm_layer):
            super().__init__()
            self.norm_1 = norm_layer(dim)
            self.attn = SelfAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop_rate=attn_drop, proj_drop_rate=drop)
            self.norm_2 = norm_layer(dim)
            self.mlp = MLP(in_features=dim, hidden_features=int(dim * mlp_radio), out_features=None, act_layer=act_layer, drop_rate=drop_path)
            self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity() # ä¸¢å¼ƒè·¯å¾„
    
        def forward(self, x):
            x = x + self.drop_path(self.attn(self.norm_1(x)))
            x = x + self.drop_path(self.mlp(self.norm_2(x)))
            return x
    

### ï¼ˆäº”ï¼‰æ¨¡å—5ï¼šViTæ•´ä½“æ¨¡å‹ç±»

Â Â Â Â Â Â Â Â æœ€åæˆ‘ä»¬æ¥å®ç°æˆ‘ä»¬å®Œæ•´çš„ViTâ€”VisonTransformerï¼Œåœ¨è¿™ä¸ª `VisonTransformer` ç±»ä¸­ï¼Œæˆ‘å°†å‰é¢ä»‹ç»çš„å„ä¸ªæ¨¡å—æ•´åˆåœ¨ä¸€èµ·ï¼Œå®ç°äº†å®Œæ•´çš„ViTç½‘ç»œã€‚é¦–å…ˆï¼Œæˆ‘ç”¨å·ç§¯å°†è¾“å…¥å›¾åƒåˆ‡åˆ†æˆå›ºå®šå¤§å°çš„Patchï¼Œå¹¶æ˜ å°„åˆ°ç‰¹å¾ç©ºé—´ï¼›æ¥ç€é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„åˆ†ç±»tokenå’Œä½ç½®ç¼–ç ï¼Œä¸ºæ¨¡å‹æä¾›ä½ç½®ä¿¡æ¯å¼¥è¡¥å·ç§¯â€œæ„Ÿå—é‡â€ç¼ºå¤±çš„é—®é¢˜ã€‚ä¹‹åï¼Œæˆ‘å †å å¤šä¸ªTransformerç¼–ç å™¨Blockï¼Œæ¯ä¸ªBlockåŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒMLPæ¨¡å—ï¼Œé€šè¿‡æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–ä¿è¯ä¿¡æ¯çš„æœ‰æ•ˆä¼ é€’ä¸ç‰¹å¾æŠ½è±¡ã€‚æœ€åï¼Œæˆ‘å–åˆ†ç±»tokençš„è¾“å‡ºï¼Œé€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ°ç›®æ ‡ç±»åˆ«ï¼Œå®ç°å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚

    class VisonTransformer(nn.Module):
        def __init__(self, input_shape, patch_size, in_channels, num_classes, num_features, depth,
                     num_heads, mlp_ratio, qkv_bias, drop_rate, attn_drop_rate, drop_path_rate,
                     norm_layer, act_layer):
            super().__init__()
            self.input_shape = input_shape # è¾“å…¥çš„ç»´åº¦
            self.patch_size = patch_size # Patch çš„å¤§å°
            self.in_channels = in_channels # è¾“å…¥çš„ç»´åº¦
            self.num_classes = num_classes # è¾“å‡ºç±»åˆ«æ•°
            self.num_features = num_features # ç‰¹å¾ç»´åº¦
            self.depth = depth # Transformerç¼–ç å™¨å±‚æ•°
            self.num_heads = num_heads # Transformeræ³¨æ„åŠ›å¤´æ•°
            self.mlp_ratio = mlp_ratio # MLP æ¯”ä¾‹ MLP:å¤šå±‚æ„ŸçŸ¥æœº,ç´§éš Self-Attention ä¹‹åï¼Œç”¨äºéçº¿æ€§å˜æ¢ï¼šå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼›ç‰¹å¾æ˜ å°„ï¼šå°† Self-Attention æå–çš„ç‰¹å¾è¿›ä¸€æ­¥è½¬æ¢ã€‚ 
            self.qkv_bias = qkv_bias # æ˜¯å¦ä½¿ç”¨åç½®
            self.drop_rate = drop_rate # ä¸¢å¼ƒç‡
            self.attn_drop_rate = attn_drop_rate # æ³¨æ„åŠ›ä¸¢å¼ƒç‡
            self.drop_path_rate = drop_path_rate # ä¸¢å¼ƒè·¯å¾„ç‡
            self.norm_layer = norm_layer # å½’ä¸€åŒ–å±‚
            self.act_layer = act_layer # æ¿€æ´»å‡½æ•°å±‚
    
            self.features_shape = [input_shape[1] // patch_size, input_shape[2] // patch_size]  # [14, 14]
            self.num_patches = self.features_shape[0] * self.features_shape[1]
            self.patch_embed = VisionPatchEmbedding(input_shape, patch_size, in_channels, num_features) # å°†è¾“å…¥å›¾ç‰‡åˆ†å‰²æˆpatchï¼Œå¹¶è¿›è¡Œçº¿æ€§æ˜ å°„
    
            # ViT ä¸æ˜¯ CNNï¼Œæ²¡æœ‰"æ„Ÿå—é‡"ï¼Œæ‰€ä»¥å¼•å…¥äº†ä½ç½®ç¼–ç ï¼Œæ¥ä¸ºæ¯ä¸ª patch åŠ ä¸Šä½ç½®ä¿¡æ¯ï¼›
            self.pretrained_features_shape = [224 // patch_size, 224 // patch_size] # é¢„è®­ç»ƒçš„ç‰¹å¾å›¾å°ºå¯¸
    
            self.cls_token = nn.Parameter(torch.zeros(1, 1, num_features)) # åˆ†ç±» token 196, 768 -> 197, 768
            self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, num_features)) # ä½ç½®ç¼–ç  197, 768 -> 197, 768
    
            self.pos_drop = nn.Dropout(drop_rate) # ä¸¢å¼ƒç‡
            self.norm = norm_layer(self.num_features) # å½’ä¸€åŒ–
    
            self.dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] # ä¸¢å¼ƒè·¯å¾„ç‡
            self.blocks = nn.Sequential(
                *[
                    Block(
                        dim = num_features,
                        num_heads = num_heads,
                        mlp_radio = mlp_ratio,
                        qkv_bias = qkv_bias,
                        drop = drop_rate,
                        attn_drop = attn_drop_rate,
                        drop_path = self.dpr[i],
                        norm_layer = norm_layer,
                        act_layer = act_layer 
                    )for i in range(depth)
                ]
            )
            self.head = nn.Linear(num_features, num_classes) if num_classes > 0 else nn.Identity()
    
        def forward_features(self,x):
            x = self.patch_embed(x)
            cls_token = self.cls_token.expand(x.shape[0], -1, -1) # å°†åˆ†ç±» token æ‰©å±•åˆ°ä¸è¾“å…¥ç‰¹å¾å›¾ç›¸åŒçš„å½¢çŠ¶
            x = torch.cat((cls_token, x), dim=1) # å°†åˆ†ç±» token ä¸è¾“å…¥ç‰¹å¾å›¾æ‹¼æ¥
    
            cls_token_pos_embed = self.pos_embed[:, 0:1, :] # åˆ†ç±» token çš„ä½ç½®ç¼–ç 
            img_token_pos_embed = self.pos_embed[:, 1:, :]  # [1, num_patches, num_features]
            # å˜æˆ[1, H, W, C]
            img_token_pos_embed = img_token_pos_embed.view(1, self.features_shape[0], self.features_shape[1], -1).permute(0, 3, 1, 2)  # [1, C, H, W]
            # æ’å€¼
            img_token_pos_embed = F.interpolate(
                img_token_pos_embed,
                size=self.features_shape,  # [H, W]
                mode='bicubic',
                align_corners=False
            )
            # å˜å›[1, num_patches, C]
            img_token_pos_embed = img_token_pos_embed.permute(0, 2, 3, 1).reshape(1, -1, img_token_pos_embed.shape[1])
    
            pos_embed = torch.cat((cls_token_pos_embed, img_token_pos_embed), dim=1) # å°†åˆ†ç±» token çš„ä½ç½®ç¼–ç ä¸å›¾åƒ token çš„ä½ç½®ç¼–ç æ‹¼æ¥
            
            x = self.pos_drop(x + pos_embed) # å°†ä½ç½®ç¼–ç ä¸è¾“å…¥ç‰¹å¾å›¾ç›¸åŠ 
    
            x = self.blocks(x)
            x = self.norm(x)
    
            return x[:, 0] # è¿”å›åˆ†ç±» token çš„ç‰¹å¾
        
        def forward(self, x):
            x = self.forward_features(x)
            x = self.head(x)
            return x
    

**è‡³æ­¤æˆ‘ä»¬å®Œæ•´çš„ViTä¾¿æ­å»ºå®Œæˆäº†ï¼**

### ï¼ˆå…­ï¼‰å®ç°æ•°æ®åŠ è½½ä»£ç ï¼ˆæ•°æ®åŠ è½½ã€lossã€ä¼˜åŒ–å™¨ï¼‰

Â Â Â Â Â Â Â Â æ•°æ®é›†åŠ è½½éƒ¨åˆ†æ¯”è¾ƒç®€ç­”ï¼Œä¸è¿‡å¤šèµ˜è¿°ï¼Œæˆ‘çš„æ•°æ®é›†ç»“æ„åŠå…·ä½“ä»£ç å¦‚ä¸‹ï¼š

![image-20250614015610919](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614015615748-1709429269.png)

    import os 
    from torch.utils.data import Dataset, DataLoader 
    from PIL import Image 
    import torchvision.transforms as transforms 
    
    class ViTDataset(Dataset):
        def __init__(self, root, split, transform=None, target_transform=None, img_size=224):
            super().__init__()
            self.split = split 
            self.img_size = img_size  # å›¾åƒå¤§å°
            self.transform = transform if transform is not None else transforms.ToTensor()
            self.target_transform = target_transform  # æ ‡ç­¾å˜æ¢
            # æ„å»ºæ•°æ®é›†æ ¹ç›®å½•
            self.data_dir = os.path.join(root, split)  # è®­ç»ƒé›†æˆ–æµ‹è¯•é›†ç›®å½•
            # è·å–æ‰€æœ‰ç±»åˆ«
            self.classes = sorted(os.listdir(self.data_dir))
            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}
            # æ”¶é›†æ‰€æœ‰å›¾åƒæ–‡ä»¶è·¯å¾„å’Œå¯¹åº”çš„æ ‡ç­¾
            self.images = []
            self.labels = []
            for class_name in self.classes:
                class_dir = os.path.join(self.data_dir, class_name)
                if not os.path.isdir(class_dir):
                    continue
                for img_name in os.listdir(class_dir):
                    if img_name.endswith(('.jpg', '.jpeg', '.png')):
                        img_path = os.path.join(class_dir, img_name)
                        self.images.append(img_path)
                        self.labels.append(self.class_to_idx[class_name])
            print(f"åŠ è½½äº† {len(self.images)} å¼ å›¾åƒç”¨äº{split}é›†ï¼Œå…±{len(self.classes)}ä¸ªç±»åˆ«")
    
        def __len__(self):
            return len(self.images)
        
        def __getitem__(self, index):
            # è·å–å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
            img_path = self.images[index]
            label = self.labels[index]
            # åŠ è½½å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            # è°ƒæ•´å›¾åƒå¤§å°
            image = image.resize((self.img_size, self.img_size), Image.Resampling.BILINEAR)
            # åº”ç”¨å˜æ¢
            image = self.transform(image)
            if self.target_transform is not None:
                label = self.target_transform(label)
                
            return image, label
        
    
    def ViTDataLoad(root, batch_size, num_workers, img_size):
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        train_dataset = ViTDataset(
            root=root,
            split='train',  # ä½¿ç”¨è®­ç»ƒé›†åˆ’åˆ†
            img_size=img_size
        )
        
        # åˆ›å»ºéªŒè¯æ•°æ®é›†
        val_dataset = ViTDataset(
            root=root,
            split='val',  # ä½¿ç”¨éªŒè¯é›†åˆ’åˆ†
            img_size=img_size
        )
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,  # éšæœºæ‰“ä¹±æ•°æ®
            num_workers=num_workers,  # å¤šçº¿ç¨‹åŠ è½½
            pin_memory=True,  # æ•°æ®é¢„åŠ è½½åˆ°å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
            drop_last=True  # ä¸¢å¼ƒæœ€åä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        )
        
        # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,  # ä¸æ‰“ä¹±æ•°æ®
            num_workers=num_workers,
            pin_memory=True
        )
        
        return train_loader, val_loader
    

### ï¼ˆä¸ƒï¼‰å®ç°è®­ç»ƒä»£ç 

Â Â Â Â Â Â Â Â æ¥ä¸‹æ¥æˆ‘ä»¬æ¥å®Œæˆæˆ‘ä»¬çš„è®­ç»ƒä»£ç ï¼Œæˆ‘é€šè¿‡ä¸Šä¸€èŠ‚å®šä¹‰çš„`ViTDataLoad`æ¥åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œè®­ç»ƒä¸­æˆ‘é‡‡ç”¨å¸¸ç”¨çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆ`CrossEntropyLoss`ï¼‰æ¥è¡¡é‡åˆ†ç±»æ•ˆæœï¼Œä¼˜åŒ–å™¨ä½¿ç”¨äº†æ›´é€‚åˆTransformerçš„`AdamW`ï¼Œå…·ä½“å®ç°å¦‚ä¸‹ï¼Œä¸è¿‡å¤šèµ˜è¿°ï¼š

    from model.transformer_net import VisonTransformer
    from dataset_load import ViTDataLoad
    import torch 
    import torch.nn as nn
    import torch.optim as optim
    from tqdm import tqdm
    import matplotlib.pyplot as plt
    
    def train(
        root="/home/xq/Working/dockertrain_test/input/timmdataset/african-wildlife", # æ•°æ®é›†æ ¹ç›®å½•
        img_size=224,
        patch_size=16,
        in_channels=3,
        num_features=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4.0,
        qkv_bias=True,
        drop_rate=0.1,
        attn_drop_rate=0.1,
        drop_path_rate=0.1,
        epochs=50,
        batch_size=4,
        num_workers=4,
        lr=1e-4,
        device=None
    ):
        device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        # æ•°æ®åŠ è½½
        train_loader, val_loader = ViTDataLoad(root, batch_size, num_workers, img_size)
        num_classes = len(train_loader.dataset.classes)
        input_shape = (in_channels, img_size, img_size)
    
        # æ¨¡å‹
        model = VisonTransformer(
            input_shape=input_shape,
            patch_size=patch_size,
            in_channels=in_channels,
            num_classes=num_classes,
            num_features=num_features,
            depth=depth,
            num_heads=num_heads,
            mlp_ratio=mlp_ratio,
            qkv_bias=qkv_bias,
            drop_rate=drop_rate,
            attn_drop_rate=attn_drop_rate,
            drop_path_rate=drop_path_rate,
            norm_layer=nn.LayerNorm,
            act_layer=nn.GELU
        ).to(device)
    
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=lr)
        best_acc = 0
        train_loss_list, val_loss_list = [], []
        train_acc_list, val_acc_list = [], []
    
        for epoch in range(epochs):
            model.train()
            total_loss, correct, total = 0, 0, 0
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
            for images, labels in pbar:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                total_loss += loss.item() * images.size(0)
                _, preds = outputs.max(1)
                correct += preds.eq(labels).sum().item()
                total += labels.size(0)
            train_loss = total_loss / total
            train_acc = correct / total
            train_loss_list.append(train_loss)
            train_acc_list.append(train_acc)
    
            # éªŒè¯
            model.eval()
            val_loss, val_correct, val_total = 0, 0, 0
            with torch.no_grad():
                for images, labels in val_loader:
                    images, labels = images.to(device), labels.to(device)
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    val_loss += loss.item() * images.size(0)
                    _, preds = outputs.max(1)
                    val_correct += preds.eq(labels).sum().item()
                    val_total += labels.size(0)
            val_loss = val_loss / val_total
            val_acc = val_correct / val_total
            val_loss_list.append(val_loss)
            val_acc_list.append(val_acc)
    
            print(f"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}")
            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            if val_acc > best_acc:
                best_acc = val_acc
                torch.save(model.state_dict(), "best_vit.pth")
    
        # å¯è§†åŒ–losså’Œacc
        plt.figure()
        plt.plot(train_loss_list, label="Train Loss")
        plt.plot(val_loss_list, label="Val Loss")
        plt.legend()
        plt.title("Loss Curve")
        plt.savefig("loss_curve.png")
        plt.figure()
        plt.plot(train_acc_list, label="Train Acc")
        plt.plot(val_acc_list, label="Val Acc")
        plt.legend()
        plt.title("Accuracy Curve")
        plt.savefig("acc_curve.png")
        print("è®­ç»ƒå®Œæˆï¼Œæœ€ä¼˜éªŒè¯å‡†ç¡®ç‡ï¼š", best_acc)
    
    if __name__ == "__main__":
        train()
    

### ï¼ˆå…«ï¼‰å®ç°éªŒè¯ä»£ç 

Â Â Â Â Â Â Â Â éªŒè¯ä»£ç ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œæœ‰PyTorchåŠæ·±åº¦å­¦ä¹ åŸºç¡€çš„åŒå­¦å¯ä»¥å¾ˆå¿«å®ç°ï¼Œæ•…è¿™é‡Œä¹Ÿä¸å†èµ˜è¿°ï¼š

    import torch
    from model.transformer_net import VisonTransformer
    import torchvision.transforms as transforms
    from PIL import Image
    import sys
    import os
    
    img_size = 224
    patch_size = 16
    in_channels = 3
    num_features = 768
    depth = 12
    num_heads = 12
    mlp_ratio = 4.0
    qkv_bias = True
    drop_rate = 0.1
    attn_drop_rate = 0.1
    drop_path_rate = 0.1
    
    classes = ['cat', 'dog']  
    num_classes = len(classes)
    input_shape = (in_channels, img_size, img_size)
    
    def load_model(device):
        model = VisonTransformer(
            input_shape=input_shape,
            patch_size=patch_size,
            in_channels=in_channels,
            num_classes=num_classes,
            num_features=num_features,
            depth=depth,
            num_heads=num_heads,
            mlp_ratio=mlp_ratio,
            qkv_bias=qkv_bias,
            drop_rate=drop_rate,
            attn_drop_rate=attn_drop_rate,
            drop_path_rate=drop_path_rate,
            norm_layer=torch.nn.LayerNorm,
            act_layer=torch.nn.GELU
        ).to(device)
        model.load_state_dict(torch.load("best_vit.pth", map_location=device))
        model.eval()
        return model
    
    def predict(img_path, model, device):
        transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
        ])
        img = Image.open(img_path).convert('RGB')
        img = transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            output = model(img)
            pred = output.argmax(dim=1).item()
        return classes[pred]
    
    if __name__ == "__main__":
        img_path = sys.argv[1]
        if not os.path.exists(img_path):
            print(f"å›¾ç‰‡ä¸å­˜åœ¨: {img_path}") 
            sys.exit(1)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = load_model(device)
        pred_class = predict(img_path, model, device)
        print(f"å›¾ç‰‡ {img_path} çš„é¢„æµ‹ç±»åˆ«ä¸º: {pred_class}") 
    

ä¸‰ã€ViTï¼šåœ¨è‡ªå®šä¹‰æ•°æ®é›†åŠCIFAR-10è¿›è¡Œè®­ç»ƒä¸æµ‹è¯•
-----------------------------

### ï¼ˆä¸€ï¼‰è‡ªå®šä¹‰æ•°æ®é›†

Â Â Â Â Â Â Â Â æˆ‘ä»¬é¦–å…ˆåœ¨æˆ‘ä»¬è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¯·ä¿è¯æ•°æ®é›†æ ¼å¼ä¸ºï¼š

![image-20250614015610919](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614015615748-1709429269.png)

Â Â Â Â Â Â Â Â æ¥ç€åœ¨è®­ç»ƒä»£ç ä¸­ä¿®æ”¹æ•°æ®é›†è·¯å¾„å¹¶è¿è¡Œä¸‹è¿°å‘½ä»¤å³å¯å¼€å§‹è®­ç»ƒï¼š

![image-20250614020702703](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614020707019-134992667.png)

    python3 train.py
    

![image-20250614021641186](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614021703517-1895368011.png)

Â Â Â Â Â Â Â Â è®­ç»ƒå®Œæˆåæ–‡ä»¶å†…ä¼šæœ‰ä¸€ä¸ªbest\_vit.pthä»¥åŠä¸¤ä¸ªè®­ç»ƒçš„AccåŠLosså›¾ç”¨äºåˆ†æ

![image-20250614021732228](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614021736970-201144847.png) ![image-20250614021741707](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614021746883-2043602797.png)

æ¥ä¸‹æ¥æˆ‘ä»¬è¿è¡Œ`python3 predict.py [img_path]`å³å¯æ‰§è¡Œæ¨ç†å•¦ï¼

![image-20250614022155334](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022159568-687739081.png)

### ï¼ˆäºŒï¼‰CIFAR-10æ•°æ®é›†

Â Â Â Â Â Â Â Â åœ¨å®Œæˆä¸Šè¿°è‡ªå®šä¹‰æ•°æ®é›†ä¹‹åæˆ‘ä»¬ä¾¿å¯ä»¥ç»§ç»­å°è¯•CIFAR-10å•¦ï¼æˆ‘ä»¬é¦–å…ˆæ¥ä¸‹è½½æˆ‘ä»¬çš„CIFAR-10æ•°æ®é›†ï¼ŒCIFAR-10æ•°æ®é›†å·²ç»é›†æˆè¿›äº†Torchï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨PyTorchæ¥å£ç›´æ¥ä¸‹è½½ï¼Œå…·ä½“ä¸‹è½½æ–¹å¼å¦‚ä¸‹ï¼Œä¸è¿‡å¤šèµ˜è¿°ï¼š

    # CIFAR-10å…¨é‡
    import torchvision.datasets as datasets
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True)
    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True)
    

![image-20250614020604397](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614020608805-1211984873.png)

Â Â Â Â Â Â Â Â ç”±äºæˆ‘ä»¬ä¸‹è½½ä¸‹æ¥çš„CIFAR-10æ•°æ®é›†æ ¼å¼å¦‚ä¸‹ï¼ˆdata\_batch\_1 ~ data\_batch\_5ï¼šè®­ç»ƒæ•°æ®ï¼ˆæ¯ä¸ª10,000å¼ å›¾åƒï¼‰ï¼Œtest\_batchï¼šæµ‹è¯•æ•°æ®ï¼ˆ10,000å¼ å›¾åƒï¼‰ï¼Œbatches.metaï¼šå…ƒæ•°æ®æ–‡ä»¶ï¼ˆåŒ…å«ç±»åˆ«åç§°ç­‰ä¿¡æ¯ï¼‰ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å¯¹æˆ‘ä»¬çš„`dataset_load.py`ä»£ç è¿›è¡Œä¿®æ”¹ä»¥é€‚é…æˆ‘ä»¬çš„CIFAR-10æ•°æ®é›†

![image-20250614021347738](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614021352130-1121553942.png)

    import os
    import pickle
    import numpy as np
    import torch
    from torch.utils.data import Dataset, DataLoader
    from PIL import Image
    import torchvision.transforms as transforms
    
    class CIFAR10Dataset(Dataset):
        def __init__(self, root, train=True, transform=None, target_transform=None):
            super().__init__()
            self.root = root
            self.train = train
            self.transform = transform
            self.target_transform = target_transform
            
            # CIFAR-10ç±»åˆ«åç§°
            self.classes = [
                'airplane', 'automobile', 'bird', 'cat', 'deer',
                'dog', 'frog', 'horse', 'ship', 'truck'
            ]
            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}
            
            self.data = []
            self.targets = []
            
            if self.train:
                for i in range(1, 6):
                    batch_file = os.path.join(root, f'data_batch_{i}')
                    with open(batch_file, 'rb') as f:
                        batch_data = pickle.load(f, encoding='bytes')
                        self.data.append(batch_data[b'data'])
                        self.targets.extend(batch_data[b'labels'])
                self.data = np.vstack(self.data)
            else:
                test_file = os.path.join(root, 'test_batch')
                with open(test_file, 'rb') as f:
                    test_data = pickle.load(f, encoding='bytes')
                    self.data = test_data[b'data']
                    self.targets = test_data[b'labels']
            
            # å°†æ•°æ®reshapeä¸ºå›¾åƒæ ¼å¼ (N, 32, 32, 3)
            self.data = self.data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)
            
            print(f"åŠ è½½äº† {len(self.data)} å¼ CIFAR-10å›¾åƒç”¨äº{'è®­ç»ƒ' if train else 'æµ‹è¯•'}ï¼Œå…±{len(self.classes)}ä¸ªç±»åˆ«")
    
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, index):
            img = self.data[index]
            target = self.targets[index]
            img = Image.fromarray(img)
            if self.transform is not None:
                img = self.transform(img)
            
            if self.target_transform is not None:
                target = self.target_transform(target)
                
            return img, target
    
    def CIFAR10DataLoad(root, batch_size, num_workers=4, img_size=224):
        train_transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),  
            transforms.RandomHorizontalFlip(p=0.5), 
            transforms.RandomRotation(10), 
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) 
        ])
        
        test_transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = CIFAR10Dataset(
            root=root,
            train=True,
            transform=train_transform
        )
        
        test_dataset = CIFAR10Dataset(
            root=root,
            train=False,
            transform=test_transform
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=True
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        )
        
        return train_loader, test_loader
    
    
    if __name__ == "__main__":
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        root = "/home/xq/Temp/cifar-10-batches-py"
        train_loader, test_loader = CIFAR10DataLoad(root, batch_size=32)
        # æµ‹è¯•ä¸€ä¸ªbatch
        for images, labels in train_loader:
            print(f"å›¾åƒbatchå½¢çŠ¶: {images.shape}")
            print(f"æ ‡ç­¾batchå½¢çŠ¶: {labels.shape}")
            print(f"æ ‡ç­¾èŒƒå›´: {labels.min()} - {labels.max()}")
            break 
    

Â Â Â Â Â Â Â Â æ¥ç€ä¿®æ”¹æˆ‘ä»¬è®­ç»ƒä»£ç ï¼Œä¸»è¦ä¿®æ”¹æ•°æ®é›†è·¯å¾„ã€æ•°æ®åŠ è½½å™¨çš„è°ƒç”¨å³å¯ï¼š

![image-20250614022640209](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022644488-1443084450.png)

![image-20250614022706545](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022710649-285381462.png)

    python3 train.py
    

![image-20250614022753939](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022758350-1870577983.png)

![image-20250614022817463](https://img2023.cnblogs.com/blog/3505969/202506/3505969-20250614022821932-833062892.png)

Â Â Â Â Â Â Â Â **ç”±äºCIFAR-10æ•°æ®é›†æ¯”è¾ƒå¤§ï¼Œè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œæˆ‘ä»¬è€å¿ƒç­‰å¾…å³å¯**ï¼Œè®­ç»ƒå®Œæˆåè¿è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯æ‰§è¡Œæ¨ç†ï¼š

    # å•ä¸ªé¢„æµ‹
    python3 predict_cifar10.py <å›¾ç‰‡è·¯å¾„>
    # Top-Ké¢„æµ‹
    python3 predict_cifar10.py <å›¾ç‰‡è·¯å¾„> --top-k 3