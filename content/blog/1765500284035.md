---
layout: post
title: 'NGD-SLAM(一)'
date: "2025-12-12T00:44:44Z"
---
NGD-SLAM(一)

NGD-SLAM介绍.

NGD-SLAM是论文[NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU](https://arxiv.org/pdf/2405.07392)描述的一种基于ORB-SLAM3的适用于动态场景的稀疏SLAM类型，代码仓库的下载地址是：[https://github.com/yuhaozhang7/NGD-SLAM](https://github.com/yuhaozhang7/NGD-SLAM)

 

这里不打算翻译这篇论文，[这里有一篇翻译成中文的](https://m.elecfans.com/article/5960009.html),在此只讲讲个人理解。以前的SLAM大多只适用于静态场景，对于动态场景不能很好地处理，直到最近几年，动态场景的处理才变成可能，这也是本文的一个亮点之一，当然你要说这篇论文有多创新那也算不上，估计这样的论文放到CVPR大概率不会被收纳，因为从学术研究的角度看，这篇论文还不够新颖，那这里为什么着重介绍这篇论文那？因为它在实际项目运用上很有价值。我们常常会看到很多新技术在研究领域获得了很高的评价，但在实际项目中却乏人问津。这实际上是科学研究和实际项目遵循两套完全不同的哲学。科学研究强调新颖性，创新性，而不强调实用价值，实际项目则常常只有有限的资源和条件，创新的技术需要做出很多取舍。在这个到处都在鼓吹“算力”的年代，新技术不使用GPU，多少有点另类，但这也突出了这个论文在非常有限的硬件条件下做出了不错的效果，这也是我非常喜欢这篇论文的原因。  
下面讲解这篇论文如何实现在不使用GPU的情况下处理动态场景的问题。

论文主要通过两个方面实现上述目的：第一，关于动态场景部分遮罩的生成，因为论文使用YOLO模型进行动态物体的识别，这是个相对比较耗时的操作，所以这里使用上一帧生成的动态物体的包围盒，避免每一帧都进行YOLO计算带来的资源消耗。第二，在做相机跟踪的时候，对于非关键桢采用光流的方法，而对于关键桢使用ORB特征点的方式进行计算，这样可以减少计算量，并且有很好的鲁棒性。

如图所示，在DETECTION阶段YOLO模型生产动态物体的包围盒，在SEGMENTATION阶段利用深度信息对包围盒进行细化的分割，SAMPLING对于分割的区域进行15\*15的点提取，TRACKING则使用Lucas-Kanade光流法对这些特征点进行跟踪，CLUSTERING对outlier进行剔除，最后一步PREDICTION完成遮罩的预测。

对比光流法和ORB特征跟踪法的区别。对于上一帧没出现而当前帧出现的动态人物，ORB不能正确的识别。  
最后谈一下该论文的一些限制。论文在计算遮罩的时候需要用到深度信息，所以使用的图片源为RGB-D，这就需要对于单目相机或者立体相机进行深度信息的补充计算.