---
layout: post
title: '探秘Transformer系列之（35）--- 大模型量化基础'
date: "2025-06-03T00:42:44Z"
---
探秘Transformer系列之（35）--- 大模型量化基础
===============================

从零开始解析Transformer，目标是：(1) 解析Transformer如何运作，以及为何如此运作，让新同学可以入门；(2) 力争融入一些比较新的或者有特色的论文或者理念，让老鸟也可以有所收获。

探秘Transformer系列之（35）--- 大模型量化基础
===============================

目录

*   [探秘Transformer系列之（35）--- 大模型量化基础](#探秘transformer系列之35----大模型量化基础)
    *   [0x00 概述](#0x00-概述)
    *   [0x01 outlier](#0x01-outlier)
        *   [1.1 定义](#11-定义)
        *   [1.2 特点](#12-特点)
        *   [1.3 出现过程](#13-出现过程)
        *   [1.4 分布规律](#14-分布规律)
        *   [1.5 出现原因](#15-出现原因)
            *   [1.5.1 softmax](#151-softmax)
                *   [LLM模型](#llm模型)
                *   [VIT模型](#vit模型)
            *   [1.5.2 RoPE](#152-rope)
                *   [现象](#现象)
                *   [分析](#分析)
            *   [1.5.3 FFN](#153-ffn)
                *   [矩阵乘法的动态性分析](#矩阵乘法的动态性分析)
                *   [FFN主要计算](#ffn主要计算)
            *   [1.5.4 LayerNorm](#154-layernorm)
            *   [1.5.5 RMSNorm](#155-rmsnorm)
        *   [1.6 作用](#16-作用)
            *   [1.6.1 负面作用](#161-负面作用)
            *   [1.6.2 正面作用](#162-正面作用)
                *   [保持性能](#保持性能)
                *   [上下文理解](#上下文理解)
        *   [1.7 难点](#17-难点)
    *   [0x02 超异常值](#0x02-超异常值)
        *   [2.1 超级权重](#21-超级权重)
            *   [2.2.1 作用](#221-作用)
            *   [2.2.2 识别](#222-识别)
        *   [2.2 massive outlier](#22-massive-outlier)
            *   [2.2.1 定义](#221-定义)
            *   [2.2.2 作用](#222-作用)
                *   [特点](#特点)
                *   [论证](#论证)
            *   [2.2.3 难点](#223-难点)
    *   [0x03 Transformer量化](#0x03-transformer量化)
        *   [3.1 原理分析](#31-原理分析)
            *   [3.1.1 推理特点](#311-推理特点)
            *   [3.1.2 对框架的要求](#312-对框架的要求)
        *   [3.2 量化模块](#32-量化模块)
        *   [3.3 分类](#33-分类)
            *   [3.3.1 PTQ](#331-ptq)
                *   [weight-only quantization](#weight-only-quantization)
                *   [weight-activation quantization](#weight-activation-quantization)
            *   [3.3.2 QAT](#332-qat)
                *   [减少数据需求](#减少数据需求)
                *   [减少计算量](#减少计算量)
            *   [3.3.3 常见方案](#333-常见方案)
                *   [优化](#优化)
                *   [转移](#转移)
                *   [旋转](#旋转)
                *   [可学习](#可学习)
                *   [查表和VQ](#查表和vq)
                *   [基于Attention Sink的量化方法](#基于attention-sink的量化方法)
        *   [3.4 效果](#34-效果)
            *   [3.4.1 对比实验与分析](#341-对比实验与分析)
            *   [3.4.2 量化准确度](#342-量化准确度)
            *   [3.4.3 QAT和PTQ](#343-qat和ptq)
    *   [0xFF 参考](#0xff-参考)

0x00 概述
-------

将现有的量化技术直接应用于大模型存在困难，会出现较大量化误差和精度下降。这主要是因为大模型的特点是规模和复杂性。与较小的模型相比，大模型的权重和激活通常表现出更多的outlier（离群值），并且具有更宽的分布范围。LLM.int()的作者就发现：

*   与较小的模型不同，LLM 表现出独特的权重和激活分布，其特点是存在大量的outlier。因为outlier 的存在，如果我们使用 INT8 量化，大多正常数值将被清零。
    
*   Emergent Features的phase shift出现在模型参数量达到6.7B时，这说明参数量在6.7B之前和在6.7B之后的模型表现非常不一样。
    

这些异常值对量化过程有显著影响，因为它们会增加量化步长，同时降低中间值的精度。因此，将参数量在6.7B之下的transformer模型上适用的方法泛化到参数量在6.7B以上的模型时要十分谨慎。比如，针对小型模型的最好的量化裁剪（clipping）方法对于LLM来说并不是开箱即用的。 这需要我们开发量身定制的量化技术，以便在不影响模型性能或效率的情况下处理这些独特的特征。

0x01 outlier
------------

在模型内部，数据通常以多维张量（可以理解为多维数组）表示。对于注意力机制，这些张量包含批次大小、序列长度、注意力头数量和头部维度等维度。而大规模值是指在LLM的注意力头部维度中，某些元素的数值明显高于该头部其他维度平均值的情况（通常为5倍以上）。

### 1.1 定义

LLM.int8()作者给出了outlier的如下定义：

对于一个有 l 层的transformer，隐状态(hidden state)为\\(X\_l ∈ R^{s×ℎ},l=0…L\\)，其中s是sequence维度，ℎ是feature维度。我们定义一个feature \\(h\_i\\)为隐状态\\(X\_{l\_i}\\)里面一个特定的维度。我们在所有层里面跟踪每个维度\\(h\_i\\)，0≤i≤ℎ， 如果某个维度满足以下条件，我们称之为一个outlier：

*   至少有一个值的绝对值（magnitude）大于等于6；
*   满足条件1的\\(h\_i\\)在transformer的至少25%的层里出现；
*   满足条件1的\\(h\_i\\)至少在6%的sequence dimension的隐状态中出现。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220545361-880558297.jpg)

上面是一个outlier示例（一共有4个黄色的outliear feature），横轴是hidden\_dim维度，纵轴是sequence维度。上图中seq\_len = 3，所以一个outlier feature是一个3×1的向量。

### 1.2 特点

研究人员发现，量化前权重和激活值分布的平坦度 (flatness) 是影响 LLM 量化误差的关键因素。直观来看，分布越平坦，离群值就越少，量化时的精度也就越高。激活和权重分布越不均匀时候，量化误差越大。

下图展示了激活和权重上异常值的特点。红色为激活，绿色为异常值。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220614231-1964961391.jpg)

**激活**

对于激活来说，其异常值特点如下：

*   在训练开始后，残差激活值迅速从高斯分布转变为逻辑分布（Logistic Distribution），并且出现较大的outlier。
*   当模型参数超过某个阈值之后，异常点比例突然增大。
*   激活的每个channel在不同token分布相似，比如在某个token权重数值量级很大的channel，在其它token上数值量级也很大。

**权重**

对于权重来说，其异常值特点如下：

*   权重分布相当均匀，平坦，易于量化。用 INT8 甚至 INT4 量化 LLM 的权重不会降低精度。
*   FFN部分权重从随机初始化的高斯分布，开始时较为稳定；在训练一定阶段后开始剧烈变化；随后整体分布再次稳定下来。权重整体保留了高斯分布，但是存在一些不是非常大的outlier。
*   LLM的离群值很少，集中于确定的几列，这几列可能存储了一些上下文无关的信息。
*   另外，在具有门控线性单元（GLU）的模型中，激活和权重大多是对称分布的，使得使用对称量化成为最佳选择。

**梯度**

梯度分布的变化趋势与权重类似，训练过程也未出现较大的outlier，说明梯度本身也具备较好的稳定性，存在低精度计算和存储的可能性。

### 1.3 出现过程

LLM.int8()作者[Tim Dettmers](https://timdettmers.com/)在其博客 [LLM.int8() and Emergent Features](https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/) 中提到了Emergent Features这个概念。Emergent Features就是Emergent outlier features。

> The other pitch talks about emergent outliers in transformers and how they radically change what transformers learn and how they function.
> 
> This blog post is a more speculative version of the paper that teases out the super curious details about the fascinating properties surrounding the emergent outlier features I found.

“Emergent”描述的是这些离群值逐渐增长，并在经历phase shift（指离群值会突然迅速增长的现象）之后，对模型性能产生严重影响的现象。此处的增长是指：离群值的数值变大，数量变多，受该离群值影响的token数和模型层数变多。

LLM.int8()论文中的实验发现，在我们把transformers规模扩大到6B的时候，异常值首次在25%的transformer层出现，然后逐渐向其它层扩散。当模型达到6.7B的时候，所有的transformer层都会受到异常值的影响，受到影响的sequence维度也从35%扩大到75%。它们的分布在所有的transformer层中集中在6个feature维度。下图展示了 Transformer 中，受到异常值特征影响的层的百分比和序列维度的百分比，这些数值和模型大小存在相关性。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220625324-607830875.jpg)

论文作者在博客中也介绍了Emergent Features随着模型参数量的增加而增长的过程。

*   即使在参数量为125M的比较小的Transformer模型中，Emergent Features也是存在的。但此时Emergent Features只存在注意力投影层（attention projection, query/key/value/output）的输出中。
*   当Transformer模型的参数量提高到350M至1.3亿，Emergent Features开始出现在注意力和FFN的输出中，并且出现在同一个维度上，但在不同的mini-batch或者不同层中出现的位置是不一样的。原博客认为这种一致性在一定程度上代表了模型各个层的协作。同时，Emergent Features分布开始呈现一些规律。
*   当模型的参数量达到2.7B至6B时，在60%的层中，Emergent Features出现在同样的维度上。
*   Transformer 所有层上的大幅异常值特征突然出现在 6B 和 6.7B 参数之间，即出现了phase shift。原博客还描述了在这种情况下，模型内出现的一些变化：
    *   受到异常值影响的层的数量百分比从 65% 增加到 100%，受到异常值影响的 token 的数量百分比从 35% 增加到 75%。同时，量化开始失败。量化方法从 6.7B 开始失败的核心原因可能是：量化分布的范围太大，导致大多数量化 bins 为空，小的量化值被量化为零，基本上消除了有效信息。
    *   注意力层变得非常稀疏；FFN层变得更密集；Transformer变得更稳定。如果将离群值从模型中分离出来，剩下的部分可以用8-bit甚至更低的精度进行运算。

### 1.4 分布规律

新出现的离群特征（Emergent Features）的分布是有规律的。LLM.int8()作者在博客总结了原论文中报告的关于Emergent Features的几个现象：

*   Emergent Features在大型模型中是系统性的：要么出现在大多数层中，要么不出现。但是，在小型模型中是概率性出现的：只是有时出现在某些层中。
*   Emergent Features容易出现的潜在位置包括：注意力投影层（attention projection, query/key/value/output）、FFN的第一层。
*   激活中的异常值集中在一小部分通道中。通常，这些离群特征只分布在 Transformer 层的少数几个维度。比如对于一个参数量为6.7亿的transformer模型来说，如果输入句子序列长度是2048，每个序列在整个模型中找到大约 150k 个异常值特征，但它们仅集中在 6 个不同的特征维度中。
*   这些激活上的离群点会出现在几乎所有的 token 上，但是局限于隐层维度上的固定的 channel 中；给定一个 token，不同 channels 间的方差会很大，但是对于不同的 token，相同 channel 内的方差很小。考虑到激活中的这些离群点通常是其他激活值的 100 倍，这使得激活量化变得困难。
*   Emergent Features是随着ppl指数增长的，与模型大小无关。

如下图所示，大量异常值特征的出现呈现一种平滑的趋势，而且基本体现为随着困惑度变化的指数函数。这表明异常值的出现并不是突然的。并且通过研究较小模型中的指数趋势，论文作者能够在相移（相位移动，Phase shift，它指的是一个波形在时间上发生的移位现象）发生之前就检测到异常值特征的出现。这也表明，异常值的出现不仅与模型大小有关，还涉及困惑度，也与所使用的训练数据量和数据质量等多个附加因素有关。论文作者推测模型大小只是离散特征出现所需的众多协变量中的一个重要协变量。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220732843-1319226094.jpg)

另外，这些大规模值还有个奇特现象：它们在不同的注意力头部中呈现出惊人的一致性，集中分布在相似的位置索引上。这打破了我们的传统理解——各注意力头部应该独立运作，处理不同类型的信息。想象一下，如果10个人思考同一个问题，正常情况下他们会关注不同的角度，但现在研究发现，他们都不约而同地关注了相同的几个点，这非常反直觉。你可能注意到，这些值的分布并非随机，而是遵循某种结构化模式，这暗示着它们在模型的信息处理中扮演着特定且关键的角色。

### 1.5 出现原因

关于outlier出现的原因，人们也做了研究，提出了很多观点。总结大致如下：softmax 和 RoPE 是产生异常值的起点，当token在Transformer架构中流动时，FFN、Norm等模块会对异常值做进一步放大，我们接下来就逐一分析。

#### 1.5.1 softmax

有一种观点认为，outlier和softmax的机制有关系。在 LLM 上下文中，outlier产生的原因是，因为某个注意力头不想关注某些有实际语义的token，所以就把更多的关注放在非语义 token（逗号等）上。如何才能把更多的关注放在非语义token上呢？这就是通过进行大量加权来产生outlier。我们使用论文“Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing”和“StableMask: Refining Causal Masking in Decoder-only Transformer”来进行学习。

##### LLM模型

论文“Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing”研究结果如下：

*   超过97%的离群值都是分割符token，如\[SEP\]，逗号"，"和句号"."等。
    
    ![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220748194-1617407502.jpg)
    
*   注意力头部将几乎所有的概率分配给\[SEP\]token和其他信息量较小的token，如点/逗号（下图(a)中左侧），而这些token对应的value很小（下图(a)中间）。这导致两者之间的乘积很小（下图(a)中右侧）。
    
*   在其他情况下（图b和c），我们观察到，很大一部分注意力概率仍然分配给了分隔符token。然而，通过在其他token上分配一些概率，这会引起hidden representation的（软）选择性更新。
    

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220801934-1912859044.jpg)

上图其实就展示了Attention的“不注意/不更新”机制，具体解释如下：

*   序列中存在一些不相关的token，如初始token或标点符号等非功能性单词，这些token更常被其他token观察到，我们可以称之为非语义 token。
*   注意力机制通常需要很少的重要token，其他token可能只是干扰。注意力机制应该只更新这些重要token的权重参数。对于这种干扰token，理想状态是把它们的注意力得分归零。毕竟不是所有token都需要学的，即不是所有token参与权重参数的更新。
*   在某些情况下，模型可能会发现“没什么token值得注意的”，这时它选择将不成比例的注意力（disproportional attention）放到非语义 token上，尽量不给其它token分配注意力概率，这种非语义 token会输出较小的value。这样才不会误更新那些有意义的token，起到“不注意/不更新”的作用。作者把这种机制称为attention的“no-op” 现象。

论文“Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing”作者对“no-op” 现象给出了几点假设和推理，我们结合论文“StableMask: Refining Causal Masking in Decoder-only Transformer”的思路一并分析：

*   为了让注意力模块在残差上不更新一个token的表示，一些注意力头希望将它们大部分的注意力概率分配给一些固定且常见的具有低信息量（例如，分隔符token或背景块）的token，这些token在学习之后可以产生小的value。
    
*   从softmax函数的定义中可以很容易地看出，这将使得softmax的输入具有相对较大的动态范围。实际上，在softmax恰好为零的极限情况下，此动态范围将无限大。
    
    ![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220815534-320488570.jpg)
    
*   由于层归一化（Layer Normalization）会归一化离群值，前一层FFN输出的大小必须非常高，这样才可以在LayerNorm之后，仍然产生足够大的动态范围。注意，这也适用于在自注意力或线性变换之前应用LayerNorm的Transformer模型。
    
*   在原始的softmax函数中，所有的输入都会被映射到0到1之间，并且所有的输出值之和为1，因此需要在所有可见token上不可避免地分配注意力。在这种情况下，softmax函数所施加的要求阻止了模型有效地将无关token的注意力得分归零。这意味着即使某些输入值非常小，它们在softmax函数处理后也会有一个非零的输出值。由于softmax永远不会输出精确的零，它总是会反向传播梯度信号以产生更大的离群值。因此，网络训练的时间越长，异常值的幅度就越大。
    
*   在Decoder Only模型中，序列最前面的token更容易成为离群值。这可能因为在序列逐渐被mask的过程中，序列最前面的token所占注意力较高，因为刚开始输入时还没有几个token，做softmax后由于分母的项少所以更容易分得更多注意力。而随着token数量的增加，更多的token参与了softmax操作，甚至为每个token分配非常小的概率也会导致显著的累积概率。因此，sink token不能像在序列开始时那样获得那么多的关注值。而且，因为初始token对几乎所有后续的token都是可见的，因此最初的token更容易被训练成为注意力汇集点，吸引一些不必要的注意力。
    

下图是BERT中注意力层的示意图，该图展示了前一层中的异常值如何影响下一层中注意力机制的行为。隐激活张量用x表示。生成幅度最大异常值的FFN的输出以红色来突出显示。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220831814-1013968153.jpg)

##### VIT模型

论文“Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing”通过分析ViT模型的特征图，发现其也是有离群值的。ViT中的离群值主要出现在图片背景上，背景token也都是没什么信息的。下图给出了在ImageNet验证集中的随机图像上展示的ViT异常值分析摘要。(a) 输入图像。(b) 第11层输出中的异常值。(c)在注意力头#1、层#12的每个补丁上花费的累积注意力权重（按行求和的注意力概率矩阵）。(d) 相应的注意力概率矩阵。(e) 异常值和非异常值补丁（patches）的平均值大小。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220840689-1423339079.jpg)

论文“Vision Transformers Need Registers”的观点如下：

*   和Quantizable Transformer一样，作者观察几种视觉Transformer模型发现：其特征图中有许多离群值特征，离群值特征就是L2范数很大的特征，并且更多地出现在更深的层、较大型模型的长时间训练中。
*   通过测量离群特征和相邻的四个token特征的余弦相似度，作者发现这些特征之间是高度相似的，进而暗示着离群特征的信息是高度冗余的。
*   作者推测离群特征包含更少的位置信息与像素信息。
*   作者又设计了一个实验，拿单个的离群特征和正常特征去预测其token所属图片的类别，结果是离群token分类的准确率更高，从而说明了离群token可能包含更多的全局信息。
*   作者的猜测和之前的研究差不多：充分训练的大模型在训练的时候会识别一些冗余的token，用这些token来处理、存储、检索一些全局信息。作者假定这种行为模式本身并不坏，但是对于模型的输出如果包含这种 tokens 就不太可取。事实上，这种异常的 tokens 会引导模型丢弃局部的信息，导致密集预测任务性能的降低。

#### 1.5.2 RoPE

论文"Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding"作者认为，RoPE 是驱动 QK 表征中结构化“离群值”出现的根本原因。

##### 现象

论文发现的现象如下：

*   在使用RoPE的模型中（如Llama、Gemma、Qwen等），注意力机制的查询（Q）和键（K）组件中存在显著的大规模值（massive values），这些值集中分布在特定区域，而在值（V）组件中却不存在此类现象。
*   而未使用 RoPE的模型（如GPT-2、OPT等）中并未观察到上述特性。
*   那些特意保留这些极端数值的量化技术（例如 AWQ 和 SmoothQuant）能够维持模型的原有性能；反之，若采用未能保留这些数值的方法（例如 GPTQ），模型的上下文推理能力将遭受重创。
*   在涵盖自回归 LLM（大语言模型）及多模态模型等多种 Transformer 架构中，作者均一致地观察到了这种“巨大值”集中的现象。
*   大规模值从第一层就开始出现，且在应用RoPE前后保持相对一致的模式。这表明大规模值的形成是模型训练过程中逐步形成的结果，而非简单由RoPE添加导致。

##### 分析

论文作者认为，RoPE 机制通过将嵌入维度划分为成对并应用不同频率的旋转操作，使得低频区域编码了丰富的语义内容而非位置信息，从而促成了大规模值的集中分布。这种模式在未采用 RoPE 的 LLMs（大语言模型）中则不会存在。

#### 1.5.3 FFN

下面文字摘录自 [从Training Dynamics到Outlier——LLM模型训练过程中的数值特性分析](https://zhuanlan.zhihu.com/p/29160636368)。

##### 矩阵乘法的动态性分析

给定矩阵乘法 \\(C = A \\times B \\in \\mathbb{R}^{m×n}\\)，可将其元素分解为：

\\\[C = A \\times B = \\begin{bmatrix} a\_0 \\\\ a\_1 \\\\ \\vdots \\end{bmatrix} \\times \\begin{bmatrix} b\_0 ; b\_1; \\dots \\end{bmatrix} = \\begin{bmatrix} a\_0 b\_0 & a\_0 b\_1 & \\dots \\\\ a\_1 b\_0 & a\_1 b\_1 & \\dots \\\\ \\vdots & \\vdots & \\ddots \\\\ \\end{bmatrix} = \[c\_{ij}\] = \[a\_i \\cdot b\_j\] \\\]

进一步有：

\\\[c\_{ij} = \\|a\_i\\| \\|b\_j\\| \\left \[ \\frac{a\_i \\cdot b\_j}{\\|a\_i\\| \\|b\_j\\|} \\right \] = \\|a\_i\\| \\|b\_j\\|\\cos{\\theta\_{ij}} c\_{ij} = \\langle a\_i, b\_j \\rangle = \\|a\_i\\| \\|b\_j\\| \\left( \\frac{\\langle a\_i, b\_j \\rangle}{\\|a\_i\\| \\|b\_j\\|} \\right) = \\underbrace{\\|a\_i\\|\\|b\_j\\|}\_{\\text{能量项}} \\cdot \\underbrace{\\cos\\theta\_{ij}}\_{\\text{相关性项}} \\\]

其中 \\(\\theta\_{ij}\\) 为两个向量的夹角，若两者均值为零，则 \\(\\cos{\\theta\_{ij}}\\) 与两向量线性相关性 \\(\\rho\\) 相等。根据上述分析，可以将矩阵乘法分解成两部分：

\\\[C = \\underbrace{\[\\|a\_i\\| \\|b\_j\\|\]}\_{能量矩阵} \\odot \\overbrace{\[\\cos \\theta\_{i,j}\]}^{相关性矩阵} = \\mathbf{E} \\odot \\mathbf{R} \\\]

其中 \\(a\_i\\) 表示的是每个token的能量， \\(b\_j\\) 表示权重矩阵对每个特征通道的固有缩放。两者张成的能量矩阵 \\(\\mathbf{E}\\) 表示了输入到矩阵乘法环节的总能量分布，而相关性矩阵 \\(\\mathbf{R}\\) 则表示了能量传输效率与信息选择。通常来说，能量矩阵 \\(\\mathbf{E}\\) 具有较高的动态范围，而相关性矩阵 \\(\\mathbf{R}\\) 需要较高的计算精度。

##### FFN主要计算

FFN的主要计算如下：

\\\[y = \\left\[ \\text{Swish}(xW\_1)\\odot xW\_2 \\right\] W\_3 \\\]

其中三次线性变换 \\(W\_1\\) 被称为gate projection， \\(W\_2\\) 被称为up projection，而 \\(W\_3\\) 被称为down projection。这三次projection的能量矩阵与相关性矩阵之间的特性如下：

*   权重矩阵分布基本服从正态分布且较为稳定，而激活输入则经常出现显著的异常值(outlier)；
*   相关性矩阵与能量矩阵之间没有相关性；
*   能量矩阵 \\(\\mathbf{E}\\)与输出激活值之间相关性较弱，对异常值的产生影响有限；
*   相关性矩阵\\(\\mathbf{R}\\)与输出激活值呈明显的线性相关关系，是产生大幅异常值的主导因素；

我们再来对SwiGLU的异常值产生过程进行数学机理分析：SwiGLU可看作一个门控选择放大单元，其计算可以分解为：

\\\[\\overbrace{\\text{Swish}(W\_{gate}x)}^{门控向量} \\odot \\underbrace{W\_{up}x}\_{特征向量} \\\]

这使得门控向量成为特征选择器，仅允许正相关特征通过。门控放大单元的机制也导致了以下作用：

*   异常值协同放大作用，当门控单元处于线性区，即\\(W\_{\\text{gate}}x > 0\\)时，\\(\\text{Swish}(z) \\approx z\\)，此时门控输出对up projection进行线性放大。当门控值与up projection都比较大时，up projection与gate projection中的outlier“撞”到了一起，相乘之后输出了非常大的数值，致使down projection输入激活值分布通常呈现高度尖锐特性。
*   零值聚集效应，当门控单元处于饱和区， 当 \\(W\_{\\text{gate}}x < 0\\) 时， \\(\\text{Swish}(z) \\rightarrow 0\\)，导致大量激活值被拉到零附近，进而导致了down projection的输入的动态范围被压缩。至此，down projection即包含了远远偏离零的outlier，又包含了非常靠近零的小值，最后导致该input对低精度训练来讲非常难以处理。

#### 1.5.4 LayerNorm

LayerNorm结构中的尺度参数γ作为一个放大器，也会放大输出中的离群值。

下图给出了BERT-ST-2上LayerNorm的异常值表示，可以看出，维度308处具有更尖锐的值，而且，乘数γ和输出\\(\\widetilde X\\)在相同的嵌入维度上包含异常值。如果删除γ，则可以发现\\(X'\\)的分布就更加平缓。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220910649-314745921.jpg)

#### 1.5.5 RMSNorm

RMSNorm层是当前主流大模型所使用的归一化层，相较传统的LayerNorm减少了对均值的计算，进而是的归一化层的计算更加高效。RMSNorm定义如下：

\\\[\\bar{x\_i} = \\frac{x\_i}{RMS(x)}w\_i, RMS(x) = \\sqrt{\\frac{1}{H}\\sum\_{i=0}^H x\_i^2} \\\]

在大模型中，RMSNorm先对每个token的表达进行归一化，再对每个channel进行缩放。我们可以直观的看到归一化与缩放两个阶段对数值范围的影响。归一化能够较好的抑制outlier，而缩放过程会放大某些重要的channel，但也会导致部分数值变得很大，反而加剧了outlier的情况。总的来说，RMSNorm主要作用是对outlier进行动态范围的压缩，这种压缩作用大多数情况能成功，但部分layer只能提供很少的压缩比。在这种情况下，就会对离群值

### 1.6 作用

#### 1.6.1 负面作用

异常值（离群值）会给量化带来什么影响？根据 Llm.int8() 和 Smoothquant 中的发现，LLM中，权重和激活都存在显著的异常值 。 这些异常值对量化过程有显著影响，因为它们会增加量化步长，同时降低中间值的精度。保留这些稀疏离群值也会导致速度大幅下降（例如1.5%的离群值导致SpQR的速度下降超过30%）。

假设有一个向量A=\[-0.10, -0.23, 0.08, -0.38, -0.28, -0.29, -2.11, 0.34, -0.53, -67.0\]。我们可以看到异常值是有害的：

*   如果我们在保留emergent feature -67.0的情况下对该向量做量化和反量化，处理后的结果是：\[ -0.00, -0.00, 0.00, -0.53, -0.53, -0.53, -2.11, 0.53, -0.53, -67.00\]，大部分信息在处理后都丢失了。
*   如果我们去掉emergent feature -67.0对向量A做量化和反量化，处理后的结果是：\[-0.10, -0.23, 0.08, -0.38, -0.28, -0.28, -2.11, 0.33, -0.53\]。出现的误差只有其中一个数值-0.29变成了-0.28。

我们再用数学公式来推导下。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220927462-1557200023.jpg)

请注意，计算 \\(\\Delta\\) 时使用的是最大值，因此 X 中的超离群值会大大增加步长。导致离群值平均会被舍入到更远的值，从而增加量化误差。随着超离群值的增加，离群值被舍入到更少的离散值中，更多的量化 bin 未被使用。这样，超离群值就会导致量化保真度降低。

#### 1.6.2 正面作用

##### 保持性能

LLM.int8()作者发现，异常值特征强烈影响 Attention 和 Transformer 的整体性能。事实证明，在量化过程中裁剪这些异常值不利于 LLM 的性能。保留异常值对于大语言模型的性能至关重要。 比如：

*   在训练的初始阶段，任何基于裁剪的方法都会导致异常高的困惑度（perplexity）分数（即> 10000），从而导致大量信息丢失，很难通过微调来恢复。而且，对于越大的模型，离群值对于模型性能的影响越大，模型对outlier的依赖更强。
*   保留稀疏离群值可以提高准确率。如果去除异常值，即使最多有 7 个异常值特征维度，top-1 softmax 概率就会从约 40% 降低到约 20%，验证集困惑度增加了 600-1000%。当改为删除 7 个随机特征维度时，top-1 概率仅下降 0.02-0.3%，困惑度增加 0.1%。这些结果突出了异常值特征的关键性质。这些异常值特征的量化精度至关重要，因为即使是微小的误差也会极大地影响模型性能。

##### 上下文理解

在理解大规模值的作用前，我们需要区分两种知识类型。"参数知识"是指模型在训练过程中学到并存储在其参数中的知识，例如"巴黎是法国首都"；而"上下文知识"是指模型从当前输入上下文中获取的信息，例如理解一篇文章后回答其中提到的细节。

论文"Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding"作者认为，大规模值在处理上下文知识（contextual knowledge）方面扮演至关重要的角色，而对参数知识（parametric knowledge）的影响相对较小。

*   此类极端数值对模型的上下文理解能力至关重要，相较之下，其对于参数化知识的倚重程度则较低。实验显示，若此类数值受到干扰，模型仍能回忆既有事实（例如，回答“中国首都是哪里？”），但在需要依赖上下文的任务中（如 GSM8K 数学推理测试），其表现则会显著下滑。
*   当作者刻意让上下文信息与模型的内在知识产生冲突时（例如"地理知识已改变，纽约现已成为英国的一个城市"），发现 LLMs（大语言模型）的表现与随机猜测无异。然而，当大规模值被破坏后，模型的准确率反而显著高于随机水平。这暗示 LLMs（大语言模型）在默认情况下更倾向于依赖其内部知识，而这些“巨大值”则在引导模型理解上下文方面扮演着关键角色。当大规模值被破坏后，模型失去了处理误导性上下文信息的能力，转而默认使用其参数知识，有效地忽略了矛盾的上下文。
*   这些“巨大值”对于需要依赖上下文的任务——例如密钥信息检索、文本情感分析以及逻辑推理——具有不可或缺的作用，而对于参数化知识的直接调取，其影响则相对有限。

因此，人们往往选择保留这些异常值。 朴素且高效的量化方法（W8A8、ZeroQuant等）会导致量化误差增大，精度下降。而GPTQ不特别保护大规模值，在上下文知识理解任务上表现显著下降（准确率降至约75%，归一化后），但在参数知识检索任务上依然表现不错。为了更有效地处理异常值，LLM.int8()提出了混合精度分解，以确保激活中的一些异常值的准确性。SmoothQuant通过平滑激活中的异常值来降低量化的难度。SpQR识别敏感权重以确保其精度，同时将其他权重量化到较低的比特宽度。AWQ通过在量化过程中选择性地保护"重要"权重来维持大规模值，在所有任务上保持较强的性能表现。

### 1.7 难点

我们总结大模型量化困难的原因为如下几点：

*   激活比权重更难量化。权重分布较均匀，易于量化。有研究表明 LLMs 的权重量化到 INT8 甚至 INT4 并不会影响精度。
    
*   激活难以量化的原因是因为异常值。在 per-tensor 量化的情况下，大的异常值主导了最大幅度，异常值规模比大多数激活值大约 100 倍。这压缩了非异常通道的有效量化位数，导致非离群点 channel 的有效量化 bits/levels 较低：假设 channel 𝑖 的最大幅值为 \\(𝑚\_𝑖\\) ，整个矩阵的最大值为 𝑚 ，channel 𝑖 的有效量化级别为 \\(2^8·𝑚\_𝑖/𝑚\\) 。对于非离群点 channel，有效的量化级别将非常小 (2-3)，导致量化误差较大。
    
*   可以认为异常值对应了涌现特征。这些异常值代表了模型进行预测所依赖的重要特征，在模型的决策过程中起着至关重要的作用，可以有效地充当模型已学会识别的特征的指示符，有助于网络处理和解释输入数据。因此，应该以原始精度保留这些异常值，这样才能让网络保留其所学知识，保持模型的性能和准确性。
    
*   异常值持续存在于一小部分特定的通道（channel）中（固定通道存在异常值，并且异常值通道值较大）。如果一个 channel 有一个异常值，它持续出现在所有 token 中。对于特定 token，不同通道的激活值差异很大（少部分通道激活值很大，大部分通道较小），但同一通道内不同 tokens 的激活值幅度差异小（异常值通道幅度持续较大）。
    
    由于异常值的持久性和每个 channel 内部的小方差，如果可以做 activation 的 per-channel 量化 (对每个 channel 使用不同的量化 step)，与 per-tensor 量化相比，量化误差会小得多，而 per-token 量化帮助不大。
    

上述现象总结起来就是，离群值跟通道相关，跟 token 无关。因此应该对激活采用逐通道量化 （即每个通道使用不同的量化系数），这可以大幅降低量化误差。逐 token 量化则帮助不大。但是，逐通道激活量化并不适合硬件加速的 `GEMM` 内核（线性层计算），因为这些内核依赖于高吞吐量的连续操作（如 Tensor Core MMAs），无法容忍低吞吐量指令（如转换或 CUDA Core FMAs）的插入，而量化公式中无论是量化系数还是浮点数到定点数的转换都是用 CUDA Core 计算单元。

0x02 超异常值
---------

在实际工作中，研究人员发现在异常值中有些另类个体，其特点是：1. 数值非常大，通常比其他的异常值大很多 。 2. 它们的数量非常少。有研究者将这些有问题的，数量很少，但是绝对值又特别大的异常值称为超异常值，这里包括（包括超级权重和超级激活值）。

### 2.1 超级权重

我们接下来看看超级权重。有研究人员发现，在大模型中，有一小部分特别重要的特征（称之为「超权重」），它们虽然数量不多，但对模型的表现非常重要。如果去掉其他一些不那么重要的特征，模型的表现只会受到一点点影响。如果去掉这些「超权重」，模型就开始胡言乱语，可能都不会生成文本。

#### 2.2.1 作用

为了量化「超权重」对模型的影响有多大，研究团队修剪了所有的离群值权重，结果发现，去掉一个「超权重」的影响，比去掉其他 7000 个离群值权重加起来还要严重。研究团队发现超级权重有两种主要影响：

*   引发「超激活」。超权重会放大输入 token 激活的离群值，这种现象研究者们称之为「超激活」（super activation）。论文发现，在降维投影之前，门控和上投影的 Hadamard 乘积产生了一个相对较大的激活，而「超权重」进一步放大了这个激活并创造了「超激活」。无论输入什么提示词，「超激活」在整个模型中都以完全相同的幅度和位置持续存在。而这源于神经网络中的「跨层连接」。
*   抑制了停用词（stopword）的生成概率。含有「超权重」的原始模型能够以 81.4% 的高概率正确预测。然而，移除「超权重」后，模型预测的最多的词变成了停用词「the」，并且「the」的概率仅为 9.0%，大多数情况是在胡言乱语。这表明，「超权重」对于模型正确且有信心地预测具有语义的词汇至关重要。

研究团队还分析了超级权重幅值变化对模型质量的影响，通过将超级权重按 0.0 到 3.0 的缩放因子放大。结果表明，适度放大幅值可以提升模型准确率。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529220941007-1041719157.jpg)

#### 2.2.2 识别

不同的大模型的「超权重」的分布却出奇地相似，比如：它们总是出现在mlp.down\_proj。

但是如何识别具体的「超权重」？论文提出了一种高效的方法：通过激活的峰值可以进一步定位「超权重」。具体而言，我们可以通过检测层间降维投影输入和输出分布中的峰值来定位「超权重」。这种方法只需要输入一个提示词，非常简单方便，不再需要一组验证数据或具体示例了。

假设存在降维投影（down proj）权重矩阵\\(W \\in R^{D \\times H}\\)，其中 D 表示激活特征的维度，H 是中间隐藏层的维度。设\\(X \\in R^{L \\times H}\\)为输入矩阵，其中 L 表示序列长度。定义输出矩阵为\\(Y=XW^T\\)，「超激活」为\\(Y\_{ij}\\)，\\(Y\_{ij} = \\sum\_{k=1}^d x\_{ik}W\_{jk}\\)。如果 \\(X\_{ik}\\) 和 \\(W\_{jk}\\)都是远大于其他值的异常值，那么 \\(Y\_{ij}\\) 的值将主要由这两个异常值的乘积决定。在这种情况下，j 和 k 是由 \\(X\_{ik}\\) 和 \\(Y\_{ij}\\) 的值决定的。因此，可以首先绘制出 mlp.down proj 层的输入和输出激活中的极端异常值。接着确定超权重所在的层和坐标。一旦检测到一个超权重，将其从模型中移除并重复上述过程，直到抑制住较大的最大激活值。

下图给出了超级权重的行为。

*   I：超级权重通常出现在模型早期层的向下投影中，用蓝紫色框表示。超级权重会立即产生令人难以置信的超大激活值。
*   II： 超级激活通过skip connection 进行传播，用蓝紫色线表示。
*   III： 这有一个净效应（net effect），即抑移除超权重会导致生成停用词的可能性飙升，用蓝紫色堆叠条表示。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221039793-941249564.jpg)

这些超离群值对模型质量非常重要，因此在量化过程中保留它们至关重要。

### 2.2 massive outlier

事实上，人们更多的将超级激活叫做massive activation，或者massive outlie（大规模异常值）。

人们常说的LLM的Outliers更多是指激活值的Outliers，因为权重的整体变化幅度还是比较小的。早期对Outliers是不做区分的，人们认为的Outliers一般指Normal Outliers，其更多地出现在Channel维度，这也是之前的通道级量化方法想解决的。最近的工作如Massive Activations in Large Language Models等将在token维度出现的值特别大，但又非常少的Outliers视为Massive outlier，这是之前通道级量化方法解决不了的，但是可以通过旋转矩阵或者CushionCache（Prompt）吸收的方式来解决。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221053878-876412437.jpg)

#### 2.2.1 定义

为了定量的分析，研究人员提供了一个宽松但广泛的定义： 如果激活的幅值超过100，并且至少或大约是其隐藏状态的中值幅度的1000倍，则认为该激活是 massive activations。

Massive Outliers特点是它们的值非常高，在token子集中的出现有限。下图分别给出了不同模型中 Massive Outliers 的现象。

*   上方是 LLaMA2-7B中的激活幅度（Magnitudes ）（z轴）。x和y轴是序列和特征维度。对于这个特定的模型，我们观察到具有巨大幅度的激活出现在两个固定的特征维度（1415和2533）和两种类型的token中——起始token和第一个句点（.）或换行符（\\n）。
*   中间是LLaMA2-13B中的massive activation。在这个模型中，它们出现在两个固定的特征维度（2100和4743）中，并且仅限于起始token。
*   下方是Mixtral-8x7B中的massive activation。在这个模型中，它们位于两个特征维度（2070和3398）中，并且位于起始token、分隔符token和某些单词token（“and”和“of”）中。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221101944-1193221644.jpg)

#### 2.2.2 作用

论文“Massive Activations in Large Language Models” 认为，massive activations 起到了类似 bias 的作用。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221110014-1360356207.jpg)

##### 特点

针对bias，massive activations 展示出的具体特点如下：

*   massive activations主要出现在：
    *   起始token。这可以归因为，因为起始token是序列所有token都会使用的，所以适合将更多的 bias信息放在这里。
    *   delimiter token（标点符号）和弱语义token。这些 token 的语义值相对较低，因此可以作为存储bias的低成本选项。

*   大部分的massive activations从前几层后开始出现并几乎保持常数不变，最后在最后几层开始消散。ViT 中的 massive activation 比 LLM要少，但是仍然存在。massive activations 仅仅出现在几个初始层之后，是因为LLM将需要一些初始层来处理与大规模激活相关联的token的含义。在这些层处，token的语义可以经由自注意机制被转移到其他token上，并且前向传播中被保留。
*   massive activations 非常重要，大多数 token 会把注意力放在 这些 massive Activations上。仅仅将 四个 massive activations 设置为零，就会导致模型性能的灾难性崩溃，当然将它们们设置为平均值并不会损害模型，这表明massive activation非常重要，但并不是具体精确值重要，而是作为一个偏置项bias对整体计算非常重要。

论文作者认为，massive activations实际上很像ViT中的Register Token，作者指出它们同样拥有超大激活值。为了说明它们的实质作用，他们对所有的Register Token拉平均以破坏其含有的信息，但是性能却没有变化。

##### 论证

论文作者跟踪了这些token的embeds在attention层的表现。发现超大激活值经过LayerNorm的Scale之后消失。此外，这些token提供的信息“看起来很类似”，比较稳定。

下图展示了从输入隐藏状态到查询、键和值状态的激活轨迹。

图a展示了在LLM中，在每一层，输入特征都经过层归一化处理3，然后通过线性投影转换为查询、键和值状态。

图b显示了此示意图中计算的所有隐藏状态（LLaMA2-7b，第3层）。我们发现，在所有阶段，与大规模激活相关的两个token的特征与其他token截然不同。具体来说，在第一个“归一化”步骤之后，这两个token的嵌入表现为具有两个不同非零元素的稀疏向量。值得注意的是，后续的QKV状态在每个嵌入中表现出相当小的变化。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221118537-517058390.jpg)

作者猜测这些token目标是在attention中提供一个固定的偏置。鉴于注意力也集中在与大规模激活相关的token上，作者因此分离出这些token，并研究它们对注意力输出的影响（注意力矩阵层乘以值向量）。

下图显示了LLaMA2-7B中的分解值更新和注意力输出。在下图的等式中，作者将每个 token k 处的注意力输出分解为两部分：来自token集C的值更新（value updates），这些注意力被集中化；以及从其他token聚集而来的值更新。下图的输入提示为“Summer is warm. Winter is cold.”。在这种情况下，集合C由tokenSummer和第一个周期token组成。我们可以看到，C的值更新在token之间几乎是相同的，也就是说，它们充当了加性偏差项，尽管没有明确施加。此外，我们注意到，这种值更新模式在各种输入中非常相似。总体而言，论文的结果表明，LLM使用大量激活来在某些代币上分配大量注意力。然后，在计算注意力输出时，这些token被用来形成一个恒定的偏差项。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221126343-1731475930.jpg)

#### 2.2.3 难点

在LLM量化方法中，一个主要问题是存在激活异常值，这会扩大量化步长，从而导致显著的精度损失。为了缓解这个问题，目前的研究已经开发了各种方法来解决激活中的正常异常值。不幸的是，现有的LLM量化方法很难有效地解决Massive Outliers。通道级量化方法大多在通道维度对权重和激活值做等价变换，从而降低通道上Outlier的幅度，使得量化更容易进行。但是并没有考虑到Outlier并不只在通道维度上，这么做还可能引入新的Outlier。

例如，SmoothQuant 尽管使用平滑因子将一些激活异常值转移到权重部分，但仍然无法有效处理具有极大值的Massive Outliers。具体如下图所示，SmoothQuantit的相应权重变化导致了新异常值的出现。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221136068-1211784150.jpg)

因此，迫切需要一种LLM量化方法，有效地解决正态和大规模异常值问题。这就是旋转矩阵或者CushionCache（Prompt）吸收的方式，我们会在后续专门论述。

0x03 Transformer量化
------------------

### 3.1 原理分析

我们首先从效率分析开始，说明量化技术如何减少大模型的端到端推理延迟。

#### 3.1.1 推理特点

LLMs模型的推理可以大致分为两个stage: prefill和decoding. 两个stage有其各自的鲜明的特点,

*   prefill阶段的行为可以类比训练的前向过程。在多数情况下, prefill阶段是compute bound。
*   decoding阶段的sequence length恒等于1，是IO bound。很多情况下, decoding较prefill在应用中出现频率更高, 推理的自回归阶段实际上是受内存带宽限制的，所以能够更快地计算并没有太多价值。由于推理受内存带宽限制，我们实际上只对减少内存占用感兴趣，因为这意味着数据传输更少。

#### 3.1.2 对框架的要求

上述特点要求推理框架需要支持两套计算逻辑以适配其不同的特点.

*   权重激活量化。在prefilling阶段，大模型通常处理长token序列，主要操作是通用矩阵乘法(GEMM)。Prefilling阶段的延迟主要受到高精度CUDA内核执行的计算操作的限制。为了解决这个问题，现有的研究方法对权重和激活都进行量化，以使用低精度Tensor核来加速计算。在每次GEMM操作之前会在线执行激活量化，从而允许使用低精度Tensor核(例如INT8)进行计算。这种量化方法被称为权重激活量化。
*   仅权重量化。在解码阶段，大模型在每个生成步中只处理一个token，其使用通用矩阵向量乘法(GEMV)作为核心操作。解码阶段的延迟主要受到加载大权重张量的影响。为了解决这个问题，现有的方法主要关注使用量化权重来加速内存访问。首先对权重进行离线量化，然后将低精度权重去量化为FP16格式进行计算。而量化模型由于其低位宽的权重表征, 可以大大缓解IO bound现象。

下图中，(a)是仅权重量化推理流程。(b)是权重激活量化推理流程。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221148190-709655679.jpg)

### 3.2 量化模块

基于上述信息，研究人员对Transformer进行了量化改造。因为对于大语言模型而言，其性能瓶颈在于矩阵乘法算子（占系统运行时间的80%以上），以及Self Attention（占系统显存开销的50%以上） ，所以，目前大多LLMs模型的量化算法可以理解为仅关心Linear算子的量化，对于其他部件(norm, embedding, softmax, add...)都没有进行研究。

下图是LLM的量化推理过程。紫色矩形框是量化的模块。量化主要针对具有权重的线性层，即自注意力模块中的Q、K、V和O层以及FFN模块中的Up、Gate和Down层。图的右侧显示了3种量化类型，包括weight-activation量化、weight-only量化和KV-cache量化。其中，X、 K和V以per-token的方式量化。W的范围在部署前进行了静态校准。对于仅权重（weight-only）设置，W采用per-group量化，对于权重激活量化，W采用per-channel量化。

假设输入为\[batchsize, seq\_len, dim\]，将batchsize乘以seq\_len进行合并，我们可以得到\[num\_tokens， dim\]，进行per token量化的话就是按行量化，即每一个token都有一个对应的scale。对于一个权重\[input\_dim, output\_dim\]，执行per channel量化就是对output\_dim进行按列量化，权重的每一列都对应一个scale。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221159589-1768903169.jpg)

下图则给出了量化的推理数据流（data-flow）。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221258257-994014126.jpg)

下图给出了INT量化的范式。

*   FP32作为基准，提供了最大的数值范围和零精度损失，但存储开销最大。
*   如果用户不太关心效率，那么INT16格式是最佳选择。INT16格式是最精确的，如果是转换FP32，INT16甚至比FP16更精确。
*   对于对实时性要求高的服务，建议采用INT8量化方案，可以在保持较高精度的同时获得显著的性能提升。如果你的网络中某些层需要更高的精度，可以使用W8A16来解决这个问题。
*   在资源受限但对精度要求相对较低的场景，则可以采用INT4方案以获得最大的资源效益。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221354038-1406627911.jpg)

### 3.3 分类

下面分类来自三个不同的论文，我们可以比对进行学习。可以看到，这些分类基本都是从量化进程进行区分，基本都是从QAT（Quantization-Aware Training）和PTQ（Post-Training Quantization）两个分类往下展开。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221410944-1316448335.jpg)

而再往下就是对PTQ进行细分，比如可以分为：仅量化权重；权重与激活同时量化；KV Cache量化三种。具体可以参见下图。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221417216-131969366.jpg)

#### 3.3.1 PTQ

PTQ 的主要优势在于其简单和高效，无需对 LLM 架构进行修改或重新训练，仅使用有限的校准数据进行校准即可。然而，值得注意的是，PTQ可能会在量化过程中引入一定程度的精度损失。PTQ特别适合需要大幅度压缩模型的场景。对于通常包含数十亿参数的 LLMs 来说，量化感知训练（QAT）的训练成本过高，PTQ 因此成为一种更实际的替代方案。尽管PTQ方法已经在较小的模型中得到了很好的探索，但是，大模型以其规模和复杂性为特征，需要用专门的方法来有效地处理量化过程。

我们可以将LLM的PTQ分为三组：仅权重量化、权重激活量化和KV缓存量化，这些组之间的差异在于它们的量化目标。

*   仅权重量化仅关注量化权重。先前的工作发现，激活量化通常更容易受到权重量化的影响，因此我们可以仅靠权重量化就可以实现较低的位宽。然而，由于量化权重在与激活相乘之前需要反量化，因此仅权重量化在推理过程中不可避免地会引入额外的计算开销，并且无法享受特定硬件支持的加速低位操作。
*   权重激活量化将其目标扩展到权重和激活。
*   kv缓存量化针对的kv缓存。KV缓存通常会消耗大量内存，成为内存瓶颈。实现kv缓存量化可以提高吞吐量，并更有效地容纳具有更长token的输入。

下图给出了代表性算法，具体从四个维度进行了分类。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221432064-140214643.jpg)

我们接下来介绍仅权重量化和权重激活量化的一些典型方案，会在后续专门开篇来介绍KV缓存量化。

##### weight-only quantization

在PTQ中，某些方法专注于仅对LLM的权重进行量化，以提高效率并减少计算需求。

*   GPTQ：GPTQ是一种训练后权重量化方法，使用基于二阶信息的逐层量化，成功将每个权重量化至 3-4 位，几乎没有精度损失。GPTQ 对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。 GPTQ 量化需要准备校准数据集。
    
*   QuantEase的工作建立在GPTQ之上。QuantEase在对每一层进行量化时，会用一种基于坐标下降的方法来更精确地补偿未量化的权重。此外，QuantEase可以利用来自GPTQ的量化权重作为初始化，并进一步完善补偿过程。
    
*   LUT- GEMM：LUT- GEMM是一种利用查找表(Look-Up Table, LUT)的去量化方法，它采用了一种称为二进制编码量化(BCQ)的非均匀量化方法，该方法包含了可学习的量化区间。
    
*   AWQ ：AWQ观察到权重通道对性能的重要性各不相同，通过保留1%的显著权重可以大大减少量化误差。基于此观察，AWQ采用了激活感知权重量化来量化LLM，具体会专注于激活值较大的权重通道，并通过每通道缩放实现最佳量化效果。
    
*   OWQ：OWQ作者观察到，激活异常值会放大权重量化损失。他们提出了异常值感知权重量化（OWQ）来识别那些具有激活异常值的脆弱权重，并为它们分配更高的精度，同时以较低的精度级别量化其余权重。
    
*   SpQR：SpQR在量化过程中识别并分离容易产生较大量化误差的离群值权重，把这些离群值权重以更高的精度来存储，而其余权重被压缩到3-4位。此外，他们引入了一种专为SpQR格式量身定制的解码方案，提高了逐个token的推理效率。
    
*   SqueezeLLM提出将离群值存储在全精度稀疏矩阵中，并对剩余权重应用非均匀量化。根据量化灵敏度确定非均匀量化的值，能够提高量化模型的性能。
    
*   FineQuant采用了一种基于经验的启发式方法，将不同级别的粒度分配给不同的权重矩阵。
    
*   LLM-MQ采用FP16格式来保存权重异常值，并将其存储在压缩稀疏行(CSR)格式中，以提高计算效率。此外，LLM-MQ将每个层的位宽分配，建模为整数规划问题，并采用高效的求解器在几秒内求解。
    

##### weight-activation quantization

许多PTQ中的工作尝试对LLM的权重和激活同时进行量化。

*   ZeroQuant采用细粒度量化权重和激活（对权重做group-wise，对激活值做token-wise），并利用核融合来最小化量化过程中的内存访问成本，并逐层进行知识蒸馏以恢复性能。
*   LLM.int8() 发现激活中的异常值集中在一小部分通道中。基于这一点，LLM.int8() 根据输入通道内的离群值分布将激活和权重分成两个不同的部分。包含激活值和权重的异常数据的通道以FP16格式存储，其他通道则以INT8格式存储。
*   SmoothQuant观察到不同的token在它们的通道上展示出类似的变化，因此SmoothQuant引入了逐通道缩放变换，有效地平滑了幅度，使得模型更易于量化。
*   FlexGen将权重和KV缓存直接量化到INT4中，以减少大批量推理期间的内存占用。
*   OliVe观察到离群值附近的普通值不那么关键。因此，它将每个离群值与一个普通值配对，用牺牲普通值来获得更大的离群值表示范围。
*   OS+观察到异常值的分布不对称的，并且集中在特定通道中，这对大模型的量化提出了挑战。为了解决这个问题，OS+引入了一种通道级别的移动和缩放技术。在搜索过程去确定移动和缩放参数，能有效地处理集中和不对称的离群值分布。
*   LLM-FP4努力将整个模型量化为FP4格式，并引入了预移位指数偏置技术。该方法将激活值的比例因子与权重相结合，以解决异常值带来的量化问题。
*   Omniquant与先前依赖量化参数的经验设计的方法不同。相反，它优化了权值裁剪的边界和等效变换的缩放因子，以最小化量化误差。
*   QLLM通过实现通道重组来解决异常值对量化的影响。此外，QLLM还设计了可学习的低秩参数，来减小post-quantized模型的量化误差。
*   RPTQ发现不同激活通道的分布，实质上是变化的，这给量化带来了挑战。为了缓解这个问题，RPTQ将具有相似激活分布的通道进行聚类分组，并在每个组内独立地应用量化，有效地减轻了通道范围的差异。
*   Outlier Suppression+ 通过确认激活中的有害异常呈现出不对称分布，主要集中在特定通道中。因此，Outlier 引入了一种新的涉及通道级的平移和缩放操作的策略，以纠正异常的不对称呈现。

#### 3.3.2 QAT

在量化感知训练（`QAT`）中，量化过程被无缝地集成到大型语言模型（LLMs）的训练中，使这些模型能够适应低精度表示，从而减轻了精度损失。因为通常涉及整个模型的重新训练，通常需要大量的训练数据和计算资源，这对QAT的实施构成了潜在的瓶颈。不幸的是，当前最好的低于8比特的PTQ方法也会导致模型质量急剧下降。因此，对于更高的量化水平，人们发现有必要使用量化感知训练（QAT）。

当前针对LLM的QAT方法分为两类：全参数重新训练和参数-高效再训练。

*   全参数重新训练是一种在量化LLM时对LLM进行完整的参数重新训练的方法，通过使用数据生成方法以及QAT和蒸馏技术，这样可以保留原始模型的涌现能力，同时减少内存使用和计算量。
*   参数-高效再训练是指采用参数高效的方法重新训练LLM。其中，QLoRA和QA-LoRA提出了将组量化集成到QLoRA中，缓解量化与低秩适应之间的不平衡问题。此外，还有如冻结量化指数并仅微调量化参数、使用二进制量化等。这些方法在具有相对可接受的计算预算下，采用低秩适应来重新训练量化LLM。

使用QAT量化LLM在几个主要方面面临挑战：

*   对数据要求高。如果训练数据域太窄或者与原始预训练数据分布存在显著不同，则可能会损害模型的性能。
*   对算力要求高。大模型的训练对于算力资源要求比较集中，通常需要大量的训练数据和计算资源。
*   由于LLM训练及其复杂，因此，很难准确地复现原始的训练设置。

我们用优化点来进行分类。

##### 减少数据需求

为了减少数据需求，LLM-QAT引入了一种无数据的方法，利用原始FP16的大模型生成训练数据，然后利用预训练模型生成的结果来实现无数据蒸馏。具体来说，LLM-QAT使用词表中的每个token作为生成句子的起始token。基于生成的训练数据，LLM-QAT应用了基于蒸馏的工作流来训练量化的LLM，以匹配原始FP16大模型的输出分布。

##### 减少计算量

为了减少计算量，许多方法采用高效参数微调(parameter-efficient tuning，PEFT)策略来加速QAT。

*   QLoRA将大模型的权重量化为4位，随后在BF16中对每个4位权重矩阵使用LoRA来对量化模型进行微调。
*   LoftQ指出，在QLoRA中用零初始化LoRA矩阵对于下游任务是低效的。作为一种替代方案，LoftQ建议使用原始FP16权重与量化权重之间差距的奇异值分解(Singular Value Decomposition，SVD)来初始化LoRA矩阵。LoftQ迭代地应用量化和奇异值分解来获得更精确的原始权重近似值。
*   PEQA是一种新的量化感知 PEFT 技术，可以促进模型压缩并加速推理。它采用了双阶段过程运行。在第一阶段，每个全连接层的参数矩阵被量化为低比特整数矩阵和标量向量。在第二阶段，对每个特定下游任务的标量向量进行微调。这种策略大大压缩了模型的大小，从而降低了部署时的推理延迟并减少了所需的总体内存。 同时，快速的微调和高效的任务切换成为可能。

#### 3.3.3 常见方案

总体来说，对于权重量化，可以通过区分不同的敏感度来进行差异性量化。对于敏感度的度量，可以分为用激活值来进行度量，用Hessian矩阵度量，用近似的Fisher information度量。在处理方式上，有平滑的方式，有混合精度稠密稀疏分解的表示方式，或者围绕混合精度设计不同的硬件友好的方式。对于激活值量化，则需要对离群值做特殊处理，处理的方式有平滑的方式、内存重排的方式、混合精度的处理方式（离群值高精度+普通值低精度）和裁剪的方式，

我们接下来从量化的技术方案角度来看看一些常见方法，此处和上面的PTQ/QAT分类不是正交的。

##### 优化

GPTQ 的核心思想是通过最小化量化引入的输出误差，实现高精度低比特量化。GPTQ使用优化的方法，在量化前面的元素的时候，解析地更新后面的元素，使得量化的优化目标公式最小化。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221502340-1609877190.jpg)

##### 转移

大模型量化的难点在于outlier的存在，AWQ和Smooth Quant使用一个平滑系数，将activation的outlier转移到weight上，一方面可以提升显著channel上的weight量化范围，另一方面可以减缓activation量化误差。

##### 旋转

但是，这种转移outlier并不能消灭outlier；于是研究人员想出了旋转方案。这类方法一般利用单位随机矩阵和权重变换之后再量化，使得变换后的权重更容易量化。Spinquant使用旋转矩阵（Hadamard矩阵或者其他可学习的正交矩阵）来分别左乘和右乘weight和activation矩阵，由于旋转矩阵的正交性，该变换是等价变换。DuQuant通过学习旋转变换和通道置换变换，在激活矩阵内部将 outliers 转移到其他通道，最终得到平滑的激活矩阵，从而大幅度降低了量化难度。

##### 可学习

很多方法都是手动设计或者网格搜索搜出来的缩放参数，有研究人员认为这是次优的。解决方式应该是在量化过程中学习转换参数和裁剪参数。

[OmniQuant](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2308.13137)将平滑因子和量化截断阈值都设置为可学习的，在block范围内对其进行训练，能取得不错的效果。

##### 查表和VQ

LUT（Look Up Table）方法和前面方法的最大区别是映射关系的转变。因为现实模型的权重矩阵不可能是均匀分布的。有相当大的一部分权重是“聚集”在一起的，故考虑将这些权重聚类，用一个聚类中心来映射这些权重，保存聚类中心（FP16)和聚类索引(INT）即可。当我们需要推理时，可以使用索引Table或者说Vector里的FP16的值进行运算。这是一种理论上更优的压缩方式。

在LLM的极低比特(2-3bit)量化方向上，常规的标量量化方法由于数值表达范围的限制，通常很难达到一个可以接受的精度。近年来有不少研究者开始采用VQ（Vector Quantization）的方法来进行LLM的weight-only 量化。Vector Quantization，属于一种数据压缩技术，具体而言，这是一种把高维向量映射至一组预先设定好的低维向量之上，并且低维向量事先被存储于码本（codebook）当中的技术。获取codebook是一个涉及数据驱动的学习过程，目的是找到一组最优的码词，以便高效地表示和压缩数据向量。编码时每个数据点都由码本中相应向量的索引表示；解码时使用这些索引对原始数据进行近似。这种方法大大降低了数据的存储要求，同时允许通过简单的索引引用来快速重建原始向量。

从理论上来讲，VQ就是在数据中寻找共性以及去除数据中的冗余，因此，它可以实现比标量量化更高的数据压缩效率，这也就是把VQ用于LLM权重压缩的理论基础。虽然VQ可以突破标量量化的瓶颈，但是将VQ方法应用于LLM权重压缩还面临着以下挑战：

*   在极低比特压缩率的情况下保证压缩的精度；
    
*   由于LLM参数庞大，在LLM上如何高效的应用压缩量化算法；
    
*   从量化压缩的权重之中还原原有的模型权重带来的计算开销；
    

我们使用VPTQ为例来进行学习。VPTQ主要是面向极低bit(eg. 2bit)的场景，是向量量化(Vector Quantization)和GPTQ的结合体，可以简单理解为将GPTQ的逐列scale量化替换为vq量化。向量量化核心想法就是将数据按照向量 (vector) 组织聚类，并将聚类结果保存为查找表(codebook)，而原始数据就可以用 Index 表示。 压缩后的模型权重可以压缩成 index + codebook。在 LLM 推理的时候只需要在当前算子执行之前，用 index和codebook 解压就可以得到想要的原始权重。

VPTQ将量化问题视为优化问题，通过二阶优化方法指导量化算法的设计。该方法的创新点主要体现在以下几个方面：

*   通道独立的二阶优化：VPTQ对每一列的权重矩阵进行独立量化，使用二阶优化来指导量化算法设计。
*   初始代码本的优化：采用加权Kmeans聚类方法初始化codebook，确保量化过程中的错误最小化。
*   低解量化开销：VPTQ在推理过程中只需读取codebook中的中心点来进行解量化，显著提高了推理吞吐量。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221523182-969135927.jpg)

##### 基于Attention Sink的量化方法

massive outlier 的特点是，其在所在的 token 中，自己都是比较大的数值，这样就没办法做 per-tensor量化，所以 PrefixQuant的目标就是彻底去除这些massive outlier。

考虑到异常值标记的数量是有限的，并且它们通常出现在输入序列的开头。PrefixQuant 在识别到 Massive Token之后，离线地将它们放在了KVCache的前缀中，防止在推理过程中产生Massive Outlier。

该方法主要源于如下现象：在Transformer模型的输入前加入可学习的"prefix token"或者叫"register token"，这些token就能吸收多余的注意力，也就是说量化激活值或者KV-Cache的时候把这些token去掉，剩下的值就更容易量化了。

PrefixQuant不学习这个"prefix token"，而是将大于设定阈值的outlier token直接存储到KV-Cache前，从而实现更高效的计算。具体来说，给定一个LLM模型，PrefixQuant首先计算N个异常token数，然后在KVCache中选择Top-N个高频离群token作为前缀。这样再结合smooth、rotate、reorder等经典等效操作后，剩下的outlier就可以忽略不计了，这样针对activation就可以用per-tensor量化来替换per-token量化，从而在保证推理精度的前提下提升了推理速度。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221533320-1053437256.jpg)

这个方法的问题在于添加了可学习的"prefix token"或者叫"register token"后，如果不在下游数据集上微调的话，这些token可能会对性能起副作用，所以找个好的前缀是比较重要的。这些token一般是". \\n \[BOS\]"等特殊符号。下图展示了KV缓存中不同模型的前缀令牌。\[BOS\]表示序列开始的特殊标记（例如Llama-2的`<s>`和Llama-3的`|begin of text|`）。请注意，以下'‘’表示空格。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221541838-1255160898.jpg)

### 3.4 效果

我们接下来通过几篇论文来看看量化的效果。

#### 3.4.1 对比实验与分析

论文“A Survey on Efffcient Inference for Large Language Models”对不同场景下的weight-only quantization技术所产生的加速效果进行了分析。模型使用了LLaMA-2-7B和LLaMA-2-13B，并使用AWQ将它们的权重量化至4-bit；GPU卡是NVIDIA A100；部署框架是TensorRT-LLM和LMDeploy。

论文评估了这些推理框架在不同的输入序列上实现的加速，这些序列是批大小和上下文长度不同的。实验结果表明：

*   Weight-only quantization可以在decoding阶段加速，进而实现端到端的加速。这种提升主要源于从高带宽内存（High Bandwidth Memory，HBM）可以更快地加载具有低精度权重张量的量化模型，量化显著减少了内存访问开销。
*   对于prefilling阶段，weight-only quantization可能会增加延迟。这是因为prefilling阶段的瓶颈是计算成本，而不是内存访问开销。因此，只量化没有激活的权重对延迟的影响最小。此外，weight-only quantization需要将低精度权重去量化到FP16，这会导致额外的计算开销，从而减慢prefilling。
*   随着批量大小和输入长度的增加，weight-only quantization的加速程度逐渐减小。这主要是因为，对于更大的批处理大小和输入长度，计算成本构成了更大比例的延迟。虽然weight-only quantization主要降低了内存访问成本，但随着批量大小和输入长度增大，计算需求变得更加突出，它对延迟的影响变得不那么显著。
*   由于内存访问开销与模型的参数量规模相关，weight-only quantization为参数规模较大的模型提供了更大的好处。因为随着模型的复杂度与尺寸的增长，存储和访问权重所需的内存量也会成比例地增加。通过量化模型权重，weight-only quantization可以有效地减少内存占用和内存访问开销。

![](https://img2024.cnblogs.com/blog/1850883/202505/1850883-20250529221553889-680123560.jpg)

#### 3.4.2 量化准确度

尽管 LLM 量化在推理加速方面非常流行，但各种量化格式的准确性-性能之间的权衡仍然存在很大的不确定性。论文"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization"对量化准确度进行了全面的研究，评估了整个 LLaMA-3.1 系列模型在学术基准和现实任务中的常见量化格式（FP8、INT8、INT4）。此外，作者还研究考察了量化模型与未量化模型生成的文本之间的差异。作者的实验涵盖了 500,000 多次单独评估，主要评估了 3 种量化类型：

*   W8A8-FP：所有线性层的权重和激活都是 FP8 格式。
    
*   W8A8-INT8：Transformer Block 中所有线性层的权重和激活都使用 INT8 格式。
    
    *   对于权重：采用对称式 Per Channel 的 GPTQ 量化方法。
    *   对于激活：采用 Per Token 的动态量化技术。
*   W4A16-INT4：Transformer Block 中所有线性层权重被量化为 INT4，而激活值则保持在16 位精度。权重通过 GPTQ 量化进行压缩，应用于每组 128 个连续元素，并采用均方误差（MSE）最优的裁剪因子。
    

并得出了几个关键发现：

*   FP8 权重和激活量化（W8A8-FP）在所有模型上基本都是无损的。
    
*   INT8 权重和激活量化（W8A8-INT）在适当调整后，准确度下降幅度很低，仅为 1%-3%。
    
*   INT4 权重量化（W4A16-INT4）与 W8A8-INT 不相上下。
    

为了解决在给定环境的“最佳”格式问题，作者使用流行的开源 vLLM 框架在各种 GPU 上进行推理分析，发现 W4A16 适合 Latency 敏感场景（Synchronous Inference）以及中端 GPU 上的 Throughout 敏感场景（Asynchronous Inference）。同时，W8A8 很适合高端 GPU 上的 Throughout 敏感场景。

#### 3.4.3 QAT和PTQ

论文 A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B 对量化技术在指令调优大语言模型（LLMs）中的表现进行了全面评估。这篇文章回应了一个关键问题：在资源受限的环境中，如何通过量化技术有效部署超大规模LLM。

论文分析了两大主流量化方法——量化感知训练（QAT）和训练后量化（PTQ）。虽然PTQ能够显著减少内存和计算需求，但可能会牺牲模型的精度。为此，研究者使用13个不同的基准数据集，对量化后的模型在各类复杂任务上的表现进行了测试，特别关注模型的推理、数学和知识能力。 通过实验，论文发现了几个关键结论：

*   量化后的模型通常表现优于更小的模型，尤其是在非真值检测和指令跟随任务上。例如，4-bit量化后的Llama-2-13B在多数任务上超过了原始的Llama-2-7B。
*   不同量化方法和模型规模对性能有不同的影响。权重量化方法通常保持更高的精度，其中AWQ比GPTQ表现更佳。
*   量化不会显著影响任务难度与准确性之间的关系，无论任务是简单还是复杂，量化后的模型基本能够保持与全精度模型相似的性能。
*   量化方法在不同模型上的表现差异明显，例如，AWQ在大多数测试中都优于GPTQ。

这些结果对未来LLM的压缩与优化具有重要意义。论文指出，虽然量化能有效压缩模型并提升性能，但并不是所有任务都能从中受益。在特定任务上，比如指令跟随和复杂推理，部分高精度模型仍然有优势。这篇论文为未来的量化研究提供了强大的实验基础，也为大模型的实际部署提出了重要参考。

0xFF 参考
-------

\[EMNLP2023\] \[W8A8\] [Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling](https://link.zhihu.com/?target=https%3A//aclanthology.org/2023.emnlp-main.102/)

[NeurIPS 2024 Oral：用 DuQuant 实现 SOTA 4bit 量化](https://zhuanlan.zhihu.com/p/4390995615) [青稞](https://www.zhihu.com/people/yue-shang-66-69)

[压缩神经网络的艺术： MIT 韩松教授的两篇经典论文解析](https://zhuanlan.zhihu.com/p/1897239667784541890) [Yixin](https://www.zhihu.com/people/hola-98-83)

[从Training Dynamics到Outlier——LLM模型训练过程中的数值特性分析](https://zhuanlan.zhihu.com/p/29160636368) [Reiase](https://www.zhihu.com/people/reiase)

[【读点论文】A Survey of Quantization Methods for Efficient Neural Network Inference](https://blog.csdn.net/weixin_43424450/article/details/134815744) [羞儿](https://blog.csdn.net/weixin_43424450)

[A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/pdf/2103.13630)

[https://www.armcvai.cn/2023-03-05/model-quantization.html](https://www.armcvai.cn/2023-03-05/model-quantization.html)

[模型量化技术综述：揭示大型语言模型压缩的前沿技术](https://mp.weixin.qq.com/s?__biz=MzU5OTM2NjYwNg==&mid=2247508744&idx=1&sn=5269e30963c74c95c82c9c6b55643a34&chksm=ffb45c05d935ca03946acd67d2459f237abe9370ede9503a719b98874b0f2cec0490d7edd5e4&mpshare=1&scene=1&srcid=1001UpB3lHtNCC6Fg1gYat9z&sharer_shareinfo=742a64104ffd1d13cd146d2cbeae6441&sharer_shareinfo_first=742a64104ffd1d13cd146d2cbeae6441#rd) \[DeepHub IMBA\](javascript:void(0)😉

[大模型性能优化（一）：量化从半精度开始讲，弄懂fp32、fp16、bf16](https://zhuanlan.zhihu.com/p/667163603)

[便捷的post training quantization方案: GPTQ](https://zhuanlan.zhihu.com/p/668968587)

[【AI不惑境】模型量化技术原理及其发展现状和展望](https://zhuanlan.zhihu.com/p/141641433) [龙鹏-笔名言有三](https://www.zhihu.com/people/long-peng-11)

[AWQ, Activation-aware Weight Quantization](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2306.00978)

[LLM.int8()](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2208.07339)

[SqueezeLLM](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2306.07629)

[SmoothQuant](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2211.10438)

[GPTQ](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2210.17323)

[大模型量化感知训练开山之作：LLM-QAT](https://zhuanlan.zhihu.com/p/647589650) [吃果冻不吐果冻皮](https://www.zhihu.com/people/liguodong-iot)

ZeroQuant系列([v1](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2206.01861), [v2](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2303.08302))

[LLM-QAT: Data-Free Quantization Aware Training for Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.17888)

[https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/LLM-QAT](https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/LLM-QAT))

[关于大模型推理的量化算法总结](https://zhuanlan.zhihu.com/p/645308698) [孙培钦](https://www.zhihu.com/people/sun-pei-qin)

[、王文广万字长文揭秘大模型量化的GPTQ方法：从OBS经OBQ到GPTQ，海森矩阵的魔力](http://mp.weixin.qq.com/s?__biz=MzA3MDE0NzEzNA==&mid=2647615252&idx=1&sn=59cf88d4876477e36bad861e84510fa2&chksm=86fa8a45b18d0353793c65314a83af2452165cf537bb4d8a30a97cdc5589c24e6b1044d9abc0&scene=21#wechat_redirect)

[王文广万字长文揭秘大模型量化技术：探究原理，理解大模型高效推理最重要的技术](http://mp.weixin.qq.com/s?__biz=MzA3MDE0NzEzNA==&mid=2647615251&idx=1&sn=e4f9a5f46ac9b292aa30fcf6bbf7168b&chksm=86fa8a42b18d035487cdbf0e5da137b8ddd3d8bd0694aa1384f7fe0d3b34497b0b534eeeb33a&scene=21#wechat_redirect)

[akaihaoshuai：从0开始实现LLM：6、模型量化理论+代码实战（LLM-QAT/GPTQ/BitNet 1.58Bits/OneBit）](https://zhuanlan.zhihu.com/p/686161543)  
[akaihaoshuai：从0开始实现LLM：6.1、模型量化（AWQ/SqueezeLLM/Marlin）](https://zhuanlan.zhihu.com/p/697992170)

[https://zhuanlan.zhihu.com/p/703513611](https://zhuanlan.zhihu.com/p/703513611)

[AI大模型高效推理的技术综述！](https://mp.weixin.qq.com/s?__biz=Mzk0MDY2ODM3NQ==&mid=2247485051&idx=1&sn=216de9046f5ef8aa75e09b4ff05b004d&chksm=c3082967b5284c3243504ac49d8beb30223af889a546f170a7a333c226ceed8f26f159c6bbc3&mpshare=1&scene=1&srcid=08201fwmKPZjtuslUNA2e6Wf&sharer_shareinfo=fabc639e7bc12dea5f97923d82b787fc&sharer_shareinfo_first=fabc639e7bc12dea5f97923d82b787fc#rd) 花哥 \[AI大模型前沿\](javascript:void(0)😉

Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Braverman, V., Beidi Chen, & Hu, X. (2023). KIVI : Plug-and-play 2bit KV Cache Quantization with Streaming Asymmetric Quantization.

Databricks 博文: LLM Inference Performance Engineering: Best Practices

Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, & Amir Gholami. (2024). KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization.

T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, (2022). LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.

A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, (2021). A Survey of Quantization Methods for Efficient Neural Network Inference.

KIVI: A Tuning-Free Asymmetric 2bit Quantization for kv Cache :[https://arxiv.org/abs/2402.02750](https://arxiv.org/abs/2402.02750)

Xiao, Guangxuan, et al. “Smoothquant: Accurate and efficient post-training quantization for large language models.” International Conference on Machine Learning. PMLR, 2023.

\[2\] Ashkboos, Saleh, et al. "Quarot: Outlier-free 4-bit inference in rotated llms." arXiv preprint arXiv:2404.00456 (2024).

Sun, Mingjie, et al. "Massive Activations in Large Language Models." arXiv preprint arXiv:2402.17762 (2024).

Liu, Ruikang, et al. "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact."arXiv preprint arXiv:2403.01241(2024).

\[5\] Liu, Zechun, et al. "SpinQuant--LLM quantization with learned rotations."arXiv preprint arXiv:2405.16406(2024).

[A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B](https://arxiv.org/abs/2409.11055)

\[2411.02355\] "Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization \[1\]

Integer Quantization for Deep Learning Inference Principles and Empirical Evaluation

[深度学习Int8的部署推理原理和经验验证](https://cloud.tencent.com/developer/article/2011179)

Quantization and training of neural networks for effificient integer-arithmetic-only inference

Quantizing deep convolutional networks for effificient inference: A whitepaper

Discovering low-precision networks close to full-precision networks for effificient embedded inference

Pact: Parameterized clipping activation for quantized neural networks

[TensorFlow 模型优化：模型量化-张益新](https://mp.weixin.qq.com/s/9QeVVESP3_rBZ6n_D96lwg)

[干货：深度学习模型量化（低精度推理）大总结](https://mp.weixin.qq.com/s/LR3Z2rlkxdl-1KnsU734VQ)

[The Super Weight in Large Language Models](https://arxiv.org/pdf/2411.07191)

[2w字解析量化技术，全网最全的大模型量化技术解析](https://mp.weixin.qq.com/s?__biz=MzkxMDcwMDExOQ==&mid=2247486415&idx=1&sn=6a5e2c0e57affb38e805e08e260f3d1f&chksm=c0b8d334324592e362ea430f3c006a9a8250eb449f11b2e5befe16e959479c975a6ef3119c6f&mpshare=1&scene=1&srcid=0402qLuEDEMvk5dP8IOFW1uE&sharer_shareinfo=bff637265e71d5baace2ad054da2b33d&sharer_shareinfo_first=bff637265e71d5baace2ad054da2b33d#rd) 柏企阅文

[白话版Scaling Laws for Precision 解读](https://zhuanlan.zhihu.com/p/6848989432) 贾斯丁不姓丁

[Efficient Deep Learning-学习笔记-4-Model Quantization](https://zhuanlan.zhihu.com/p/679631413) [回溯的猫](https://www.zhihu.com/people/hui-su-de-mao)

[LLMs量化系列|LLMs Quantization Need What ?](https://zhuanlan.zhihu.com/p/703573964) [回溯的猫](https://www.zhihu.com/people/hui-su-de-mao)

[LLMs量化系列|MiLo：如何利用LoRA补偿MoE模型的量化损失](https://zhuanlan.zhihu.com/p/28205691314) [回溯的猫](https://www.zhihu.com/people/hui-su-de-mao)

[LLMs量化系列|LLM量化方法小结](https://zhuanlan.zhihu.com/p/12568191406) [回溯的猫](https://www.zhihu.com/people/hui-su-de-mao)

[我做模型量化的那些年](https://zhuanlan.zhihu.com/p/680567656) [星月野](https://www.zhihu.com/people/xing-yue-ye-95-16)

[\[LLM量化系列\] VQ之路：从AQLM、GPTVQ到VPTQ](https://zhuanlan.zhihu.com/p/15335162833) [进击的Killua](https://www.zhihu.com/people/zeroine-68)

[\[LLM量化系列\] PTQ量化经典研究解析](https://zhuanlan.zhihu.com/p/695267503) [进击的Killua](https://www.zhihu.com/people/zeroine-68)

[AttnSink相关论文分享](https://zhuanlan.zhihu.com/p/1893057243601156073) [金琴](https://www.zhihu.com/people/jie-xi-31-98)

[decoupleQ 2bit 量化技术介绍](https://zhuanlan.zhihu.com/p/23967788151) 行云流水

[最新发现：大规模值，注意力机制的关键密码。ICML2025](https://mp.weixin.qq.com/s?__biz=Mzg4MzYxODkzMg==&mid=2247499791&idx=1&sn=91ae8f8a5f24602bd3d34c718bf2c350&chksm=ce2e7b17d1a1f52d291969ada35a4ff5f890920aa624529a4e086a45b7c195a86668f86cee0a&mpshare=1&scene=1&srcid=0506HfTigj57QDlRhMkfIS4p&sharer_shareinfo=c3e51ea3de9eb6e299e60a06af0c1e03&sharer_shareinfo_first=c3e51ea3de9eb6e299e60a06af0c1e03#rd) AI修猫Prompt

[Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/pdf/2502.01563)

[ICML25研究发现：RoPE又立大功了！](https://www.xiaoqiedun.com/posts/2025-05-11-massive-values-in-self-attention/) 一只小茄墩

\[ICML2024\] [StableMask: Refining Causal Masking in Decoder-only Transformer](https://link.zhihu.com/?target=https%3A//openreview.net/forum%3Fid%3DGFfWzAReAc)

[Massive Activation in LLM](https://zhuanlan.zhihu.com/p/8440004243) [菜鸟脱贫户](https://www.zhihu.com/people/pu-ti-zi-77-2)

[Transformers need glasses! Information over-squashing in language tasks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2406.04267v1)

[大模型量化压缩是条歧路](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/mPl0ByKSceh4zGGBG3B9lw)

[读论文【大语言模型量化】Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models](https://zhuanlan.zhihu.com/p/672739196) [Mr.Liu](https://www.zhihu.com/people/liu-xiao-yu-12-48)

[massive activation](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.17762)

Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing

[Massive Activations in Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.17762)

[A Survey on Model Compression for Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2308.07633)

\[ICLR2024\] \[W4A4\] [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](https://link.zhihu.com/?target=https%3A//openreview.net/forum%3Fid%3D8Wuvhh0LYW)

\[进击的Killua：[LLM量化系列\] VQ之路：从AQLM、GPTVQ到VPTQ41 赞同 · 2 评论文章](https://zhuanlan.zhihu.com/p/15335162833)

\[MLSys2024\] [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models.](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.16795v1)

[VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2409.17066)

[大模型低比特量化新思路 AQLM](https://zhuanlan.zhihu.com/p/706103629) [认输你就真了](https://www.zhihu.com/people/mrdoghead)

[狂降90%！参数压缩还能这样玩？VQ量化三大奇招，模型瘦身新思路](https://zhuanlan.zhihu.com/p/23069567467) [深蓝学院](https://www.zhihu.com/people/shen-lan-xue-yuan)

[VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models](https://arxiv.org/pdf/2409.17066)

PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs

[PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs](https://zhuanlan.zhihu.com/p/1160626143) [锤炼小助手](https://www.zhihu.com/people/user-of-zhihu)

[旋转矩阵在LLM量化中的使用](https://zhuanlan.zhihu.com/p/1908961205546095160) [西西嘛呦](https://www.zhihu.com/people/gong-ou-bo)