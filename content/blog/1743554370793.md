---
layout: post
title: '小了 60,500 倍，但更强；AI 的“深度诅咒”'
date: "2025-04-02T00:39:30Z"
---
小了 60,500 倍，但更强；AI 的“深度诅咒”
==========================

作者：Ignacio de Gregorio

![](https://img2024.cnblogs.com/blog/3524016/202504/3524016-20250401192042932-2119149514.jpg)

                                          图片来自 Unsplash 的 Bahnijit Barman
    

几周前，我们看到 Anthropic 尝试训练 Claude 去通关宝可梦。模型是有点进展，但离真正通关还差得远。

但现在，一个独立的小团队用一个只有一千万参数的模型通关了宝可梦，比主流前沿 AI 模型小了几千倍。

举个例子，它比最先进的 DeepSeek V3 模型小了 60,500 倍。

但这怎么可能呢？这么小的模型怎么能比所谓的“前沿模型”表现得还好？难道 AI 实验室的钱都打水漂了吗？

答案是：深度诅咒。这是一个很有意思的现象，任何想了解 AI 中最反直觉的问题之一，以及业界打造 AGI 最靠谱路径的人，都值得看看。

训练 AI 处理长线任务

这个小模型是通过强化学习算法（Reinforcement Learning，简称 RL）训练出来的。就像我以前说过很多次的，这个技术就是给 AI 一个目标和一些约束条件，让它学会一套能达成目标的行动策略。

但这次我想聚焦在三件事上：

为什么 RL 和过去两年我们用大语言模型（LLM）做的事不一样，

为什么它对于推动 AI 到新高度至关重要，

还有，为什么这么小的模型能打败大块头们？

我们来深入看看。

从模仿到探索

如果我们看看 AI 的最前沿，有两种主要的训练范式：模仿学习和探索学习。

模仿学习顾名思义就是让模型模仿它的训练数据。通过这种模仿，模型能识别出数据中的底层模式，然后学着去模仿它们。

在 LLM 的情况下，这个训练过程叫做“预训练”，模型会被暴露在互联网级别的大数据集上，它要学会如何模仿这些内容（当然我们也会加些小技巧，让模型在推理阶段生成相似的内容，而不是一模一样的句子；不然它就只是个数据库了）。

模仿学习在让 AI 行为像人方面非常优秀，而且在我们手上有大量可供模仿的数据时，是最佳选择。

但它也会促进记忆式的训练（说到底，就是让模型模仿数据嘛），这也解释了为什么 LLM 的表现主要依赖于它们的记忆能力，而不是真正的智能。

也就是说，模仿学习终究是有上限的。因为有很多应用场景，我们希望 AI 能处理的，恰恰卡在两个问题上：

我们没那么多数据给它模仿；

我们也不想让它去“模仿”，尤其是那种背诵式的，而是要它“真正推理”。

说到这，举个最好的例子就是：推理类任务。

AI 推理的科学

首先，推理类数据（就是人类会明确写出他们怎么推理的过程）非常少。再者，前面说过了，我们不希望 AI 是模仿，我们希望它“跳出框框”，或者更准确地说，在记忆不起作用的时候，探索出不同的解决方式。

基本上我在讲的就是为什么普通的非推理类 LLM 在推理任务上很拉胯——它们不是被训练来“推理”的，而是训练来“复读”的，所以它们只能“执行”它们记住的任务，本质上就是死记硬背而不是逻辑思考。

换句话说，有些任务是需要探索的，就像你也不是每道数学题都能一眼解出来。但是你有那个直觉——数学的“先验知识”——你能通过尝试去探索直到找到答案。

所以最近我们就把一个探索阶段，也就是 RL 阶段，加到了 LLM 上面，让它们去“探索”。

那这到底是怎么工作的？

理解推理训练

探索训练的基本思路就是让模型输出不同的答案，然后我们在训练时实时给予反馈，看哪个答案好，哪个不好。这样模型就能学会什么行为会带来好结果，什么不会。

你可以把这个训练想象成“热还是冷”游戏：我们告诉模型“热”或者“冷”，这样它就能一步步靠近目标（当然实际比这复杂多了，但基本逻辑就是这样）。在实际操作中，这就变成了大规模的试错游戏。

第一个真正用上探索训练的 LLM 是 DeepSeek R1（可能 o3 更早，但他们后来才承认）。

可以想象，这个方法极大地提高了 AI 在推理任务上的表现，于是我们才有了所谓的“推理模型”，像前面提到的 OpenAI 的 o1/o3 或 DeepSeek 的 R1。

在 LLM 的世界里，这种探索训练让模型发展出了推理技巧，比如反思（模型能反省自己的“想法”）、回溯（模型承认错误并自行纠正）等等。

通俗点说，就是靠“蛮力”试错，模型学会了怎么最有效地解决问题。这也是为什么 DeepSeek 的结果被认为是重大突破。

在 DeepSeek/OpenAI 出现之前，我们所谓的 RL 其实只是“人类反馈的强化学习”（RLHF），就是模型在两个选项中学会挑出更合适的那个，以此符合工程师希望的行为。但这当中没有探索，所以其实说是 RL 有点名不副实。

注意：大多数实验室现在仍然会用 RLHF，但只是作为进入“真正 RL”前的一个阶段。

总结一下，现在这些前沿推理模型的训练流程分两个步骤：

通过模仿学习把知识“塞进”模型，造出一个非推理模型（也就是传统的 LLM）；

然后基于这个“认知基础”（或者说是直觉引擎，毕竟这个模型对问题处理还是有点直觉的），我们跑一轮探索训练，让它靠这些直觉去探索、去学会推理，最终造出一个推理模型。

如果这样理解更容易，那你可以把“推理”看作：直觉（内置知识和经验）+ 搜索。

换句话说，推理 = 直觉驱动的探索。

说清楚 RL 在现在 AI 世界的重要性之后，我们还没回答这个问题：

一个小得不能再小的纯 RL 模型，怎么能打败用 RL 训练过的、像 Claude 3.7 Sonnet 这样的推理 LLM？

广度 vs 深度

几十年来，AI 一直在“广”与“深”之间拉扯。

LLM 是“广”的代表。它们是超大规模的模型，被喂进各种你能找到的数据，目标是实现泛化，也就是在没见过的数据任务上也能表现不错。

相对的，像 AlphaGo/AlphaZero 或这次的宝可梦模型，就是“深”的代表。它们只用 RL 训练，而且只聚焦在一个任务上。

在“基础模型”出现之前（它们之所以被叫这个名字就是这个原因），AI 一直是“深”的游戏：每个模型只专注一个任务。

而如今，大部分资金都砸在“广”的模型上。为什么？这样做有什么代价？

你大概已经猜到了，答案就是：AGI（通用人工智能）之梦。

主流观点是，超级智能的 AI 应该是通用的。不需要它对每个任务都训练得很深入（这也不现实），但它应该有一套足够好的“先验知识”，能在没训练的任务上也有 decent 的表现。

有趣的是，虽然这个观点没错（也确实有证据，比如 AlphaZero 在多个棋类游戏上都超过了专精模型），但超级 AI 的表现却反着来。

人类历史上所有达到“超人水平”的 AI（就是远远超越人类的）全是单任务模型，比如 AlphaGo（围棋）。

而到目前为止，没有一个“广”的模型，在任何一个任务上做到超人。

这就解释了为什么我们今天讨论的这个宝可梦模型，虽然比 SOTA 小了四个数量级，却轻松打爆它们：

这个模型放弃了“广”，换来了在一个任务上的极致表现。

换句话说，它小巧、灵活，只专注一个任务，所以才能练出这个任务的终极能力，甚至反过来打败“全能型”的选手。它靠的是“开挂式”的探索学习。

这又意味着什么？

不像 LLM，因为太贵，无法让它们跑非常长时间的探索来找最优策略；小模型恰好相反：它可能在多个任务上都拉，但在那个唯一训练过的任务上，它能打出神级表现。

总结一句话，这个小团队之所以能训练出一个能打爆主流模型的宝可梦 AI，就是因为虽然我们知道 RL 很强，但我们还没学会怎么在“基础模型”上正确地跑 RL。

这能不能做到、能不能把 RL 训练应用到大模型上，就是现在所有顶级 AI 实验室都在努力搞清楚的问题。

所以，RL 是答案吗？

这项研究看起来可能有点泄气：

我们 AI 的路是不是走错了？

是不是在大模型上烧钱没意义？

AGI 是不是应该由一堆小的、单任务的模型组成？

我能理解你有这些想法，但我其实恰恰相反地看：

这又一次证明了 RL 是有效的，我们只需要找到方法，把它扩展到更大规模上。

如果我们能在 LLM 的基础上跑出纯 RL，那我们可能就找到了通往新时代 AI 的路：不再是“模仿”智能，而是真正拥有某种程度的“智能”。

这会不会把 AI 推向真正的智能？我们希望如此，但也不能确定。不过这是我们唯一已知的靠谱赌注，那就只能希望它能成功了。

但我们现在搞清楚怎么让这一步发生了吗？没有，那些被吹成“博士水平”的 LLM 连井字棋都玩不好。

总的来说，本文最重要的 takeaway 是：RL，或者说探索学习，依然是唯一一个在某些情况下能做到“超人表现”的方法。

虽然手段不同、技术各异，但从基本原理上看，所有 AI 实验室走的其实是一条路：直觉驱动的搜索。

你只需要知道这一点，就能明白现在前沿 AI 的真相。他们全都在玩同一个游戏。

剩下的，就只是工程和资本分配而已。