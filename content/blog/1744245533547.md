---
layout: post
title: '让 LLM 来评判 | 技巧与提示'
date: "2025-04-10T00:38:53Z"
---
让 LLM 来评判 | 技巧与提示
=================

> 这是 **让 LLM 来评判** 系列文章的第六篇，敬请关注系列文章:
> 
> *   [基础概念](https://www.cnblogs.com/huggingface/p/18666189)
> *   [选择 LLM 评估模型](https://www.cnblogs.com/huggingface/p/18670887)
> *   [设计你自己的评估 prompt](https://www.cnblogs.com/huggingface/p/18741971)
> *   [评估你的评估结果](https://www.cnblogs.com/huggingface/p/18710538)
> *   [奖励模型相关内容](https://www.cnblogs.com/huggingface/p/18715798)
> *   技巧与提示

LLM 评估模型已知偏差及缓解措施:
------------------

*   **缺乏内部一致性**：同一 prompt 输入评估模型执行多次得到的结果可能不一样 (如果温度参数不设为 0)。
    *   缓解措施：遵循 “自我一致性 (self-consistency)” 设置 prompt，输入模型执行多次并保留多数结果
*   **自我偏好**：LLM 评估模型更 [偏好自己的输出模式](https://arxiv.org/abs/2404.13076)，因此会对模式相似的结果评分偏高。
    *   缓解措施：采用陪审团机制
*   **输入扰动不敏感**：评估模型对 [扰动输入](https://arxiv.org/abs/2406.13439) 的辨识效果较差，[难以提供一致的评分范围](https://twitter.com/aparnadhinak/status/1748368364395721128) (更多实验结果可以参考 [这个链接](https://github.com/LeonEricsson/llmjudge/blob/main/README.md))。例如对于施加了相同程度噪声的文本，使用评估模型评估文本质量的评分无法反映噪声的程度。
    *   缓解措施：
        *   要求模型先输出详细的推理过程 [再输出评分](https://twitter.com/seungonekim/status/1749289437165769177)
        *   在 prompt 中添加一致的评分标准
*   **位置偏差**：评估模型更 [偏好特定位置的答案](https://arxiv.org/abs/2306.05685)。例如在成对比较时，Claude 和 GPT3.5 在多次测试中通常会偏好某一个位置，例如第一个或第二个答案。
    *   缓解措施：
        *   随机调整答案位置
        *   计算所有选项的对数概率并归一化
*   **冗长偏好 (长度偏差)**：评估模型更偏好冗长的答案。
    *   缓解措施：[考虑答案中的长度差异](https://arxiv.org/abs/2404.04475)
*   **[难以对齐人类答案](https://arxiv.org/abs/2308.15812)**：
    *   在所有评估中，[人工评估是否可以作为一个不错的基线尚有争议](https://arxiv.org/abs/2202.06935)。例如在某些特定领域 (如医学、法律、数学等)，如果标注员专业性不够，那么得到的结果可能跟直接采用 LLM 一样差。
*   **格式偏差**：如果输入模型的 prompt 格式与其训练数据的格式 [相差甚远](https://arxiv.org/pdf/2310.17631)，可能导致模型的评估结果不准确。例如，成对比较模型的训练集数据格式中提供了参考答案，如果在评估时没有给定参考答案或者给定的参考答案格式有误，那么评估结果就不可信。
    *   缓解措施：仔细遵循评估模型训练集 prompt 格式 (比如指令微调模型的格式)。

选择合适的 LLM 评估任务
--------------

LLM 评估特性：

*   **很难识别幻觉**：尤其是部分幻觉 (与事实非常相近，仅有微小的区别而导致错误)。(可以参考这两篇论文：[链接 1](https://arxiv.org/abs/2305.11747) 和 [链接 2](https://arxiv.org/abs/2303.08896))。
*   **许多任务上与人工评估一致性不高**：如 [总结任务](https://arxiv.org/abs/2304.02554) (也可以参考 [这篇](https://arxiv.org/abs/2303.16634))、[输入遵循忠实度](https://arxiv.org/abs/2307.16877)，更多任务请参考 [这篇论文](https://arxiv.org/abs/2406.18403)。

* * *

> 英文原文: [evaluation-guidebook/contents/model-as-a-judge/tips-and-tricks.md](https://github.com/huggingface/evaluation-guidebook/blob/main/translations/zh/contents/model-as-a-judge/tips-and-tricks.md)
> 
> 原文作者: clefourrier
> 
> 译者: SuSung-boy
> 
> 审校: adeenayakup