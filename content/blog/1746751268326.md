---
layout: post
title: 'DeepSeek 多模态模型 Janus-Pro 本地部署'
date: "2025-05-09T00:41:08Z"
---
DeepSeek 多模态模型 Janus-Pro 本地部署
=============================

一、概述
====

Janus-Pro是DeepSeek最新开源的多模态模型，是一种新颖的自回归框架，统一了多模态理解和生成。通过将视觉编码解耦为独立的路径，同时仍然使用单一的、统一的变压器架构进行处理，该框架解决了先前方法的局限性。这种解耦不仅缓解了视觉编码器在理解和生成中的角色冲突，还增强了框架的灵活性。Janus-Pro 超过了以前的统一模型，并且匹配或超过了特定任务模型的性能。

代码链接：[https://github.com/deepseek-ai/Janus](https://github.com/deepseek-ai/Janus)

模型链接：[https://modelscope.cn/collections/Janus-Pro-0f5e48f6b96047](https://modelscope.cn/collections/Janus-Pro-0f5e48f6b96047)

体验页面：[https://modelscope.cn/studios/AI-ModelScope/Janus-Pro-7B](https://modelscope.cn/studios/AI-ModelScope/Janus-Pro-7B)

二、虚拟环境
======

环境说明
----

本文使用WSL2运行的ubuntu系统来进行演示，参考链接：https://www.cnblogs.com/xiao987334176/p/18864140

创建虚拟环境
------

conda create --name vll-Janus-Pro-7B python=3.12.7

激活虚拟环境，执行命令：

conda activate vll-Janus-Pro-7B

查看CUDA版本，执行命令：

\# nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005\-2025 NVIDIA Corporation
Built on Wed\_Jan\_15\_19:20:09\_PST\_2025
Cuda compilation tools, release 12.8, V12.8.61
Build cuda\_12.8.r12.8/compiler.35404655\_0

三、安装Janus-Pro
=============

创建项目目录

mkdir vllm
cd vllm

克隆代码

git clone https://github.com/deepseek-ai/Janus

安装依赖包，注意：这里要手动安装pytorch，指定版本。

pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

安装其他依赖组件

pip3 install transformers attrdict einops timm

下载模型

可以用modelscope下载，安装modelscope，命令如下：

pip install modelscope

modelscope download \--model deepseek-ai/Janus-Pro-7B

效果如下：

\# modelscope download --model deepseek-ai/Janus-Pro-7B
Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B
Downloading \[config.json\]: 100%|███████████████████████████████████████████████████| 1.42k/1.42k \[00:00<00:00, 5.29kB/s\]
Downloading \[configuration.json\]: 100%|████████████████████████████████████████████████| 68.0/68.0 \[00:00<00:00, 221B/s\]
Downloading \[README.md\]: 100%|█████████████████████████████████████████████████████| 2.49k/2.49k \[00:00<00:00, 7.20kB/s\]
Downloading \[processor\_config.json\]: 100%|███████████████████████████████████████████████| 210/210 \[00:00<00:00, 590B/s\]
Downloading \[janus\_pro\_teaser1.png\]: 100%|██████████████████████████████████████████| 95.7k/95.7k \[00:00<00:00, 267kB/s\]
Downloading \[preprocessor\_config.json\]: 100%|████████████████████████████████████████████| 346/346 \[00:00<00:00, 867B/s\]
Downloading \[janus\_pro\_teaser2.png\]: 100%|███████████████████████████████████████████| 518k/518k \[00:00<00:00, 1.18MB/s\]
Downloading \[special\_tokens\_map.json\]: 100%|███████████████████████████████████████████| 344/344 \[00:00<00:00, 1.50kB/s\]
Downloading \[tokenizer\_config.json\]: 100%|███████████████████████████████████████████████| 285/285 \[00:00<00:00, 926B/s\]
Downloading \[pytorch\_model.bin\]:   0%|▏                                            | 16.0M/3.89G \[00:00<03:55, 17.7MB/s\]
Downloading \[tokenizer.json\]: 100%|████████████████████████████████████████████████| 4.50M/4.50M \[00:00<00:00, 6.55MB/s\]
Processing 11 items:  91%|█████████████████████████████████████████████████████▋     | 10.0/11.0 \[00:19<00:00, 14.1it/s\]  
Downloading \[pytorch\_model.bin\]: 100%|█████████████████████████████████████████████| 3.89G/3.89G \[09:18<00:00, 7.48MB/s\]
Processing 11 items: 100%|███████████████████████████████████████████████████████████| 11.0/11.0 \[09:24<00:00, 51.3s/it\]

可以看到下载目录为/root/.cache/modelscope/hub/models/deepseek-ai/Janus-Pro-7B

把下载的模型移动到vllm目录里面

mv /root/.cache/modelscope/hub/models/deepseek-ai /home/xiao/vllm

四、测试图片理解
========

vllm目录有2个文件夹，结构如下：

\# ll
total 20
drwxr\-xr-x 4 root root 4096 May  8 18:59 ./
drwxr\-x--- 5 xiao xiao 4096 May  8 14:50 ../
drwxr\-xr-x 8 root root 4096 May  8 18:59 Janus/
drwxr\-xr-x 4 root root 4096 May  8 16:01 deepseek-ai/

进入deepseek-ai目录，会看到一个文件夹Janus-Pro-7B

\# ll
total 16
drwxr\-xr-x 4 root root 4096 May  8 16:01 ./
drwxr\-xr-x 4 root root 4096 May  8 18:59 ../
drwxr\-xr-x 2 root root 4096 May  7 18:32 Janus-Pro-7B/

返回上一级，在Janus目录，创建image\_understanding.py文件，代码如下：

import torch
from transformers import AutoModelForCausalLM
from janus.models import MultiModalityCausalLM, VLChatProcessor
from janus.utils.io import load\_pil\_images

model\_path = "../deepseek-ai/Janus-Pro-7B"

image='aa.jpeg'
question='请说明一下这张图片'
vl\_chat\_processor: VLChatProcessor = VLChatProcessor.from\_pretrained(model\_path)
tokenizer = vl\_chat\_processor.tokenizer

vl\_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from\_pretrained(
    model\_path, trust\_remote\_code=True
)
vl\_gpt = vl\_gpt.to(torch.bfloat16).cuda().eval()

conversation = \[
    {
        "role": "<|User|>",
        "content": f"<image\_placeholder>\\n{question}",
        "images": \[image\],
    },
    {"role": "<|Assistant|>", "content": ""},
\]

# load images and prepare for inputs
pil\_images = load\_pil\_images(conversation)
prepare\_inputs = vl\_chat\_processor(
    conversations=conversation, images=pil\_images, force\_batchify=True
).to(vl\_gpt.device)

# # run image encoder to get the image embeddings
inputs\_embeds = vl\_gpt.prepare\_inputs\_embeds(\*\*prepare\_inputs)

# # run the model to get the response
outputs = vl\_gpt.language\_model.generate(
    inputs\_embeds=inputs\_embeds,
    attention\_mask=prepare\_inputs.attention\_mask,
    pad\_token\_id=tokenizer.eos\_token\_id,
    bos\_token\_id=tokenizer.bos\_token\_id,
    eos\_token\_id=tokenizer.eos\_token\_id,
    max\_new\_tokens=512,
    do\_sample=False,
    use\_cache=True,
)

answer = tokenizer.decode(outputs\[0\].cpu().tolist(), skip\_special\_tokens=True)
print(f"{prepare\_inputs\['sft\_format'\]\[0\]}", answer)

下载一张图片，地址：https://pics6.baidu.com/feed/09fa513d269759ee74c8d049640fcc1b6f22df9e.jpeg

![](https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508192923746-2079946853.jpg)

将此图片，重命名为aa.jpeg，存放在Janus目录

最终Janus目录，文件如下：

\# ll
total 2976
drwxr-xr-x 8 root root    4096 May  8 18:59 ./
drwxr-xr-x 4 root root    4096 May  8 18:59 ../
drwxr-xr-x 8 root root    4096 May  7 18:11 .git/
-rw-r--r-- 1 root root     115 May  7 18:11 .gitattributes
-rw-r--r-- 1 root root    7301 May  7 18:11 .gitignore
-rw-r--r-- 1 root root    1065 May  7 18:11 LICENSE-CODE
-rw-r--r-- 1 root root   13718 May  7 18:11 LICENSE-MODEL
-rw-r--r-- 1 root root    3069 May  7 18:11 Makefile
-rwxr-xr-x 1 root root   26781 May  7 18:11 README.md\*
-rw-r--r-- 1 root root   62816 May  8 14:59 aa.jpeg
drwxr-xr-x 2 root root    4096 May  7 18:11 demo/
drwxr-xr-x 2 root root    4096 May  8 17:19 generated\_samples/
-rw-r--r-- 1 root root    4515 May  7 18:11 generation\_inference.py
-rw-r--r-- 1 xiao xiao    4066 May  8 18:50 image\_generation.py
-rw-r--r-- 1 root root    1594 May  8 18:58 image\_understanding.py
drwxr-xr-x 2 root root    4096 May  7 18:11 images/
-rw-r--r-- 1 root root    2642 May  7 18:11 inference.py
-rw-r--r-- 1 root root    5188 May  7 18:11 interactivechat.py
drwxr-xr-x 6 root root    4096 May  7 19:01 janus/
drwxr-xr-x 2 root root    4096 May  7 18:11 janus.egg-info/
-rw-r--r-- 1 root root 2846268 May  7 18:11 janus\_pro\_tech\_report.pdf
-rw-r--r-- 1 root root    1111 May  7 18:11 pyproject.toml
-rw-r--r-- 1 root root     278 May  7 18:11 requirements.txt

运行代码，效果如下：

\# python image\_understanding.py
Python version is above 3.10, patching the collections module.
/root/anaconda3/envs/vll-Janus-Pro-7B/lib/python3.12/site-packages/transformers/models/auto/image\_processing\_auto.py:604: FutureWarning: The image\_processor\_class argument is deprecated and will be removed in v4.42. Please use \`slow\_image\_processor\_class\`, or \`fast\_image\_processor\_class\` instead
  warnings.warn(
Using a slow image processor as \`use\_fast\` is unset and a slow processor was saved with this model. \`use\_fast\=True\` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with \`use\_fast=False\`.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization\_llama\_fast.LlamaTokenizerFast'\>. This is expected, and simply means that the \`legacy\` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set \`legacy=False\`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 \[00:10<00:00,  5.18s/it\]
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.

<|User|>: <image\_placeholder>
请说明一下这张图片

<|Assistant|>: 这张图片展示了一位身穿传统服饰的女性，她正坐在户外，双手合十，闭着眼睛，似乎在进行冥想或祈祷。背景是绿色的树木和植物，阳光透过树叶洒在她的身上，营造出一种宁静、祥和的氛围。她的服装以淡雅的白色和粉色为主，带有精致的花纹，整体风格非常优雅。

描述还是比较准确的

五、测试图片生成
========

在Janus目录，新建image\_generation.py脚本，代码如下：

import os
import torch
import numpy as np
from PIL import Image
from transformers import AutoModelForCausalLM
from janus.models import MultiModalityCausalLM, VLChatProcessor

model\_path \= "../deepseek-ai/Janus-Pro-7B"
vl\_chat\_processor: VLChatProcessor \= VLChatProcessor.from\_pretrained(model\_path)
tokenizer \= vl\_chat\_processor.tokenizer

vl\_gpt: MultiModalityCausalLM \= AutoModelForCausalLM.from\_pretrained(
    model\_path, trust\_remote\_code\=True
)
vl\_gpt \= vl\_gpt.to(torch.bfloat16).cuda().eval()

conversation \= \[
    {"role": "<|User|>", "content": "超写实8K渲染，一位具有东方古典美的中国女性，瓜子脸，西昌的眉毛如弯弯的月牙，双眼明亮而深邃，犹如夜空中闪烁的星星。高挺的鼻梁，樱桃小嘴微微上扬，透露出一丝诱人的微笑。她的头发如黑色的瀑布般垂直落在减胖两侧，微风轻轻浮动发色。肌肤白皙如雪，在阳光下泛着微微的光泽。她身着乙烯白色的透薄如纱的连衣裙，裙摆在海风中轻轻飘动。"},
    {"role": "<|Assistant|>", "content": ""},
\]

sft\_format \= vl\_chat\_processor.apply\_sft\_template\_for\_multi\_turn\_prompts(
    conversations\=conversation,
    sft\_format\=vl\_chat\_processor.sft\_format,
    system\_prompt\=""
)
prompt \= sft\_format + vl\_chat\_processor.image\_start\_tag

@torch.inference\_mode()
def generate(
        mmgpt: MultiModalityCausalLM,
        vl\_chat\_processor: VLChatProcessor,
        prompt: str,
        temperature: float \= 1,
        parallel\_size: int \= 1, # 减小 parallel\_size
        cfg\_weight: float = 5,
        image\_token\_num\_per\_image: int \= 576,
        img\_size: int \= 384,
        patch\_size: int \= 16,
):
    input\_ids \= vl\_chat\_processor.tokenizer.encode(prompt)
    input\_ids \= torch.LongTensor(input\_ids)

    tokens \= torch.zeros((parallel\_size \* 2, len(input\_ids)), dtype=torch.int).cuda()
    for i in range(parallel\_size \* 2):
        tokens\[i, :\] \= input\_ids
        if i % 2 != 0:
            tokens\[i, 1:-1\] = vl\_chat\_processor.pad\_id

    inputs\_embeds \= mmgpt.language\_model.get\_input\_embeddings()(tokens)

    generated\_tokens \= torch.zeros((parallel\_size, image\_token\_num\_per\_image), dtype=torch.int).cuda()

    for i in range(image\_token\_num\_per\_image):
        outputs \= mmgpt.language\_model.model(inputs\_embeds=inputs\_embeds, use\_cache=True,
                                             past\_key\_values\=outputs.past\_key\_values if i != 0 else None)
        hidden\_states \= outputs.last\_hidden\_state

        logits \= mmgpt.gen\_head(hidden\_states\[:, -1, :\])
        logit\_cond \= logits\[0::2, :\]
        logit\_uncond \= logits\[1::2, :\]

        logits \= logit\_uncond + cfg\_weight \* (logit\_cond - logit\_uncond)
        probs \= torch.softmax(logits / temperature, dim=-1)

        next\_token \= torch.multinomial(probs, num\_samples=1)
        generated\_tokens\[:, i\] \= next\_token.squeeze(dim=-1)
        next\_token \= torch.cat(\[next\_token.unsqueeze(dim=1),
                                next\_token.unsqueeze(dim\=1)\], dim=1).view(-1)
        img\_embeds \= mmgpt.prepare\_gen\_img\_embeds(next\_token)
        inputs\_embeds \= img\_embeds.unsqueeze(dim=1)
        # 添加显存清理
        del logits, logit\_cond, logit\_uncond, probs
        torch.cuda.empty\_cache()

    dec \= mmgpt.gen\_vision\_model.decode\_code(generated\_tokens.to(dtype=torch.int),
                                             shape\=\[parallel\_size, 8, img\_size // patch\_size, img\_size // patch\_size\])
    dec \= dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)

    dec \= np.clip((dec + 1) / 2 \* 255, 0, 255)

    visual\_img \= np.zeros((parallel\_size, img\_size, img\_size, 3), dtype=np.uint8)
    visual\_img\[:, :, :\] \= dec

    os.makedirs('generated\_samples', exist\_ok=True)
    for i in range(parallel\_size):
        save\_path \= os.path.join('generated\_samples', f"img\_{i}.jpg")
        img \= Image.fromarray(visual\_img\[i\])
        img.save(save\_path)

generate(
    vl\_gpt,
    vl\_chat\_processor,
    prompt,
)

**注意：提示词是可以写中文的，不一定非要是英文。**

**代码在默认的基础上做了优化，否则运行会导致英伟达5080显卡直接卡死。**

运行代码，效果如下：

# python image\_generation.py
Python version is above 3.10, patching the collections module.
/root/anaconda3/envs/vll-Janus-Pro-7B/lib/python3.12/site-packages/transformers/models/auto/image\_processing\_auto.py:604: FutureWarning: The image\_processor\_class argument is deprecated and will be removed in v4.42. Please use \`slow\_image\_processor\_class\`, or \`fast\_image\_processor\_class\` instead
  warnings.warn(
Using a slow image processor as \`use\_fast\` is unset and a slow processor was saved with this model. \`use\_fast=True\` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with \`use\_fast=False\`.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization\_llama\_fast.LlamaTokenizerFast'\>. This is expected, and simply means that the \`legacy\` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set \`legacy=False\`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 1/1 \[00:09<00:00,  4.58s/it\]

注意观察一下GPU使用情况，这里会很高。

![](https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508191414571-1353256890.png)

 RTX 5080显卡，16GB显存，几乎已经占满了。

等待30秒左右，就会生成一张图片。

打开小企鹅

![](https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508192208441-1598612409.png)

进入目录\\home\\xiao\\vllm\\Janus\\generated\_samples

这里会出现一张图片

![](https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508191630711-1160295926.png)

 打开图片，效果如下：

![](https://img2024.cnblogs.com/blog/1341090/202505/1341090-20250508192331575-235520069.jpg)

效果还算可以，距离真正的8k画质，还是有点差距的。

**注意提示词，尽量丰富一点，生成的图片，才符合要求。**

如果不会写提示词，可以让deepseek帮你写一段提示词。