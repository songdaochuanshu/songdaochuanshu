---
layout: post
title: '彩笔运维勇闯机器学习--最小二乘法的数学推导'
date: "2025-08-23T00:39:21Z"
---
彩笔运维勇闯机器学习--最小二乘法的数学推导
======================

前言
--

今天我们来讨论一下回归算法当中的数学实现。本人数学也是渣，大学时期概率论一直挂到清考才勉强通过，+\_+ !!，如今勇闯机器学习，硬着头皮重新学习了微积分和线代，也是为了记录自己最近的状态，避免过段时间忘记了。描述的时候有不周全的地方，请各位大佬们多担待了

本节将会运用一些数学知识来解释一下相关的回归算法的合理性，虽有些枯燥，但知其然也知其所以然，多了解一些总是好的

最小二乘法
-----

最小二乘法的核心思想是找到一组参数，使得模型预测值与实际观测值之间的误差平方和最小。最小二乘法是回归模型中非常常用的计算回归系数的方法：

\\\[\\text{f} = \\sum\_{i=1}^{n} (y\_i - \\hat{y}\_i)^2 \\\]

其中\\(y\_i\\)是真实值，\\(\\hat{y}\_i\\)是预测值

#### 推导过程

先用最简单的一元线性回归，一元线性回归的数学模型为：

\\\[\\hat{y\_i}=β\_0+β\_1x\_i \\\]

带入公式：

\\\[\\text{f} = \\sum\_{i=1}^{n} (y\_i - (β\_0+β\_1x\_i))^2 = \\sum\_{i=1}^{n} (y\_i - β\_0 - β\_1x\_i)^2 \\\]

由于要讨论的是\\(β\_0\\)和\\(β\_1\\)，这是一个多变量函数，为了研究单独变量，可以分别对其求偏导

\\\[\\frac{\\partial f}{\\partial β\_0} = (\\sum\_{i=1}^{n} (y\_i - β\_0 - β\_1x\_i)^2)' \\\]

首先，有限个数的求和之后的导数=有限个数导数之后求和，把\\((y\_i - β\_0 - β\_1x\_i)^2\\)看成一个整体

\\\[\\frac{\\partial f}{\\partial β\_0} = \\sum\_{i=1}^{n} ((y\_i - β\_0 - β\_1x\_i)^2)' \\\]

这是复合函数求导，那就来个剥洋葱法则，先对平方求导，再对加法求导

\\\[\\frac{\\partial f}{\\partial β\_0} = \\sum\_{i=1}^{n} 2(y\_i - β\_0 - β\_1x\_i)⋅(y\_i - β\_0 - β\_1x\_i)' \\\]

由于是对\\(β\_0\\)求导，其余可认为是常数，求导为0

\\\[\\frac{\\partial f}{\\partial β\_0} = \\sum\_{i=1}^{n} 2(y\_i - β\_0 - β\_1x\_i) ⋅ -1 =-2\\sum\_{i=1}^{n} β\_0(y\_i - β\_0 - β\_1x\_i) \\\]

导数是函数切线的斜率，要找到函数的最小值，就是其导数为0的地方

\\\[\\frac{\\partial f}{\\partial β\_0}=-2\\sum\_{i=1}^{n} (y\_i - β\_0 - β\_1x\_i)=0 \\\]

整理一下：

\\\[\\sum\_{i=1}^{n} (y\_i - β\_0 - β\_1x\_i)=\\sum\_{i=1}^{n}y\_i - \\sum\_{i=1}^{n}β\_0 - \\sum\_{i=1}^{n}β\_1x\_i=0 \\\]

方程1： $$\\sum\_{i=1}^{n}y\_i = nβ\_0 + β\_1⋅\\sum\_{i=1}^{n}x\_i$$

同理对\\(β\_1\\)求偏导

\\\[\\frac{\\partial f}{\\partial β\_1} = \\sum\_{i=1}^{n} 2(y\_i - β\_0 - β\_1x\_i) ⋅ -x\_i =0 \\\]

整理一下：

\\\[\\sum\_{i=1}^{n} 2(y\_i - β\_0 - β\_1x\_i) ⋅ -x\_i=-2(\\sum\_{i=1}^{n} x\_iy\_i-\\sum\_{i=1}^{n}β\_0x\_i-\\sum\_{i=1}^{n}β\_1x\_i^2)=0 \\\]

方程2：

\\\[\\sum\_{i=1}^{n} x\_iy\_i = β\_0⋅\\sum\_{i=1}^{n}x\_i+β\_1⋅\\sum\_{i=1}^{n}x\_i^2 \\\]

我们将样本数据\\((x\_i, y\_i)\\)求平均值，就是样本均值

\\\[\\bar{x}=\\frac{1}{n}\\sum\_{i=1}^{n}x\_i \\\]

\\\[\\bar{y}=\\frac{1}{n}\\sum\_{i=1}^{n}y\_i \\\]

带入方程1：

\\\[\\bar{y} = β\_0 + β\_1\\bar{x} \\\]

将\\(β\_0\\)带入方程2计算\\(β\_1\\)：

\\\[\\sum\_{i=1}^{n} x\_iy\_i = (\\bar{y} - β\_1\\bar{x})⋅\\sum\_{i=1}^{n}x\_i+β\_1⋅\\sum\_{i=1}^{n}x\_i^2 = n\\bar{x}\\bar{y}-nβ\_1\\bar{x}^2+β\_1⋅\\sum\_{i=1}^{n}x\_i^2 \\\]

\\\[\\sum\_{i=1}^{n} x\_iy\_i - n\\bar{x}\\bar{y} = β\_1(-n\\bar{x}^2+\\sum\_{i=1}^{n}x\_i^2) \\\]

\\\[β\_1=\\frac{\\sum\_{i=1}^{n} x\_iy\_i - n\\bar{x}\\bar{y}}{\\sum\_{i=1}^{n}x\_i^2-n\\bar{x}^2} \\\]

经过漫长的推导：

\\\[β\_1=\\frac{\\sum\_{i=1}^{n} x\_iy\_i - n\\bar{x}\\bar{y}}{\\sum\_{i=1}^{n}x\_i^2-n\\bar{x}^2} \\\]

\\\[β\_0 = \\bar{y} - β\_1\\bar{x} \\\]

#### 小结

通过最小二乘法，一步一步计算出截距与回归系数的公式，这其中用到的数学知识主要有：多元函数求偏导、导数的计算

多元回归下的最小二乘法
-----------

#### 推导过程

多元线性回归的数学模型：

\\\[y = β\_0 + β\_1x\_1 + β\_2x\_2 + \\dots + β\_nx\_n \\\]

相比于一元回归的最小二乘法，多元回归可谓有一点复杂，因为特征数量的增加，带来的样本与特征的快速上升

比如有3个样本，2个特征，记为：\\(y = β\_0 + β\_1x\_1 + β\_2x\_2\\)

\\\[x^{(1)} = \[1,2\] \\\]

\\\[x^{(2)} = \[3,4\] \\\]

\\\[x^{(3)} = \[5,6\] \\\]

用矩阵表达：

\\\[X=\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\\]

假设有m个特征，n个样本

\\\[\\hat{y}\_i = β\_0 + β\_1x\_1^{(1)} + β\_2x\_2^{(1)} + \\dots + β\_nx\_n^{(1)} \\\]

\\\[\\hat{y}\_i = β\_0 + β\_1x\_1^{(2)} + β\_2x\_2^{(2)} + \\dots + β\_nx\_n^{(2)} \\\]

\\\[... \\\]

\\\[\\hat{y}\_i = β\_0 + β\_1x\_1^{(m)} + β\_2x\_2^{(m)} + \\dots + β\_nx\_n^{(m)} \\\]

\\\[X=\\begin{bmatrix} 1 & x\_1^{(1)} & x\_2^{(1)} & \\dots & x\_n^{(1)} \\\\ 1 & x\_1^{(2)} & x\_2^{(2)} & \\dots & x\_n^{(2)} \\\\ ... \\\\ 1 & x\_1^{(m)} & x\_2^{(m)} & \\dots & x\_n^{(m)} \\\\ \\end{bmatrix} \\\]

\\\[β=\\begin{bmatrix} β\_0 \\\\ β\_1 \\\\ ... \\\\ β\_n \\\\ \\end{bmatrix} \\\]

所以通过矩阵的点积，可以将公式改写为，在m个特征，n个样本下：

\\\[\\hat{y}\_i=Xβ \\\]

带入最小二乘法公式：

\\\[\\text{f} = \\sum\_{i=1}^{n} (y\_i - \\hat{y}\_i)^2 = \\sum\_{i=1}^{n} (y\_i - Xβ)^2 = \\| {y\_i} - Xβ \\|\_2 = (y\_i - Xβ)^T(y\_i - Xβ) \\\]

展开矩阵：

\\\[(y\_i - Xβ)^T(y\_i - Xβ) = y\_i^Ty\_i-y\_i^TXβ-X^Tβ^Ty\_i+X^Tβ^TXβ \\\]

由于 \\(y\_i^TXβ\\) 的转置矩阵就是 \\(X^Tβ^Ty\_i\\) ：

\\\[= y\_i^Ty\_i-2X^Tβ^Ty\_i+X^Tβ^TXβ \\\]

为了找到β最小值，先求导然后令导数为0

\\\[\\frac{\\partial f}{\\partial β} = (y\_i^Ty\_i-2X^Tβ^Ty\_i+X^Tβ^TXβ)' = -2X^Ty\_i+2X^TXβ = 0 \\\]

`=>`

\\\[X^Ty\_i=X^TXβ \\\]

两边同时乘以\\(X^TX\\)逆矩阵，换句话说，\\(X^TX\\)是可逆矩阵：

\\\[β=(X^TX)^{-1}X^Ty\_i \\\]

#### 小结

这其中用到的数学知识主要有：导数、矩阵等方面的知识

用MathJax语法写公式真的太费劲了！还不如在纸上手写

联系我
---

*   联系我，做深入的交流

* * *

至此，本文结束  
在下才疏学浅，有撒汤漏水的，请各位不吝赐教...

本文来自博客园，作者：[it排球君](https://www.cnblogs.com/MrVolleyball/)，转载请注明原文链接：[https://www.cnblogs.com/MrVolleyball/p/19052292](https://www.cnblogs.com/MrVolleyball/p/19052292)

本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须在文章页面给出原文连接，否则保留追究法律责任的权利。