---
layout: post
title: '13. LangChain4j + 加入检索增加生成 RAG(知识库)'
date: "2025-09-21T00:43:31Z"
---
13\. LangChain4j + 加入检索增加生成 RAG(知识库)
====================================

13\. LangChain4j + 加入检索增加生成 RAG(知识库)
====================================

@

目录

*   [13\. LangChain4j + 加入检索增加生成 RAG(知识库)](#13-langchain4j--加入检索增加生成-rag知识库)
    *   [RAG 的概念](#rag-的概念)
    *   [LangChain4j RAG 的使用理论](#langchain4j-rag-的使用理论)
    *   [LangChain4j RAG 的实战](#langchain4j-rag-的实战)
*   [最后：](#最后)

RAG 的概念
-------

*   官网：[https://docs.langchain4j.dev/tutorials/rag/](https://docs.langchain4j.dev/tutorials/rag/)

**核心设计理念：** RAG技术就像给AI大模型装上了「实时百科大脑」，为了让大模型获取足够的上下文，以便获得更加广泛的信息源，通过先查资料后回答的机制，让AI摆脱传统模型的"知识遗忘和幻觉回复"困境。

**一句话简单的说：就是让大模型有一个类似于小抄的，当大模型在回答你的问题的时候，先检索你给大模型提供的(RAG 小抄的资料)，结合你提供的小抄的资料，在回答你的问题，减少了大模型的“幻觉”，同时也让大模型更加专业，准确。** 大模型是基于网络公开的数据信息资料，回答你的，但是一些行业专业术语，以及公司内部的数据资料，大模型是无法获取到的，而我们的 RAG(小抄)就解决了，这个问题。我们可以将我们的内部资料设置为(RAG 小抄)，让大模型回答的时候，根据我们给它提供的 RAG(小抄)回答我们的问题。

LangChain4j RAG 的使用理论
---------------------

RAG 流程分为两个不同的阶段：**索引 + 检索**

*   **索引：**[https://docs.langchain4j.dev/tutorials/rag/#indexing](https://docs.langchain4j.dev/tutorials/rag/#indexing)

在索引阶段，文档会被预处理，以便在检索阶段进行高效搜索。

这个过程可能因使用的信息检索方法而异。 对于向量搜索，这通常涉及清理文档、用额外数据和元数据丰富文档、 将文档分割成更小的片段（也称为分块）、嵌入这些片段，最后将它们存储在嵌入存储（也称为向量数据库）中。

索引阶段通常是离线进行的，这意味着最终用户不需要等待其完成。 例如，可以通过定时任务在周末每周重新索引一次公司内部文档来实现。 负责索引的代码也可以是一个单独的应用程序，只处理索引任务。

然而，在某些情况下，最终用户可能希望上传自己的自定义文档，使 LLM 能够访问这些文档。 在这种情况下，索引应该在线进行，并成为主应用程序的一部分。

以下是索引阶段的简化图表：

**简单的理解就是：一般我们的提供给大模型的 RAG(小抄)的内容是很大的，数据量大，同时要提高大模型查找我们的 RAG(小抄)，我们就需要将我们的 RAG(小抄)，通过向量大模型将 RAG(小抄)转换为向量数据，存储到向量数据库当中。减少空间的占用，以及提高效率。**

同时：索引阶段通常是离线进行的，这意味着最终用户不需要等待其完成。 例如，可以通过定时任务在周末每周重新索引一次公司内部文档来实现。 负责索引的代码也可以是一个单独的应用程序，只处理索引任务。

*   **检索：**[https://docs.langchain4j.dev/tutorials/rag#retri](https://docs.langchain4j.dev/tutorials/rag#retri)

检索阶段通常在线进行，当用户提交一个应该使用索引文档回答的问题时。

这个过程可能因使用的信息检索方法而异。 对于向量搜索，这通常涉及嵌入用户的查询（问题） 并在嵌入存储中执行相似度搜索。 然后将相关片段（原始文档的片段）注入到提示中并发送给 LLM。

以下是检索阶段的简化图表： 

*   [https://docs.langchain4j.dev/tutorials/rag/#core-rag-apis](https://docs.langchain4j.dev/tutorials/rag/#core-rag-apis)

*   **EmbeddingStorelngestor组织结构分析：**

*   **Document Loader(文档加载器)：**

    FileSystemDocumentLoader: 从文件系统加载文档
    UrlDocumentLoader: 从 URL 加载文档
    AmazonS3DocumentLoader: 从 Amazon S3 加载文档
    AzureBlobStorageDocumentLoader: 从 Azure Blob 存储加载文档
    GitHubDocumentLoader: 从 GitHub 仓库加载文档
    TencentCosDocumentLoader: 从腾讯云 COS 加载文档
    

*   **Document Parser(文档解析器）：**

*   **Document Transformer(文档转换器)：**

DocumentTransformer 用于对文档执行各种转换,如清理、过滤、增强或总结。

*   **Document Splitter(文档拆分器)：**

    DocumentByParagraphSplitter: 按段落拆分
    
    DocumentBySentenceSplitter: 按句子拆分
    
    DocumentByWordSplitter: 按单词拆分
    
    DocumentByCharacterSplitter: 按字符拆分
    
    DocumentByRegexSplitter: 按正则表达式拆分
    

**使用 LangChain4j 构建 RAG 的一般步骤：**

1.  加载文档：使用适当的DocumentLoader和DocumentParser加载文档
2.  转换文档：使用DocumentTransformer清理或增强文档(可选）
3.  拆分文档：使用DocumentSplitter将文档拆分为更小的片段(可选)
4.  嵌入文档：使用EmbeddingModel将文档片段转换为嵌入向量
5.  存储嵌入:使用EmbeddingStoreIngestor存储嵌入向量
6.  检索相关内容：根据用户查询,从EmbeddingStore检索最相关的文档片段
7.  生成响应：将检索到的相关内容与用户查询一起提供给语言模型，生成最终响应

LangChain4j RAG 的实战
-------------------

*   [https://docs.langchain4j.dev/tutorials/rag](https://docs.langchain4j.dev/tutorials/rag)

**LangChain4j 提供了三种 RAG 风格：**

*   [Easy RAG](https://docs.langchain4j.info/tutorials/rag/#easy-rag)：开始使用 RAG 的最简单方式
*   [Naive RAG](https://docs.langchain4j.info/tutorials/rag/#naive-rag)：使用向量搜索的基本 RAG 实现
*   [Advanced RAG](https://docs.langchain4j.info/tutorials/rag/#advanced-rag)：一个模块化的 RAG 框架，允许额外的步骤，如 查询转换、从多个来源检索和重新排序

这里我们使用 Easy RAG ，同时这里我们在使用“内存向量”，作为向量数据库，存储我们 RAG(小抄)转化为的向量数据。我们设计给大模型提供一个 RAG(内容是：阿里巴巴 Java 开发手册当中的错误码)，配置好后，问大模型 错误码：00000，A0001 是什么含义。

1.  **创建对应项目的 module 模块内容：**
2.  导入相关的 pom.xml 的依赖，这里我们采用流式输出的方式，导入\_ 整合 Spring Boot ，_`langchain4j-open-ai-spring-boot-starter`，`langchain4j-spring-boot-starter` ，同时我们加入我们的操作 RAG 的 jak 依赖。因为我们这里使用的是“内存的向量数据库”所以不需要额外的引入其他的向量数据库包。_这里我们不指定版本，而是通过继承的 pom.xml 当中获取。\_

            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-starter-web</artifactId>
            </dependency>
            <dependency>
                <groupId>dev.langchain4j</groupId>
                <artifactId>langchain4j</artifactId>
            </dependency>
            <dependency>
                <groupId>dev.langchain4j</groupId>
                <artifactId>langchain4j-open-ai</artifactId>
            </dependency>
            <!--easy-rag-->
            <dependency>
                <groupId>dev.langchain4j</groupId>
                <artifactId>langchain4j-easy-rag</artifactId>
                <version>1.2.0-beta8</version>
            </dependency>
    

3.  **设置 applcation.yaml / properties 配置文件，其中指明我们的输出响应的编码格式，因为如果不指定的话，存在返回的中文，就是乱码了。**

    server.port=9012
    spring.application.name=langchain4j-12chat-rag
    
    # 设置响应的字符编码，避免流式返回输出乱码
    server.servlet.encoding.charset=utf-8
    server.servlet.encoding.enabled=true
    server.servlet.encoding.force=true
    
    # https://docs.langchain4j.dev/tutorials/spring-boot-integration
    #langchain4j.open-ai.chat-model.api-key=${aliQwen-api}
    #langchain4j.open-ai.chat-model.model-name=qwen-plus
    #langchain4j.open-ai.chat-model.base-url=https://dashscope.aliyuncs.com/compatible-mode/v1
    # 大模型调用不可以明文配置，你如何解决该问题
    # 1 yml：                ${aliQwen-api}，从环境变量读取
    # 2 config配置类：      System.getenv("aliQwen-api")从环境变量读取
    

4.  **编写让大模型做什么事情——>这里是聊天的，接口类 ChatAssistant**

    package com.rainbowsea.langchain4jchatrag.service;
    
    /**
     */
    public interface ChatAssistant {
    
        /**
         * 聊天
         *
         * @param message 消息
         * @return {@link String }
         */
        String chat(String message);
    }
    
    

5.  **编写大模型三件套（大模型 key，大模型 name，大模型 url） 三件套的大模型配置类。同时也需要配置，我们的向量数据库(内存向量数据库)，将我们的 RAG(小抄(Alibaba Java 开发手册)) 信息存储到内存向量数据库当中，供大模型读取使用。**

    package com.rainbowsea.langchain4jchatrag.config;
    
    import com.rainbowsea.langchain4jchatrag.service.ChatAssistant;
    import dev.langchain4j.data.segment.TextSegment;
    import dev.langchain4j.memory.chat.MessageWindowChatMemory;
    import dev.langchain4j.model.chat.ChatModel;
    import dev.langchain4j.model.openai.OpenAiChatModel;
    import dev.langchain4j.rag.content.retriever.EmbeddingStoreContentRetriever;
    import dev.langchain4j.service.AiServices;
    import dev.langchain4j.store.embedding.EmbeddingStore;
    import dev.langchain4j.store.embedding.inmemory.InMemoryEmbeddingStore;
    import org.springframework.context.annotation.Bean;
    import org.springframework.context.annotation.Configuration;
    
    /**
     */
    @Configuration
    public class LLMConfig
    {
        @Bean
        public ChatModel chatModel()
        {
            return OpenAiChatModel.builder()
                        .apiKey(System.getenv("aliQwen_api"))
                        .modelName("qwen-plus")
                        .baseUrl("https://dashscope.aliyuncs.com/compatible-mode/v1")
                    .build();
        }
    
    
        /**
         * 需要预处理文档并将其存储在专门的嵌入存储（也称为矢量数据库）中。当用户提出问题时，这对于快速找到相关信息是必要的。
         * 我们可以使用我们支持的 15 多个嵌入存储中的任何一个，但为了简单起见，我们将使用内存中的嵌入存储：
         *
         * https://docs.langchain4j.dev/integrations/embedding-stores/in-memory
         *
         * @return
         */
        @Bean
        public InMemoryEmbeddingStore<TextSegment> embeddingStore() {
            return new InMemoryEmbeddingStore<>();
        }
    
        
    
    
        @Bean
        public ChatAssistant assistant(ChatModel chatModel, EmbeddingStore<TextSegment> embeddingStore)
        {
            return AiServices.builder(ChatAssistant.class)
                        .chatModel(chatModel)
                        .chatMemory(MessageWindowChatMemory.withMaxMessages(50))
                        .contentRetriever(EmbeddingStoreContentRetriever.from(embeddingStore))
                    .build();
        }
    }
    
    

6.  **编写对外访问的 ctroller 层：**将我们的**将我们的 RAG(小抄(Alibaba Java 开发手册)) 信息存储到内存向量数据库当中，供大模型读取使用。再向大模型提问，大模型就会结合其中 Alibaba Java 开发手册当中的内容回答。**

    package com.rainbowsea.langchain4jchatrag.controller;
    
    import com.rainbowsea.langchain4jchatrag.service.ChatAssistant;
    import dev.langchain4j.data.document.Document;
    import dev.langchain4j.data.document.parser.apache.tika.ApacheTikaDocumentParser;
    import dev.langchain4j.store.embedding.EmbeddingStoreIngestor;
    import dev.langchain4j.store.embedding.inmemory.InMemoryEmbeddingStore;
    import jakarta.annotation.Resource;
    import lombok.extern.slf4j.Slf4j;
    import dev.langchain4j.data.segment.TextSegment;
    import org.springframework.web.bind.annotation.GetMapping;
    import org.springframework.web.bind.annotation.RestController;
    
    import java.io.File;
    import java.io.FileInputStream;
    import java.io.FileNotFoundException;
    
    /**
     */
    @RestController
    @Slf4j
    public class RAGController
    {
        @Resource
        InMemoryEmbeddingStore<TextSegment> embeddingStore;
    
        @Resource
        ChatAssistant chatAssistant;
    
        // http://localhost:9012/rag/add
        @GetMapping(value = "/rag/add")
        public String testAdd() throws FileNotFoundException
        {
            //Document document = FileSystemDocumentLoader.loadDocument("D:\\44\\alibaba-java.docx");
            File file = new File(getClass().getClassLoader().getResource("static/Alibaba_Java.docx").getFile());
            FileInputStream fileInputStream = new FileInputStream(file);;
    
            Document document = new ApacheTikaDocumentParser().parse(fileInputStream);
    
            EmbeddingStoreIngestor.ingest(document, embeddingStore);
    
            String result = chatAssistant.chat("错误码00000和A0001分别是什么");
    
            System.out.println(result);
    
            return result;
        }
    }
    
    
    

运行测试：

最后：
===

> “在这个最后的篇章中，我要表达我对每一位读者的感激之情。你们的关注和回复是我创作的动力源泉，我从你们身上吸取了无尽的灵感与勇气。我会将你们的鼓励留在心底，继续在其他的领域奋斗。感谢你们，我们总会在某个时刻再次相遇。”