---
layout: post
title: 'Solon AI 开发学习4 - chat - 模型实例的构建和简单调用'
date: "2025-11-28T00:40:55Z"
---
Solon AI 开发学习4 - chat - 模型实例的构建和简单调用
====================================

Solon框架的聊天模型接口(ChatModel)提供了多种交互方式，包括同步调用(call)、流式调用(stream)、工具调用(Tool Call)等功能。支持通过YAML配置或手动方式构建模型，可设置API地址、提供商、模型类型等参数。接口支持同步获取完整响应和异步流式响应(基于reactivestreams规范)，并能与Solon-Flow集成。内置的模型日志功能会记录请求和响应信息，便于调试。该接口设计灵活，可适配多种响应式框架如Mutiny、RxJava或Reactor。

聊天模型接口（ChatModel）支持：

*   同步调用（call），一次性返回结果
*   支流式调用（stream，基于 reactivestreams 规范）。通过 `sse` 或 `x-ndjson` 流式返回结果。
*   Tool Call（或 Function Call） 与本地数据互动（需要 llm 支持）
*   提示语多消息输入输出（记忆体）
*   带图片消息
*   与 solon-flow 结合使用

### 1、聊天模型的构建

*   配置方式构建

    solon.ai.chat:
      demo:
        apiUrl: "http://127.0.0.1:11434/api/chat" # 使用完整地址（而不是 api_base）
        provider: "ollama" # 使用 ollama 服务时，需要配置 provider
        model: "llama3.2"
        headers:
          x-demo: "demo1"
    

    import org.noear.solon.ai.chat.ChatConfig;
    import org.noear.solon.ai.chat.ChatModel;
    import org.noear.solon.annotation.Bean;
    import org.noear.solon.annotation.Configuration;
    import org.noear.solon.annotation.Inject;
    
    @Configuration
    public class DemoConfig {
        @Bean
        public ChatModel build(@Inject("${solon.ai.chat.demo}") ChatConfig config) {
            return ChatModel.of(config).build();
        }
    }
    

*   手动方式构建

    @Configuration
    public class DemoConfig {
        @Bean
        public ChatModel build() {
            return ChatModel.of("http://127.0.0.1:11434/api/chat") //使用完整地址（而不是 api_base）
                    .provider("ollama")
                    .model("llama3.2")
                    .headerSet("x-demo", "demo1")
                    .defaultOptionAdd("stream_options", Utils.asMap("include_usage", true))
                    .build();
        }
    }
    

### 2、同步调用（call）

    public void case1() throws IOException {
        ChatResponse resp = chatModel.prompt("hello").call();
    
        //打印消息
        log.info("{}", resp.getMessage());
    }
    

### 3、异步流式或响应式调用（stream）

流式返回为 `org.reactivestreams.Publisher`（reactivestreams 规范）

    public void case2() throws IOException {
      Publisher<ChatResponse> publisher = chatModel.prompt(ChatMessage.ofUser("hello")).stream();
      
      //return publisher; //使用 solon-web-rx 时可直接返回；或者对接 solon-web-sse 或 websocket
    
      publisher.subscribe(new SimpleSubscriber<ChatResponse>()
              .doOnNext(resp -> {
                  log.info("{}", resp.getMessage());
              }).doOnComplete(() -> {
                  log.debug("::完成!");
              }).doOnError(err -> {
                  log.error("{}", err);
              }));
    }
    

可以直接订阅消费（如上）。也可对接各种流行的响应式框架，比如 mutiny、rxjava 或 reactor：

    @Produces(MimeType.TEXT_EVENT_STREAM_UTF8_VALUE)
    @Mapping("case2")
    public Flux<SseEvent> case2(String prompt) throws IOException {
        return Flux.from(chatModel.prompt(prompt).stream())
                .map(resp -> resp.getMessage())
                .map(msg -> new SseEvent().data(msg.getContent()))
                .doOnError(err->{
                     log.error("{}", err);
                });
    }
    

### 4、模型日志

内部默认会打印 llm 请求与响应的日志，分别以 `ai-request:` 和 `ai-response:` 开头。日志级别为：DEBUG。