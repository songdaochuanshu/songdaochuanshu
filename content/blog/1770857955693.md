---
layout: post
title: '[深度学习] 大模型学习6-模型量化与推理部署'
date: "2026-02-12T00:59:15Z"
---
\[深度学习\] 大模型学习6-模型量化与推理部署
=========================

在[大语言模型基础知识](https://www.cnblogs.com/luohenyueji/p/18644847)一文中，已简要介绍了模型量化与高效推理部署，二者是实现大语言模型（Large Language Model，LLM）低成本、高效落地的关键路径。本文将系统阐述模型量化的基本原理，并详细介绍LLM高效推理的核心技术与常用框架。

目录

*   [1 LLM中的量化技术](#1-llm中的量化技术)
    *   [1.1 量化背景](#11-量化背景)
    *   [1.2 量化基本原理](#12-量化基本原理)
        *   [1.2.1 核心思想](#121-核心思想)
        *   [1.2.2 量化时机](#122-量化时机)
        *   [1.2.3 策略选型](#123-策略选型)
    *   [1.3 量化算法在LLM中的实现与工具链](#13-量化算法在llm中的实现与工具链)
        *   [1.3.1 适用于LLM的量化方法](#131-适用于llm的量化方法)
        *   [1.3.2 主流LLM量化工具链](#132-主流llm量化工具链)
        *   [1.3.3 Qwen3系列模型中的量化](#133-qwen3系列模型中的量化)
*   [2 推理部署](#2-推理部署)
    *   [2.1 部署参数](#21-部署参数)
    *   [2.2 推理优化](#22-推理优化)
        *   [2.2.1 计算优化](#221-计算优化)
        *   [2.2.2 显存优化](#222-显存优化)
        *   [2.2.3 延迟与吞吐优化](#223-延迟与吞吐优化)
        *   [2.2.4 从Prefill与Decoder视角理解LLM推理](#224-从prefill与decoder视角理解llm推理)
    *   [2.3 推理框架](#23-推理框架)
*   [3 参考](#3-参考)

1 LLM中的量化技术
===========

本部分将系统介绍如何通过模型量化（Quantization）技术压缩LLM。首先，从量化背景出发，说明当前模型压缩的现实需求；其次，概述深度学习中的通用量化原理；最后，结合LLM的特点，详解其专用的量化方法与工具链。

1.1 量化背景
--------

深度学习模型的训练与推理主要包括前向计算、反向传播与参数更新三个阶段，本质上都属于大规模数值运算，其中以矩阵乘法和张量运算为主。这类运算具有高度并行性，对计算效率要求较高。相比而言，CPU虽然通用性强，但并行能力和内存带宽有限，难以高效处理大规模矩阵运算；而GPU采用大规模并行架构，尤其擅长执行矩阵乘法等高密度计算任务，因而成为当前深度学习的主流计算平台。

随着深度学习模型规模不断扩大，尤其是LLM参数量从7B、14B、34B增长至数百亿甚至更高，模型对显存容量和计算资源的需求急剧增加。由于LLM模型参数、激活值及中间结果均需占用显存，将完整模型加载至单张GPU上在实际中几乎不可行。即便是高端消费级GPU（如RTX 5090，约32GB显存）或数据中心级GPU（如H100，约80GB显存），在超大LLM模型面前仍存在明显瓶颈。虽然多GPU并行技术可以分担计算与存储压力，但会显著增加显存占用、通信开销和系统复杂度，甚至影响推理效率。因此，在有限算力和显存条件下，如何降低计算量和显存占用，同时尽量保持模型推理性能，已成为LLM训练与部署中的关键问题。

在这一背景下，数值表示格式的选择对LLM的计算效率和显存占用具有决定性影响。当前深度学习训练与推理中常用的浮点格式（如FP32、FP16、BF16）提供了大动态范围和高数值精度，但同时带来显著的存储与计算开销。相比之下，定点或低比特整数（如INT8、INT4）可大幅减少模型参数与中间激活的存储需求，从而缓解显存与算力瓶颈，但也会带来数值精度下降的问题。不同数值格式对比如下图所示，其中：

*   红色块（Sign）：就1位，管数字是正还是负，
*   蓝色块（Exponent）：管数字的大小范围，位数越多，能存的数的范围越大，
*   绿色块（Mantissa）：管数字的精度细节，比如是存1.2还是1.2345，位数越多，数字的精度就越高，
*   黄色块（Regime，只在Posit16里有）：类似Exponent的加强版，也是管数字的大小范围。

![https://semiengineering.com/data-formats-for-inference-on-the-edge/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/1.jpg)

FP32凭借更多的指数位和尾数位兼顾了动态范围与精度，却带来了较高的存储和计算开销；FP16与BF16通过缩减位宽在效率与精度间实现折中，其中BF16会保留更多指数位，以此维持动态范围；INT8与INT4则几乎不单独表示指数部分，仅保留有限精度，适合对性能和存储开销敏感的推理场景；而Posit16通过Regime机制在有限比特下提供了一种更灵活的数值表示方案。下面的Python示例直观展示了不同数值格式在相同数组长度下的存储开销差异：

    import numpy as np
    import matplotlib.pyplot as plt
    
    # -----------------------------
    # 1️⃣ 定义数组长度
    # -----------------------------
    N = 1_000_000
    
    # -----------------------------
    # 2️⃣ 创建不同数值格式的数组
    # -----------------------------
    arr_fp32 = np.zeros(N, dtype=np.float32)    # 32 位浮点
    arr_fp16 = np.zeros(N, dtype=np.float16)    # 16 位浮点
    arr_bfloat16 = np.zeros(N, dtype=np.uint16) # 使用 uint16 存储 BF16
    arr_int8 = np.zeros(N, dtype=np.int8)       # 8 位整型
    arr_int4 = np.zeros((N + 1) // 2, dtype=np.uint8)  # 4 位整型打包存入 uint8
    arr_posit16 = np.zeros(N, dtype=np.uint16)  # 16 位 Posit 格式（此处为占位）
    
    # -----------------------------
    # 3️⃣ 计算存储大小（字节）
    # -----------------------------
    storage_bytes = {
        'FP32': arr_fp32.nbytes,
        'FP16': arr_fp16.nbytes,
        'BF16': arr_bfloat16.nbytes,
        'INT8': arr_int8.nbytes,
        'INT4': arr_int4.nbytes,
        'Posit16': arr_posit16.nbytes
    }
    
    # 转换为 MB
    storage_mb = {k: v / (1024 ** 2) for k, v in storage_bytes.items()}
    
    print("各数值格式存储大小（MB）：")
    for k, v in storage_mb.items():
        print(f"{k}: {v:.2f} MB")
    
    # -----------------------------
    # 4️⃣ 绘制柱状图对比
    # -----------------------------
    plt.figure(figsize=(8, 5))
    types = list(storage_mb.keys())
    sizes = list(storage_mb.values())
    
    bars = plt.bar(types, sizes, color=['red', 'blue', 'green', 'orange', 'purple', 'cyan'])
    plt.ylabel("Storage Size (MB)")
    plt.title(f"Storage Comparison of Different Numeric Formats (Array length={N})")
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    
    # 在柱上方标注数值
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.05, f"{yval:.2f}", 
                 ha='center', va='bottom')
    
    plt.show()
    

通常，模型训练阶段常采用较高精度（如FP32/BF16），而在推理阶段则可通过精度转换以提升效率。基于不同数值表示的特点，研究者提出了模型量化方法，即将模型中的高精度浮点数映射至低比特定点数或整数表示，在尽量保持模型性能的前提下，显著降低模型的存储需求与计算开销。量化不仅能减少模型参数与中间激活值所占用的显存，还可充分发挥GPU、TPU等硬件对低精度运算的加速能力，从而为资源受限环境下的模型部署提供有效支持。

![https://developer.nvidia.com/blog/optimizing-llms-for-performance-and-accuracy-with-post-training-quantization/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/1.gif)

1.2 量化基本原理
----------

### 1.2.1 核心思想

量化的核心思想可以概括为用更少的比特，在可接受的误差范围内近似表示原本的高精度浮点数值。一般是通过将高精度浮点数（如FP32）映射为低比特整数（如INT8/INT4）。类似于将高清图像转为缩略图：在保留整体结构的前提下丢失部分细节，从而提升处理与传输效率。关于量化更详细和更加生动可视化的介绍见：[A Visual Guide to Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)。

从工程实现角度看，量化方法通常分为线性量化和非线性量化：

*   线性量化Linear quantization（主流方案）：将浮点数区间线性映射到固定的整数区间，计算简单、数值稳定，易于在现有CPU/GPU/NPU上高效实现，是当前训练后量化和推理部署中最常用的方法。
*   非线性量化Non-linear quantization：采用非线性映射或离散化策略（如对数量化、分段量化、k-means量化），在特定分布下可降低量化误差，但实现复杂、推理支持受限，工程落地成本较高。

常见的线性量化公式如下：

1.  使浮点数零点与整数零点对齐，计算时先把原始数值按比例缩放，再四舍五入成整数，也就是对称量化：
    
    \\\[q = \\text{round}\\left(\\frac{x}{s}\\right) \\\]
    
    其中\\(x\\)是原始浮点值，\\(s\\)是缩放因子scale，\\(q\\)是量化后的整数。
2.  当浮点数的取值全部为正时，若仍采用对称量化方式，整数表示范围内近一半的负数位将无法被利用，从而降低量化精度。为此，可通过引入零点，将浮点数的取值范围整体平移到整个整数区间上，以充分利用所有整数位来更好地保持精度，这种方法即非对称量化。后续将重点介绍非对称量化，对称量化仅需将零点设为0即可：
    
    \\\[q = \\text{round}\\left(\\frac{x}{s}\\right) + z \\\]
    
    其中\\(z\\)为零点（zero-point，整数），用于把浮点的零对齐到整数域的某个值，让浮点取值范围适配整数表示范围。
3.  常见的反量化（把整数变回浮点）：
    
    \\\[\\hat{x} = s \\cdot (q - z) \\\]
    
    量化误差即 (\\(x\\) - \\(\\hat{x}\\))，其来源于取整（round）操作以及可能的数值截断，这也是模型精度损失的主要来源。
4.  计算缩放因子\\(s\\)和零点\\(z\\)：
    
    *   目标整数范围：  
        通常设定为\\(\[q\_{\\min}, q\_{\\max}\]\\)，8位有符号整数的标准范围是\\(\[-128,127\]\\)，为了简化对称量化的处理，很多框架在对称量化时会把有效映射范围设为\\(\[-127,127\]\\)，使得零点严格为0。8位无符号整数的标准范围则为\\(\[0,\\,255\]\\)。
    *   缩放因子：
    
    \\\[s = \\frac{x\_{\\max} - x\_{\\min}}{q\_{\\max} - q\_{\\min}} \\\]
    
    其中\\(x\_{\\min}\\)和\\(x\_{\\max}\\)指的是当前要量化的那一组具体浮点数数据的实际最大值、实际最小值。
    
    *   零点是浮点数0对应的量化后整数，计算时一般以原始浮点数的最小值\\(x\_{\\min}\\)​和目标整数范围的最小值\\(q\_{\\min}\\)​为基准：
    
    \\\[z = q\_{\\min} - \\text{round}\\left(\\frac{x\_{\\min}}{s}\\right) \\\]
    
    其中\\(z\\)会被计算并存储为整数，以确保浮点零值\\(x = 0\\)能够精确对应到整数\\(q = z\\)。

![https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/2.jpg)

举例：假设权重的浮点取值范围为 (\[-1.0, 0.9\])，使用非对称量化，目标整数范围为 (\[0, 255\])。

1.  计算缩放因子（scale）

\\\[s = \\frac{\\max - \\min}{\\text{整数范围}} = \\frac{0.9 - (-1.0)}{255 - 0} = \\frac{1.9}{255} \\approx 0.00745 \\\]

2.  计算零点（zero-point）

\\\[ z = \\text{round}\\left(-\\frac{\\min}{s}\\right) = \\text{round}\\left(-\\frac{-1.0}{0.00745}\\right) \\approx 134 \\\]

3.  量化权重

\\\[q = \\text{round}\\left(\\frac{w}{s}\\right) + z \\\]

对 w = 0.123：

\\\[q = \\text{round}\\left(\\frac{0.123}{0.00745}\\right) + 134 \\approx \\text{round}(16.5) + 134 = 151 \\\]

4.  反量化回浮点

\\\[\\hat{w} = (q - z) \\cdot s = (151 - 134) \\cdot 0.00745 \\approx 17 \\cdot 0.00745 \\approx 0.1267 \\\]

5.  计算误差

\\\[\\epsilon = w - \\hat{w} = 0.123 - 0.1267 \\approx -0.0037 \\\]

可以看到，非对称量化引入的误差仍然很小（约0.0037），并且零点的存在让浮点0能精确对应整数值，从而在推理中避免了偏移误差累积。在此基础上，为了进一步平衡计算效率与模型精度，还可以根据实际需求，将量化精细化为2比特（INT2）、4比特（INT4）以及8比特（INT8）等多种位宽类型。

![](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/3.jpg)

此外，还需理解量化粒度（granularity）的概念。它决定了权重或激活是以逐张量（per-tensor）、逐通道（per-channel）还是按组（per-group）为单位进行量化。粒度越细，量化参数越贴合数据分布，通常能提高精度，但也会增加存储与计算开销。

另一个相关概念是饱和量化（saturated quantization）与不饱和量化（non-saturated quantization）。饱和量化会裁剪分布两端的极端值，将更多的整数表示空间留给数据的主要分布区域，从而提升整体精度，但会损失极端值信息；不饱和量化则保留全部数据范围，但可能降低常用值的分辨率。

在实际部署中，为平衡精度与硬件效率，通常对权重采用逐通道量化（per-channel），对激活采用逐张量量化（per-tensor）。理解这些概念的基本作用即可，无需深入细节。

### 1.2.2 量化时机

量化可以在不同阶段对模型进行操作，主要分为训练后量化和量化感知训练。

**PTQ（Post-Training Quantization，训练后量化）**

*   原理：PTQ是指在模型训练完成后，对权重或激活进行量化。可以理解为模型已经训练好了，然后再用更少的比特表示数据，以减少存储和计算成本。
*   特点：
    *   优点：流程简单，通常不需要重新训练，或者只需极少量校准数据。
    *   缺点：当量化位宽很低（如 4-bit、2-bit）或者模型本身对量化操作较为敏感时，模型的推理精度可能下降明显。
*   实现方式：
    1.  Data-free（无数据校准）
        *   方法：不依赖真实数据，只用一些基于权重的固有统计或假设就算出量化参数（scale/zero-point）。
        *   优势：速度快，操作简单。
        *   劣势：精度可能较低。
    2.  Calibration（基于校准数据）
        *   方法：使用少量代表性数据，通过统计方法（如 min/max、percentile、KL 散度等）计算最佳量化参数。
        *   优势：精度更高，量化效果更稳定。
        *   劣势：需要额外时间和数据样本。
*   极简示例：

    def post_training_quantize(model, bits=8):
       """
       这是weight-only、per-layer、symmetric的int8量化
       forward仍然使用 float（通过反量化后的权重）
       """
       # int8 对称量化的整数范围
       qmin = -(2 ** (bits - 1))
       qmax =  (2 ** (bits - 1)) - 1
       for layer in model.layers:
          # 仅对包含权重参数的层进行量化（如 Conv / Linear）
          if hasattr(layer, 'weights'):
                # 将权重展平成一维，便于统计最大绝对值
                w = layer.weights.flatten()
                # 使用权重的最大绝对值来确定量化scale
                # 这是 PTQ 中常见的per-layer symmetric量化方法
                max_abs = np.max(np.abs(w))
                # 避免全0权重导致除零
                if max_abs < 1e-8:
                   scale = 1.0
                else:
                   # float_value ≈ int_value * scale
                   scale = max_abs / qmax
    
                # ---------- 量化（float → int） ----------
                # 将 float 权重映射到整数域
                q = np.round(w / scale)
                # 裁剪到 int8 可表示范围
                q = np.clip(q, qmin, qmax).astype(np.int8)
                # ---------- 反量化（int → float） ----------
                # 用反量化后的 float 权重近似原始权重
                w_hat = q.astype(np.float32) * scale   
       return model
    

**QAT（Quantization-Aware Training，量化感知训练）**

*   原理：QAT在训练过程中模拟量化影响，使模型在量化噪声中进行训练或微调，提前适应压缩，从而在低比特量化下仍保持高精度。
*   实现方式：
    *   伪量化（fake-quant）：在前向传播中模拟量化效果，让模型看到量化后的数据。
    *   反向传播：把量化操作当作恒等映射（即假设它不改变数值）来传递梯度，从而正常更新浮点权重。
*   特点：
    *   优点：在低比特量化下精度恢复明显，效果接近原始模型。
    *   缺点：训练成本高，实现复杂，需要额外计算资源。
*   极简示例：

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    class QuantizedLinear(nn.Module):
       def __init__(self, in_features, out_features, bits=8):
          super().__init__()
          # 浮点权重参数
          self.weight = nn.Parameter(torch.randn(out_features, in_features))
          self.bits = bits
          # 量化比例（scale），在模型保存/加载时保持
          self.register_buffer("scale", torch.tensor(1.0))
       def quantize_weights(self, w):
          """
          将浮点权重压缩到整数范围，再映射回浮点
          """
          qmax = 2 ** (self.bits - 1) - 1      # 对称量化最大值
          scale = w.abs().max() / qmax          # 动态计算比例
          q = torch.clamp(torch.round(w / scale), -qmax, qmax)  # 整数化并限制范围
          # STE：保持梯度流
          w_quantized = (q * scale).detach() - w.detach() + w
          return w_quantized, scale
    
       def forward(self, x):
          if self.training:
                # 训练阶段
                w_q, scale = self.quantize_weights(self.weight)
                self.scale.copy_(scale)  # 保存 scale
                return F.linear(x, w_q)
          else:
                # 推理阶段：直接使用量化后的权重
                qmax = 2 ** (self.bits - 1) - 1
                q = torch.clamp(torch.round(self.weight / self.scale), -qmax, qmax)
                return F.linear(x, q * self.scale)
    

实际部署首选PTQ量化，因为它简单高效，用少量数据校准就能让模型在8-bit下基本不掉精度；只有对超低精度（4-bit以下）或精度损失敏感的场景才用QAT，它通过训练让模型提前适应量化，但代价是重新训练成本高。

![https://arxiv.org/abs/2112.06126](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/4.jpg)

### 1.2.3 策略选型

量化是在精度与效率间权衡的工程方法，关键在于理解缩放因子（scale）、零点（zeropoint）、量化粒度（逐张量/逐通道）和量化时机（PTQ/QAT）。通过有效的校准与工程实践，可在节省资源的同时控制精度损失。量化通常不会导致精度大幅下降，但无法完全无损。非线性量化在特定分布或极低位宽下或有优势，但往往牺牲硬件友好性。对于4bit及以下量化，常需配合剪枝、蒸馏或专门的QAT流程才能获得可用精度。关于这些技术的详细解析，可参考：[一文详尽大型语言模型的四种量化技术](https://mp.weixin.qq.com/s/KQxhrYtDTudRZkxgu5yN5g)。在工程实践中，量化策略的选择通常受具体约束条件影响，可总结如下：

首要约束

推荐技术

核心优势

潜在挑战

上线时间紧

PTQ

数小时内完成部署

极低比特（<4-bit）时精度崩塌

显存严重不足

4-bit Finetuning

显存需求大幅度降低

需要一定的微调数据和时间

必须保证准确率

QAT

几乎无精度损失

训练开销极大，需大量数据

综合性能最优

Mixed Precision

硬件加速效果最好

实现逻辑相对复杂

提升量化效果可遵循以下要点：

1.  从PTQ校准开始：用少量代表性数据校准模型，常可显著提高精度；
2.  权重用逐通道，激活用逐张量：硬件支持时可对激活尝试逐通道；
3.  敏感层或首尾层用混合精度：如保留首尾层为FP16/FP32；
4.  PTQ不足再做QAT：如精度不满足要求，QAT下微调通常可恢复大部分精度；
5.  量化前做统计分析：依据权重与激活分布选择合适截断策略；
6.  实测推理性能：需测端到端延迟并确认硬件支持整型运算。

![https://dataman-ai.medium.com/four-quantization-techniques-for-large-language-models-d4ff478074a0](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/5.jpg)

1.3 量化算法在LLM中的实现与工具链
--------------------

### 1.3.1 适用于LLM的量化方法

随着LLM的规模扩大，其参数量通常达到数十亿至上千亿级别。在这种规模下，传统量化方法面临以下挑战：

1.  显存占用高：即使8-bit权重量化，部分模型仍难以在单GPU部署；
2.  低位量化误差累积：4-bit或更低精度的量化中，误差在网络层间传播，显著影响生成质量；
3.  激活分布不均：某些层如注意力矩阵或前馈网络激活值范围大，统一量化比例容易产生严重截断；
4.  层间特性差异大：不同层对量化的敏感度不同，需要差异化策略。

因此，传统量化方法在LLM上的应用效果受限，亟需更为精细的量化方案。为应对上述挑战，业界与开源社区在传统量化方法的基础上，发展出多种适应LLM的量化技术，涵盖推理优化、低显存微调和激活量化等关键场景。以下内容基于开源社区采纳度、工程实践应用及相关文献引用情况进行梳理，旨在提供一份简要的技术概览，仅供参考：

**GPTQ（流行的PTQ方法之一）**

*   核心思路：对权重分组，逐组最小化量化误差，并进行局部调整
*   优点：4bit量化精度高、推理速度快、社区生态成熟
*   受欢迎程度：★★★★★
*   典型采用者：
    *   LLaMA 系列（社区量化版）
    *   Qwen3（社区量化）
    *   DeepSeek系列

**BitsAndBytes（最常用的8bit/4bit加载方案）**

*   核心思路：加载时直接转换为8bit或NF4权重格式，支持快速推理
*   优点：简单、兼容性强，HuggingFace默认支持
*   受欢迎程度：★★★★★
*   典型采用者：
    *   HuggingFace上几乎所有主流模型
    *   LLaMA、Qwen等

**QLoRA（低显存微调方案）**

*   核心思路：4bit权重+LoRA适配器微调
*   优点：显存占用极低，可微调百亿参数模型
*   受欢迎程度：★★★★★
*   典型采用者：
    *   开源模型指令微调（LLaMA、Qwen等）
    *   企业内部定制模型

**GGUF（通用量化文件格式）**

*   核心思路：统一量化模型存储格式，便于快速加载与推理
*   优点：兼容性强，简化工程化流程，可和GPTQ/BitsAndBytes/QLoRA等配合使用
*   受欢迎程度：★★★★★
*   典型采用者：
    *   社区模型分发
    *   工程化部署

**KV-Cache量化**

*   核心思路：量化KV缓存（Key-Value缓存）以降低推理显存占用
*   优点：显著降低LLM生成时的显存需求
*   受欢迎程度：★★★★☆
*   典型采用者：
    *   低显存部署LLM
    *   高速生成场景

**AWQ（高质量 4bit 推理方案之一）**

*   核心思路：保护重要通道，减少量化对敏感激活的影响
*   优点：4bit性能稳定，尤其适合指令微调模型
*   受欢迎程度：★★★★☆
*   典型采用者：
    *   Qwen2.5/Qwen3
    *   LLaMA3/LLaMA3.1

**SmoothQuant（主流激活量化方法）**

*   核心思路：平滑激活峰值，使权重和激活更易量化
*   优点：有效应对量化中精度与效率失衡的问题
*   受欢迎程度：★★★☆☆
*   典型采用者：
    *   企业推理引擎
    *   大规模在线服务

**HQQ（快速工程化量化方案）**

*   核心思路：快速优化算法，几乎无需校准数据
*   优点：量化速度极快，支持2bit/1bit
*   受欢迎程度：★★★☆☆
*   典型采用者：
    *   边缘部署
    *   需要快速量化的LLM模型

**LLM-QAT（超低比特量化方案）**

*   核心思路：训练中模拟量化，使模型适应低比特误差
*   优点：2bit 性能最佳
*   受欢迎程度：★★☆☆☆（研究或企业内部使用）
*   典型采用者：
    *   企业内部模型（搜索、广告等）
    *   边缘低比特部署模型

![](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/6.jpg)

关于这些技术的详细介绍见：[LLM Quantization Techniques](https://corefragment.com/blog/llm-quantization-techniques)和[大模型量化技术解析和应用](https://modelscope.cn/learn/399?pid=488)。

对于大多数只需部署应用或进行简单模型研究的用户，关键是理解量化的类型与适用场景，无需过分钻研具体方法细节；常见路径是先用BitsAndBytes进行加载与推理，再用GPTQ实现高质量4bit推理，若需微调则采用QLoRA，最后可研究激活量化和KV-cache优化以进一步降低显存占用。

### 1.3.2 主流LLM量化工具链

基于前文介绍的量化方法，社区已经形成一套成熟的量化工具链与推理生态，使得LLM的低比特部署从研究探索逐步演进为工程常规实践。总体上，这些工具将复杂的量化细节封装起来，工程师可以把精力集中在模型选择、部署资源与业务场景上，而无需重复实现量化算法或内核优化。多数工具同时提供与Hugging Face Transformers的对接接口或支持导出标准化的量化权重格式，便于模型复现与分发。在实际使用中，掌握bitsandbytes与AutoGPTQ即可覆盖大多数需求：快速实验优先选bitsandbytes，追求高精度的4-bit推理选择AutoGPTQ，若面向生产部署可考虑Hugging Face Optimum，追求低延迟、高吞吐则可评估AutoAWQ。

![](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/7.jpg)

各类工具详细介绍如下：

**bitsandbytes**

*   特点：无需校准数据的动态量化，常用于显存受限的QLoRA训练。推理时加载速度快
*   使用频率：★★★★★（最常用）
*   优点：与transformers库集成极简，一行代码启用 (load\_in\_4bit=True)，是进行高效微调的首选工具之一
*   缺点：由于缺乏数据校准，极低比特下量化误差较大；其通用CUDA内核在特定架构上可能非最优

    from transformers import AutoModelForCausalLM
    
    model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-2-7b",
        device_map="auto",
        load_in_4bit=True  # 或 load_in_8bit=True
    )
    

**AutoGPTQ**

*   特点：基于GPTQ的离线权重量化，需要校准数据，4-bit精度与推理速度表现优秀
*   使用频率：★★★★☆
*   优点：推理速度较快，4-bit精度稳定；量化结果可复现、易于分发
*   缺点：量化流程需要校准，不适合在线或频繁更新模型的场景；量化后的模型进行额外的参数高效微调（如添加新的LoRA模块）较为困难或不被官方支持

    from auto_gptq import AutoGPTQForCausalLM
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained("model")
    model = AutoGPTQForCausalLM.from_quantized(
        "model-gptq",
        device_map="auto",
        use_safetensors=True,   # 如果模型是 safetensors
        low_cpu_mem_usage=True
    )
    

**Hugging Face Optimum**

*   特点：统一GPTQ、SmoothQuant等量化流程，面向生产部署场景
*   使用频率：★★★★☆
*   优点：工作流标准化，便于跨平台部署和实验对比
*   缺点：性能依赖具体后端实现，框架抽象可能屏蔽了一些底层调优选项

    from optimum.gptq import GPTQQuantizer
    
    quantizer = GPTQQuantizer(bits=4, dataset="wikitext2")
    quantizer.quantize_model("model", save_dir="model-gptq")
    

**AutoAWQ**

*   特点：采用激活感知权重量化（AWQ），在4-bit下具有较高推理效率
*   使用频率：★★★☆☆（生态发展迅速）
*   优点：4-bit下通常比GPTQ有更高的吞吐量或更低的延迟，但精度相当
*   缺点：模型适配性与生态成熟度仍在完善中

    from awq import AutoAWQForCausalLM
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained("model")
    model = AutoAWQForCausalLM.from_quantized(
        "model-awq",
        device_map="auto"
    )
    

### 1.3.3 Qwen3系列模型中的量化

上述量化工具不仅在通用场景中得到广泛验证，还获得了多种开源LLM系列模型的原生集成或官方支持，其中最具代表性的是Qwen3系列。该系列由阿里巴巴通义千问团队于2025年4月推出，其规模设计与工程实践体现了当前主流模型的典型发展方向。Qwen3系列同时涵盖稠密模型与混合专家模型，能够适应从轻量化部署到超大规模推理的多样化需求。

从模型结构上看，Qwen3系列主要分为两类：

*   稠密模型：推理过程中激活全部参数，结构简单、行为稳定。Qwen3已开源多个参数规模的稠密模型，包括Qwen3-0.6B、1.7B、4B、8B、14B以及32B，覆盖从端侧到高性能推理的主要应用区间。
*   混合专家模型（Mixture of Experts, MoE）在推理时仅激活部分专家网络，能够在保持高性能的同时，显著降低计算与推理成本。Qwen3系列中包含以下MoE模型：
    *   Qwen3-30B-A3：拥有约300亿总参数，但在推理时仅激活约30亿参数。
    *   Qwen3-235B-A22B：作为旗舰模型，其总参数超过2350亿，激活参数约为220亿。

在功能层面，Qwen3模型支持在思考模式与非思考模式之间无缝切换：前者适用于复杂逻辑推理、数学计算与编程任务，后者则面向通用对话场景，提供更低延迟的响应体验。除文本模型外，Qwen3体系还包含面向多模态的Qwen3-Omni、视觉理解的Qwen3-VL，以及文本向量与排序模型（Qwen3-Embedding、Qwen3-Reranker）等开源模型。注意该系列目前最强的旗舰模型Qwen3-Max为闭源模型，主要通过官方API与应用服务提供。模型选型可参考：[一文看懂Qwen3本地部署的配置要求](https://zhuanlan.zhihu.com/p/1902792207527838412)，更详细的性能数据可查阅：[Qwen3 Speed Benchmark](https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html)。

![https://qwen.ai/blog?id=qwen3](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/8.jpg)

为使模型在消费级硬件等资源受限环境中高效运行，Qwen3系列不仅提供多类量化版本以加速部署，还支持多种主流量化方法与模型格式：

格式名称

位宽(Bit)

特点

适用场景

硬件依赖

FP8

8-bit

精度近乎无损，官方提供标准量化版（如Qwen3-0.6B-FP8），适合LLM推理

高吞吐量推理、大规模生成任务

支持FP8的GPU（H100/A100）

AWQ

4-bit

硬件友好型，高压缩比，节省约70%显存

GPU推理优化

兼容多数现代GPU（不依赖FP8支持）

GPTQ

4-bit

经典PTQ量化，利用二阶信息补偿量化误差，兼容性好

老旧GPU、本地部署、AutoGPTQ框架

对显卡要求低，适合中低端GPU

GGUF

2~8bit（视导出配置）

通用量化封装格式，可选择不同精度（Q4\_K\_M、Q5\_K\_M、Q8\_0等），支持CPU/GPU混合推理

本地部署、轻量化推理、跨平台使用

CPU/GPU均可，依赖后端推理库支持

在量化效果方面，Qwen3模型在8-bit量化下几乎无损性能，可作为生产部署的可靠选择；4-bit量化虽会导致模型性能小幅下降，但仍能满足多数任务需求，并大幅降低部署门槛。例如，Qwen3-32B的4-bit AWQ量化版本，使得原本需多张高端显卡运行的模型，能够在单张消费级显卡上流畅推理。

![https://developer.aliyun.com/article/1666422](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/9.jpg)

这些Qwen3模型是官方为方便用户部署所提供的，那么其中的量化模型是否可以用于继续训练或微调？方法可归纳如下：

*   直接微调量化权重：对低精度量化权重进行全量微调既罕见又困难。量化是一种有损压缩，会将连续权重映射到离散值，导致梯度更新的小幅调整可能被离散化吞掉，从而出现梯度信号失真或消失，训练容易不稳定且难以收敛。
*   更常见的方案是使用参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）：
    1.  量化模型上使用Adapter或LoRA：冻结基座模型权重，仅训练额外添加的小型适配模块Adapter或LoRA，这些模块通常以高精度FP16保存。推理时，将适配模块与量化基座模型结合使用。训练后模型由量化基座+高精度Adapter/LoRA组成，原基座保持不变，可复用或合并后再量化部署。
    2.  使用QLoRA等变体：将基座模型量化为4-bit或直接使用官方提供的4-bit模型，仅训练高精度的LoRA/Adapter参数。训练后模型同样由量化基座+高精度模块组成，显存占用更低，微调效果接近全量微调，适合显存受限或多任务共享基座模型的场景。
*   稳妥方案：若硬件资源充足，可先加载高精度版本FP16/BF16进行全参数或混合精度微调，微调完成后再进行量化部署，这种方式最为稳妥。

2 推理部署
======

即使在量化压缩后，LLM在实际应用里仍然面临计算开销高、显存占用量大等挑战。这主要是因为量化虽然能缩小模型体积、减轻部分算力负担，却很难彻底化解参数规模庞大和注意力机制复杂所带来的资源压力。要想进一步实现高效、低成本的在线服务，推理部署就成为至关重要的一环。本部分将系统介绍LLM推理部署的核心技术路径，主要包括部署参数配置、多维度优化策略以及主流推理框架的选型。

2.1 部署参数
--------

训练完成的LLM模型通常需进一步用于推理或部署。推理是指利用模型从输入数据生成输出结果的过程；部署则是将模型集成到持续运行的生产环境中，以提供高效、稳定的推理服务。在技术实现上，LLM推理可直接基于PyTorch等原生代码执行，但这通常仅适用于研究或测试场景。在生产环境中，为了获得更优性能，业界普遍采用专门为LLM优化的推理框架。

LLM推理的核心是文本生成，其主流机制为自回归解码 (Autoregressive Decoding)。该过程以Prompt为初始输入，模型迭代式地逐个预测下一个token，并将其追加到输入序列中，循环此过程直至生成终止符或达到最大长度。自回归机制仅规定了逐词生成的顺序，但未指定如何在每一步从成千上万个候选token中进行选择。这一决策由解码策略或采样方法控制，直接影响生成文本的质量、确定性与多样性。常见的策略包括：

*   贪婪搜索 (Greedy Search)：每一步挑选概率最高的token，简单却易陷入重复（如"非常非常非常..."），导致生成结果缺乏多样性。
*   集束搜索 (Beam Search)：通过同时维护多条候选序列，缓解贪婪搜索容易陷入局部最优的问题。每一步仅保留概率最高的若干序列，在所有序列生成结束标记或达到预设长度后，选择对数概率之和最大的序列作为输出。该策略适用于注重连贯性的生成任务，但由于每一步倾向于选择当前最稳妥的token，生成的文本通常较为保守，新颖性不足。
*   随机采样 (Sampling)：根据词表中各token的概率分布随机选取token，概率越高的token被选中的可能性越大。该方法有助于提升生成文本的多样性，是当前主流的文本生成方式之一。常见的随机采样优化策略包括：
    *   Top‑k采样：从模型预测的概率分布中，选取概率最高的k个token作为候选池，并在此池内重新归一化概率后随机采样。
    *   Top‑p采样：将token按概率从高到低排序，依次累加概率，直到累积概率首次超过阈值p，仅使用这部分token重新归一化概率后随机采样。

在LLM中，Top-k与Top-p常结合使用。一般流程是先通过Top-k从大规模词表中快速选出概率最高的k个token以降低计算开销，再在此基础上运用Top-p进行精筛，仅保留累积概率达到阈值p的token。优先使用Top-k是为了快速缩小候选池、降低计算开销，但其存在明显局限：如果概率分布集中，会引入冗余候选；如果分布分散，又可能遗漏关键token。此时Top-p能进行动态调整，使采样池更紧凑合理。因此实际调节时，通常建议优先调整Top-p，它可以根据概率分布自适应地确定候选范围，对不同上下文具有更好的适应性。

![https://felixrante.com/%F0%9F%8E%B2-top-k-vs-top-p-sampling-the-secret-behind-ai-text-generation/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/10.jpg)

然而，无论是单独使用还是组合使用Top-k与Top-p，它们本质上都是基于模型输出的概率分布进行选择，但这个分布本身的陡峭程度会直接决定采样的随机性，也就是说如果分布过于集中、高概率词占比极大，采样仍接近贪婪搜索，多样性不足；如果分布过于分散，又容易选中低概率的无意义词，导致高度随机。此时，可通过调节temperature（温度）参数来控制概率分布的形态，在多样性与可控性之间实现更好的平衡。temperature参数的工作原理如下：

模型在每一步生成时，会先输出一组未归一化的得分logits，表示对不同候选token的偏好程度。接着，使用温度参数\\(T\\)对logits进行缩放，再将其转换为概率分布。核心计算公式为：

\\\[\\text{新logits} = \\frac{\\text{原始logits}}{T} \\\]

缩放后的logits经过softmax函数得到最终用于采样的概率分布：

\\\[P(\\text{token}\_i) = \\frac{e^{\\text{logits}\_i / T}}{\\sum\_j e^{\\text{logits}\_j / T}} \\\]

从该公式可以看出：

*   当\\(T \\to 0\\)时，概率分布高度集中于logits最大值对应的token，其他token的概率趋近于0。此时softmax近似于one-hot分布，模型输出几乎确定，随机性极低。
*   当\\(T \\to +\\infty\\)时，所有logits的差异被极大削弱，概率分布趋近均匀分布，输出接近随机等可能选择。
*   通常温度取值在\\((0, 1\]\\)之间，越接近0，模型越倾向于选择最高概率的token；越接近1，模型保留更多原始预测分布。

因此，温度参数\\(T\\)实质上调节的是**高概率token的相对优势程度**，从而控制生成过程中的随机性与多样性。取值越高，概率分布越平滑，输出越多样、越不可预测；取值越低，概率分布越尖锐，输出越集中、越稳定。

![https://github.com/sukhitashvili/GenAI_parameters_temperature_topK_topP/blob/main/generated_images/temperature.png](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/11.jpg)

以上推理策略构成了LLM的核心推理超参数体系，虽然还存在其他常见参数，但以下是最常被调整的一组：

采样参数：

*   do\_sample（布尔型）  
    采样总开关：设置为True时启用随机采样，设置为False时采用确定性解码（贪婪搜索或集束搜索）。
*   temperature（浮点型）  
    采样随机性调节：取值范围通常为0~1，数值越低生成结果越确定，越高则越多样。
*   top\_k（整型）  
    固定候选池采样：从概率最高的k个token中采样，k=0表示禁用此策略，常用取值范围为10~100。
*   top\_p（浮点型）  
    动态候选池采样：从累积概率达到p的token集合中采样，p=1.0表示禁用此策略，常用取值范围为0.7~0.95。

生成控制参数：

*   max\_new\_tokens（整型）  
    生成长度控制：限制模型新生成的token数量上限，需适配模型上下文窗口大小，常用取值范围为500~2000。
*   repetition\_penalty（浮点型）  
    重复惩罚系数：当某个token在历史生成序列中出现过时，对其对应logits按比例进行缩放处理，以降低其再次生成的概率，从而缓解循环输出与冗余表达。1.0表示关闭该机制，推荐取值为1.1～1.3，过高可能导致文本不自然或语义断裂。该参数在开源推理框架（如Transformers、vLLM、llama.cpp等）中应用更为广泛。
*   presence\_penalty（浮点型）  
    存在惩罚系数：只要某个token在当前生成序列中出现过，便对其后续再次生成施加固定幅度的惩罚，以鼓励模型引入新的词汇与内容，提高生成文本的多样性。0表示不启用该机制，常用取值为0～1.0。该参数在OpenAI系列接口及其兼容API中更为常见。
*   seed（整型）  
    可复现性控制：仅在do\_sample=True时生效，固定随机种子可保证相同输入与参数下生成结果完全一致。

![](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/12.jpg)

2.2 推理优化
--------

为了提升LLM在实际部署中的性能，推理优化主要从三个维度展开：计算优化以削减冗余运算，显存优化以降低资源占用，延迟与吞吐优化以提升服务效率与用户体验。目前已涌现出多种技术来应对这些挑战。

![](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/13.jpg)

### 2.2.1 计算优化

计算优化的核心目标是减少推理过程中的冗余计算，从而直接提升运算效率。其关键瓶颈在于Transformer架构中的注意力（Attention）层。该层旨在捕捉词与词之间的依赖关系，Attention计算方式要求每个token与序列中所有其他token进行交互。对于一个长度为n的序列，计算复杂度为O(n²)。这意味着随着对话历史增长或上下文长度增加，计算开销会呈平方级上升，成为推理速度的主要瓶颈。

为了缓解这一问题，业界引入了多种优化技术，具体方法可参见[大模型量化及低成本部署最佳实践](https://modelscope.cn/learn/399?pid=344)，其中最基础且广泛应用的是KVCache（Key-Value缓存），它的核心思想很简单：

*   在模型生成新词时，前面已经处理过的词，它们的注意力中间结果（Key和Value）其实是可以重复使用的；
*   所以把这些结果存下来（就像做笔记一样），下次再用的时候直接拿出来，不用重新算一遍；
*   这样当模型生成很多词的时候，它只需要计算新词相关的内容，把历史部分都从缓存里直接拿出来，用起来非常快。

可以将KVCache类比为做读书笔记：已经记录过的内容下次无需重读，只需关注新增部分。几乎所有的LLM推理框架都支持KVCache，其本质就是缓存历史计算结果，避免重复计算。

不过，即使用了KVCache，注意力机制仍面临一个突出的效率瓶颈：每当需要处理新词时，不仅要计算新词与已有历史词之间的关系，还需频繁将历史数据从容量较大但读写速度较慢的GPU显存中，读取到GPU上速度极快但容量非常有限的缓存中进行计算，完成计算后再将结果写回显存。这种频繁的数据搬运在硬件层面非常耗时，有时甚至比实际的计算过程更慢。这就像搬运一整箱书所花费的时间，反而超过真正阅读的时间。

![https://www.dailydoseofds.com/p/kv-caching-in-llms-explained-visually/](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/2.gif)

为了解决这个问题，又出现了另一种聪明的优化方法，叫FlashAttention。它和KVCache的思路不同：KVCache是通过记住过去算过的内容来避免重复计算，而FlashAttention则专注于减少计算过程中对显存的反复读写。它的核心思想是：既然高速缓存太小，装不下全部历史数据，那就不要试图一次性处理所有数据，而是把整个注意力计算任务拆解成多个小任务，每个小任务只涉及一小部分历史数据：

*   FlashAttention会把历史的Key和Value按顺序切成若干小块，每一块的大小刚好能完整放进GPU的高速缓存里；
*   处理当前新词时，它依次将每一个小块从显存加载到缓存中，在这个小块内部完成新词与该部分历史词的所有计算，并立即把中间结果累加到最终输出中；
*   由于每个小块只被读取一次、用完就丢，而且不需要把中间过程的数据存回显存，整个计算过程就大大减少了在显存和缓存之间来回搬运数据的次数。

能这么做的关键是先重新安排数据的访问顺序和计算顺序，再把几步操作合并起来一起处理，这样GPU的处理效率就被大大提高了。现代的很多主流推理框架都已经支持或集成了FlashAttention相关优化，这是当前提升LLM推理性能的常见做法之一。

### 2.2.2 显存优化

显存优化的目标是在有限硬件资源下支持更大参数规模或更长上下文模型的运行。由于LLM的参数与中间计算状态均需占用显存，高效的内存管理至关重要。主要优化手段可归纳为三类，包括模型量化、模型并行以及显存卸载。其中，前文已提及的模型量化技术通过降低数值精度来压缩模型体积，是目前最常用的显存压缩方法。

模型并行是在单块GPU显存无法容纳整个模型时的解决方案。例如，一个70B参数的模型，如果以FP16格式存储，大约需要140GB显存，而一块A100显卡只有80GB显存，显然无法一次性加载整个模型。这时就需要将模型拆分到多张GPU上运行。主流的并行方案有两种：

1.  张量并行（Tensor Parallelism, TP）  
    核心思路是将Transformer层中的大型矩阵（如Q、K、V的投影矩阵或前馈网络FFN的权重矩阵）按行或列切分，然后分配到多张GPU上。每张GPU同时参与同一层的计算，从而分摊显存压力。
    
2.  流水线并行（Pipeline Parallelism, PP）  
    核心思路是将Transformer的层按顺序拆分到不同GPU上。例如，将前20层放在第一张GPU，中间20层放在第二张GPU，后20层放在第三张GPU。推理请求依次经过这些GPU，就像流水线一样完成计算。
    

![https://arxiv.org/abs/2504.14775](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/14.jpg)

当量化与并行策略仍无法满足显存需求时，可采用显存卸载（LLM Offloading）作为折中方案。其核心思想是将不频繁访问的数据，例如部分模型权重、推理过程中生成的KVCache或中间激活值，临时迁移至CPU内存，甚至进一步卸载到磁盘中，仅在需要时再加载回GPU显存。这种以时间换空间的策略，能够在显存受限的硬件环境下，有效支持更大规模模型的运行。

### 2.2.3 延迟与吞吐优化

在服务端部署场景中，推理优化需同时追求高吞吐（降低单位请求成本）与低延迟（提升用户体验），这两者直接关系到服务的可行性与竞争力。常见技术包括：

1.  Continuous Batching（动态批处理）：提升GPU利用率 → 主要优化吞吐  
    传统静态批处理存在明显弊端：整个批次需等待最慢的请求完成后才能进行下一批次处理。例如，当批次大小设为8，若前7个请求各生成100个token，而第8个请求生成1000个token，则即便前7个请求早已完成，仍需等待第8个请求结束，这导致GPU在大部分时间处于空闲状态。  
    相较之下，动态批处理策略旨在通过即时将新请求添加到当前处理批次中，并持续填充以维持GPU的高利用率来克服上述问题。为有效管理因请求长度差异导致的显存分配挑战，该策略引入了PagedAttention机制。该机制将每个请求的KVCache在逻辑上划分为固定大小的块（例如每块存储16个token的键值对），并依据序列增长的实际需求，动态分配或释放物理上非连续的显存块。这种按需分配非连续内存页的方式，不仅减少了显存碎片，也显著提升了系统整体吞吐量。
    
2.  Speculative Decoding（推测解码）：降低生成延迟 → 主要优化延迟  
    自回归生成在生成长文本时通常延迟较高（例如生成1000个token可能需要约10秒）。为改善此问题，推测解码技术引入了轻量级的草稿模型（Draft Model）。该模型预先生成候选token序列，随后由主LLM模型对token序列进行验证和修正，从而减少重复计算，显著降低整体生成延迟。流程如下：
    
    *   由草稿模型基于当前上下文快速生成后续k个候选token（如k=5）。
    *   将当前上下文及候选token一次性输入主LLM模型，主LLM模型会检查每个候选token的生成概率（logits）是否达标，以判断其在当前上下文中是否合理：若概率较高，则视作合理；若概率较低，则视为不合理。
    *   如果候选token合理，则直接保留；若部分不合理，则回滚到合理位置，主LLM模型继续生成后续token。
3.  Prefix Caching（前缀缓存）：固定Prompt的加速技巧 → 可优化延迟与吞吐  
    在许多实际应用中（如客服系统、代码生成），用户请求常包含一段固定的提示词前缀。前缀缓存技术将该固定前缀对应的KVCache计算结果进行缓存。当后续请求包含相同前缀时，可直接复用缓存结果，仅需计算用户输入的动态部分。前缀缓存避免了每次请求都重复计算固定提示词，从而让用户输入后几乎立刻就能看到首个回应，并让服务器同时服务更多人。
    

![](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/15.jpg)

### 2.2.4 从Prefill与Decoder视角理解LLM推理

前面介绍的各类推理优化方法从计算、显存和延迟等多个维度加速了LLM的文本生成过程。这些技术的本质目标是解决同一个核心问题，即如何高效完成自回归生成。  
与常见的深度学习分类或回归模型通常只需一次前向计算得到结果不同，LLM的文本生成过程是自回归的。LLM模型每次只生成一个token，然后将其加入上下文，再基于更新后的上下文生成下一个token，也就是说，输出不是一次性产生，而是逐步累积生成的。这种生成方式带来两个主要挑战：

1.  每生成一个新token都需要参考前面已生成的上下文；
2.  如果每一步都重新计算整个上下文，计算量将非常大，效率低下。

为应对这些问题，LLM的推理过程通常被拆解为Prefill（预填充）和Decoder（自回归解码）两个阶段。理解这两个阶段有助于理解各种优化技术的设计逻辑：

*   Prefill 阶段：一次性完整计算，为生成做准备  
    Prefill阶段对已有输入进行完整计算，为后续的文本生成准备好所有必需的上下文信息。可以把Prefill想象成写论文前将所有参考资料整理好放在桌上，方便后续查阅。Prefill阶段的特点是对整个输入序列进行完整计算，计算量与输入长度成正比。如果Prompt很长，例如多轮对话或长文档，Prefill本身可能会消耗较多时间。具体操作包括：
    
    1.  将用户输入的完整Prompt一次性送入模型；
    2.  计算每个token的隐藏状态以及注意力所需的Key和Value向量；
    3.  将这些结果存储到KVCache中，后续生成文本时直接复用。
*   Decoder阶段：N次小计算，逐个生成新token  
    Decoder逐个生成新token，每生成一个token都参考前面计算好的信息并更新状态。这个阶段就像边写文章边查资料，每次写一个词：
    
    1.  使用已有上下文和KVCache；
    2.  预测下一个token的概率分布；
    3.  选择一个token并添加到序列中；
    4.  更新KVCache；
    5.  重复上述步骤直到生成完成。

![https://www.themoonlight.io/en/review/ghidorah-fast-llm-inference-on-edge-with-speculative-decoding-and-hetero-core-parallelism](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/16.jpg)

由于Decoder每生成一个token都需要进行一次计算，生成N个token就需要进行N次计算。这也解释了LLM生成延迟通常较高，输出是逐步产生而非一次性完成。通过这种分阶段理解，可以更清楚地看到LLM推理优化的逻辑：Prefill主要应对长输入序列带来的高效计算挑战，Decoder主要解决逐步生成的延迟问题，而各类优化方法正是针对这两类计算瓶颈进行设计的：

*   KVCache/PagedAttention：减少Decoder对历史token的重复计算；
*   FlashAttention：加速Prefill和Decoder的注意力计算；
*   模型量化：降低计算成本和显存占用；
*   Speculative Decoding：减少Decoder步骤，降低延迟；
*   Prefix Caching：对固定前缀进行Prefill缓存，避免重复计算。

2.3 推理框架
--------

前文介绍了LLM推理阶段的关键配置参数及相关优化技术，但在实际应用中，开发者通常无需从头实现这些技术，可直接采用已集成此类优化技术的LLM推理框架。常见的LLM推理框架可参阅：[Choosing the right inference framework](https://bentoml.com/llm/getting-started/choosing-the-right-inference-framework)，这些框架通过深度优化计算、显存与通信，在保障输出正确性的同时，实现高吞吐、低延迟，并便于以API或微服务形式部署。

![](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/17.jpg)

框架的选择并无放之四海皆准的答案，通常取决于具体部署场景、使用需求及基础设施条件。下面将按典型场景对常见的推理框架进行分类介绍：

**🚀面向高并发在线服务**

适用于需同时处理大量请求的生产环境，重点关注高吞吐、低延迟与可扩展性：

*   vLLM（非常常用）：侧重高吞吐与高效内存管理，适合大批量并发请求的低延迟API服务。
*   TGI（Text Generation Inference）（常用）：Hugging Face推出的推理方案，支持动态批处理、多模型管理与量化，适合云端部署。
*   Triton Inference Server（较常用）：NVIDIA的企业级推理平台，支持多框架模型与动态更新，适合复杂生产环境。
*   DeepSpeed‑Inference / DeepSpeed‑MII（较常用）：微软推出的优化引擎，专注于显存优化与推理性能提升。
*   XInference（较常用/新兴）：支持高吞吐、多线程优化与量化，适合GPU高并发服务。

**🖥️ 面向本地离线与快速实验**

适合个人电脑、开发机或资源受限环境，强调快速上手与本地推理：

*   llama.cpp（非常常用）：极轻量的C/C++实现，可在无GPU的笔记本上运行，常用于本地测试与快速实验。
*   Ollama（常用）：基于llama.cpp，提供更友好的CLI/GUI体验，便于本地部署与验证。
*   MLC‑LLM（较常用）：支持跨平台（如手机、浏览器等）高效运行，适合边缘推理与轻量级离线应用。

**🔧 面向特定硬件优化**

这些框架通常针对GPU或特定加速器进行了深度优化：

*   TensorRT‑LLM（常用，适用于 NVIDIA GPU）：基于TensorRT的推理引擎，在NVIDIA GPU上性能优异，适合高性能负载。
*   LMDeploy（常用）：注重高性能推理与分布式部署，适合多GPU与LLM推理场景。
*   SGLang（较常用）：专注于结构化任务与复杂生成流程，在特定场景下显著提升吞吐量与响应速度，兼顾性能与可控性。

**🎉 其他常用生态与工具**

这些工具和格式在推理框架之外，常用于完善开发流程、简化部署或保证兼容性：

*   GGUF/GGML（非常常用）：轻量级模型存储格式，保证模型在不同推理框架间的兼容性，广泛用于本地与边缘部署。
*   Text Generation WebUI（常用）：提供完整的Web界面，方便快速搭建和调试对话模型，支持多种后端推理框架。
*   LobeChat（常用）：开源、现代化的聊天机器人框架，专注于提供美观且功能强大的用户界面，支持多种后端模型服务和丰富的插件生态，适合构建面向团队或社区的生产级LLM应用前端。
*   FastChat（较为常用）：方便搭建对话服务前端，支持多种推理后端接入，适合快速开发和原型验证，但是更新频率较慢。

一句话指南：根据硬件条件和场景需求选择LLM推理框架，追求高并发吞吐选vLLM，深挖NVIDIA显卡极致性能选TensorRT-LLM，最佳通用体验选TGI，若在个人电脑上快速本地运行和测试，则Ollama或llama.cpp是最佳入口。需要快速搭建包含Web UI的完整对话服务时，可根据需求选用Text Generation WebUI（侧重个人测试与模型调试）或LobeChat（侧重生产级应用与用户体验）来统一接入上述后端。

![https://pub.towardsai.net/from-local-to-production-the-ultimate-ollama-to-vllm-migration-guide-571faa8cbfde](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A06-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/imgs/18.jpg)

选型时，重点考察框架对模型架构、多GPU并行及推测解码的支持。鉴于推理框架更新快、优化差异大，建议优先选择接口标准、与环境解耦的方案，减少对单一技术栈的依赖。一个可行的路径是：可先用Ollama本地验证原型，再迁移至vLLM/TGI提升批处理和GPU利用率；当业务扩展至多GPU或跨云部署，可引入XInference、LMDeploy或TensorRT-LLM实现分布式推理与运维优化。关于更全面的框架对比分析，详见[LLM推理框架的全面分析与选型指南](https://mp.weixin.qq.com/s/vadmuBp4TVr7Lug-7ALyCg)。

最后，为方便横向对比，下表汇总了以上框架与工具的核心信息：

框架

推理性能

核心优势

典型适用场景

备注

vLLM

高

高吞吐、低延迟、内存优化

高并发在线API服务

🚀高并发首选

TGI

中上

多模型管理、动态批处理、量化

云端通用部署

🌟云端首选

Triton Inference Server

高

企业多框架支持、动态更新

企业生产、多模型部署

\-

DeepSpeed‑Inference

高

显存优化

LLM、多GPU推理

\-

DeepSpeed‑MII

高

开箱即用的模型服务

LLM、多GPU推理

基于DeepSpeed-Inference构建

XInference

高

多线程、高吞吐、量化支持

GPU高并发服务

新兴框架

llama.cpp

中低

极轻量、无GPU可运行

本地测试、快速实验

🌟本地测试首选

Ollama

中

CLI/GUI友好、本地部署方便

本地快速验证、轻量推理

🌟本地原型首选

MLC‑LLM

中

跨平台、高效

边缘设备、轻量离线

\-

TensorRT‑LLM

极高（NVIDIA GPU）

GPU深度优化、高性能负载

高性能GPU、分布式推理

🚀NVIDIA首选

LMDeploy

高

多GPU分布式

LLM、多GPU推理

\-

SGLang

中上

结构化任务优化、复杂流程

特定高吞吐场景

\-

GGUF/GGML

\-

跨框架轻量存储

本地/边缘部署

🌟跨框架存储首选

Text Generation WebUI

\-

完整Web界面、快速搭建

本地/云端快速对话服务

\-

LobeChat

依赖后端

现代化 UI、多模型切换、插件扩展

个人/团队聊天机器人界面、快速接入各类模型

🌟前端界面/应用首选

FastChat

\-

前端快速搭建、多后端支持

原型验证、快速开发

\-

3 参考
====

*   [大模型学习1-大语言模型基础知识](https://www.cnblogs.com/luohenyueji/p/18644847)
*   [A Visual Guide to Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)
*   [一文详尽大型语言模型的四种量化技术](https://mp.weixin.qq.com/s/KQxhrYtDTudRZkxgu5yN5g)
*   [LLM Quantization Techniques](https://corefragment.com/blog/llm-quantization-techniques)
*   [大模型量化技术解析和应用](https://modelscope.cn/learn/399?pid=488)
*   [一文看懂Qwen3本地部署的配置要求](https://zhuanlan.zhihu.com/p/1902792207527838412)
*   [Qwen3 Speed Benchmark](https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html)
*   [大模型量化及低成本部署最佳实践](https://modelscope.cn/learn/399?pid=344)
*   [Choosing the right inference framework](https://bentoml.com/llm/getting-started/choosing-the-right-inference-framework)
*   [LLM推理框架的全面分析与选型指南](https://mp.weixin.qq.com/s/vadmuBp4TVr7Lug-7ALyCg)

本文来自博客园，作者：[落痕的寒假](https://www.cnblogs.com/luohenyueji/)，转载请注明原文链接：[https://www.cnblogs.com/luohenyueji/p/19605118](https://www.cnblogs.com/luohenyueji/p/19605118)

![](https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/wechat/content/%E5%8A%A0%E6%B2%B9%E9%B8%AD.gif)