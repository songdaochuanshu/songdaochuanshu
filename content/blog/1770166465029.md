---
layout: post
title: '吴恩达深度学习课程五：自然语言处理  第三周：序列模型与注意力机制（四）语音识别和触发字检测'
date: "2026-02-04T00:54:25Z"
---
吴恩达深度学习课程五：自然语言处理 第三周：序列模型与注意力机制（四）语音识别和触发字检测
=============================================

此分类用于记录吴恩达深度学习课程的学习笔记。  
课程相关信息链接如下：

1.  原课程视频链接：[\[双语字幕\]吴恩达深度学习deeplearning.ai](https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&from_spmid=playlist.playlist-detail.0.0&is_story_h5=false&mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1713085655&unique_k=DfBgvFW&up_id=8654113&vd_source=e035e9878d32f414b4354b839a4c31a4)
2.  github课程资料，含课件与笔记:[吴恩达深度学习教学资料](https://github.com/robbertliu/deeplearning.ai-andrewNG)
3.  课程配套练习（中英）与答案：[吴恩达深度学习课后习题与答案](https://blog.csdn.net/u013733326/article/details/79827273)

本篇为第五课第三周的内容，[3.9](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=181)到[3.10](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=182)的内容，同时也是本周理论部分的最后一篇。

* * *

本周为第五课的第三周内容，与 CV 相对应的，这一课所有内容的中心只有一个：**自然语言处理（Natural Language Processing，NLP）**。  
应用在深度学习里，它是专门用来进行**文本与序列信息建模**的模型和技术，本质上是在全连接网络与统计语言模型基础上的一次“结构化特化”，也是人工智能中**最贴近人类思维表达方式**的重要研究方向之一。  
**这一整节课同样涉及大量需要反复消化的内容，横跨机器学习、概率统计、线性代数以及语言学直觉。**  
语言不像图像那样“直观可见”，更多是抽象符号与上下文关系的组合，因此**理解门槛反而更高**。  
因此，我同样会尽量补足必要的背景知识，尽可能用比喻和实例降低理解难度。  
本周的内容关于**序列模型和注意力机制**，这里的序列模型其实是**指多对多非等长模型**，这类模型往往更加复杂，其应用领域也更加贴近工业和实际，自然也会衍生相关的模型和技术。而注意力机制则让模型在长序列中学会主动分配信息权重，而不是被动地一路传递。二者结合，为 Transformer 等现代架构奠定了基础。

本篇的内容关于**语音识别和触发字检测**，是 seq2seq 模型在音频数据上的应用。

1\. 音频数据（Audio data）
====================

音频数据虽然和文本数据同样都为序列数据，但是如果我们希望实现相关的应用，所寻找到的数据集样本往往都是**一段段连续的录音**，无法直接输入模型。  
因此，就像为文本数据构建词典一样，在使用 seq2seq 模型完成在语音领域的任务时，我们同样需要对音频数据进行预处理，而这就涉及到音频数据本身的特点。

1.1 音位（Phoneme）
---------------

在展开音频数据的预处理方式之前，有必要先引入一个语言学中的概念：**音位（phoneme）**。

音位是一种**抽象的语音单位**，其定义并非基于声学相似性，而是基于**是否能够区分词义**。  
在某一语言中，如果两个发音单元的替换会导致词义变化，它们就属于不同的音位；反之，即使在物理发音上存在差异，只要不影响词义，它们仍被视为同一音位。  
换句话说，音位关心的是**语言系统内部能区分意义的功能**，而不是具体的发音表现。

通过大量对比，语言学家逐步归纳出某一语言的音位系统。**在英语中，音位通常借助音标来表示，但音标与音位又并不等同**：音标只是描述发音的工具，而音位是一种功能性分类结果。同一个音位在不同语音环境中可能呈现略有不同的实际发音形式（如口音差异），但只要这些差异不承担区分语义的功能，它们仍被视为同一音位的不同实现。  
![image.png](https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260203155945196-1250283629.png)  
这便是音位的基本概念，到这里，一个自然而然的想法就是**把音频数据处理为连续的音位序列**，就像文本数据一样进行处理输入模型。  
然而，我们刚刚也提到了，音位虽然可以代表语义，但是**这是我们人为归纳的特征而不是音频本身的属性**。因此，在实验中，**音位并不是可以直接从连续的音频波形中观测得到的量**。  
其获取过程高度依赖**人工标注**、规则设计或复杂的对齐模型，这也在实践中限制了其作为模型直接输入的可行性。  
这一现实，直接推动了后续更偏向信号层面的语音表示方法的发展，也为声谱图等特征形式的广泛应用奠定了背景。

1.2 声谱图（Spectrogram）
--------------------

由于音位是人为抽象的单位，无法直接从连续的音频波形中观测得到，因此在实际语音处理与建模中，我们更倾向于使用**信号层面的连续特征表示**，其中最常用的表示方式之一就是**声谱图（spectrogram）**。

声谱图是一种将音频信号在**时间与频率域**上进行表示的二维图像。  
简单来说，它将连续的音频波形分割为短时片段，并对每个片段计算频谱能量，从而得到**时间-频率矩阵**，矩阵中的每个值反映该时刻该频率成分的强度，就像这样：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260203155952737-472972628.png)

通过这种方式，原本**连续的波形被转换为一组能够揭示语音细节的特征**，既保留了声音的动态变化，也便于计算机处理。  
在计算机处理时，声谱图的每一列包含该时间片段所有频率的能量值，可以看作一个多维向量。因此，我们通常**把声谱图的每一列视为一个时间步的输入向量**。  
这样，原本二维的时间-频率矩阵就被转化为**时间序列的特征向量序列**，与文本序列类似，使模型能够在连续语音中捕捉语义和声学模式。

声谱图的关键优势在于**无需人工标注音位的同时保留了丰富的声学信息，并且其二维矩阵形式可以直接作为模型输入**，实现端到端语音识别、声学建模或语音生成。  
了解了对音频数据的基本处理逻辑后，现在就来看看其应用：

2\. 语音识别（Speech Recognition）
============================

对音频数据最常见的应用领域就是语音识别。生活中，我们最常用的例子可能是微信的语音转文字，也包括语音助手、电话客服的语音输入等。  
语音识别的核心任务是**将连续的音频信号映射为文字序列**。由于音频本身是连续信号，而文字序列是离散符号序列，因此，这一任务本质上也是一个 **seq2seq 问题**。  
一个主流且常见的训练方式就是应用我们刚刚介绍的带[注意力机制](https://www.cnblogs.com/Goblinscholar/p/19563950)的编码解码框架：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260203155927580-326667711.png)  
此外，还存在一种独特的技术，我们称为 **CTC（Connectionist Temporal Classification）** 。  
CTC 是一种**专门用于处理输入输出长度不匹配的序列学习方法**，非常适合语音识别这样的任务。它的核心思想是：**允许模型在连续的时间步上输出“空白”或重复符号，从而自动对齐输入序列与输出序列**。  
它提出于 06 年的一篇论文：[Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks](https://sferics.idsia.ch/pub/juergen/icml2006.pdf)  
可以发现，CTC 的提出较早，因此也并没有使用编码解码框架，而是**等长多对多模型**框架。不过如今 CTC 也并没有被完全淘汰，它仍常见于一些混合方案中。  
我们简单展开如下：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260203155951242-1300096674.png)  
再复述一下其核心思想： CTC 通过引入 **blank（空白）符号** 和 **重复合并规则** 来对齐输入与输出，来实现端到端训练，无需在标签中人工对齐每一帧。  
当然，你也会发现它对长距离依赖建模能力有限，对长句子性能很大可能不如注意力机制，了解即可。

3\. 触发字检测（Trigger Word Detection / Keyword Spotting）
====================================================

对于触发字检测我们也并不陌生，生活中最常见的例子包括语音助手的唤醒词“Hey Siri”“小爱同学”“Alexa”，只有检测到这些触发词后，设备才会进入完整语音识别流程。

不同于语音识别，触发字检测任务更为精简，它在建模中关注的问题是：**在连续语音流中，判断某个特定关键词是否被说出，以及它出现的大致时间位置。**  
因此，一个关键点在于：**触发字检测模型因其实时性更适合多对多等长模型，模型每个时间帧预测一个触发概率或二分类信号，表示当前帧是否属于触发词的一部分。**  
简单展开如下：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260203155928163-1676940697.png)  
传播过程并不复杂，但有一点需要注意：**触发字检测的任务特征决定了其数据往往是不平衡的**，即绝大多数标签都为 0 ，这导致即使模型全部输出 0 ，也能得到较好的指标，从而导致错误判断和部署。  
对此，一种常用缓解策略是**扩展正样本标签**：不仅将触发词对应的帧标记为 1，还将其之后若干帧也标记为 1，形成一个时间段的正样本窗口，这既平衡了样本，也符合实际触发的延迟容忍需求。

总结来说，触发字检测本质是**低延迟的序列二分类任务**。建模逻辑为：**先将音频转化为时间序列特征，再利用多对多等长模型预测每一帧的触发概率，也可通过平滑或滑窗处理得到最终触发决策**。

4.总结
====

概念

原理

比喻

**音位（Phoneme）**

语言学中的抽象单位，基于是否能区分词义进行分类；同一音位在不同环境下可有不同发音，但功能相同。

就像文字中的字母，不同字母组合产生不同单词，但同一字母在不同字体中仍表示相同字母。

**声谱图（Spectrogram）**

将音频分帧并计算每帧的频谱能量，得到时间-频率矩阵；每列作为时间步输入向量，用于模型训练。

好比把连续的声音切成一格格“照片”，每格显示不同频率的亮度，连续播放形成动态影像。

**语音识别（Speech Recognition）**

将连续音频信号映射为文字序列，可用注意力编码解码框架处理 seq2seq 问题，也可用 CTC 进行端到端训练。

就像把一段连续的河流水流（声音波形）逐段翻译成文字，注意力机制像有导游指引每段对应文字，CTC像自动对齐标记。

**CTC（Connectionist Temporal Classification）**

输入为时间序列特征；允许输出空白符和重复符号，通过合并规则对齐输出序列；无需逐帧标注。

好比在长河上放置浮标（空白符），只标记关键节点，最后整理成整段文字。

**触发字检测（Trigger Word Detection / Keyword Spotting）**

输入音频转时间序列特征；多对多等长模型预测每帧触发概率；可通过标签扩展、平滑或滑窗处理缓解数据不平衡。

就像警报系统监测连续流水声，只在听到特定声响（触发词）后报警，而非逐秒记录每滴水。