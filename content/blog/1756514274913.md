---
layout: post
title: '3. LangChain4j + 低阶 和 高阶 API的详细说明'
date: "2025-08-30T00:37:54Z"
---
3\. LangChain4j + 低阶 和 高阶 API的详细说明
==================================

3\. LangChain4j + 低阶 和 高阶 API的详细说明
==================================

@

目录

*   [3\. LangChain4j + 低阶 和 高阶 API的详细说明](#3-langchain4j--低阶-和-高阶-api的详细说明)
*   [LangChain4j + 低阶 和 高阶 API的详细说明](#langchain4j--低阶-和-高阶-api的详细说明)
    *   [LangChain4j在两个抽象层(低阶 / 高阶)提供不同的 api](#langchain4j在两个抽象层低阶--高阶提供不同的-api)
        *   [low level 低阶](#low-level-低阶)
        *   [high level 高阶](#high-level-高阶)
    *   [low level 低阶 API 的使用](#low-level-低阶-api-的使用)
        *   [大模型中的Token VS Web开发中的Token](#大模型中的token-vs-web开发中的token)
    *   [high level 高阶 API 的详细使用](#high-level-高阶-api-的详细使用)
*   [最后：](#最后)

LangChain4j + 低阶 和 高阶 API的详细说明
==============================

[https://docs.langchain4j.dev/get-started/](https://docs.langchain4j.dev/get-started/)

[https://docs.langchain4j.dev/tutorials/chat-and-language-models/](https://docs.langchain4j.dev/tutorials/chat-and-language-models/)

LLM 目前有两种 API 类型：

*   `LanguageModel`。它们的 API 非常简单 - 接受 `String` 作为输入并返回 `String` 作为输出。 这种 API 现在正在被聊天 API（第二种 API 类型）所取代。
*   ChatModel。这些接受多个 `ChatMessage` 作为输入并返回单个 `AiMessage` 作为输出。 `ChatMessage` 通常包含文本，但某些 LLM 也支持其他模态（例如，图像、音频等）。 这类聊天模型的例子包括 OpenAI 的 `gpt-4o-mini` 和 Google 的 `gemini-1.5-pro`。

**LangChain4j 不会再扩展对** `LanguageModel` **的支持， 因此在所有新功能中，我们将使用** **ChatModel** API。

`ChatModel` 是 LangChain4j 中与 LLM 交互的低级 API，提供最大的能力和灵活性。 还有一个高级 API（[AI 服务](https://docs.langchain4j.info/tutorials/ai-services)），我们将在介绍完基础知识后再讨论。

除了 `ChatModel` 和 `LanguageModel` 外，LangChain4j 还支持以下类型的模型：

*   `EmbeddingModel` - 这种模型可以将文本转换为 `Embedding`。
*   `ImageModel` - 这种模型可以生成和编辑 `Image`。
*   `ModerationModel` - 这种模型可以检查文本是否包含有害内容。
*   `ScoringModel` - 这种模型可以对查询的多个文本片段进行评分（或排名）， 本质上确定每个文本片段与查询的相关性。这对 [RAG](https://docs.langchain4j.info/tutorials/rag) 很有用。 这些将在后面介绍。

现在，让我们仔细看看 `ChatModel` API。

    public interface ChatModel {
    
        String chat(String userMessage);
        
        ...
    }
    

如您所见，有一个简单的 `chat` 方法，它接受 `String` 作为输入并返回 `String` 作为输出，类似于 `LanguageModel` 。 这只是一个便捷方法，让您可以快速轻松地进行试验，而无需将 `String` 包装在 `UserMessage` 中。

​

LangChain4j在两个抽象层(低阶 / 高阶)提供不同的 api
-----------------------------------

*   [https://docs.langchain4j.dev/intro](https://docs.langchain4j.dev/intro)

> *   LangChain4j 在两个抽象层次上运行：  
>     \- 低层次。在这个层次上，您拥有最大的自由度和访问所有低级组件的权限，如 [ChatModel](https://docs.langchain4j.dev/tutorials/chat-and-language-models),、`UserMessage`、`AiMessage`、`EmbeddingStore`、`Embedding` 等。 这些是您的 LLM 驱动应用程序的"原语"。 您可以完全控制如何组合它们，但需要编写更多的粘合代码。  
>     \- 高层次。在这个层次上，您使用高级 API（如 [AI 服务](https://docs.langchain4j.info/tutorials/ai-services)）与 LLM 交互， 它隐藏了所有复杂性和样板代码。 您仍然可以灵活地调整和微调行为，但是以声明式方式完成。

### low level 低阶

ChatModel 接口如下的默认实现的方法：

ChatModel提供的--种极其简便的方法：如下：

    default String chat(String userMessage) {
        ChatRequest chatRequest = ChatRequest.builder()
                .messages(UserMessage.from(userMessage))
                .build();
    
        ChatResponse chatResponse = chat(chatRequest);
    
        return chatResponse.aiMessage().text();
    }
     
    

    @GetMapping(value = "/langchain4j/hello")
    public String hello(@RequestParam(value = "prompt", defaultValue = "你是谁") String prompt)
    {
        String result = chatModel.chat(prompt);
    
        System.out.println("通过langchain4j调用模型返回结果：\n"+result);
    
        return result;
    }
     
    

* * *

### high level 高阶

low level 低阶 API 的使用
--------------------

导入相关的依赖：

    
        <dependencies>
    
            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-starter-web</artifactId>
            </dependency>
            <!--langchain4j-open-ai 基础-->
            <!--所有调用均基于 OpenAI 协议标准，实现一致的接口设计与规范LangChain4j 提供与许多 LLM 提供商的集成
            从最简单的开始方式是从 OpenAI 集成开始https://docs.langchain4j.dev/get-started    -->
            <dependency>
                <groupId>dev.langchain4j</groupId>
                <artifactId>langchain4j-open-ai</artifactId>
            </dependency>
            <!--langchain4j 高阶-->
            <dependency>
                <groupId>dev.langchain4j</groupId>
                <artifactId>langchain4j</artifactId>
            </dependency>
    
    
            <dependency>
                <groupId>junit</groupId>
                <artifactId>junit</artifactId>
                <version>3.8.1</version>
                <scope>test</scope>
            </dependency>
        </dependencies>
    

配置对应大模型的配置类。

    package com.rainbowsea.langchain4j02lowhighapi.config;
    
    import dev.langchain4j.model.chat.ChatModel;
    import dev.langchain4j.model.openai.OpenAiChatModel;
    import org.springframework.context.annotation.Bean;
    import org.springframework.context.annotation.Configuration;
    
    /**
     * @Date 2025-05-27 22:04
     * @Description: 知识出处 https://docs.langchain4j.dev/get-started
     */
    @Configuration
    public class LLMConfig
    {
        @Bean(name = "qwen")
        public ChatModel chatModelQwen()
        {
            return OpenAiChatModel.builder()
                    .apiKey(System.getenv("aliQwen_api"))
                    .modelName("qwen-plus")
                    .baseUrl("https://dashscope.aliyuncs.com/compatible-mode/v1")
                    .build();
        }
    
        /**
         * @Description: 知识出处，https://api-docs.deepseek.com/zh-cn/
         */
        @Bean(name = "deepseek")  // 可以在根据@Resoure(name = "deepseek") 导入不同的实体类
        public ChatModel chatModelDeepSeek()
        {
            return
                    OpenAiChatModel.builder()
                            .apiKey(System.getenv("deepseek_api"))
                            .modelName("deepseek-chat")
                            //.modelName("deepseek-reasoner")
                            .baseUrl("https://api.deepseek.com/v1")
                            .build();
        }
    }
    
    

编写 Controller ，如下，其实就是跟我们编写的第一个连接大模型的 Hello World 是一样的

启动测试：

### 大模型中的Token VS Web开发中的Token

大模型当中的 Token

Web 开发中的 Token

      // http://localhost:9002/lowapi/api02
        @GetMapping(value = "/lowapi/api02")
        public String api02(@RequestParam(value = "prompt", defaultValue = "你是谁") String prompt)
        {
            ChatResponse chatResponse = chatModelDeepSeek.chat(UserMessage.from(prompt));
    
            String result = chatResponse.aiMessage().text();
            System.out.println("通过langchain4j调用模型返回结果："+result);
    
            TokenUsage tokenUsage = chatResponse.tokenUsage();
            System.out.println("本次调用消耗Token："+tokenUsage);
    
            result = result +"\t\n"+ tokenUsage;
    
            return result;
        }
    }
    

**ChatMessage 其实就是一个 prompt ，就是被大模型封装了一层，让其更好的被大模型读取识别而已。**

运行测试：

high level 高阶 API 的详细使用
-----------------------

*   [https://docs.langchain4j.dev/tutorials/ai-services/](https://docs.langchain4j.dev/tutorials/ai-services/)

翻译：

**AI Service 的高阶 API 的使用：**

具体的编码步骤如下：[https://docs.langchain4j.dev/tutorials/ai-services/#simplest-ai-service](https://docs.langchain4j.dev/tutorials/ai-services/#simplest-ai-service)

1.  **定义 AI Service 接口：**

我们知道，按照Java开发一般习惯，有接口就要有实现类 比如接口ChatAssistant，就会有实现类ChatAssistantImpl现在用高阶api- AIServics不用你自己写 impl实现类，交给langchain4j给你搞定。（接口名是随意的，你只要见名之意即可）

    package com.rainbowsea.langchain4j02lowhighapi.service;
    
    /**
     * 我们知道，按照Java开发一般习惯，有接口就要有实现类
     * 比如接口ChatAssistant，就会有实现类ChatAssistantImpl
     * 现在用高阶api-AIServics不用你自己写impl实现类，交给langchain4j给你搞定
     * <p>
     * 本次配置用的是langchain4j原生整合，没有引入sprinboot，不需要接口头上配置@AiService注解标签
     */
    public interface ChatAssistant {
        String chat(String prompt);
    }
    
    

LLMConfig类配置当中配置调用大模型的三件套（大模型的 Key，大模型 name，大模型的 url）

    
    import com.rainbowsea.langchain4j02lowhighapi.service.ChatAssistant;
    import dev.langchain4j.model.chat.ChatModel;
    import dev.langchain4j.model.openai.OpenAiChatModel;
    import dev.langchain4j.service.AiServices;
    import org.springframework.beans.factory.annotation.Qualifier;
    import org.springframework.context.annotation.Bean;
    import org.springframework.context.annotation.Configuration;
    
    /**
     * @Date 2025-05-27 22:04
     * @Description: 知识出处 https://docs.langchain4j.dev/get-started
     */
    @Configuration
    public class LLMConfig
    {
    
    
    
        @Bean(name = "qwen")
        public ChatModel chatModelQwen()
        {
            return OpenAiChatModel.builder()
                    .apiKey(System.getenv("aliQwen_api"))
                    .modelName("qwen-plus")
                    .baseUrl("https://dashscope.aliyuncs.com/compatible-mode/v1")
                    .build();
        }
    }
    

3.  **对我们自我编写的 AI Service 的接口类，配置器实现类的配置(配置指明那个大模型实现我们这个接口类)，配置好后，调用** `AiServices.create()`**方法就好创建好我们自定义的接口实现类。**

    
    import com.rainbowsea.langchain4j02lowhighapi.service.ChatAssistant;
    import dev.langchain4j.model.chat.ChatModel;
    import dev.langchain4j.model.openai.OpenAiChatModel;
    import dev.langchain4j.service.AiServices;
    import org.springframework.beans.factory.annotation.Qualifier;
    import org.springframework.context.annotation.Bean;
    import org.springframework.context.annotation.Configuration;
    
    /**
     * @Date 2025-05-27 22:04
     * @Description: 知识出处 https://docs.langchain4j.dev/get-started
     */
    @Configuration
    public class LLMConfig
    {
    
        @Bean(name = "qwen")
        public ChatModel chatModelQwen()
        {
            return OpenAiChatModel.builder()
                    .apiKey(System.getenv("aliQwen_api"))
                    .modelName("qwen-plus")
                    .baseUrl("https://dashscope.aliyuncs.com/compatible-mode/v1")
                    .build();
        }
    
    
    
        // High-Api https://docs.langchain4j.dev/tutorials/ai-services#simplest-ai-service
        @Bean
        public ChatAssistant chatAssistant(@Qualifier("qwen") ChatModel chatModelQwen)
        {
            return AiServices.create(ChatAssistant.class, chatModelQwen);
        }
    
    }
    

AlService是如何工作的

[https://docs.langchain4j.dev/tutorials/ai-services/#how-does-it-work](https://docs.langchain4j.dev/tutorials/ai-services/#how-does-it-work)

4.  **最后编写对于业务的 Controller 类**。直接调用我们的接口类，因为该接口的实现类已经被我们通过**调用** `AiServices.create()`**方法就好创建好我们自定义的接口实现类。同时我们也将其加入** `@Bean`**加入到了 IOC 容器当中管理了，所以可以直接，通过** `@Resource` 注解注入。

    package com.rainbowsea.langchain4j02lowhighapi.controller;
    
    import com.rainbowsea.langchain4j02lowhighapi.service.ChatAssistant;
    import jakarta.annotation.Resource;
    import lombok.extern.slf4j.Slf4j;
    import org.springframework.web.bind.annotation.GetMapping;
    import org.springframework.web.bind.annotation.RequestParam;
    import org.springframework.web.bind.annotation.RestController;
    
    /**
     */
    @RestController
    @Slf4j
    public class HighApiController
    {
        @Resource
        private ChatAssistant chatAssistant;
    
        @GetMapping(value = "/highapi/highapi")
        public String highApi(@RequestParam(value = "prompt", defaultValue = "你是谁") String prompt)
        {
            return chatAssistant.chat(prompt);
        }
    }
    

运行测试：

最后：
===

> “在这个最后的篇章中，我要表达我对每一位读者的感激之情。你们的关注和回复是我创作的动力源泉，我从你们身上吸取了无尽的灵感与勇气。我会将你们的鼓励留在心底，继续在其他的领域奋斗。感谢你们，我们总会在某个时刻再次相遇。”