---
layout: post
title: '【H2O系列】包括人形机器人WBC相关论文小结'
date: "2025-02-21T00:36:40Z"
---
【H2O系列】包括人形机器人WBC相关论文小结
=======================

1\. 前言
======

这篇博客主要用于记录包括人形机器人WBC或locomotion相关论文小结。  
一方面便于日后自己的温故学习，另一方面也便于大家的学习和交流。  
如有不对之处，欢迎评论区指出错误，你我共同进步学习！  
PS：主要是备忘，不然看过就忘了。。。（汗

2\. 正文
======

先看数据集或者说动捕数据：

2.1 SMPL Skinned Multi-Person Linear (SMPL) Model
-------------------------------------------------

用数据来构建人体和mesh的，在这个上，后面的AMASS有所拓展，引入了motions，赋予了尸体灵魂（bushi  
详细查看：[https://blog.csdn.net/IanYue/article/details/127206953](https://blog.csdn.net/IanYue/article/details/127206953)  
**一个3D人体mesh由6890个网格顶点和23个关节点组成**

Skinned表示这个模型不仅仅是骨架点了，其是有蒙皮的，其蒙皮通过3D mesh表示，3D mesh如图所示，指的是在立体空间里面用三个点表示一个面，可以视为是对真实几何的采样，其中采样的点越多，3D mesh就越密，建模的精确度就越高（这里的由三个点组成的面称之为三角面片）。Multi-person表示的是这个模型是可以表示不同的人的，是通用的。Linear就很容易理解了，其表示人体的不同姿态或者不同升高，胖瘦（我们都称之为形状shape）是一个线性的过程，是可以控制和解释的（线性系统是可以解释和易于控制的）  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207133903358-427516087.png)

### 2.1.1姿态参数

pose parameters，含有\\(24\\times3\\)个参数，24个点，每个点含有相对于父节点的axis-angle 表达，也就是相对父节点的旋转角度：  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207134113886-446633494.png)  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207135104585-234034865.png)  
影响动作姿势：θ，72个参数，后69个值在-1到1之间，3\*23 + 3，影响23个关节点+1个root orientation的旋转。前三个控制root orientation，后面每连续三个控制一个关节点

### 2.1.2 形状参数

一组形状参数有着10个维度的数值去描述一个人的形状，每一个维度的值都可以解释为人体形状的某个指标，比如高矮，胖瘦等。  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207135041056-733097786.png)

![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207145605662-156476251.png)

2.2 AMASS: Archive of Motion Capture as Surface Shapes
------------------------------------------------------

2019.4.5

> 介绍了AMASS，这是一个庞大而多样的人体运动数据库，通过在一个共同的框架和参数化中表示它们，统一了15种不同的基于光学标记的运动捕捉数据集

1.  首先，我们开发了一种从标准动作捕捉（mocap）标记数据中准确恢复运动中的人的形状和姿势的方法。
2.  创建最大的公共人类运动数据库，使机器学习能够应用于动画和计算机视觉

只要知道这个是一个包含人体motions的数据集就好了，里面会有很多的dataset提供给我们下载。

见我的另一篇博客：[https://i.cnblogs.com/posts/edit;postId=18715051#postBody](https://i.cnblogs.com/posts/edit;postId=18715051#postBody)

何泰然大佬的连续三篇工作：

2.3 H2O：Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation
---------------------------------------------------------------------

IROS 2024.3.7  
CMU

> 何泰然大佬，b站也很有名！

个人总结主要贡献点包括：  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207141509132-797736365.png)

### 2.3.1 Retargeting(a)

将机器人的关节点映射，和SMPL的数据集的人体模型作距离的剃度下降，以最小化二者间的距离，这时人体的shape参数就需要改变了：  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207140934131-1816468315.png)  
shape-fitted的过程  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207141112253-565904694.png)  
作者还对比了一下如果不这么做的结果：如果直接把人体的关节点般过去，就会导致机器人的脚部距离过小 ，走路可能会绊倒。  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207141154543-1050745446.png)  
有一些动作比较特殊，机器人完成不了，所以需要去除：  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207142651510-630932729.png)

### 2.3.2 Sim-to-data

将2.1.1部分的得到的运动机器人数据集（H1）的作为输入，输入到ISSAC GYM中进行训练，让机器人可以跟踪数据点进行模仿学习

*   本体状态：

![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207143113204-1239813148.png)

*   目标状态：

![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207143145050-609815498.png)

*   奖励函数：

![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207143633153-1886385433.png)

### 2.3.3 Real-time Teleoperation

通过RGB相机输入人体动作，通过**HybrIK**进行3D人体姿态估计。

> 说到**HybrIK**，这里进行简要的记录：  
> 之前一直比较好奇他是如何通过RGB相机得到人体的3D姿态分析的：  
> ![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207145126811-637615164.png)  
> 关于HybrIK，详细查看L：[https://zhuanlan.zhihu.com/p/461640390](https://zhuanlan.zhihu.com/p/461640390)

2.4 OmniH2O: Universal and Dexterous Human-toHumanoid Whole-Body Teleoperation and Learnin
-------------------------------------------------------------------------------------------

2024.6.13  
目前我主要也在看这篇工作的开源代码。。。。。

2.4.1 abstarct
--------------

较之前进行了多种控制方式的扩展，比如：  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207152052918-1899900416.png)  
还公布了一个数据集：OmniH2O-6

2.4.2 从pipeline得到的对比
--------------------

![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250207153013563-1391564072.png)  
第一个部分几乎没有变化，还是retargeting

> (a) OmniH2O retargets large-scale human motions and filters out infeasible motions for humanoids.

第二个部分采用了模仿学习，先利用特权观测值训练一轮，然后去掉特权观测值，利用之前的几组历史本体观测值训练得到sim2real的policy网络

> (b) Our sim-to-real policy is distilled through supervised learning from an RL-trained teacher policy using  
> privileged information.

第三个部分区别在于，这里又拓展了遥操作的多样性，versital

> (c) The universal design of OmniH2O supports versatile human control interfaces  
> including VR headset, RGB camera, language, etc. Our system also supports to be controlled by autonomous  
> agents like GPT-4o or imitation learning policy trained using our dataset collected via teleoperation.

也就是先用pre obs训一个教师网络，然后再蒸馏给只能看到历史数据的学生网络。

2.5 HOVER: Versatile Neural Whole-Body Controller for Humanoid Robot
--------------------------------------------------------------------

HOVER (Humanoid Versatile Controller）  
输入的  
![image](https://img2024.cnblogs.com/blog/3481742/202501/3481742-20250123104036015-1929957410.png)  
蒸馏的结构：  
![image](https://img2024.cnblogs.com/blog/3481742/202501/3481742-20250123104003478-35896956.png)  
融合了很多的模态输入，引入了掩码。

2.6 HumanPlus
-------------

[大佬的解读博客](https://blog.csdn.net/v_JULY_v/article/details/139702814 "大佬的解读博客")  
Best Paper Award Finalist (top 6) at CoRL 2024  
Stanford  
![image](https://img2024.cnblogs.com/blog/3481742/202501/3481742-20250122194058200-1411605501.png)  
[https://humanoid-ai.github.io/](https://humanoid-ai.github.io/)  
读起来不象正经的科研论文的格式，比较奇怪。。。。。。不过代码很是简洁，就是issac的那套框架改的。  
HumanPlus的全栈人型机器人  
主要贡献点

> 1、一个实时影子系统，允许人类操作员使用单个RGB相机和Humanoid Shadowing Transformer(简称HST)来全身控制人形机器人，该HST是一种low-level策略，基于大量的模拟人体运动的数据进行训练  
> 2、人形模仿Transformer，本质就是模仿学习算法，能够通过40次演示高效学习：双目感知和高自由度控制

通过影子跟踪模仿施教：具体而言，通过使用上面收集的数据，然后执行监督行为克隆，并使用自我中心视觉训练技能策略，使人形机器人(33自由度、180cm高)通过模仿人类技能自主完成不同的任务  
最终，机器人自主完成了穿鞋、站立行走、从仓库货架卸载物品、折叠运动衫、重新排列物品、打字以及向另一台机器人打招呼等任务

2.6.1 简单总结下
-----------

`HST`：通过AMASS数据集实现能够shadow人体数据集的动作，使用HST也可以用一个单纯的RGB相机来进行shadow，不过前面训练的shadow是一个低级low level的版本，后面人类的数较有点大模型的微调的那个感觉(bushi)  
`HIT`：机器人可以自主学习技能，通过上面训练的数据，然后执行监督学习进行克隆，并使用以自我为中心的视觉技能策略，是机器人通过模仿人类动作自主完成策略。  
`代码开源了，连接已经贴上了`

2.7 Expressive Whole-Body Control for Humanoid Robots
-----------------------------------------------------

[https://expressive-humanoid.github.io/](https://expressive-humanoid.github.io/)  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250214172013873-913778650.png)

上下半身解耦，下肢主要是locomotion的任务，上肢是模仿学习的任务  
有相关的解读，我这里就补充下：  
[解读博客--知乎](https://zhuanlan.zhihu.com/p/684655285 "解读博客--知乎")

2.8 ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills
--------------------------------------------------------------------------------------------------

网站:[https://agile.human2humanoid.com/](https://agile.human2humanoid.com/)  
还是何Tairan大佬的文章  
下面是整篇文章的pipeline:  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220195006029-416694161.png)

### 2.8.1 尝试解读

奖励函数有3种类型：  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220201451694-1687079836.png)  
为了增强鲁棒性，使用了DR：  
![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220201606275-971649006.png)

### 2.8.2 总结

大概浏览了下整体的可结构，我可以得到如下的总结：

#### 阶段1

*   首先通过video数据集得到动作数据，然后retargeted到机器人身上，请注意，这时是不考虑物理引擎的，也居士机器人肯能会飞在控制模仿动作。
*   然后在虚拟环境中训练一个policy，让机器人学会这些motion。
*   然后将这个policy部署到实机上，roll out出来一个trajectories，得到一系列的<s,a>数据。
*   让仿真器得到的下一时刻的state和真实环境的state作loss，这算一个奖励函数了。

#### 阶段2

*   训练一个delta action网络根据roll out出来的trajectories训练，也就是基于real环境的训练，这是训练的是\\(\\Delta\\)A,经过仿真器得到下一时刻的state，然后以此类推。  
    我其实感觉就是在真实环境的action上进行了修正  
    ![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220204213458-874315341.png)

#### 阶段3

*   当delta action网络训练好后，冻结这个网络，训练一个policy，能够根据一个输入s0的状态，推断出action0，其实这个地方用的就是PPO的强化学习算法，然后将aciton送到delte action网络里面，输出delta action，加到输入的aciton上面，经过仿真器输出state,这个是下一时刻的，以此类推  
    ![image](https://img2024.cnblogs.com/blog/3481742/202502/3481742-20250220204643433-1816396177.png)

#### 阶段4

*   在真实环境中部署这个policy，

3\. 后记
======

这篇博客暂时记录到这里，日后我会继续补充。