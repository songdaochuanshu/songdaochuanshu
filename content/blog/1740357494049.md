---
layout: post
title: '解密prompt系列49. 回顾R1之前的思维链发展路线'
date: "2025-02-24T00:38:14Z"
---
解密prompt系列49. 回顾R1之前的思维链发展路线
============================

![解密prompt系列49. 回顾R1之前的思维链发展路线](https://img2024.cnblogs.com/blog/1326688/202502/1326688-20250224080746514-923949164.png) 我先按照自己的思路来梳理下R1之前整个模型思维链的发展过程，可以分成3个阶段：大模型能思考，外生慢思考，内生慢思考

在所有人都在谈论R1的今天，作为算法也是有些千头万绪无从抓起。所以这一章先复盘，我先按照自己的思路来梳理下R1之前整个模型思维链的发展过程。下一章再展望主要去看RL在Agent上的一些尝试，毕竟Agent规划和长思考的本质是非常像的，在优化中面临的问题也是类似的。

梳理大模型出来后的这两年时间，个人认为思维链的技术在R1出现之前，可以分成大致3个阶段（哈哈可能每个人都有自己的分类标准吧）：

*   大模型能思考，各式各样的思维链能不同程度提升模型在不同领域的效果
*   外生慢思考：Inference Scaling正式提出，推理侧的范式逐渐收敛到MCTS
*   内生慢思考：探索把推理侧的模型思考能力内化到模型内，包含以RFT为代表的各类训练方案，PRM为代表的打分方案，STaR为代表的各类样本优化方案

下面我们细说下每个阶段的一些代表方案

阶段1-模型能思考：思维链能提升任务效果
====================

> *   COT相关的各类论文可以直接看[Github](https://github.com/DSXiangLi/DecryptPrompt)

开始讨论如何使用思维链来提升大模型效果的起点就是Let's think step by step的论文，它首次提出了Chain of Thought概念，也就是让模型先思考再回答可以有效提升任务完成效果。

之后差不多1年的时间里出现了很多探索如何优化COT方案的论文，主要集中在以下几个方向

*   **是否使用few-shot**：zero-shot，few-shot
*   **自动选择few-shot样例**：AutoCOT, Active Prompting等动态few-shot构建和选择方案，提升few-shot难度和多样性可以进一步提升思维链效果
*   **重点在思考形态调整**：Tree of Thought，Graph of Thought，Tree-of-Mixed-Thought，Algorithm of Thought，
*   **重点在思考逻辑调整**：Least-to-Most（分而治之），CogTree(假设检验），Step-Back(后退思考），HtT（演绎推理+归纳推理），Abstraction of Thought（抽象思维），Plan-and-Slove（系统思维）
*   **重点在思考角度的调整**：DIVISE（多样性），Self-Consistency（一致性）

不难发现以上的论文还基本停留在，通过指令告诉模型你应该如何思考，或者通过few-shot像模型示范正确的思考方式，以及探索使用不同的逻辑思维方式来提升模型在各个领域的思考效果上。

同期结合Agent的概念，也出现了不少的论文是探索如何把COT和Action，工具调用进行融合，包括

*   **基础编排方式**：ReACT，Self-Ask
*   **进阶编排方式**：ART（多步调用），ReWOO(并发执行）
*   **领域COT领域**：ToRA（数学推理），PAL(代码），好像还是啥化学生物领域整分子，化学结构啥的忘记叫什么了

第一阶段的大模型思维链还相对基础，主要停留在面向不同领域任务的Prompt设计，面向结果居多，并未深入探讨思维链的过程，什么时候使用思维链更好，以及为何思维链能提升推理的效果。

阶段2-外生慢思考: Inference Scaling理论提出
================================

> *   DeepMind: Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters
> *   SELF-REFINE: Iterative Refinement with Self-Feedback
> *   GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements
> *   PRM: Let's verify step by step
> *   rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking

第二个阶段的代表论文是DeepMind提出的Inference Scaling概念，论文首次抽象并量化了思考的过程，以及可以带来的收益，核心观点是

*   **通过增加推理阶段的计算资源，可以显著提升LLM性能，且效率优于单纯扩大模型参数。**例如，采用优化策略后，较小规模的模型在部分任务上可超越参数规模大14倍的模型，同时减少约4倍的计算资源消耗。这表明，测试时计算的合理分配能够以更低的成本实现高性能输出。
*   **适应性“计算最优”策略**，论文提出根据问题难度动态调整测试时计算策略：
    *   简单到中等难度问题：LLM本身有能力生成合理的推理结果，那在推理阶段分配更多资源（深度or广度搜索），能显著弥补预训练截断的不足
    *   复杂问题：但是超越LLM本身能力的，即便在推理侧分配更多资源也没用，还是要更多预训练提升能力才可以
    *   通过预测问题难度并自适应分配计算资源，可以达到最优的资源分配

![image](https://img2024.cnblogs.com/blog/1326688/202502/1326688-20250224080719565-49879644.png)

那结合DeepMind以及众多其他推理搜索的论文，我们可以总结归纳出来，推理阶段的思维链生成，包含两个核心能力也就是推理链路的生活能力和打分能力

*   **推理生成能力**：推理链路的中间过程如何生成，以什么样的形态生成
    *   **深度搜索**：self-refine（RISE），self-reflection(GLoRE)， 思考本身是串行的，整个流程就是思考->反思->优化->再思考
    *   **广度搜索**：Major-Vote，Best-of-N，Beam-Search
    *   **深度+广度**：MCTS，REBEASE等相比Beam-search的局部最优，复杂度更高的全局优化方案
*   **推理打分能力**：如何选择更优的推理路径，从而指导以上的推理过程生成？
    *   **基于结果的价值函数**：ORM
    *   **基于过程的价值函数**：PRM，PRM + Lookahead
    *   **基于蒙特卡洛模拟**：根据每个节点通向最终正确答案的概率来计算节点打分

![image](https://img2024.cnblogs.com/blog/1326688/202502/1326688-20250224080719574-2005499910.png)

在Inference Scaling的基础上再进一步，很自然就会想到既然我们能通过在推理阶段海量的探索和打分找到相对更优的推理链路，那我们是否就能把这更优的推理链路直接通过训练内化大模型的参数中，这样在推理侧不就不需要这么高的推理资源了么？

那如何如何把外生推理搜索过程内化到模型中，既保留思考效果，又保证全场景泛化能力；以及是否能突破MCTS的上限，获得更稳定，质量更高的思考链路就是下一个阶段大家核心探讨的问题了。

虽然后面R1的推出让大家开始怀疑推理搜索技术的合理性，但可能殊途也能同归。如果搜索推理走到极致也能媲美O1的思考能力，例如rstart-Math已经部分跑通，在数学任务上只靠外生推理搜索就能让下模型在部分任务媲美O1的推理能力。只不过当前搜索推理的一个问题更多在非STEM问题的泛化上。

![image](https://img2024.cnblogs.com/blog/1326688/202502/1326688-20250224080719525-1227517995.png)

阶段3-内生慢思考：O1开启的Long Thought时代
=============================

> *   STaR: Self-Taught Reasoner Bootstrapping ReasoningWith Reasoning
> *   RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold
> *   Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations
> *   12月OpenAI RFT：[https://www.bilibili.com/video/BV1P2qBY2EwK/?vd\_source=52e5d8e60f1d8edf228e8fded56b41631](https://www.bilibili.com/video/BV1P2qBY2EwK/?vd_source=52e5d8e60f1d8edf228e8fded56b41631)
> *   12月字节 REFT：Reasoning with REinforced Fine-Tuning

下一个阶段，自然以跨时代的O1作为代表，也就是把更强思考能力内化到模型中的方案，可以分成以下两个部分

*   如何进一步提升思考样本的质量：这一阶段的尝试多脱离人工标注，毕竟在思考任务上Scabality Oversight是最容易出现的，就是人类标注Golden Answer的上限很低，自优化，持续迭代优化是核心。
*   如何把思考能力训练内化到模型中，如何提高训练的效率和泛化性

思考样本优化
------

首先是**思考样本质量的提升方案，主要是以STaR为代表的模型自由化Bootstrap方案和OpenAI提出的Prover-Verifier对抗优化方案。** 其中Bootstrap方案主要通过对模型生成的多个推理结果进行拒绝采样，筛选更优推理链路，再通过SFT训练进模型，再基于新的模型生成更优的推理链路再训练，这样反复迭代不断提升样本质量。**R1的训练过程中，也用了类似的方案，就是RL训练得到R1-Zero后，会使用R1-Zero通过拒绝采样来构建大规模的思维链样本重新训练DeepSeek-V3。**

而对抗优化的重心更多在Verfier,也就是随着大模型Generator能力的提升，对应Reward模型的能力也要随之提升，才能持续为模型提供有效的监督信号。所以会借助辩论，或者Proposal尝试迷惑Verifier等博弈对抗方案，来同时提升生成器和校验器的模型能力。

但最近常看R1的Reasong思考过程也让我产生了一个疑问：**在训练过程中构造的COT，真的是过程越正确的思维链更好么？**会不会基于PRM步骤打分得得到的更加正确的思维链，在一定程度上本身会抑制模型的反思能力，以及思维链的泛化性？毕竟中间过程都是正确的，类似于找到了整个思考链路中的Shortest Path，把反思，错误优化，尝试其他假设等基于错误思考节点的反思优化能力都抛弃掉了。而R1-Zero完全基于RL训练的思维链只基于目标优化，没有冷启动过程，反而不存在这个问题？

内化思考
----

其次是**如何把长思考能力内化到模型参数中，方案还是主要集中在SFT**上，但通过SFT训练的模型，在样本外的思考泛化能力较差，因此有很多方案都集中在如何提高SFT训练的泛化性。

**前期主要方案在Data Augumentation（offline Training）**，通过更充分的利用推理时生成的思维链正负样本，让模型在学习时有更多的选择，降低对单一思考链路的拟合，包括

*   充分利用正样本：生成更多正确推理路径同时进行SFT，例如RFT同时采样100个思考链路，所有正确的推理路径一起训练
*   充分利用负样本：增加关键错误节点样本进行SFT，让模型通过对比识别正确和错误的关键差异例如 V-STaR，GLoRE，Incorrect Synthetic Data等等

**中期大家开始更多考虑RL，不过RL目标的设计相对复杂，多采用了PRM的打分方案。** 这类方案主要针对Math和Code这两个垂直领域，例如

*   MATH-SHEPHERD使用MCTS蒙特卡洛模拟作为PRM打分，进行RL优化
*   DeepSeekMATH先做SFT，再使用ORM+PRM共同对问题进行优化

**后期方案同样是RL，但大道至简，RL的奖励函数重新回归简单，收敛到了基于结果（标准答案+规则打分）的优化方案。** 分别有OpenAI在24年底推出的RFT（可申请测试），和同期字节推出的ReFT。前者更多针对system2到领域模型的思维链优化，后者主要针对system1到system2思考模型的优化。其中

*   OpenAI RFT：已有o1-mini模型，经过少量领域样本，和领域评分器，通过RL就能拥有该领域复杂问题的强大推理能力，适合法律，金融，医疗等专家能对样本给出标准答案和评分，并且任务复杂度超过通用模型能力的，这里的评分器主要为各种规则打分例如ranking

![image](https://img2024.cnblogs.com/blog/1326688/202502/1326688-20250224080719573-239571986.png)

*   Bytedance REFT：两阶段训练先使用COT样本进行warm up训练，再在Policy模型基础上进行多轮采样，使用基于标准答案（0/1规则）的奖励函数进行PPO训练

![image](https://img2024.cnblogs.com/blog/1326688/202502/1326688-20250224080719632-1006108987.png)

如何就到R1了呢？
---------

不难发现在RFT，REFT的阶段大家已经开始探索基于结果（标准答案） 的RL对于提升模型思维链泛化的效果，但是和R1的实现相比，个人认为有几个核心的差异点，主要来自SFT和RL的技术定位差异。

**以SFT为主RL为辅**，RFT和ReFT本质还是以SFT为主，也就是先把准备好的思维链样本喂给模型，而RL的作用更多是进行拒绝采样，也就是从模型推理的多个候选思维链中，奖励更好的，惩罚不好的。DeepSeek V3其实就是这种训练方案。但是这种方案可能会存在一些问题

*   **前期经过大规模的COT SFT可能会影响模型思考泛化能力**，模型已经记忆了大量的思考范式，就很难再Think Out of Box了。即便有RL泛化也不会太好，同时前期标注COT的思考水平会一定程度上限制模型的上限。  
    同时前期标注COT的思考水平会一定程度上限制模型的上限。
*   **前期标注的大规模COT或许不是质量越高越好**，过优的COT中因为中间节点的推理正确概率较高，可能限制了模型的反思，优化，基于错误前置推理提出新假设的种种反思能力，Shortest Path是终极目标但不应该是初始目标。
*   **前期标注COT样本和模型分布存在偏移**，本质上非相同基座模型生成的推理样本多少都存在分布偏移，而分布差异会导致模型更倾向于模仿而非学习

而R1-Zero揭示了模型思考能力这个领域上，可以是**RL为主SFT为辅**的技术方案。前期只依赖RL，让模型在更广泛的空间中直接进行思维链编排的各种探索，即便使用SFT进行warm-up，个人认为这部分的量级和训练steps也都不会太大，本质只是为了加入RL训练的收敛。

以及**RL可以单纯依赖数学，代码，这些思考密度更高的领域训练得到。** 本质上全领域世界知识能力的获取，和思维能力的获取是存在差异的，前者依靠背，后者靠探索习得。这两种能力本质可能存在差异，只不过之前有一些思维惯性在，因此更多沿着前期SFT的范式去做，但实际可能并非如此。

**想看更全的大模型论文·微调预训练数据·开源框架·AIGC应用 >>** [**DecryPrompt**](https://github.com/DSXiangLi/DecryptPrompt/)