---
layout: post
title: 'Skill Discovery | LGSD：用描述 state 的语言 embedding 的距离，作为 metra 的 d(x,y) 距离约束'
date: "2025-07-17T00:45:10Z"
---
Skill Discovery | LGSD：用描述 state 的语言 embedding 的距离，作为 metra 的 d(x,y) 距离约束
=========================================================================

用语义距离 d\_lang(x,y) = cos\_sim\[ l(s\_1), l(s\_2)\] ，来作为 metra 的 1-Lipschitz 约束。

  

*   ICLR 2025，8 8 6 6 poster。
*   arxiv：[https://arxiv.org/abs/2406.06615](https://arxiv.org/abs/2406.06615)
*   pdf：[https://arxiv.org/pdf/2406.06615](https://arxiv.org/pdf/2406.06615)
*   html：[https://arxiv.org/html/2406.06615v2](https://arxiv.org/html/2406.06615v2)
*   open review：[https://openreview.net/forum?id=i3e92uSZCp](https://openreview.net/forum?id=i3e92uSZCp)
*   尽力找了代码，但还没找到。

1 LGSD 故事（感觉很好）
---------------

### 1.1 这篇文章关注什么，解决什么任务？

*   关注：如何在**无监督强化学习**中，让智能体自动发现 **语义上多样（Semantically Diverse）**的 skill 集合。即，我们学到的 skill，不仅在状态空间上不同，在人类理解的“含义”上也不同，比如“走到北边”vs“走到东边”，而不是“轻微晃动左腿”vs“轻微晃动右腿”。
*   任务：智能体通过自我探索，学习一整套 skill，每个 skill 对应一个潜在向量 z。学完后，这些 skill 能用于下游任务，如按指令移动到指定位置。

### 1.2 先前方法 & 问题

*   互信息（MI）方法（如 DIAYN, DADS）：
    *   训练一个判别器，区分不同 skill 产生的状态 / 状态转移。最大化技能 z 和状态 s 的互信息。
    *   问题（说的真好）：**KL 散度目标在 skill 可区分时就饱和**，导致 skill 差异微小（神经网络能区分细微差别），缺乏显著的语义差异。
*   距离最大化方法（如 LSD, CSD）：
    *   直接最大化 skill 在状态空间的距离（如欧氏距离），或关注可控性。
    *   问题（故事真好）：**使用的距离度量（如欧氏距离）不一定反映语义差异**。智能体可能只关注“容易”改变但不重要的状态维度（如机器人关节角度），忽略真正有语义的维度（如被操纵物体的位置）。同时，先前方法也难以约束**技能学习到特定的语义子空间**。
    *   欧氏距离（LSD）：机器人手臂可能疯狂挥舞自己（状态变化大），但不去碰眼前的物体（语义无变化）。
    *   可控性距离（CSD）：鼓励探索难到达的状态，但这些状态**语义上可能很平凡**（比如卡在角落）。
    *   （发现 LGSD 举例子的时候没说 metra，是 metra 不好套在这个故事里嘛？LGSD 也没有提到 dodont，不过这篇确实不强依赖于 dodont。同时，发现 Appendix B 把 metra 的一套理论重新发明了一遍。）

### 1.3 Motivation / Gap

*   Gap：现有的技能多样性度量（互信息、状态覆盖、状态距离）只是语义多样性的 **间接代理（proxy）**，不能保证学到的 skill 在人类理解的“含义”上真正不同和有用。
*   动机：1. 利用 LLM 强大的语义理解能力，直接定义和最大化技能之间的语义差异。2. 利用语言提示（Prompt），约束 skill 的搜索空间为用户感兴趣的语义子空间（如“只移动可食用物体”），并提供自然语言接口，方便使用学到的技能。

### 1.4 method

*   感觉用 semantic distance \\(d\_\\text{lang}(s\_1, s\_2)\\) 来把 state embedding \\(\\phi(s)\\) 学成一个 semantic space，非常巧妙。
*   并且，（好像模仿 seohong park 的 CSD），他们还证明了，如果仅对相邻 state 进行 semantic distance 的约束，那么就能得到一个伪度量，感觉这个证明也很好。
*   这个做法跟故事直接对的上。故事讲的是 1. 互信息 skill discovery 只能学到静态行为，2. 最大化距离 skill discovery 把行为多样性当作了语义多样性的 proxy，因此，他们要直接最大化 skill 的语义多样性。而通过训练 semantic space 并在这个 space 里跑 metra，LGSD 可以直接探索完整的、人类关注的行为空间，每个行为具有不同的语义。

2 LGSD 的 method
---------------

*   如何得到 state 的语言 embedding：
    *   LLM 输入：当前状态 \\(s\\) + 让 LLM 描述状态的 prompt \\(l\_\\text{prompt}\\)。LLM 输出：状态的自然语言描述 \\(l\_\\text{desc}(s)\\)，然后使用 Sentence-BERT 将 \\(l\_\\text{desc}(s)\\) 编码为向量。
    *   （关于 prompt：用户通过 Prompt 告诉 LLM 应该关注场景的哪些方面，例如，“只描述物体位置，忽略手臂姿态”）
*   与 metra 的结合：
    *   使用 LLM 定义语言距离 \\(d\_\\text{lang}(s, s') = 1 - \\text{cos\\\_sim}(向量(s), 向量(s'))\\) ，即，定义为这些向量之间的**余弦距离**（1 - 余弦相似度）。
    *   把语言距离放到 metra 的 \\(\\|\\phi(s\_1) - \\phi(s\_2)\\|\_2\\le d(s\_1,s\_2)\\) 的 d 位置。metra 的 intrinsic reward 不变。
    *   貌似，metra 形式里的 d 没有对 d 进行任何约束，只说它应该是一个 metric space。然而， \\(d\_\\text{lang}(s\_1, s\_2)\\) 不一定满足三角不等式之类。然而，我们可以构造 \\(\\|\\phi(s) - \\phi(s')\\|\\le d\_\\text{lang}(s, s')\\) ，即仅对一个 transition 内的 state 进行更新，从而得到一个伪度量（pseudometric）。Appendix C 有可以看懂的理论证明。
    *   看起来，LGSD 没有直接用 dodont 的权重方法，而是去训了一个跟 semantic distance 有关的 \\(\\phi(s)\\) embedding。
*   执行下游任务：
    *   可以在用 \\(l\_\\text{desc}(s)\\) 训 policy \\(\\pi(a|s,z)\\) 的同时，维护一个 \\(\\psi(l\_\\text{desc}(s))=z\\) ，这样，执行下游任务时，就可以 输入人类的自然语言描述，输出对应的 skill z，从而让 policy 执行我们的意图了。

3 LGSD 的实验
----------

### 3.1 实验 Setting (任务 / 环境)

*   基于 Isaac Gym，所有 observation 都是 state 而非 pixel。
*   任务 1 (验证技能空间约束 - Ant)：让四足机器人（Ant）在二维平面上移动。
    *   输入：机器人状态（本体位置、速度、关节角度 / 速度等，共 39 维）。输出：关节动作。
    *   目标：通过不同语言提示，如“只探索北/南/东/西半平面”，验证学到的技能 是否被约束在指定的语义子空间内。
*   任务 2 (验证技能多样性 - FrankaCube)：让 Franka 机械臂推动桌面上的立方体物体。
    *   输入：机械臂末端状态（位置、旋转）、夹爪开合、物体状态（位置、旋转），共 16 维。输出：关节动作。
    *   目标：在无明确奖励且机械臂初始距离物体最远的情况下，发现能向各个方向推动物体的 语义多样技能。
*   任务 3 (利用技能 - 下游任务)：
    *   zero-shot follow 语言指令 (FrankaCube)：用自然语言描述目标 state，如“物体位于\[0.3,0.2\]”，agent 执行相应 skill 到达该状态。
    *   训练高层策略 (Ant)：在 AntSingleGoal（单目标点）和 AntMultiGoal（连续多目标点）任务上，用预训练的低层技能（来自 LGSD 和 baseline）训练一个高层策略来选择 skill，评估学到的 skill 对解决下游任务的帮助，具体的，评估性能和学习效率。

### 3.2 Baseline

*   DIAYN (MI-based)：最大化状态与技能的互信息，依赖 skill discriminator \\(q(z|s)\\)。
*   DADS (MI-based)：预测下一状态来学习可预测的动态相关技能，还没读过。
*   LSD (Distance-based)：最大化 state space 里 欧几里得距离的 Wasserstein 依赖度量。
*   CSD (Distance-based)：最大化基于状态转移动态似然（可控性感知）的距离度量，还没读过。
*   METRA (Distance-based)：最大化在 temporal distance metric 下的 Wasserstein 依赖度量。

### 3.3 评价指标

*   Ant 任务：视觉化展示不同提示下学到的技能轨迹在二维平面上的分布（是否集中在指定半平面）。
*   Franka 任务：
    *   物体移动距离（的 curve）：评估平均推动物体移动了多远（米）。
    *   物体状态覆盖率（的 curve）：统计物体在水平桌面（1cm x 1cm 网格）上访问了多少个网格单元。
    *   轨迹可视化：展示不同 method 推物体的路径覆盖范围。
*   下游任务：
    *   zero-shot follow 语言指令：视觉化展示物体到达目标位置的轨迹（成功/失败）。
    *   高层策略 (Ant)：在 AntSingleGoal 和 AntMultiGoal 任务上的累积奖励 training curve。

### 3.4 实验结果

*   空间约束成功：Ant 能根据语言提示（n e s w）将技能约束在对应半平面内。Franka 能将物体推向指定半平面，但覆盖范围不如 Ant 完美，因推动的接触动力学更复杂。
*   多样性显著提升：在 FrankaCube 任务上：
    *   LGSD 的物体平均移动距离 远超所有 baseline，达到了最高的状态覆盖率。
    *   可视化显示 LGSD 的轨迹覆盖范围远广于 baseline。距离最大化方法（LSD / CSD / METRA）能移动物体，但不如 LGSD 多样，MI 方法（DIAYN / DADS）效果不佳。
*   高效利用技能：
    *   zero-shot follow 语言指令：LGSD 成功将物体移动到语言描述的目标位置。仅有 LGSD 可以 work 的可视化，无 curve / 数值结果 / 与其他方法的比较。
    *   高层策略：使用 LGSD 预训练技能的高层策略，在 AntSingleGoal 和 AntMultiGoal 任务上的 training curve 最高。（metra 被干的不 work，是因为 LGSD 把翻滚的 agent 都直接终止 episode 了）

### 3.5 更多实验细节

*   LLM：
    *   使用 gpt-4-turbo-2024-04-09，温度设为 0 以保证描述一致性。
    *   为了减少 LLM query 的数量，我们对状态进行离散化，并缓存这些 query 的输入输出，在训练过程中重复使用。
    *   附录的 prompt 示例显示，LGSD 用坐标规则强制对齐了语义，prompt 里有很多细节。
    *   Franka 的两阶段 prompt：第一阶段（物体未动）关注“末端执行器到物体的距离”；第二阶段（物体移动后）关注“物体的位置”。引导智能体先接近物体再推动物体。
*   技能维度：Ant 用 2 维，Franka 用 3 维。
*   下游任务的 RL 算法：PPO (稳定收敛)。

（无论如何，感觉故事真好）