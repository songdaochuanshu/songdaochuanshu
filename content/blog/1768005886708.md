---
layout: post
title: '小白也能看懂的LLM-RL算法：PPO/DPO/GRPO/GSPO'
date: "2026-01-10T00:44:46Z"
---
小白也能看懂的LLM-RL算法：PPO/DPO/GRPO/GSPO
=================================

原文: [https://mp.weixin.qq.com/s/9KT9LrMTXDGHSvGFrQhRkg](https://mp.weixin.qq.com/s/9KT9LrMTXDGHSvGFrQhRkg)

**LLM-RL往期文章推荐**

[小白也能看懂的RL-PPO](https://mp.weixin.qq.com/s/cx3qY42Lp0L3RaSOgsH77A)

[收藏！强化学习从入门到封神：5 本经典教材 + 8 大实战项目 + 7个免费视频，一站式搞定](https://mp.weixin.qq.com/s/nfN0dWT3ZfDuW7ZGfaG6dA)

[小白也能看懂的RLHF：基础篇](https://mp.weixin.qq.com/s/4_6CBXMJhqmiYKSzsAXncg)

[小白也能看懂的RLHF-PPO：原理篇](https://mp.weixin.qq.com/s/8O7W8--x14-b1d3M9IS_3w)

强化学习 (RL, Reinforcement Learning) 已成为大模型后训练的关键环节，在前几篇中，我们对LLM-RL中最基础、最核心的PPO算法做了详细的拆解，有需要的可以点击上面链接阅读，本篇将介绍其他主流的LLM-RL算法。读完这篇后，你会发现，不管是哪种LLM-RL算法，本质都是将人类偏好融入到大模型中，差异点在于以何种方式/更高效地挖掘偏好数据的价值、如何在降低算力资源需求下保证模型性能、如何提高算法稳定性。

在介绍LLM-RL各类策略算法前，我们将先解释一些基本概念，涵盖强化学习理解、价值函数、NLP中的强化学习。 随后，介绍RLHF的起源、设计思路、如何与大模型RLHF算法关联的。再对LLM-RL主流的（DPO、GRPO、GSPO）策略优化算法进行细致讲解，如各个算法原理、训练过程、优缺点等。为了能够更好地对比各个算法，也会捎带介绍下PPO算法。

1 基本概念
======

1.1 强化学习理解
----------

强化学习关键在于「**强化**」这个词，简单理解就是「**用反馈强化行为**」，即用奖惩规则让智能体自己在试错中筛选出有效行为，再通过重复反馈把这些行为巩固成最优策略，本质就是「**奖惩驱动的行为筛选与巩固**」，咱们结合之**训练小狗坐下**的例子逐词解释下这句话。

*   **奖惩驱动**：RL整个学习过程的核心动力来自**奖励** 和 **惩罚/无奖励**的即时反馈，比如小狗做对**坐下**就给零食（奖励），没做对就不给零食（无奖励），小狗的行为选择完全由这种反馈驱动。
    
*   **行为筛选**：智能体（小狗）会在动作空间里不断尝试不同行为，而奖惩就像一个筛选器，**被奖励的行为会被保留，被惩罚/无奖励的行为会被淘汰**。比如小狗一开始可能尝试蹦跳、趴着、摇尾巴等动作，这些动作都没得到零食，就会慢慢被筛选掉；只有**坐下**这个动作能拿到零食，就会被筛选出来。
    
*   **行为巩固**：被筛选出来的有效行为，会在一次次重复的「**行为—奖励**」循环中被强化，最终变成稳定的条件反射。比如小狗每次做坐下都能拿到零食，重复次数多了，它就会把**听到指令→坐下**这个行为固定下来，形成稳定的技能。
    

简单来说，「**强化**」不是增强能力，而是强化「**动作 - 奖励之间**」的关联，让智能体慢慢记住**做什么能拿到好处，做什么会吃亏**。就像训练小狗时，你不用强行掰它的腿让它坐下，只需要用零食的奖惩，让它自己悟出**坐下=有吃的**这个规律，最后形成稳定的行为习惯。

接下来，我们继续以**训练小狗坐下为例，**理解下强化学习的关键要素、学习过程。

强化学习的2个核心角色：

*   **智能体（Agent）**：要学习**坐下**技能的**小狗**
    
*   **环境（Environment）**：小狗所处的训练场景（比如家里的客厅、户外的院子等）
    

强化学习的3个关键要素：

*   **状态空间** \\(S\\)：训练环境中所有可能出现的场景集合。比如小狗可能站着/趴着/对着玩具叫，也可能身边有陌生人经过，这些不同的状态都属于状态空间
    
*   **动作空间** \\(A\\)：小狗能做出的所有动作集合。比如小狗可以站着不动、趴着休息、摇尾巴、跳跃、坐下，这些都是它动作空间里的内容
    
*   **奖励** \\(R\\)：小狗处于某个状态、做出某个动作时，给出的即时反馈。比如小狗做出**坐下**动作时，给它一块零食；如果小狗乱蹦乱跳不配合，就不给零食甚至轻声制止。
    

那小狗（智能体）和训练环境是怎么互动的，最终学会坐下的呢？我们以训练中 \\(t\\)时刻的互动过程为例：

*   当前状态\\(S\\\_t\\)：小狗正站在客厅里看着你，此时它还没做出目标动作，所以没拿到奖励，即时奖励\\(R\\\_t\\)\= 0
    
*   小狗的动作\\(A\\\_t\\)：你发出**坐下**的指令后，小狗可能做出各种反应，比如蹦跳、可能趴着，也可能偶然做出**坐下**的动作；
    
*   新状态 \\(S\\\_{t+1}\\)与新反馈与\\(R\\\_{t+1}\\)：如果小狗刚才做的动作\\(A\\\_t\\)\=坐下，训练状态就变成**小狗处于坐下状态**\\(S\\\_{t+1}\\)，主人立刻给它一块零食作为奖励\\(R\\\_{t+1}\\)\=2 的正向奖励；如果小狗做的是其他动作，\\(S\\\_{t+1}\\)可能变成小狗趴着或小狗蹦跳，主人不给零食\\(R\_{t+1}\\)\=0。
    

这样的互动会一次次重复：主人反复发出指令，小狗在不同状态下尝试不同动作，并根据是否拿到零食（奖励）总结经验。最终，小狗（智能体）就会学到一套最优玩法：当接收到**坐下**指令时，立刻做出**坐下**动作，以此稳定拿到零食奖励，这就是强化学习的核心逻辑。

1.2 价值函数
--------

前面提到的**即时奖励**\\(R\\\_t\\)，只反映了当前状态\\(S\\\_t\\)的收益，忽略了当前状态和动作对未来收益的影响。更合理的计算方式，需要兼顾**即时收益**和**未来收益**，公式如下：

\\\[V\\\_t = R\_t + \\gamma V\_{t+1} \\\]

其中：

*   \\(V\_t\\)：\\(t\\)时刻的总收益（包含即时和未来收益）
    
*   \\(R\_t\\)：\\(t\\)时刻的即时收益
    
*   \\(V\_{t+1}\\)： \\(t+1\\)时刻的总收益，对 \\(t\\)时刻而言属于未来收益
    
*   \\(\\gamma\\)：折扣因子，用来调节未来收益在当前总收益中的权重
    

此处不展开强化学习价值函数的假设与推导，仅提供简化结论，方便读者聚焦于更好的理解各类算法策略的具体做法和直觉理解。

1.3 NLP中的强化学习
-------------

对应到\\(NLP\\) 下，大语言模型就是智能体，输入的提示词 \\(prompt\\) 就对应状态 \\(S\\)，输出下一个\\(token\\) 就是动作 \\(A\\)，对 \\(prompt+response\\) 进行打分就是奖励 \\(R\\)，整体目标就是给定\\(prompt\\)，调整策略模型，生成符合人类喜好的\\(response\\)。

2 RLHF的起源
=========

论文标题：Deep Reinforcement Learning from Human Preferences

论文地址：[https://arxiv.org/pdf/1706.03741](https://arxiv.org/pdf/1706.03741)

这篇发表于2017年NIPS的论文是强化学习领域的开创性成果，其核心设计直接奠定了大模型RLHF的技术基础，我们来看看这篇论文到底做出了哪些开创新工作、以及如何对RLHF带来启发的。

2.1 文章的核心：问题、创新与方法
------------------

*   **解决的问题**：传统强化学习的核心思路是为任务手工设计明确奖励函数，用以引导智能体学习。这种模式对于简单的应用场景还行，一旦涉及复杂场景，如机器人控制、游戏，会面临奖励函数难以精准定义、奖励设计违背人类真实需求的问题，直接影响是智能体行为与人类偏好严重错位，比如机器人为适配简单奖励规则会出现跛行式行走，智能驾驶智能体为追求得分奖励而忽视人类更看重的平稳驾驶体验。
    
*   **核心创新**：舍弃手工设计奖励，提出「**用人类偏好反馈驱动强化学习**」，无需真实奖励函数就可以让智能体对齐人类需求，大幅降低人类监督成本。
    
*   **关键方法**：构建「**偏好收集→奖励建模→策略优化**」三阶段闭环。① 偏好收集：让非专家对比1-2秒短轨迹片段（如机器人后空翻成败片段），快速给出偏好判断；② 奖励建模：用Bradley-Terry模型训练奖励模型，拟合人类偏好背后的隐性规则；③ 策略优化：通过A2C（游戏场景）、TRPO（机器人场景）算法，结合拟合的奖励模型持续更新策略。
    

2.2 方法效果
--------

论文在多个场景验证了所提方法的有效性：① MuJoCo机器人任务（8项）：仅700条人类查询，性能就接近手工奖励方案，1400条查询实现反超；② Atari游戏（7项）：5500条查询让多数任务性能显著提升；③ 可训练novel行为：1小时内教会机器人连续后空翻等人类难以演示的动作。核心亮点是仅需不到1%的环境交互反馈，就能达到甚至超越传统方法。

2.3 对大模型RLHF的启发
---------------

论文的核心思想与设计，被RLHF完整继承并适配文本场景（后文会给出详细解读），成为其核心技术框架：

*   **核心逻辑复用**：RLHF直接借鉴「**用人类偏好替代手工奖励**」的思路，解决大模型对话连贯性、礼貌性等难以量化的问题，避免大模型无法对齐人类偏好。
    
*   **三步流程照抄**：让标注员快速对比两条文本，选出人类偏好，训练模型学会怎么给文本打分，将人类偏好打分融合到PPO算法中优化大模型参数
    
*   **关键突破的支撑**：大模型文本任务维度极高，全量标注不现实，得益于论文验证的「**少量反馈即可驱动模型优化**」，RLHF仅用数万条偏好数据就实现了问答质量跃升。
    

从上面的描述可以看出，论文提出的「**奖励难定义**」、「**三阶段闭环**」、「**低成本反馈**」构成了RLHF的技术基石，启发了OpenAI的研发人员将「**智能体对齐人类偏好**」这一想法，从机器人、游戏领域成功迁移至大模型，成为ChatGPT一夜爆火的重要原因。

3 RLHF-PPO
==========

论文标题：Training language models to follow instructions with human feedback 论文地址： [https://arxiv.org/pdf/2203.02155](https://arxiv.org/pdf/2203.02155)

PPO（Proximal Policy Optimization）算法是 InstructGPT 模型实现人类反馈强化学习（RLHF）的核心，核心思想是通过约束策略更新幅度，在保证训练稳定性的前提下，最大化人类偏好导向的奖励，同时避免策略过度偏离初始监督模型。

3.1 RLHF-PPO的四大模型
-----------------

相比传统PPO算法，RLHF-PPO模型架构更为复杂，一共包含4个模型，比传统PPO多出了**参考模型（Reference Model）** 和**奖励模型（Reward Model）**。

\*\*四大模型作用和参数初始化：

*   **Actor Model** \\(\\pi\_{\\theta}\\) ：用SFT后的模型作为初始模型，通过PPO训练调整参数，得到最终的模型，使其生成的回答更符合人类偏好，**参数可变**。
    
*   **Reference Model** \\(\\pi\_{base}\\)：SFT后的模型，用于限制Actor Model不要偏离Reference Model太远，**参数冻结**。
    
*   **Reward Model** \\(r\\\_{\\theta}\\)：对模型回答打分，表示贴合人类偏好的程度，训练完后，在PPO中是**参数冻结**的。
    
*   **Critic Model** \\(V\_{t}\\)：用于预测期望总收益，为优势函数 \\(A\_{t}\\)提供基线，使策略更新更稳定，**参数可变**。
    

3.2 RLHF-PPO的训练过程
-----------------

我们回顾一下RLHF里的整体流程，如上图所示：

Step 1：收集演示数据，训练监督策略（SFT）

Step 2：收集对比数据，训练奖励模型（RM）

Step 3：用强化学习（PPO）优化策略模型

整个流程的核心是**用人类偏好数据（演示、排序）指导模型训练**，最终让大模型输出既有用、又符合人类意图的内容。

下面，重点讨论PPO算法的训练过程

### 经验数据收集

*   将一个batch的prompt喂给当前的Actor模型\\(\\pi\_{\\theta}\\)，生成对应的响应序列response
    
*   对于响应序列的每个token，计算以下中间变量：
    
    *   Actor模型的对数概率\\(log(\\pi\_\\theta(a\_t | s\_t))\\)
        
    *   Reference模型的对数概率\\(log(\\pi\_{base}(a\_t | s\_t))\\)
        
    *   Critic模型根据当前状态 \\(s\\\_t\\)的价值估计 $V\_t $
        
    *   计算每个token的KL散度\\(kl = log(\\pi\_\\theta(a\_t | s\_t)) - log(\\pi\_{base}(a\_t | s\_t)) = log \\frac {\\pi\_\\theta(a\_t | s\_t)}{\\pi\_{base}(a\_t | s\_t)}\\)
        
    *   若该token位于序列的最后一个，通过Reward Model 计算人类偏好分数\\(r\_{\\theta}\\)
        
    *   结合 Reward Model 计算人类偏好分数\\(r\_{\\theta}\\)（只在最后一步）和 每个token 的 KL 惩罚 ，计算出每个token的最终即时奖励信号 $R\_t $，以deepspeed-chat的RLHF为例， $R\_t $的计算如下
        
    
    $$\\begin{align} R\_t = \\begin{cases} -kl\\\_ctl \*(log\\frac{\\pi\_\\theta(a\_t | s\_t)}{\\pi\_{base}(a\_t | s\_t)}), &\\text t\\not =T \\\\ -kl\\\_ctl \*(log\\frac{\\pi\_\\theta(a\_t | s\_t)}{\\pi\_{base}(a\_t | s\_t)}) + r\_\\theta, &\\text t=T \\end{cases} \\end{align}$$
    
    *   使用最终即时奖励信号 $R\_t $和Critic模型的价值估计 $V\_t $，通过 GAE (Generalized Advantage Estimation) 方法计算每个 token 的优势 \\(Adv\_t\\)，计算如下

\\\[\\begin{align} \\delta\_t = R\_t + \\gamma\*V\_{t+1} - V\_t \\\\ Adv\_t = \\delta\_t + \\gamma\* \\lambda\*Adv\_{t+1} \\end{align} \\\]

将前面收集到的经验数据（prompt、\\(log(\\pi\_\\theta(a\_t | s\_t))\\)、\\(log(\\pi\_{base}(a\_t | s\_t))\\) 、\\(Adv\_t\\)、...）构成批次，放到经验池中

如果对上述计算过程有疑惑，可以回看[小白也能看懂的RLHF-PPO：原理篇](https://mp.weixin.qq.com/s/8O7W8--x14-b1d3M9IS_3w)，文章有详细解读。

### 模型参数更新

*   循环epochs次
    
    *   对经验池的每个batch数据
        
        *   Actor Model参数更新： PPO更新模型 \\(\\pi\_{\\theta}\\) 参数，这里为了防止策略模型 \\(\\pi\_{\\theta}\\) 过度偏离参考模型 \\(\\pi\_{base}\\) ，有两种形式保证模型在有限的空间里微调参数，一种是引入PPO-KL散度，
            
            $$\\begin{align} L^{PPO-KL}(\\theta) = E\_{\\tau \\sim {\\pi\_{base}}}^t \[\\frac{\\pi\_\\theta(a\_t | s\_t)}{\\pi\_{base}(a\_t | s\_t)} A\_{t} - \\beta D\_{KL}(\\pi\_{base}(.|x\_t) || \\pi\_{\\theta}(.|x\_t))\] \\end {align}$$
            
            另一种PPO-CLIP比较直接，在目标函数中，限制新旧模型的差距在约定的区间内：
            
            $$\\begin{align} L^{PPO-CLIP}(\\theta) = E\_{\\tau \\sim {\\pi\_{base}}}^t \[min( \\frac{\\pi\_\\theta(a\_t | s\_t)}{\\pi\_{base}(a\_t | s\_t)} A\_{t}, \\ clip(\\frac{\\pi\_\\theta(a\_t | s\_t)}{\\pi\_{base}(a\_t | s\_t)}, 1-\\varepsilon,1+\\varepsilon)A\_{t})\] \\end {align}$$
            
        *   Critic Model参数更新：$$\\begin{align} L^{Critic}({\\phi}) &= E\_t \[(R\_t + \\gamma V\_{t+1} - V\_t)^2\] \\end {align}$$
            
            \\(V\\\_{t}\\): Critic模型对 \\(t\\)时刻收益的预估，即未来和即时收益的整体预估
            
            $ R\_t + \\gamma\*V\_{t+1}\\(: 计算得到的即时收益\\)R\_{t}$ ,\\(\\gamma\*V\_{t+1}\\) 为Critic模型预测出 \\(t+1\\)时刻之后的折损收益
            

### 模型参数同步

*   **策略参数更新**：完成经验池的数据训练迭代后，用新训练得到的Actor模型参数、Critic模型参数替换旧的模型参数。
    
*   **丢弃旧数据**：因这批数据是用旧策略模型收集的，与现有的新策略模型偏差较大，重要性采样的假设不再准确，所以必须丢弃当前这批数据。
    
*   **循环训练流程**：回到数据收集阶段，用更新后的策略模型与环境重新交互，采集全新的数据；之后重复整个训练流程，直到策略性能稳定收敛，或达到预设的训练步数。
    

以上就是PPO算法训练全流程的说明，核心环节包括：经验数据收集、模型参数更新和模型参数同步

3.3 RLHF-PPO的优缺点
----------------

**优点**：① 稳定性强：靠裁剪/KL惩罚限制策略更新幅，降低崩坏风险，易收敛；② 易落地：无需TRPO的二阶计算，用Adam优化器即可；核心超参数（如clip系数、GAE参数）易调试

**缺点**：① 训练成本高：涉及4个模型的运算/更新，是后续改进重点；② 梯度信息浪费：clip机制会截断大量梯度，拖慢训练、限制策略精细度；③ 样本利用率低：本质是on-policy架构，策略同步阶段需丢弃旧数据，新模型每次收集数据都会带来算力、时间成本。

4 DPO
=====

论文标题：Direct Preference Optimization: Your Language Model is Secretly a Reward Model 论文地址：[https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)

如前文所述，PPO算法虽被证实有效，但也暴露诸多痛点。如需同时维护和运行四个模型，对计算资源和显存的要求极高；训练过程还伴随强化学习（RL）常见的不稳定性问题；一次完整的PPO训练，覆盖多个环节，调优的难度较大。

能否找到一种方法，绕过显式训练奖励模型和复杂的强化学习环节，基于人类偏好数据直接优化策略模型呢？

DPO正是在这一背景下诞生的创新方法，由斯坦福大学研究团队提出。其核心思想是摒弃「**先训练奖励模型，再用RL优化策略**」的两步流程，直接将人类偏好数据转化为对策略参数的二分类损失优化，大幅简化了训练链路。

4.1 DPO算法的原理
------------

DPO 的关键理论支撑在于人类偏好数据里隐藏的奖励规则，而针对这个规则，存在一个**最优策略** \\(\\pi^\*\\)，

既能让模型获得的期望奖励尽可能高，又能控制策略模型\\(\\pi\_\\theta\\)和参考模型 \\(\\pi\_{ref}\\)之间的差异在合理的范围内。

论文进一步证明了，这个最优策略 \\(\\pi^\*\\)、参考策略模型 \\(\\pi\_{ref}\\)以及偏好数据里藏着的奖励函数\\(r\_{\\theta}\\) ，三者之间有明确的数学关系，详细的推导过程见论文。这意味着，不用单独训练一个奖励模型，直接拿偏好数据就能优化策略模型。

具体来说，DPO 让正在训练的模型\\(\\pi\_\\theta\\)，在面对同一个输入提示 \\(x\\)时，生成人类偏好的回答\\(y\_w\\)，而不是被否定的回答\\(y\_l\\)。对应的损失函数定义如下：

\\\[\\begin{align} L\_{DPO}(\\pi\_\\theta, \\pi\_{ref}) = -E\_{(x, y\_w, y\_l) \\sim {D}}\[log(\\sigma(\\beta \* (log\\frac{\\pi\_\\theta(y\_w | x)}{\\pi\_{ref}(y\_w | x)} - log\\frac{\\pi\_\\theta(y\_l | x)}{\\pi\_{ref}(y\_l | x)})))\] \\end {align} \\\]

我们来拆解这个公式的几个关键部分：

*   **数据来源** \\(D\\) : 是人类标注的偏好数据集，每个样本是 "提示 \\(x\\) + 更优回答\\(y\_w\\) + 较差回答\\(y\\\_l\\)" 的组合
    
*   **概率比的比较**:
    
    *   \\(log\\frac{\\pi\_\\theta(y\_w | x)}{\\pi\_{ref}(y\_w | x)}\\)：反映当前模型\\(\\pi\_\\theta\\)相比参考模型\\(\\pi\_{ref}\\)倾向于生成优质回答\\(y\_w\\)的程度，这个值越大越好
        
    *   \\(log\\frac{\\pi\_\\theta(y\_l | x)}{\\pi\_{ref}(y\_l | x)}\\)：反映当前模型\\(\\pi\_\\theta\\)相比参考模型\\(\\pi\_{ref}\\)倾向于生成差回答\\(y\_l\\)的程度，这个值越小越好
        
    *   这两个概率比的差值，就是模型学到的**优质回答比差回答好多少**的信号，两者相差越大，说明模型越能区分好坏，并且越偏爱好的回答，相当于把偏好转化成了隐式奖励。
        
*   **调控因子** \\(\\beta\\)：用来控制\\(\\pi\_\\theta\\)和\\(\\pi\_{ref}\\)的偏离程度，\\(\\beta\\)越大，表明模型需要更明显的偏好差异，才会认定一个回答比另一个好。
    
*   **损失的作用**：\\(Sigmoid\\)把奖励信号转成更优回答的概率，再用负对数似然损失来训练，本质是让模型尽可能准确地区分好回答和差回答。
    

4.2 DPO的训练过程
------------

**数据准备**：收集人类偏好数据集 (\\(x, y\_w, y\_l\\))

**模型初始化**：

*   **Actor Model** \\(\\pi\_{\\theta}\\) ：用SFT后的模型作为初始模型，**参数可变**。
    
*   **Reference Model** \\(\\pi\_{ref}\\)：SFT后的模型，**参数冻结**。
    

**训练过程**：

*   循环epochs次
    
    *   从 数据集\\(D\\)中抽取 (\\(x, y\_w, y\_l\\)) batch数据
        
        *   对于每个样本，计算公式（7）中的中间变量值 \\(\\pi\_\\theta(y\_w | x)\\)、\\(\\pi\_{ref}(y\_w | x)\\)、\\(\\pi\_\\theta(y\_l | x)\\)、\\(\\pi\_{ref}(y\_w | x)\\)及其对数概率。通常是计算序列的总对数概率， \\(log \\pi\_theta(y | x) = \\sum\_t log \\pi\_\\theta(token\_t | x, y\_{<t})\\)
            
        *   代入 DPO 损失函数公式，计算损失，使用梯度下降更新 \\(\\pi\_{\\theta}\\) 的参数。
            
*   重复此过程，直到模型收敛或达到预定训练步数。
    

4.3 DPO的优缺点
-----------

**优点**：① 训练链路极简化：直接从偏好数据里提取隐式的奖励信息，没有复杂的数据采样过程，省掉了训练奖励模型、价值函数的步骤。② 资源占用少：仅有参考模型和策略模型，显存和计算资源的消耗大幅降低，训练也更稳定、更容易落地。

**缺点**：① 依赖大量高质量二元偏好数据：虽然可以借助开源 LLM 生成优质数据，但数据收集仍有难度；② 适配场景较局限：因高度依赖偏好数据，难以覆盖复杂推理任务，能力上限不及 PPO，仅胜在流程简单、训练成本低。

5 GRPO
======

论文标题：DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

论文地址：[https://arxiv.org/pdf/2402.03300](https://arxiv.org/pdf/2402.03300)

DeepSeek团队发现，PPO算法中Critic模型，参数规模与Actor模型相当，显著增加了LLM训练的内存占用与计算成本；更关键的是，很多LLM任务（如数学推理、代码生成等））奖励信号稀疏（仅序列末尾由RM给出奖励），导致训练能精准估计每个token价值的Critic模型异常困难。

Critic模型的核心作用是为优势函数计算提供基线以降低方差，能否用更高效方式替代Critic实现该目标呢？为此，DeepSeek团队提出GRPO（Group Relative Policy Optimization）算法，突破了对Critic模型的依赖，为策略模型的高效优化开辟了新路径。

5.1 GRPO的原理
-----------

GRPO取消了PPO中用于估计价值的Critic模型，通过同一Prompt下的群体信息来构造基线。即，针对单个 Prompt，让策略模型生成 \\(G\\) 个不同输出 \\({\\{o\_1, o\_2, ..., o\_G\\}}\\)，用奖励模型给每个输出打分 \\(r={\\{r\_1, r\_2, ..., r\_G\\}}\\)，计算这组奖励的均值与标准差，再将每个输出的原始奖励归一化，得到优势

\\\[\\begin{align} \\hat{A}\_{i,t} = \\widetilde{r}\_i = \\frac{r\_i - mean(r)}{std(r)} \\end{align} \\\]

由上式得到反映群组相对好坏的分数，以此替代 Critic 提供的优势基线。

由此看出，GRPO的训练仅需三个模型：

*   **策略模型** \\(\\\\pi\_{\\theta}\\) ：由SFT模型初始化，根据组内相对奖励动态更新参数，参数变化
    
*   **参考模型** \\(\\pi\_{ref}\\)：冻结参数的SFT，参数不变
    
*   **奖励模型** \\(r\\\_{\\theta}\\)：用于评估生成的完整响应序列的质量，为策略模型提供优化方向，冻结参数
    

GRPO算法沿用 PPO 的 clip 阶段结构，优势估计被替换为组内归一化奖励：

\\\[\\begin{align} J\_{GPRO}(\\theta) = E\[q\\sim P(Q), {o\_i} \\sim \\pi\_{\\theta\_{old}(O|q)}\] \\frac{1}{G} \\sum\_{i=1}^G \\frac{1}{|o\_i|} \\sum\_{t=1}^{|o\_i|} {\\{min \[ratio\_t \*\\hat{A}\_{i,t}, clip(ratio\_t, 1-\\varepsilon, 1+\\varepsilon)\\}}\*\\hat{A}\_{i,t}\] - \\beta D\_{KL}\[\\pi\_\\theta || \\pi\_{ref}\]\\} \\end {align}\\\]

式（9）的中间变量拆解如下：

*   $\\hat{A}\_{i,t} $ 由公式（8）给出，作用于序列中每一个 token，就是说，所有 token 共享同一个优势
    
*   \\(ratio\_t=\\frac{\\pi\_\\theta(o\_{i,t} |q, o\_{i,<t})}{\\pi\_{old}(o\_{i,t} |q, o\_{i,<t})}\\), ，即新旧策略对同一动作的概率比
    
*   \\(\\beta D\_{KL}\[\\pi\_\\theta || \\pi\_{ref}\]\\)，约束 策略模型不要偏离参考模型太远
    

5.2 GRPO的训练过程
-------------

*   **输入项**：初始策略模型 \\(\\pi\_{\\theta\_{init}}\\) 、奖励模型 \\(r\_{\\theta}\\)、任务提示集 \\(D\\)，以及超参数 \\(\\varepsilon\\),\\(\\beta\\)（控制训练稳定性、更新步长等）。
    
*   **外层循环，总迭代轮次** \\(I\\)
    
    每一轮外层循环包含以下操作：
    
    *   复制当前策略模型\\(\\pi\_{\\theta}\\)的参数并冻结，将其作为参考模型\\(\\pi\_{ref}\\)
        
    *   进入内层循环（步长\\(M\\)），执行策略的更新
        
        *   采样任务与旧策略：从任务提示集 \\(D\\)中采样一个批次 \\(D\_b\\), 将当前策略模型\\(\\pi\_{\\theta}\\)暂存为旧策略 \\(\\pi\_{\\theta\_{old}}\\)（用于后续生成候选输出）
            
        *   生成候选输出并计算奖励：对批次中每个任务q，用旧策略旧策略 \\(\\pi\_{\\theta\_{old}}\\)生成G个候选输出 \\({\\{o\_1, o\_2, ..., o\_G\\}}\\)，用奖励模型 $$r\_{\\theta}$$为每个候选输出计算奖励 \\({\\{r\_1, r\_2, ..., r\_G\\}}\\)
            
        *   计算组相对优势： 由公式（8）计算群组相对优势 $\\hat{A}\_{i,t} $
            
        *   优化策略模型：公式（9)（10）的梯度更新策略模型\\(\\pi\_{\\theta}\\)的参数
            

5.3 GRPO算法的优缺点
--------------

**优点**：① 稳定性更强：无需依赖价值函数，只要奖励模型能相对判断群体内策略优劣，降低对价值估计精度的依赖降低对价值估计精度的依赖，提升算法稳定性；② 训练成本更低：没有了Critic 模型，显存占用减少大幅降低，相同硬件可训练更大模型或更大batch；③ 易实现且适配并行：组内各策略可独立与环境交互、收集数据，便于利用分布式框架实现并行训练加速。

**缺点**：①存在固有偏差：归一化易偏好短答案或冗长错误输出，且标准差归一化使模型优先优化简单问题（奖励稳定），忽视难题（奖励波动大） ；② 目标不一致问题：奖励函数针对完整序列打分（需等待完整回答），而GRPO优化目标的重要性采样基于token级别计算损失，两者目标存在偏差；③ 高方差和不稳定问题：GRPO 在 token 级别计算重要性采样权重，单个token只被采样一次，难以校正偏离原始策略的 token 分布，易引入高方差噪声，导致梯度估计不稳定。

6 GSPO
======

论文标题：Group Sequence Policy Optimization

论文地址：[https://arxiv.org/pdf/2507.18071](https://arxiv.org/pdf/2507.18071)

GSPO(Group Sequence Policy Optimization，组序列策略优化)旨在解决GRPO中训练过程中存在的固有偏差、目标不一致、高方差和不稳定的问题。GSPO将RL中重要性采样的单位从令牌级别提升至整个序列级别，又引入了序列长度归一化， 确保不同长度的问题享有公平的优化尺度，从根源解决训练稳定性问题，完美适配 MoE 了架构。

6.1 GSPO的原理和训练过程
----------------

首先，我们来看下GRPO和GSPO的目标函数

*   GRPO目标函数

其中，\\(ratio\_t=\\frac{\\pi\_\\theta(o\_{i,t} |q, o\_{i,<t})}{\\pi\_{old}(o\_{i,t} |q, o\_{i,<t})}\\)

*   GSPO目标函数

\\\[ \\begin{align} J\_{GSPO}(\\theta) = E\[x\\sim D, {\\{y\_i}\\}\_{i=1}^G \\sim \\pi\_{\\theta\_{old}(.|x)}\] \[ \\frac{1}{G} \\sum\_{i=1}^G min (s\_i(\\theta) \*\\hat{A}\_i, clip(s\_i(\\theta), 1-\\varepsilon, 1+\\varepsilon)\*\\hat{A}\_i)\] \\end {align} \\\]

其中，\\(s\_i(\\theta)=(\\frac{\\pi\_\\theta(y\_i |x)}{\\pi\_{old}(y\_i |x)}) ^{(\\frac{1}{|y\_i|})} = exp(\\frac{1}{|y\_i|} \\sum\_{t=1}^{|y\_i|}log\\frac{\\pi\_\\theta(y\_{i,t} |x, y\_{i,<t})}{\\pi\_{old}(y\_{i,t} |x, y\_{i,<t})})\\) ，\\(|y\_i|\\)是响应\\(y\_i\\) 的长度，长度归一 化是关键，确保不同长度序列的重要性比率具有可比性，防止比率因序列过长而剧烈波动。注意，论文里为了简化，在公式（11）中没有加KL，实际应用中是有用KL的。

通过对比GRPO与GSPO的核心公式，能清晰看出两者在重要性采样单位与计算逻辑上的关键差异。

先看GRPO的目标函数，其加权优势的计算核心聚焦于**token级别**的，如\\(ratio\_t=\\frac{\\pi\_\\theta(o\_{i,t} |q, o\_{i,<t})}{\\pi\_{old}(o\_{i,t} |q, o\_{i,<t})}\\), 所示，针对第 \\(i\\)个序列中的第\\(t\\)个token进行运算。

举个直观例子，若一个组包含5条样本序列，每条序列有20个token，GRPO需要为这5条序列的每一个token分别计算加权优势。

两次平均操作：内层先对单条序列的20个token取平均，外层再对5条不同序列取平均，**本质就是算术平均**，最终得到梯度更新的有效信号。

再看GSPO的目标函数，从公式\\(s\_i(\\theta)=(\\frac{\\pi\_\\theta(y\_i |x)}{\\pi\_{old}(y\_i |x)}) ^{(\\frac{1}{|y\_i|})} = exp(\\frac{1}{|y\_i|} \\sum\_{t=1}^{|y\_i|}log\\frac{\\pi\_\\theta(y\_{i,t} |x, y\_{i,<t})}{\\pi\_{old}(y\_{i,t} |x, y\_{i,<t})})\\), 可以看出，其加权优势的计算单位直接升级为**序列级别，**内层平均由算术平均本变为了几何平均，外层的计算逻辑依然不变。

GSPO的训练过程和GRPO基本一致，差别在于目标函数中重要性采样计算逻辑，这里不做过多解释。

6.2 GSPO的优缺点
------------

**优点**：① 提升训练稳定性：GRPO 在 token 级进行重要性采样时，若某时间步生成错误 token，此时优势为 \\(A\\\_t <0\\)，旧策略生成该 token 的概率极低（如\\(1e^{-8}\\)），新策略生成概率即使仅为 0.001，重要性比值也会飙升至 \\(10^5\\)，通过公式(9) (10)发现，在\\(A\_t <0\\)情况下，这么大的重要性比值不会被裁剪，进而造成损失函数的大波动，噪声会不断累积，甚至模型会训崩。而 GSPO 以序列为单位计算重要性采样比值，即便个别 token的重要性比值飙升，经过几何平均后(经过Log运算、算术平均、指数运算)，也会被平滑，不会造成比值的大幅波动，从而避免了损失函数的震荡，保障了训练稳定性；② 适配 MoE 架构模型：GRPO 采用 token 级重要性采样，需依赖路由重放等技巧才能稳定训练；而 GSPO 基于序列级计算重要性采样，无需这类复杂操作即可实现 MoE 模型的稳定训练，大幅简化了训练流程，同时释放了 MoE 模型的潜力。

**缺点**：① 计算复杂度升高：相比 GRPO 的 token 级重要性采样，GSPO 切换为序列级计算后，算法的实现难度与计算成本有所增加。

7 参考资料
======

[https://zhuanlan.zhihu.com/p/1932791271363154917](https://zhuanlan.zhihu.com/p/1932791271363154917)

[https://mp.weixin.qq.com/s/gSUw8bXraKy0RzVU\_Crwcw](https://mp.weixin.qq.com/s/gSUw8bXraKy0RzVU_Crwcw)

[https://arxiv.org/pdf/1706.03741](https://arxiv.org/pdf/1706.03741)

[https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)

[https://arxiv.org/pdf/2402.03300](https://arxiv.org/pdf/2402.03300)

[https://arxiv.org/pdf/2507.18071](https://arxiv.org/pdf/2507.18071)