---
layout: post
title: '手把手教你在个人电脑部署本地知识库（基于RAGFlow + DeepSeek [+ Ollama]）'
date: "2025-02-13T00:35:56Z"
---
手把手教你在个人电脑部署本地知识库（基于RAGFlow + DeepSeek \[+ Ollama\]）
====================================================

1.    实现方案及准备工作
===============

按照教程一步一步操作，基本没有什么太大难度，稍显麻烦的可能就是因网络问题有些资源无法下载，对于镜像无法下载的问题，文中也提供了替代的方法，但是github访问不稳定这点，如果你不是上网达人，只能找朋友求助了。

本文档提供了两种部署方法：半本地化部署、完全本地化部署，你可以根据自己的需求选择。

部署本地知识库的工具有很多，RAGFlow、FastGPT、DIFY、AnythingLLM等，他们各有特点，本文以RAGFlow为例。

本文讲述的是在Windows操作系统中进行的部署，但实际上在Linux系统中部署差别也不是很大，因为本文的部署主要是基于docker-compose。

1.1.   安装docker桌面版
------------------

docker桌面版用于容器化运行ragflow相关组件。

官网下载地址：[https://www.docker.com/](https://www.docker.com/)

下载后安装，一直下一步，直至安装成功。

这里有一点需要注意，很多虚拟机软件包括docker默认会使用Hyper-V，比如先安装vmware再安装docker后，可能就会引起冲突，建议先卸载本机上的虚拟机软件，再安装docker。

安装完成后，打开docker desktop（下文称docker桌面版），点右上角的设置（齿轮）按钮，选择Resource -> Advanced，修改此处的路径，下载的镜像、docker运行期间占用的虚拟磁盘空间，都会使用这个目录。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108536-480477532.png)

2.    半本地化部署
============

半本地化部署是指知识库工具部署在本地，但语言模型不部署在本地，而是接入互联网模型提供商提供的接口。这种方式对本机性能要求不算太高，但需要按用量付费，且文件安全性较完全本地化部署要差一些。

2.1.   克隆ragflow仓库
------------------

选择一个盘符，确保其剩余空间大于100GB。

打开docker桌面版，点击右下角的“Terminal”进入终端模式。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108601-1397820138.png)

输入下面命令进入选择的盘符。

cd e:\\

然后输入下面命令克隆仓库。

git **clone** **https**://github.com/infiniflow/ragflow.git

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108674-963801887.png)

如果提示git命令不存在，可能是本机没有安装git，通过下列地址下载安装：[https://git-scm.com/](https://git-scm.com/)

因为github连接的不稳定性，这里可能会有一些问题，可根据报错自行上网寻找解决方法或向朋友求助。

克隆成功后，会创建E:\\ragflow目录，执行如下命令。

cd ragflow

git checkout -f v0.16.0

2.2.   修改.env文件
---------------

在“我的电脑”中，进入刚刚克隆的目录，进入其子目录docker下，用文本编辑器打开.env文件。

决定要下载的版本，配置文件中提供了两个版本，一个是v0.16.0-slim（第84行），一个是v0.16.0（第87行），两者的区别是，前者不会预下载嵌入模型，默认会安装前者，为了方便这里改为后者。

修改前。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108514-1113807626.png)

但因为DockerHub官方镜像源很早开始就无法访问了，直接修改还不行。如果你能正常下载，如上面修改就行；但大概率是不能正常下载的，这里提供了一种解决办法，可以按[docker镜像无法下载问题解决](https://www.cnblogs.com/ffzj/p/18711315)中的步骤来操作。

操作完成后，将新的镜像地址填写进来。最终如下图。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108614-447663380.png)

.env的第二处修改，是将HF\_ENDPOINT=https://hf-mirror.com这一行取消注释，因为一般情况下我们是访问不了huggingface。修改后如下。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108487-1927697916.png)

第三处修改，包括多个组件的密码，比如MYSQL\_PASSWORD（第46行），等号后面的值改为自己要设定的密码。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108516-432590877.png)

2.3.   修改docker-compose-base.yml文件
----------------------------------

这个文件的修改并非强制，主要是指定挂载文件的路径，比如MySQL的数据文件存放目录。

如下图，我配置了将MySQL文件存放到e:\\docker\\ragflow\\mysql目录下。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108534-706377321.png)

以下是代码文本，注意driver\_opts:这一行必须与上一行对齐，type: none以及下面两行缩进两个字符间距。

    driver\_opts:

      type: none

      device: /e/docker/ragflow/mysql/

      o: bind

还有一种情况，即默认的镜像无法下载成功时，使用[docker官网镜像无法下载解决](https://www.cnblogs.com/ffzj/p/18711315)方法后，需要在这里修改镜像地址。

2.4.   启动容器
-----------

回到docker桌面版，确保此时位于ragflow目录。因为我用的是nvidia显卡，所以用docker-compose-gpu.yml文件来启动，如果是其他情况，可以用docker-compose.yml文件。

docker compose -f docker/docker-compose-gpu.yml up -d

初次运行，docker会下载多个镜像。如果出同下面这种错误，说明拉取镜像失败。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108549-2102920399.png)

在前面修改.env文件时我们已经提到了，目前docker官网的镜像直接访问不了，请按[docker镜像无法下载问题解决](https://www.cnblogs.com/ffzj/p/18711315)中的步骤来操作。

启动成功后，在Containers中就会出现ragflow相关组件。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108621-629322384.png)

2.5.   删除容器
-----------

只有需要修改配置文件时，才需要执行此操作。

使用下面的命令删除容器。

docker compose -f docker/docker-compose-gpu.yml down

修改配置文件后，再重新启动容器。

docker compose -f docker/docker-compose-gpu.yml up -d

3.    首次登录
==========

在docker桌面版的Containers中，找到ragflow-server，点击其右侧的Show all ports。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108578-1809572487.png)

点击80:80。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108126-879667899.png)

系统会在默认浏览器中打开登录地址[http://localhost/login](http://localhost/login)。

点击注册，来注册一个账号。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108672-2081967226.png)

用注册的账号登录后，点击右上角English右侧的下拉图标，可将语言切换为简体中文。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108548-181778398.png)

4.    完全本地化部署
=============

完全本地化部署是指所有组件都部署在本地，这一步并不是必须的，取决于你的需求，同时对电脑配置有一定要求。如果不需要，可以跳过这一步，继续从添加互联网模型开始。

4.1.   部署要求
-----------

系统：Windows、Linux均可，本文以Windows为例。

显卡：7B参数模型需要NVIDIA GeForce RTX 3060 Ti或更高端显卡，14B参数模型需要NVIDIA GeForce RTX 4080或更高端显卡。

内存：7B参数模型推荐32GB内存，14B参数模型推荐64GB内存。

CPU：Intel Core i5 12400或同级别以上CPU。

磁盘：剩余磁盘空间大于100GB。

4.2.   下载ollama并运行本地模型
----------------------

ollama是一个模型管理工具，可以通过它下载多种不同模型。ollama官网地址：[https://ollama.com/](https://ollama.com/)。

下载后安装，安装成功后在浏览器打开[http://127.0.0.1:11434/](http://127.0.0.1:11434/)。如果页面上显示Ollama is running，表示安装成功。如果下载页面加载失败，可以到github中下载OllamaSetup.exe，下载页面：https://github.com/ollama/ollama/releases。

仅下载ollama还不行，还必须下载需要的模型。

ollama默认将模型下载到C盘，因此我们需要修改模型下载目录。

1.打开系统设置，进入系统属性或系统变量设置。

2.创建一个新的系统变量或修改现有的环境变量，变量名为OLLAMA\_MODELS。

3.将变量的值设置为ollama模型文件下载路径，例如d:\\ollama。

设置完以后重启Ollama服务即可。

打开ollama官网，在导航栏上找到Models，点击进入。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108550-347042299.png)

在这个页面中可以搜索到各类模型，找到我们需要下载的模型。点击进入该模型详情页面。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108560-2028157495.png)

在模型页面选择需要下载的参数规模，一般个人电脑最多只能支持到14b，然后点击右侧的“复制”按钮，复制下载模型的命令。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108556-1518763096.png)

打开windows命令提示符工具（WIN +R快捷键，然后输入cmd，回车）。点击右键粘贴命令，回车，ollama会自动下载模型。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108165-357147352.png)

安装完成后，可以通过CTRL+D快捷键退出会话。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108158-986402273.png)

可以通过ollama list命令查看已安装的模型。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108676-905163484.png)

4.3.   添加本地模型
-------------

点击右上角的头像，在设置页面点击“模型供应商”，在列表中找到Ollama，点击其下方的“添加模型”。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108231-1396087378.png)

模型类型根据情况选择。

chat：语言（会话、聊天）模型。

embedding：嵌入模型，用于将文本向量化。

rerank：重排模型，用于对检索结果进行再次排序，在ragflow中创建助理时，默认使用矢量余弦相似性进行这一处理，除非指定了rerank模型。

image2text：图片转文本模型。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108501-1962659021.png)

模型名称必须与ollama中的名称一致，如想将deepseek-r1:7b添加为本地模型，就需要将其完整名称复制到文本框中。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108578-1779044029.png)

基础Url：因为ragflow是基于docker运行，但是ollama是直接安装在宿主机的，因此基础Url填写：http://host.docker.internal:11434

API-Key：留空。

最大token数：根据需要填写，不超过8192。

填写完成后单击右下角“确定”。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108529-817354883.png)

如果要继续添加本地模型，可以点击ollama中的“添加模型”按钮，进行添加。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108204-870817050.png)

5.    添加互联网模型
=============

本章中只描述半本地化部署的模型添加方法，完全本地化部署的模型添加方法请参考的添加本地模型步骤。

5.1.   获取API KEY
----------------

本文以DeepSeek为例，进入DeepSeek开放平台，[https://platform.deepseek.com/](https://platform.deepseek.com/)。

在API Keys中，创建API key。然后复制并保存。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108561-481068398.png)

5.2.   添加模型
-----------

点击右上角头像。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108514-426085858.png)

点击“模型提供商”，在列表中找到自己期望的模型供应商，以DeepSeek为例，点击“添加模型”。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108583-1147831535.png)

在弹出的窗口中填写上一步获取的API KEY，确定。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108529-1677769736.png)

添加成功后，可以在页面上半部分的已添加模型列表中，看到刚刚添加的DeepSeek了。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108604-1607240466.png)

6.    添加知识库与对话
==============

6.1.   修改系统模型
-------------

在模型供应商中，点击右上角的“系统模型设置”。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108517-1511516733.png)

在其中选择各类模型的默认模型，比如聊天模型选deepseek等。

嵌入模型和Rerank模型一般不必修改。

Img2txt、Sequence2txt、TTS这三种模型，可以等需要用到的时候，添加模型后，再来配置。其中img2txt模型用于识别图片，Sequence2txt模型用于识别语音，TTS模型用于生成语音。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108537-463963162.png)

6.2.   添加知识库
------------

点击上方导航栏的“知识库”，进入知识库页面，点击右侧的“创建知识库”按钮。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108500-1141379817.png)

在弹出窗口中，输入名称后确定，进入知识库配置页面。

设置知识库的各项参数。

文档语言：“中文”。

嵌入模型：默认不变。

解析方法：取决后面需要上传到知识库的文件类型，每一种在页面右侧都有说明和示例，我要上传的是一个存放在EXCEL中的问答对，所以我选择Q&A。

其他信息如果不懂就不调整。

确认后保存。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108602-753767816.png)

在数据集中新增文件，选择本地文件。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108570-824693133.png)

在弹出的窗口中，上传准备好的知识库文档。

上传成功后，单击文件右侧的解析按钮开始解析文件，解析成功后，解析状态会变为“成功”。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108571-747645857.png)

单击“检索测试”，在该页面中进行问答测试。通过调整相似度阈值和关键字相似度权重两个参数，来测试什么样的设置能获得相对精准的回答。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108588-1655744730.png)

6.3.   创建对话
-----------

点击上方导航栏的“聊天”，再点击“新建助理”。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108486-884479618.png)

给助理设置一个姓名，最底部的知识库中选择刚刚创建的知识库。

然后点击窗口上方的“提示引擎”。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108618-1344850913.png)

在窗口中根据需要修改提示词，因为我需要助手严格按知识库中的内容作答，不要回答其他内容，所以在默认的提示词基础上做了一点修改。

再根据上一步添加知识库时的测试，设置合适的相似度阈值和关键词相似度权重。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108244-256847833.png)

提示引擎设置完成后，点击窗口上方的“模型设置”。

选择期望使用的模型，以及模型的回复风格。

注意模型下拉列表中，Ollama下面的是本地模型，上面的DeepSeek是接入供应商的API，需要按用量付费的。实际使用中，我们要么选择半本地化部署，要么选择完全本地化部署，一般是不太会出现二者皆有的情况，这里只是为了方便演示。

最大token数，如果助手给出的回答中预估内容比较长，可以适当增加最大token数。

一切确定后，单击“确定”按钮，创建助手成功。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108621-684537968.png)

点击聊天右侧的“+”号，创建新的对话。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108570-1661272063.png)

然后就可以开始对话了。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108615-184221860.png)

6.4.   分享助手
-----------

有时候助手需要分享给别人使用，点击右上角头像，进入API页面，点击API KEY按钮。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108223-921205432.png)

在弹出的窗口中点击“创建新密钥”，然后确定。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108501-2034200216.png)

点击上方导航栏“聊天”，回到聊天页面。

鼠标光标停留在智能助手上方时，会出现一个设置按钮，将光标移动上去，在弹出的列表中选择“嵌入网站”。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108602-488404346.png)

将src后面引号内的内容复制出来，将localhost替换为本机IP或域名，就可以分享给别人使用了。

![](https://img2024.cnblogs.com/blog/1959921/202502/1959921-20250212123108587-1436552883.png)