---
layout: post
title: 'DeepSeek+PageAssist实现本地大模型联网'
date: "2025-02-15T00:35:48Z"
---
DeepSeek+PageAssist实现本地大模型联网
============================

![DeepSeek+PageAssist实现本地大模型联网](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214165527991-2115786552.png) 这篇文章主要介绍了通过使用PageAssist，来使得本地部署的DeepSeek模型可以联网搜索的方案。

技术背景
====

在前面的几篇博客中，我们分别介绍过[在Ubuntu上部署DeepSeek](https://www.cnblogs.com/dechinphy/p/18699554/deepseek)、[在Windows上部署DeepSeek](https://www.cnblogs.com/dechinphy/p/18702523/deepseek2)、[使用AnythingLLM构建本地知识库](https://www.cnblogs.com/dechinphy/p/18703831/deepseek3)的方法，其中还包含了ChatBox的基本安装和使用。这里我们要介绍的是PageAssist，一个包含了大模型Chat、网络检索和知识库搭建的浏览器插件，不依赖于操作系统，而且功能配置非常丰富，可玩性很高，推荐使用。

安装PageAssist
============

访问[PageAssist官网](https://pageassist.xyz/)，点击install now：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214160344106-923285767.png)

添加到chrome：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214160455174-993602990.png)

点击插件图标：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214160538993-50400171.png)

第一个图标是固定到任务栏的意思，固定以后就可以直接在任务栏打开PageAssist窗口：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214160643248-880441373.png)

PageAssist设置
============

打开右上角的设置`Settings`按钮，先设置一个中文语言：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214160807746-1839403907.png)

根据自己的需求，配置一下网络访问（国内一般设置搜索引擎为百度或者搜狗，但是百度里面可能会有广告）：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214160933078-1603538556.png)

在RAG设置里面设置本地Ollama的Embedding模型，比如`nomic-embed-text`，本地没有该模型的可以去[模搭](https://www.modelscope.cn/models/nomic-ai/nomic-embed-text-v1.5-GGUF/files)下载一个GGUF模型文件，参考[这篇文章](https://www.cnblogs.com/dechinphy/p/18702523/deepseek2)里面的方法进行本地模型构建，或者直接运行`ollama pull nomic-embed-text`从Ollama的官方Hub里面下载该模型文件。

配置本地的大模型是在Ollama设置里面配置url：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214161943230-1017726023.png)

如果只有本地的模型，用默认配置就可以了。在Ollama设置里面还可以支持很多细节设置，可以按照自己本地的硬件条件进行设置（不能使用中文，我这里特地改成英文查看下单位设置）：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214164318054-490373293.png)

在这里可以手动配置加载进GPU的模型层数`num_gpu`，和回答的严谨程度`temperature`等，对于模型的性能提升和场景适配非常有帮助，配置完要记得保存。

在PageAssist里面还可以直接管理模型，例如拉取某个模型，或者删除某个本地模型（谨慎操作）：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214162436412-450984523.png)

然后也可以像[前一篇文章](https://www.cnblogs.com/dechinphy/p/18703831/deepseek3)文章里面提到的，可以部署本地知识库：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214162541782-987806236.png)

PageAssist里面支持按照提示词进行对话分类，这个特点就有点像[ChatBox](https://www.cnblogs.com/dechinphy/p/18699554/deepseek)里面不同的角色，可以进行不同风格的记忆和回答：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214162647351-1902787859.png)

联网功能测试
======

选择一个模型，开始聊天：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214163650870-603789728.png)

可以用一些比较有时效性的问题进行网页浏览测试，例如“北京最近7天的天气怎么样？”：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214164625347-812434517.png)

在回答的末尾还可以看到他的citation：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214164709216-26083239.png)

在最底下这一行的按钮里面，可以查看每一次回答的tokens的速度：

![](https://img2024.cnblogs.com/blog/2277440/202502/2277440-20250214164853676-2069790182.png)

建议模型的tokens速度要达到`5 tokens/s`以上，使用体验才会好一些，如果速度低于`1 tokens/s`，用起来会很难受。

总结概要
====

这篇文章主要介绍了通过使用PageAssist，来使得本地部署的DeepSeek模型可以联网搜索的方案。

版权声明
====

本文首发链接为：[https://www.cnblogs.com/dechinphy/p/ds-pa.html](https://www.cnblogs.com/dechinphy/p/ds-pa.html)

作者ID：DechinPhy

更多原著文章：[https://www.cnblogs.com/dechinphy/](https://www.cnblogs.com/dechinphy/)

请博主喝咖啡：[https://www.cnblogs.com/dechinphy/gallery/image/379634.html](https://www.cnblogs.com/dechinphy/gallery/image/379634.html)