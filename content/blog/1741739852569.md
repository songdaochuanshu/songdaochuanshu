---
layout: post
title: 'æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ12ï¼‰--- å¤šå¤´è‡ªæ³¨æ„åŠ›'
date: "2025-03-12T00:37:32Z"
---
æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ12ï¼‰--- å¤šå¤´è‡ªæ³¨æ„åŠ›
==============================

ä»é›¶å¼€å§‹è§£æTransformerï¼Œç›®æ ‡æ˜¯ï¼š(1) è§£æTransformerå¦‚ä½•è¿ä½œï¼Œä»¥åŠä¸ºä½•å¦‚æ­¤è¿ä½œï¼Œè®©æ–°åŒå­¦å¯ä»¥å…¥é—¨ï¼›(2) åŠ›äº‰èå…¥ä¸€äº›æ¯”è¾ƒæ–°çš„æˆ–è€…æœ‰ç‰¹è‰²çš„è®ºæ–‡æˆ–è€…ç†å¿µï¼Œè®©è€é¸Ÿä¹Ÿå¯ä»¥æœ‰æ‰€æ”¶è·ã€‚

æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ12ï¼‰--- å¤šå¤´è‡ªæ³¨æ„åŠ›
==============================

ç›®å½•

*   [æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ12ï¼‰--- å¤šå¤´è‡ªæ³¨æ„åŠ›](#æ¢ç§˜transformerç³»åˆ—ä¹‹12----å¤šå¤´è‡ªæ³¨æ„åŠ›)
    *   [0x00 æ¦‚è¿°](#0x00-æ¦‚è¿°)
    *   [0x01 ç ”ç©¶èƒŒæ™¯](#0x01-ç ”ç©¶èƒŒæ™¯)
        *   [1.1 é—®é¢˜](#11-é—®é¢˜)
        *   [1.2 æ ¹æº](#12-æ ¹æº)
        *   [1.3 è§£å†³æ–¹æ¡ˆ](#13-è§£å†³æ–¹æ¡ˆ)
    *   [0x02 åŸç†](#0x02-åŸç†)
        *   [2.1 æ¶æ„å›¾](#21-æ¶æ„å›¾)
            *   [åç½®](#åç½®)
            *   [æƒé‡çŸ©é˜µ](#æƒé‡çŸ©é˜µ)
            *   [\\(W^O\\)çŸ©é˜µ](#çŸ©é˜µ)
        *   [2.2 è®¾è®¡æ€è·¯](#22-è®¾è®¡æ€è·¯)
            *   [å­ç©ºé—´&åˆ†æ²»](#å­ç©ºé—´åˆ†æ²»)
            *   [ensemble&èåˆ](#ensembleèåˆ)
            *   [ç¼“è§£ç¨€ç–](#ç¼“è§£ç¨€ç–)
        *   [2.3 è®¡ç®—](#23-è®¡ç®—)
            *   [è®¡ç®—æµç¨‹](#è®¡ç®—æµç¨‹)
            *   [è®¡ç®—å¼ºåº¦](#è®¡ç®—å¼ºåº¦)
        *   [2.4 æ•ˆæœ](#24-æ•ˆæœ)
        *   [2.5 èåˆæ–¹å¼](#25-èåˆæ–¹å¼)
        *   [2.6 åˆ†æ](#26-åˆ†æ)
        *   [2.7 ä¼˜ç‚¹](#27-ä¼˜ç‚¹)
    *   [0x03 å®ç°](#0x03-å®ç°)
        *   [3.1 å®šä¹‰](#31-å®šä¹‰)
        *   [3.2 è¿ç®—é€»è¾‘](#32-è¿ç®—é€»è¾‘)
            *   [è¾“å…¥](#è¾“å…¥)
            *   [æŠ•å½±](#æŠ•å½±)
            *   [åˆ‡åˆ†æ•°æ®](#åˆ‡åˆ†æ•°æ®)
                *   [é€»è¾‘è§’åº¦](#é€»è¾‘è§’åº¦)
                *   [ç‰©ç†è§’åº¦](#ç‰©ç†è§’åº¦)
                *   [å°ç»“](#å°ç»“)
            *   [è°ƒæ•´ç»´åº¦](#è°ƒæ•´ç»´åº¦)
            *   [ä¸ºæ¯ä¸ªå¤´è®¡ç®—æ³¨æ„åŠ›](#ä¸ºæ¯ä¸ªå¤´è®¡ç®—æ³¨æ„åŠ›)
                *   [å•ç‹¬åˆ†ç»„](#å•ç‹¬åˆ†ç»„)
                *   [å¹¶è¡Œ](#å¹¶è¡Œ)
            *   [èåˆæ¯ä¸ªå¤´çš„Z](#èåˆæ¯ä¸ªå¤´çš„z)
            *   [forward()å‡½æ•°](#forwardå‡½æ•°)
        *   [3.3 è°ƒç”¨](#33-è°ƒç”¨)
            *   [ç¼–ç å™¨](#ç¼–ç å™¨)
            *   [è§£ç å™¨](#è§£ç å™¨)
    *   [0x04 æ”¹è¿›](#0x04--æ”¹è¿›)
        *   [4.1 MOHSA](#41-mohsa)
        *   [4.2 MoH](#42-moh)
        *   [4.3 DCMHA](#43-dcmha)
            *   [ç ”ç©¶èƒŒæ™¯](#ç ”ç©¶èƒŒæ™¯)
            *   [åŠ¨æœº](#åŠ¨æœº)
            *   [æ€è·¯](#æ€è·¯)
    *   [0xFF å‚è€ƒ](#0xff-å‚è€ƒ)

0x00 æ¦‚è¿°
-------

MHSAï¼ˆå¤šå¤´è‡ªæ³¨æ„åŠ›ï¼‰ æ˜¯ Transformer æ¨¡å‹çš„æ ¸å¿ƒæ¨¡å—ã€‚Transformeræœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé€šç”¨çš„å¯å¾®è®¡ç®—æœºï¼Œé›†å¤šç§ä¼˜ç§€ç‰¹æ€§äºä¸€èº«ã€‚

*   Transformer ç±»ä¼¼æ¶ˆæ¯ä¼ é€’çš„æ¶æ„å…·æœ‰é€šç”¨æ€§ï¼ˆå³å®Œæ•´æ€§ï¼‰å’Œå¼ºå¤§åŠŸèƒ½ï¼ˆå³æ•ˆç‡ï¼‰ï¼Œèƒ½å¤Ÿæ¶µç›–è®¸å¤šç°å®ä¸–ç•Œçš„ç®—æ³•ï¼Œå› æ­¤Transformerå…·å¤‡éå¸¸å¼ºå¤§çš„è¡¨ç°åŠ›ï¼ˆåœ¨å‰å‘ä¼ æ’­ä¸­ï¼‰ã€‚
*   é€šè¿‡åå‘ä¼ æ’­å’Œæ¢¯åº¦ä¸‹é™ï¼ŒTransformerå¯ä»¥æŒç»­ä¸æ–­çš„ä¼˜åŒ–ã€‚
*   å› ä¸ºTransformerçš„è®¡ç®—å›¾æ˜¯æµ…è€Œå®½çš„ï¼Œè€Œä¸”è‡ªæ³¨æ„åŠ›æœºåˆ¶è®©æˆ‘ä»¬åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶ï¼Œèƒ½å¤Ÿå¹¶è¡Œè®¡ç®—åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œæ‰€ä»¥Transformerèƒ½å¤Ÿæ›´å¥½åœ°æ˜ å°„åˆ°æˆ‘ä»¬çš„é«˜å¹¶è¡Œè®¡ç®—æ¶æ„ï¼ˆæ¯”å¦‚GPUï¼‰æ¥è¿›è¡Œé«˜æ•ˆè®¡ç®—ã€‚

*   å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡å¹¶è¡Œè¿è¡Œå¤šä¸ªè‡ªæ³¨æ„åŠ›å±‚å¹¶ç»¼åˆç»“æœï¼Œèƒ½åŒæ—¶æ•æ‰è¾“å…¥åºåˆ—åœ¨ä¸åŒå­ç©ºé—´çš„ä¿¡æ¯ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ç§ç‰¹æ€§ä½¿å¾—Transformerå¯ä»¥æ›´å¥½åœ°ç†è§£æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å¤šé¢†åŸŸéƒ½èƒ½å‡ºè‰²åº”ç”¨ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºã€‚

å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯è›‹ç³•ä¸Šçš„æ¨±æ¡ƒã€‚å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å·§å¦™ä¹‹å¤„åœ¨äºï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å¹¶è¡Œè¿è¡Œå¤šä¸ªå…·æœ‰ç‹¬ç‰¹è§†è§’çš„æ³¨æ„åŠ›å¤´æ¥åŒæ—¶å¤„ç†æ•°æ®ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»å¤šä¸ªè§’åº¦åˆ†æè¾“å…¥åºåˆ—ï¼Œæ•æ‰ä¸°å¯Œçš„ç‰¹å¾å’Œä¾èµ–å…³ç³»ã€‚ç±»ä¼¼äºä¸€ç»„ä¸“å®¶åˆ†æå¤æ‚é—®é¢˜çš„å„ä¸ªæ–¹é¢ã€‚æˆ–è€…åƒåŒæ—¶æœ‰å¤šä¸ªè§†è§’åœ¨çœ‹åŒä¸€ä¸ªä¸œè¥¿ï¼Œæ¯ä¸ªè§†è§’éƒ½èƒ½çœ‹åˆ°ä¸€äº›ä¸åŒçš„ç»†èŠ‚ã€‚ä¸‹å›¾å½¢è±¡åŒ–çš„è§£é‡Šäº†å¤šå¤´æ³¨æ„åŠ›è¿è¡Œæœºåˆ¶ï¼ŒQueryã€Keyå’ŒValue è¢«åˆ†ä¸ºä¸åŒçš„Headï¼Œå¹¶åœ¨æ¯ä¸ªHeadä¸­ç‹¬ç«‹è®¡ç®—è‡ªæ³¨æ„åŠ›ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125346078-1875409742.jpg)

0x01 ç ”ç©¶èƒŒæ™¯
---------

### 1.1 é—®é¢˜

è¿„ä»Šä¸ºæ­¢ï¼Œæ³¨æ„åŠ›æœºåˆ¶çœ‹èµ·æ¥å¾ˆç¾å¥½ï¼Œä½†æ˜¯ä¹Ÿæš´éœ²å‡ºæ¥äº†ä¸€äº›ç¼ºé™·ï¼š

æ¯”å¦‚ï¼Œæ¨¡å‹åœ¨ç¼–ç æ—¶ï¼Œå®¹æ˜“ä¼šè¿‡åº¦çš„å°†æ³¨æ„åŠ›é›†ä¸­äºå½“å‰çš„ä½ç½®ï¼Œè€Œå¿½ç•¥äº†å…¶å®ƒä½ç½®çš„ä¿¡æ¯ï¼Œä»è€Œé”™è¿‡æŸäº›é‡è¦çš„ä¾èµ–å…³ç³»æˆ–ç‰¹å¾ã€‚ç”¨ç¨‹åºåŒ–çš„è¯­è¨€æ¥è¯´ï¼Œå› ä¸ºQã€Kã€Véƒ½æ¥è‡ªè¾“å…¥Xï¼Œåœ¨è®¡ç®—\\(QK^T\\)æ—¶ï¼Œæ¨¡å‹å®¹æ˜“å…³æ³¨åˆ°è‡ªèº«çš„ä½ç½®ä¸Šï¼Œå³\\(QK^T\\)å¯¹è§’çº¿ä¸Šçš„æ¿€æ´»å€¼ä¼šæ˜æ˜¾æ¯”è¾ƒå¤§ï¼Œè¿™æ ·ä¼šå‰Šå¼±æ¨¡å‹å…³æ³¨å…¶å®ƒé«˜ä»·å€¼ä½ç½®ä¸Šçš„èƒ½åŠ›ï¼Œé™åˆ¶äº†æ¨¡å‹çš„ç†è§£å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

å†æ¯”å¦‚ï¼Œæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä½¿ç”¨Qå»æ‰¾ç›¸å…³çš„Kï¼Œä½†æ˜¯â€ç›¸å…³â€œå¯ä»¥æœ‰ä¸åŒå½¢å¼å’Œå®šä¹‰ï¼Œæ¯”å¦‚ä¸€é¡¹äº‹ç‰©å¾€å¾€æœ‰å¤šä¸ªæ–¹é¢ï¼Œåº”è¯¥ç»¼åˆåˆ©ç”¨å„æ–¹é¢çš„ä¿¡æ¯/ç‰¹å¾ï¼Œä»å¤šä¸ªè§’åº¦è¿›è¡Œè¡¡é‡ã€‚æ¯”å¦‚ä¸‹é¢å¥å­ä¸­å°±æœ‰å­—ä½“å¤§å°ï¼ŒèƒŒæ™¯é¢œè‰²ï¼Œå­—ä½“é¢œè‰²ï¼ŒåŠ ç²—/ä¸‹åˆ’çº¿/æ–œçº¿è¿™å‡ ä¸ªä¸åŒçš„å¼ºè°ƒç»´åº¦ï¼Œéœ€è¦å¤šæ–¹è€ƒè™‘ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125357312-204475925.jpg)

å¦å¤–ï¼Œäººç±»æ³¨æ„åŠ›æœºåˆ¶æœ¬èº«å°±æ˜¯å¤©ç„¶å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªæ–¹é¢çš„ä¿¡æ¯çš„ã€‚è®¾æƒ³ä½ åœ¨ä¸€ä¸ªæ‹¥æŒ¤çš„å…¬äº¤è½¦ä¸Šçœ‹ä¹¦ï¼Œä½ çš„å¤§è„‘èƒ½è‡ªåŠ¨å…³æ³¨åˆ°ä¹¦çš„å†…å®¹ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ç•™æ„å‘¨å›´çš„ç¯å¢ƒå£°ï¼Œè­¬å¦‚æœ‰äººå«ä½ çš„åå­—æˆ–æ˜¯å…¬äº¤è½¦åˆ°ç«™æ’­æŠ¥å£°ã€‚

è€Œè¿„ä»Šä¸ºæ­¢ï¼Œåœ¨æˆ‘ä»¬çš„å­¦ä¹ å†ç¨‹ä¸­ï¼Œå½“å‰çš„Transformeræ³¨æ„åŠ›æœºåˆ¶åªæ˜¯æ³¨é‡äº‹ç‰©çš„å•ç‹¬æ–¹é¢ï¼Œè€Œéæ³¨æ„å¤šä¸ªæ–¹é¢ã€‚

### 1.2 æ ¹æº

Embedding æ‰æ˜¯å¤šå¤´æ³¨æ„åŠ›èƒŒåçš„çœŸæ­£å†…åœ¨æˆå› ã€‚Embedding æ˜¯äººç±»æ¦‚å¿µçš„æ˜ å°„ï¼Œæˆ–è€…è¯´æ˜¯è¡¨è¾¾äººç±»æ¦‚å¿µçš„é€”å¾„æˆ–è€…æ–¹æ³•ã€‚äººç±»çš„æ¦‚å¿µæ˜¯ä¸€ä¸ªåŠå…¶å¤æ‚çš„ç³»ç»Ÿï¼Œå› ä¸ºæ¦‚å¿µéœ€è¦æœ‰è¶³å¤Ÿçš„å†…éƒ¨å¤æ‚åº¦æ‰èƒ½åº”å¯¹å¤–éƒ¨ä¸–ç•Œçš„å¤æ‚åº¦ã€‚æ¯”å¦‚å¯¹äºä¸€ä¸ªè¯æ¥è¯´ï¼Œå…¶å°±æœ‰è¯­ä¹‰é€»è¾‘ã€è¯­æ³•é€»è¾‘ã€ä¸Šä¸‹æ–‡é€»è¾‘ã€åœ¨å…¨å¥ä¸­ä½ç½®é€»è¾‘ã€åˆ†ç±»é€»è¾‘ç­‰å¤šç§ç»´åº¦ã€‚è€Œä¸”ï¼Œè¯ä¸è¯ä¹‹é—´çš„å…³ç³»è¿˜ä¸ä»…ä»…é™äºè¯­ä¹‰ä¸Šçš„åˆ†ç±»æ‰€å¯¼è‡´çš„å®šä½è¿œè¿‘è¿™ä¹ˆç®€å•ã€‚ä¸€ä¸ªè¯æ‰€ä»£è¡¨çš„äº‹ç‰©ä¸å…¶ä»–è¯æ‰€ä»£è¡¨çš„äº‹ç‰©ä¹‹é—´èƒ½äº§ç”Ÿå†…åœ¨è”ç³»çš„å¾€å¾€æœ‰æˆç™¾ä¸Šåƒä¸Šä¸‡ç§ä¹‹å¤šã€‚

æˆ–è€…è¯´ï¼Œæ¦‚å¿µæ˜¯è¢«é…ç½®ä¸ºèƒ½å¤Ÿè·¨ä»»åŠ¡å·¥ä½œçš„å‘é‡ï¼Œæ˜¯å»é™¤éæœ¬è´¨ä¿¡æ¯ï¼Œä¿ç•™æœ€ç¡®å®šæ€§çš„ç»“æœã€‚åœ¨è¿™ç§åŸºç¡€ä¸Šï¼Œå­˜å‚¨åœ¨é•¿æœŸè®°å¿†ä¸­çš„å•ä¸ªæ¦‚å¿µå‘é‡å¯ä»¥é€šè¿‡ä¸åŒçš„å‡½æ•°è¿›è¡ŒæŠ•å½±ï¼Œä»¥ç”¨äºä¸åŒç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡å…¶å®å¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å‘é‡ç©ºé—´ã€‚æ¯”å¦‚å¯¹äºä¸Šé¢çš„ä¾‹å­ï¼Œå­—ä½“å’Œé¢œè‰²å°±æ˜¯ä¸¤ä¸ªä¸åŒçš„å­ç©ºé—´ï¼ˆä½ç»´ç©ºé—´ï¼‰ã€‚

è€Œç›®å‰æ³¨æ„åŠ›åªæ³¨é‡å•ç‹¬æŸä¸ªå‘é‡ç©ºé—´ï¼ŒåŠ¿å¿…å¯¼è‡´è™½ç„¶æœ€ç»ˆç”Ÿæˆçš„å‘é‡å¯ä»¥åœ¨è¯¥ç©ºé—´ä¸Šæœ‰æ•ˆå°†äººç±»æ¦‚å¿µè¿›è¡Œæ˜ å°„ï¼Œä½†æ˜¯æ— æ³•æœ‰æ•ˆåæ˜ å¤–éƒ¨ä¸°å¯Œçš„ä¸–ç•Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§å¯ä»¥å…è®¸æ¨¡å‹åœ¨ä¸åŒçš„å­ç©ºé—´ä¸­è¿›è¡Œä¿¡æ¯é€‰æ‹©çš„æœºåˆ¶ã€‚

### 1.3 è§£å†³æ–¹æ¡ˆ

å¤šå¤´æ³¨æ„åŠ›å°±æ˜¯ç ”ç©¶äººå‘˜ç»™å‡ºçš„è§£å†³æ–¹æ¡ˆã€‚å¤šå¤´æ³¨æ„åŠ›å¯ä»¥ç†è§£ä¸ºé«˜ç»´å‘é‡è¢«æ‹†åˆ†æˆ–è€…è½¬åŒ–ä¸ºHä»½ä½ç»´å‘é‡ï¼Œå¹¶åœ¨Hä¸ªä½ç»´ç©ºé—´é‡Œæ±‚è§£å„è‡ªçš„æ³¨æ„åŠ›ã€‚è¿™æ ·æ¨¡å‹å°±å¯ä»¥ä»ä¸åŒè§’åº¦æ¥åˆ†æå’Œç†è§£è¾“å…¥ä¿¡æ¯ï¼Œæœ€ç»ˆè¾“å‡ºåŒ…å«æœ‰ä¸åŒå­ç©ºé—´ä¸­çš„ç¼–ç è¡¨ç¤ºä¿¡æ¯ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚Transformerè®ºæ–‡ä¸­å¯¹äºå¤šæ³¨æ„åŠ›æœºåˆ¶çš„è®ºè¿°å¦‚ä¸‹ã€‚

> Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.

å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶åŸºç¡€ä¸Šè¿›è¡Œæ‰©å±•ã€‚åœ¨ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½ åªèƒ½ä½¿ç”¨ä¸€ç»„æŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰å’Œå€¼ï¼ˆVï¼‰æ¥è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚ä½†æ˜¯ï¼Œåœ¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨å¤šç»„ä¸åŒçš„Qã€Kå’ŒVæ¥è¿›è¡Œè®¡ç®—ã€‚æ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„ä¸€ç»„Qã€Kå’ŒVï¼Œå¤šç»„Qã€Kå’ŒVé€šè¿‡ç‹¬ç«‹çš„çº¿æ€§å˜æ¢æ¥ç”Ÿæˆã€‚

ä¸åŒçš„Qå»æŸ¥æ‰¾ä¸åŒæ–¹é¢çš„ç›¸å…³æ€§ï¼Œæ¯”å¦‚æŸä¸ªQå»æ•æ‰è¯­æ³•ä¾èµ–ï¼Œå¦ä¸€ä¸ªQå»æ•æ‰è¯­ä¹‰ä¾èµ–ï¼Œè¿™æ ·æ¯ä¸ªæ³¨æ„åŠ›å¤´å¯ä»¥å…³æ³¨æ–‡æœ¬ä¸­ä¸åŒçš„æ–¹é¢å’Œç‰¹å¾ï¼Œæ‰èƒ½ä¸ä»…æŠ“ä½ä¸»æ—¨ï¼ŒåŒæ—¶ä¹Ÿèƒ½ç†è§£å„ä¸ªè¯æ±‡é—´çš„å…³è”ï¼Œè¿›è€Œä»å¤šè§’åº¦æ•æ‰ä¸Šä¸‹æ–‡å’Œå¾®å¦™ä¹‹å¤„ï¼Œå¹¶è¡Œåœ°å­¦ä¹ å¤šç»„è‡ªæ³¨æ„åŠ›æƒé‡ã€‚æœ€åï¼Œå¤šä¸ªæ³¨æ„åŠ›å¤´çš„ç»“æœä¼šè¢«æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå¹¶é€šè¿‡å¦ä¸€ä¸ªçº¿æ€§å˜æ¢è¿›è¡Œæ•´åˆï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å…·ä½“å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å…¶ä¸­ï¼ŒD è¡¨ç¤º hidden sizeï¼ŒH è¡¨ç¤º Head ä¸ªæ•°ï¼ŒL è¡¨ç¤ºå½“å‰æ˜¯åœ¨åºåˆ—çš„ç¬¬ L ä¸ª Tokenã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125413019-1260747646.jpg)

é’ˆå¯¹ä¸Šæ–¹å¥å­çš„ä¾‹å­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›å°±æ˜¯åŒæ—¶å…³æ³¨å­—ä½“å’Œé¢œè‰²ç­‰å¤šæ–¹é¢ä¿¡æ¯ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´å…³æ³¨ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´ï¼Œè¿™æ ·å³å¯ä»¥æœ‰æ•ˆå®šä½ç½‘é¡µä¸­å¼ºè°ƒçš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥çµæ´»é€‰æ‹©æ–‡å­—ä¸­çš„å„ç§å…³ç³»å’Œç‰¹å¾ï¼Œä»è€Œæå–æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚æ¨¡å‹æœ€ç»ˆçš„â€œæ³¨æ„åŠ›â€å®é™…ä¸Šæ˜¯æ¥è‡ªä¸åŒâ€œè¡¨ç¤ºå­ç©ºé—´â€çš„æ³¨æ„åŠ›çš„ç»¼åˆï¼Œå‡è¡¡å•ä¸€æ³¨æ„åŠ›æœºåˆ¶å¯èƒ½äº§ç”Ÿçš„åå·®ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125421251-1871283983.jpg)

æœ‰ä¸¤ä¸ªæ¯”è¾ƒç¡®åˆ‡çš„ä¾‹å­ï¼Œå¯ä»¥è®©å¤§å®¶å¯¹å¤šå¤´è‡ªæ³¨æ„åŠ›æœ‰ç›´è§‚çš„æ„Ÿå—ã€‚

*   ä¾‹å­1æ˜¯ä»ä¸“å®¶çš„ä¸“å®¶è§’åº¦æ¥çœ‹ã€‚ä¸€ä¸ªå›¢é˜Ÿåˆä½œå®Œæˆä¸€ä¸ªè½¯ä»¶é¡¹ç›®ï¼Œæ¯ä¸ªå›¢é˜Ÿæˆå‘˜è´Ÿè´£è‡ªå·±æ“…é•¿çš„é¢†åŸŸã€‚äº§å“ç»ç†è´Ÿè´£æ•´ä½“é¡¹ç›®è§„åˆ’å’Œéœ€æ±‚åˆ†æï¼›é¡¹ç›®ç»ç†è´Ÿè´£é¡¹ç›®æŠŠæ§ï¼›å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆè´Ÿè´£ä¸ç”¨æˆ·ç•Œé¢ç›¸å…³çš„å·¥ä½œï¼›åç«¯å·¥ç¨‹å¸ˆè´Ÿè´£æœåŠ¡å™¨é€»è¾‘å’Œæ•°æ®åº“ç®¡ç†ï¼›æµ‹è¯•å·¥ç¨‹å¸ˆè´Ÿè´£é¡¹ç›®è´¨é‡ä¿è¯ã€‚æ¯ä¸ªå›¢é˜Ÿæˆå‘˜ç”¨è‡ªå·±çš„ä¸“ä¸šèƒ½åŠ›ç‹¬ç«‹çš„å¯¹é¡¹ç›®ä»˜å‡ºä¸åŒçš„è´¡çŒ®ï¼Œæœ€ç»ˆå°†å„è‡ªçš„æˆæœæ•´åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„è½¯ä»¶äº§å“ã€‚
    
*   ä¾‹å­2æ›´å€¾å‘äºä»åˆä½œçš„è§’åº¦æ¥çœ‹ã€‚åœ¨æ©„æ¦„çƒé¢†åŸŸå†…æœ‰ä¸€ç§è¯´æ³•ï¼Œä¸€åœºæ¯”èµ›è¦çœ‹å››éï¼Œç¬¬ä¸€éä»æ€»ä½“ä¸Šç²—ç•¥çœ‹ï¼Œç¬¬äºŒéä»è¿›æ”»çƒå‘˜è§’åº¦çœ‹ï¼Œç¬¬ä¸‰éä»é˜²å®ˆçƒå‘˜è§’åº¦çœ‹ï¼Œç¬¬å››éåˆ™ç»¼åˆä¹‹å‰çš„ç†è§£å†æ€»ä½“çœ‹ä¸€éã€‚ä½†æ˜¯è¿™æ ·è¦çœ‹å››éã€‚ä¸å¦‚è®©å‡ ä¸ªäººä¸€èµ·æ¥çœ‹ä¸€éæ¯”èµ›ï¼Œè§‚çœ‹è¿‡ç¨‹ä¸­ï¼Œæœ‰äººè´Ÿè´£ä»ä»è¿›æ”»çƒå‘˜è§’åº¦çœ‹ï¼Œæœ‰äººè´Ÿè´£ä»é˜²å®ˆçƒå‘˜è§’åº¦çœ‹ï¼Œæœ‰äººè´Ÿè´£æ€»ä½“æŠŠæ¡ï¼Œæœ‰äººè´Ÿè´£çœ‹é‡ç‚¹çƒå‘˜ï¼Œæœ‰äººçœ‹æ•™ç»ƒéƒ¨ç½²ï¼Œæœ€ç»ˆæœ‰äººå°†ä¸åŒçš„æ„è§å’Œè§è§£æ•´åˆèµ·æ¥ï¼Œå½¢æˆå¯¹æ¯”èµ›çš„å®Œæ•´ç†è§£ã€‚
    

0x02 åŸç†
-------

### 2.1 æ¶æ„å›¾

å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å˜ä½“ï¼Œå¤šå¤´æ³¨æ„åŠ›çš„æ¶æ„åŠå…¬å¼å¦‚ä¸‹å›¾ï¼Œh ä¸ª Scale Dot-Product Attentionï¼ˆå·¦ï¼‰å¹¶è¡Œç»„åˆä¸º Multi-Head Attentionï¼ˆå³ï¼‰ã€‚æ¯ä¸ªScaled Dot-Product Attention ç»“æ„å¯¹è¾“å…¥ä¸Šä¸‹æ–‡ç‰¹å¾å•ç‹¬åšäº† ä¸€æ¬¡ ä¸Šä¸‹æ–‡ä¿¡æ¯èåˆã€‚åœ¨æ­¤åŸºç¡€ä¹‹ä¸Šï¼Œæˆ‘ä»¬æŠŠå¤šä¸ªè¿™æ ·çš„ç‰¹å¾èåˆæ“ä½œå¹¶è”èµ·æ¥ï¼Œå¾—åˆ°å¤šä¸ªç‹¬ç«‹çš„è¾“å‡ºç‰¹å¾å¼ é‡ï¼Œå†æŠŠè¿™äº›å¼ é‡è”æ¥ï¼ˆconcatenateï¼‰èµ·æ¥ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125429776-796238888.jpg)

ä¸Šå›¾ä¸­ï¼Œ\\(W^Q\\)ï¼Œ\\(W^K\\)ï¼Œ\\(W^V\\) è¿™ä¸‰ä¸ªçŸ©é˜µåˆ—æ•°å¯ä»¥ä¸åŒï¼Œä½†æ˜¯è¡Œæ•°éƒ½æ˜¯\\(d\_{model}\\)ã€‚\\(d\_{model}\\)ä¸ºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—è¾“å…¥ä¸è¾“å‡ºå¼ é‡çš„é€šé“ç»´åº¦ï¼Œhä¸ºheadä¸ªæ•°ã€‚è®ºæ–‡ä¸­h=8ï¼Œå› æ­¤\\(d\_k=d\_v=d\_{model}/h=64\\)ï¼Œ\\(d\_{model}=512\\)ã€‚

#### åç½®

\\(W^Q\\)ï¼Œ\\(W^K\\)ï¼Œ\\(W^V\\)è¿™ä¸‰ä¸ªæŠ•å½±å±‚ä»¥åŠæœ€åçš„æŠ•å½±å±‚\\(W^O\\)ï¼ˆZ \* Output\_weightsï¼‰å¯ä»¥é€‰æ‹©æ·»åŠ æˆ–è€…ä¸æ·»åŠ åç½®ã€‚

ä¸¾ä¾‹ï¼šæ ¹æ®LLaMA3æºç æ¥çœ‹ï¼Œå…¶æ²¡æœ‰åŠ å…¥biasã€‚

    class Attention(nn.Module):
        def __init__(self, args: ModelArgs):
            super().__init__()
            self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
            model_parallel_size = fs_init.get_model_parallel_world_size()
            self.n_local_heads = args.n_heads // model_parallel_size
            self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
            self.n_rep = self.n_local_heads // self.n_local_kv_heads
            self.head_dim = args.dim // args.n_heads
    
            self.wq = ColumnParallelLinear(
                args.dim,
                args.n_heads * self.head_dim,
                bias=False, # æ²¡æœ‰åç½®
                gather_output=False,
                init_method=lambda x: x,
            )
            self.wk = ColumnParallelLinear(
                args.dim,
                self.n_kv_heads * self.head_dim,
                bias=False,  # æ²¡æœ‰åç½®
                gather_output=False,
                init_method=lambda x: x,
            )
            self.wv = ColumnParallelLinear(
                args.dim,
                self.n_kv_heads * self.head_dim,
                bias=False,  # æ²¡æœ‰åç½®
                gather_output=False,
                init_method=lambda x: x,
            )
            self.wo = RowParallelLinear(
                args.n_heads * self.head_dim,
                args.dim,
                bias=False,  # æ²¡æœ‰åç½®
                input_is_parallel=True,
                init_method=lambda x: x,
            )
    

å¦å¤–ï¼Œ[PaLM: Scaling Language Modeling with Pathways](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2204.02311) è¿™ç¯‡è®ºæ–‡é‡Œæåˆ°ï¼Œå¦‚æœå¯¹å…¨è¿æ¥å±‚ä»¥åŠ layer norm ä¸åŠ åç½®é¡¹ï¼Œå¯ä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚

> No Biases â€“ No biases were used in any of the dense kernels or layer norms. We found this to result in increased training stability for large models.

#### æƒé‡çŸ©é˜µ

å¦‚æœæ˜¯Scaled Dot-Product Attentionï¼Œå³å•å¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶è¦å­¦çš„å‚æ•°å…¶å®å°±æ˜¯ä¸‰ä¸ªçŸ©é˜µ \\(W^Q,W^K,W^V\\)ï¼Œè¿™ä¸ªå‚æ•°é‡å¾€å¾€ä¸å¤šï¼Œä¸”å®¹æ˜“æ˜¯ç¨€ç–çŸ©é˜µã€‚å½“è¯­ä¹‰é€æ¸å¤æ‚åï¼Œå®¹æ˜“å› ä¸ºå‚æ•°é‡è¾¾åˆ°å®¹é‡ä¸Šé™è€Œé€ æˆæ¨¡å‹æ€§èƒ½ä¸è¶³ã€‚

å¤šå¤´å°±æ„å‘³ç€éœ€è¦æŠŠè¯åµŒå…¥åˆ†æˆè‹¥å¹²çš„å—ï¼Œå³æ¯ä¸ªå­—éƒ½è½¬æ¢ä¸ºè‹¥å¹²`512/H`ç»´åº¦çš„ä¿¡æ¯ã€‚ç„¶åæˆ‘ä»¬å°†è¿™äº›å—åˆ†é…åˆ°ä¸åŒçš„å¤´ä¸Šï¼Œæ¯ä¸ªå¤´å°†ç‹¬ç«‹åœ°è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚å¯¹äºæ¯ä¸ªå¤´å¾—åˆ°çš„Qã€Kå’ŒVï¼Œæˆ‘ä»¬éƒ½éœ€è¦åˆ†åˆ«è¿›è¡Œçº¿æ€§å˜æ¢ã€‚è®¡ç®— Qã€K å’Œ V çš„è¿‡ç¨‹è¿˜æ˜¯ä¸€æ ·ï¼Œä¸è¿‡ç°åœ¨æ‰§è¡Œå˜æ¢çš„æƒé‡çŸ©é˜µä»ä¸€ç»„\\((W^Q, W^K, W^V)\\)å˜æˆäº†å¤šç»„ï¼š\\((W\_0^Q, W\_0^K, W\_0^V)\\)ï¼Œ\\((W\_1^Q, W\_1^K, W\_1^V)\\)ï¼Œ....\\((W\_h^Q, W\_h^K, W\_h^V)\\)ã€‚é€šè¿‡è¿™äº›æƒé‡çŸ©é˜µçš„è½¬æ¢ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®©å¤šç»„å…³æ³¨ä¸åŒçš„ä¸Šä¸‹æ–‡çš„ Qã€K å’Œ Vã€‚

å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡æ›´å¤šçš„æƒé‡çŸ©é˜µæ¥å¢åŠ äº†æ¨¡å‹çš„å®¹é‡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å¤æ‚çš„è¡¨ç¤ºã€‚åœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´åªå…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„ä¸€ä¸ªç‹¬ç«‹å­ç©ºé—´ï¼Œä¸åŒå¤´ï¼ˆè§’åº¦ï¼‰æœ‰ä¸åŒçš„å…³æ³¨ç‚¹ï¼Œç»¼åˆå¤šä¸ªå¤´å¯ä»¥è®©æ¨¡å‹å°±èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£è¾“å…¥æ•°æ®ã€‚æˆ–è€…è¿™ä¹ˆç†è§£ï¼šä¸åŒçš„æ³¨æ„åŠ›å¤´å¯ä»¥å­¦ä¹ åˆ°åºåˆ—ä¸­ä¸åŒä½ç½®ä¹‹é—´çš„ä¸åŒä¾èµ–å…³ç³»ï¼Œç»„åˆå¤šå¤´æ³¨æ„åŠ›å¯ä»¥æ•æ‰å¤šç§ä¾èµ–å…³ç³»ï¼Œæä¾›æ›´ä¸°å¯Œã€æ›´å¼ºå¤§çš„è¡¨ç¤ºã€‚ä»è€Œä½¿å¾—å¤šå¤´çš„Qã€Kã€Væƒé‡å¯ä»¥åœ¨å‚æ•°é‡ç›¸åŒçš„æƒ…å†µæå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

è¿™äº›è‡ªæ³¨æ„åŠ›â€œå¤´â€çš„å…³æ³¨ç‚¹å¹¶éé¢„è®¾ï¼Œè€Œæ˜¯ä»éšæœºå¼€å§‹ï¼Œé€šè¿‡å¤„ç†å¤§é‡æ•°æ®å¹¶è‡ªæˆ‘å­¦ä¹ ï¼Œè‡ªç„¶è€Œç„¶åœ°è¯†åˆ«å‡ºå„ç§è¯­è¨€ç‰¹å¾ã€‚å®ƒä»¬å­¦ä¹ åˆ°çš„ä¸€äº›ç‰¹å¾æˆ‘ä»¬èƒ½å¤Ÿç†è§£ï¼Œæœ‰äº›åˆ™æ›´åŠ éš¾ä»¥æ‰æ‘¸ã€‚

#### \\(W^O\\)çŸ©é˜µ

ä¸Šé¢çš„æ“ä½œç›¸å½“äºæŠŠä¸€ä¸ªè¿›ç¨‹æ‹†åˆ†æˆ8ä¸ªç‹¬ç«‹çš„å­è¿›ç¨‹è¿›è¡Œæ“ä½œï¼Œæ¯ä¸ªè¿›ç¨‹å¤„ç†åŸå§‹Embeddingçš„1/nã€‚æœ€ç»ˆæ¯ä¸ªè¿›ç¨‹å¾—åˆ°çš„å‘é‡é•¿åº¦æ˜¯åŸæ¥embeddingé•¿åº¦çš„1/nã€‚æ€æ ·æŠŠä¸åŒæ³¨æ„åŠ›å¤´çš„è¾“å‡ºåˆèµ·æ¥å‘¢ï¼Ÿç³»ç»Ÿä¼šåœ¨dè¿™ä¸ªç»´åº¦ï¼Œé€šè¿‡ Concat æ–¹å¼æŠŠ8ä¸ªå­è¿›ç¨‹çš„ç»“æœä¸²è”èµ·æ¥ï¼Œç›´æ¥æ‹¼æ¥æˆä¸€ä¸ªé•¿å‘é‡ã€‚æ­¤æ—¶ Concat åçš„çŸ©é˜µå®é™…ä¸Šå¹¶ä¸æ˜¯æœ‰æœºåœ°èåˆ 8 ä¸ªâ€œå°Embeddingâ€ï¼Œè€Œåªæ˜¯ç®€å•åœ°åšäº†çŸ©é˜µçš„å‰åé“¾æ¥ Concatã€‚è¿™å°±å¸¦æ¥äº†å‡ ä¸ªé—®é¢˜ï¼š

*   å¤šä¸ªå¤´ç›´æ¥æ‹¼æ¥çš„æ“ä½œï¼Œ ç›¸å½“äºé»˜è®¤äº†æ¯ä¸ªå¤´æˆ–è€…è¯´æ¯ä¸ªå­ç©ºé—´çš„é‡è¦æ€§æ˜¯ä¸€æ ·çš„ï¼Œ åœ¨æ¯ä¸ªå­ç©ºé—´é‡Œé¢å­¦ä¹ åˆ°çš„ç›¸ä¼¼æ€§çš„é‡è¦åº¦æ˜¯ä¸€æ ·çš„ï¼Œå³è¿™äº›å¤´çš„æƒé‡æ˜¯ä¸€æ ·çš„ã€‚ç„¶è€Œï¼Œå„ä¸ªå¤´çš„æƒé‡äº‹å®ä¸Šè‚¯å®šä¸åŒï¼Œå¦‚ä½•æœ‰æœºèåˆï¼Ÿæˆ–è€…è¯´ï¼Œå¦‚ä½•è°ƒæ•´ä¸åŒå¤´ä¹‹é—´çš„æƒé‡æ¯”ä¾‹ï¼Ÿ
    
*   è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¨¡å—ä¼šæ¥åˆ°å…¨è¿æ¥ç½‘ç»œï¼ŒFFNéœ€è¦çš„è¾“å…¥æ˜¯ä¸€ä¸ªçŸ©é˜µè€Œä¸æ˜¯å¤šä¸ªçŸ©é˜µã€‚è€Œä¸”å› ä¸ºæœ‰æ®‹å·®è¿æ¥çš„å­˜åœ¨ï¼Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦åº”è¯¥æ˜¯ä¸€æ ·çš„ã€‚
    

ç»¼ä¸Šï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‹ç¼©ã€è½¬æ¢å’Œèåˆçš„æ‰‹æ®µï¼ŒæŠŠ 8 ä¸ªå°çš„è¯­ä¹‰é€»è¾‘å­ç©ºé—´æœ‰æœºåœ°æ•´åˆæˆä¸€ä¸ªæ€»ä½“çš„ Embeddingï¼Œè€Œä¸”éœ€è¦æŠŠå¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºæ¢å¤ä¸ºåŸ Embedding çš„ç»´åº¦å¤§å°ï¼Œå³512ç»´çš„å‘é‡é•¿åº¦ã€‚ä½†æ˜¯æœ‰æœºèåˆæ˜¯ä¸ªå¤æ‚çš„æƒ…å†µï¼Œåªå‡­å€ŸäººåŠ›éš¾ä»¥åšå¥½ã€‚å› æ­¤ç ”å‘äººå‘˜æå‡ºæ¥æŠŠèåˆç›´æ¥åšæˆå¯å­¦ä¹ ã€å¯è®­ç»ƒçš„ã€‚å³è®¾å®šä¸€ä¸ªå¯å­¦ä¹ å‚æ•°ï¼Œå¦‚æœå®ƒè§‰å¾—æŸä¸ªå¤´é‡è¦ï¼Œ é‚£å¹²è„†è®©é‚£ä¸ªå¤´å¯¹åº”çš„å¯å­¦ä¹ å‚æ•°å¤§äº›ï¼Œè¾“å‡ºçš„çŸ©é˜µå¤§äº›ï¼Œè¿™å°±ç±»ä¼¼äºå¢åŠ äº†å¯¹åº”å¤´çš„æƒé‡ã€‚

æœ€ç»ˆå°±å¾—åˆ°æ˜¯\\(W^O\\)æ–¹æ¡ˆã€‚åˆ©ç”¨\\(W^O\\) å¯¹å¤šå¤´çš„è¾“å‡ºè¿›è¡Œå‹ç¼©å’Œèåˆæ¥æå‡ç‰¹å¾è¡¨å¾å’Œæ³›åŒ–èƒ½åŠ›ã€‚\\(W^{O}\\)ç±»ä¼¼ \\(W^{Q}\\)ï¼Œ\\(W^{K}\\)ï¼Œ$W^{V} \\(ï¼Œä¹Ÿæ˜¯åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µä¸€åŒè®­ç»ƒå‡ºæ¥çš„æƒé‡çŸ©é˜µï¼ˆå³ä¸Šè§’ O æ„ä¸ºè¾“å‡º Output çš„æ„æ€ï¼‰ã€‚\\)W^O$æ“ä½œå‰åï¼Œç»´åº¦æ²¡æœ‰å˜åŒ–ã€‚å³æœ€ç»ˆè¾“å‡ºçš„ç»“æœå’Œè¾“å…¥çš„è¯åµŒå…¥å½¢çŠ¶ä¸€æ ·ã€‚

### 2.2 è®¾è®¡æ€è·¯

æˆ‘ä»¬æ¥åæ¨æˆ–è€…çŒœæµ‹ä¸€ä¸‹Transformerä½œè€…çš„è®¾è®¡æ€è·¯å¤§è‡´ä¸ºï¼šä»¥åˆ†æ²»+èåˆçš„æ¨¡å¼å¯¹æ•°æ®è¿›è¡ŒåŠ å·¥ã€‚åˆ†æ²»æ˜¯å¯¹æ•°æ®è¿›è¡Œæœ‰å·®åˆ«çš„å¯¹å¾…ï¼Œè€Œèåˆæ˜¯åšæ•°æ®èåˆã€‚

#### å­ç©ºé—´&åˆ†æ²»

**Embedding**

å‰é¢æåˆ°ï¼ŒEmbedding æ‰æ˜¯å¤šå¤´èƒŒåçš„çœŸæ­£å†…åœ¨æˆå› ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬å†çœ‹çœ‹è¿™ä¸ª Embedding ä¸­çš„è¯­ä¹‰é€»è¾‘å­ç©ºé—´ã€‚æˆ‘ä»¬å‡è®¾æœ‰8ä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ¯ä¸ªæ³¨æ„å¤´éƒ½æœ‰è‡ªå·±çš„å¯å­¦ä¹ æƒé‡çŸ©é˜µ\\(W\_i^Q\\), \\(W\_i^K\\)å’Œ\\(W\_i^V\\)ã€‚$W^{Q} \\(ï¼Œ\\)W{K}$ï¼Œ$W$ å‡æ˜¯ Transformer å¤§æ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µæ—¶ï¼Œé€šè¿‡æµ·é‡çš„å¯¹ç…§è¯­æ–™è®­ç»ƒé›†è®­ç»ƒå‡ºæ¥çš„ï¼Œä»–ä»¬æ˜¯ä¸“é—¨ç”¨æ¥æ‹†è§£æ¯ä¸ª token åœ¨ Embedding ç©ºé—´ä¸­çš„é€»è¾‘ç»†åˆ†å­ç©ºé—´ç”¨çš„ã€‚

é€šè¿‡è¿™äº›æƒé‡çŸ©é˜µå¯ä»¥æŠŠåŸå§‹é«˜ç»´å‘é‡åˆ†è§£æˆ 8 ä¸ªç»†åˆ†çš„ Embedding å‘é‡ï¼Œæ¯ä¸ªå‘é‡å¯¹åº”åˆ°ä¸€ä¸ªç»†åˆ†è¯­ä¹‰é€»è¾‘å­ç©ºé—´ï¼ˆè¯­ä¹‰é€»è¾‘ã€è¯­æ³•é€»è¾‘ã€ä¸Šä¸‹æ–‡é€»è¾‘ã€åˆ†ç±»é€»è¾‘ç­‰ï¼‰ã€‚å®é™…ä¸Šä¾¿æ˜¯æŠŠ Attention æœºåˆ¶åˆ†å‰²åœ¨ Embedding ä¸­çš„ä¸åŒç»†åˆ†é€»è¾‘å­ç©ºé—´ä¸­æ¥è¿ä½œäº†ã€‚æ¯ä¸ªæ³¨æ„åŠ›å¤´äº’ç›¸ç‹¬ç«‹çš„å…³æ³¨åˆ°ä¸åŒçš„å­ç©ºé—´ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶è€ƒè™‘è¯¸å¤šé—®é¢˜ï¼Œä»è€Œè·å¾—æ›´ä¸°å¯Œçš„ç‰¹å¾ä¿¡æ¯ã€‚

**ç‰¹å¾æå–**

Transformerçš„å¤šå¤´æ³¨æ„åŠ›åº”è¯¥ä¹Ÿå€Ÿé‰´äº†CNNä¸­åŒä¸€å·ç§¯å±‚å†…ä½¿ç”¨å¤šä¸ªå·ç§¯æ ¸çš„æ€æƒ³ã€‚CNNä¸­ä½¿ç”¨äº†ä¸åŒçš„å·ç§¯æ ¸æ¥å…³æ³¨å›¾åƒä¸­çš„ä¸åŒç‰¹å¾ï¼Œå­¦ä¹ ä¸åŒçš„ä¿¡æ¯ã€‚ç„¶åCNNä¸­é€é€šé“å·ç§¯æœ€åæ²¿ç€é€šé“æ±‚å’Œåšç‰¹å¾èåˆã€‚

Transformerçš„è§’è‰²å®šä½æ˜¯ç‰¹å¾æŠ½å–å™¨æˆ–è€…ä¸‡èƒ½å‡½æ•°é€¼è¿‘å™¨ã€‚æˆ‘ä»¬æœŸæœ›æ•æ‰æ›´å¤šçš„æ¨¡å¼ï¼Œä»è€Œåˆ©äºä¸‹æ¸¸å¤šæ ·çš„ä»»åŠ¡å¾®è°ƒæ—¶ï¼Œä¸€æ—¦è¿™ç±»æ¨¡å¼æœ‰ç”¨ï¼Œå°±å¯ä»¥æ¿€æ´»å‡ºæ¥è®©ä¸‹æ¸¸ä»»åŠ¡å¯ä»¥å­¦ä¹ åˆ°ã€‚æ‰€ä»¥Transformerä½¿ç”¨å¤šå¤´å¯¹ä¸€ä¸ªå‘é‡åˆ‡åˆ†ä¸åŒçš„ç»´åº¦æ¥æ•æ‰ä¸åŒçš„æ¨¡å¼ï¼Œè®©æ¨¡å‹å¯èƒ½ä»å¤šç§ç»´åº¦å»ç†è§£è¾“å…¥å¥å­çš„å«ä¹‰ã€‚å•ä¸ªæ¦‚å¿µå‘é‡å¯ä»¥é€šè¿‡ä¸åŒçš„å‡½æ•°è¿›è¡ŒæŠ•å½±ï¼Œä»¥ç”¨äºä¸åŒç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ã€‚ç„¶åä¹Ÿä¼šæ¥ç€ä¸€ä¸ªç‰¹å¾èåˆè¿‡ç¨‹ã€‚æ˜ å°„åˆ°ä¸åŒå­ç©ºé—´å…¶å®å°±æ˜¯åœ¨æ¨¡ä»¿å·ç§¯ç¥ç»ç½‘ç»œä»¥æ”¯æŒå¤šé€šé“æ¨¡å¼çš„è¾“å‡ºã€‚

#### ensemble&èåˆ

ä¸Šé¢é‡ç‚¹è¯´çš„æ˜¯å°†è¾“å…¥åˆ‡åˆ†ï¼Œç„¶åæå–ä¸åŒå­ç©ºé—´çš„ä¿¡æ¯ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ä»å¦ä¸€ä¸ªæ–¹é¢æ¥è§£é‡Šï¼Œå¤šå¤´çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯ensembleã€‚

å¤§é‡å­¦æœ¯è®ºæ–‡è¯æ˜ï¼Œå¾ˆéš¾åªä¾é å•ä¸ªå¤´å°±å¯ä»¥åŒæ—¶æ•æ‰åˆ°è¯­æ³•/å¥æ³•/è¯æ³•ä¿¡æ¯ï¼Œå› æ­¤éœ€è¦å¤šå¤´ã€‚ä½†æ˜¯å¤šå¤´ä¸­æ¯ä¸ªå¤´çš„åŠŸèƒ½ä¸åŒï¼Œæœ‰çš„å¤´å¯èƒ½è¯†åˆ«ä¸åˆ°å•¥ä¿¡æ¯ï¼Œæœ‰çš„å¤´å¯èƒ½ä¸»è¦è¯†åˆ«ä½ç½®ä¿¡æ¯ï¼Œæœ‰çš„å¤´å¯èƒ½ä¸»è¦è¯†åˆ«è¯­æ³•ä¿¡æ¯ï¼Œæœ‰çš„å¤´ä¸»è¦è¯†åˆ«è¯æ³•ä¿¡æ¯ã€‚multi-headçš„ä½œç”¨å°±æ˜¯ä¸ºäº†ä¿è¯è¿™äº›patternéƒ½èƒ½å¤Ÿè¢«æŠ½å–å‡ºæ¥ã€‚

æˆ‘ä»¬å¯ä»¥æŠŠMHAçš„å¤šä¸ªattentionè®¡ç®—è§†ä¸ºå¤šä¸ªç‹¬ç«‹çš„å°æ¨¡å‹ï¼Œæ¯ä¸ªheadå°±åƒæ˜¯ä¸€ä¸ªå¼±åˆ†ç±»å™¨ï¼Œæœ€ç»ˆæ•´ä½“çš„concatè®¡ç®—ç›¸å½“äºæŠŠæ¥è‡ªå¤šä¸ªå°æ¨¡å‹çš„ç»“æœè¿›è¡Œäº†èåˆï¼Œä»è€Œè®©æœ€åå¾—åˆ°çš„embeddingå…³æ³¨å¤šæ–¹é¢ä¿¡æ¯ã€‚è€Œä¸”ï¼Œå•å¤´å®¹æ˜“åªå…³æ³¨è‡ªèº«çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¤šå¤´ï¼ˆéœ€è¦è®©å…¶æœ‰ä¸€å®šçš„å¤´çš„åŸºæ•°ï¼‰æ— ç–‘æ˜¯é€šè¿‡å¤šæ¬¡æŠ•ç¥¨é™ä½è¿™ç§æ¦‚ç‡ï¼Œè¿™æ ·æ•ˆæœæ¯”è¾ƒå¥½ä¹Ÿæ˜¯æ¯”è¾ƒç¬¦åˆç›´è§‰çš„ã€‚åšä¸ªæ¯”å–»æ¥è¯´ï¼Œè¿™å°±å¥½åƒæ˜¯å…«ä¸ªæœ‰ä¸åŒé˜…è¯»ä¹ æƒ¯çš„ç¿»è¯‘å®¶ä¸€åŒç¿»è¯‘åŒä¸€ä¸ªå¥å­ï¼Œä»–ä»¬æ¯ä¸ªäººå¯èƒ½ç¿»è¯‘æ—¶é˜…è¯»é¡ºåºå’Œå…³æ³¨ç‚¹éƒ½æœ‰æ‰€ä¸åŒï¼Œç»¼åˆä»–ä»¬å…«ä¸ªäººçš„æ„è§ï¼Œæœ€ç»ˆå¾—å‡ºæ¥çš„ç¿»è¯‘ç»“æœå¯èƒ½ä¼šæ›´åŠ å‡†ç¡®ã€‚

#### ç¼“è§£ç¨€ç–

é€šè¿‡è§‚å¯Ÿå¤§é‡æ ·æœ¬çš„attentionçŸ©é˜µæˆ‘ä»¬å‘ç°ï¼Œå…¶å®å‡ ä¹æ¯ä¸€ä¸ªtokenåœ¨å…¨å¥ä¸­çš„æ³¨æ„åŠ›éƒ½æ˜¯ç¨€ç–çš„ï¼Œå³æ¯ä¸ªtokenåªå…³æ³¨éå¸¸æœ‰é™ä¸ªå…¶ä»–tokenï¼Œå…¶ä½™æ³¨æ„åŠ›åŸºæœ¬å¯ä»¥çœ‹æˆæ˜¯0ï¼ˆsoftmaxæ— æ³•ä¸¥æ ¼ä¸º0ï¼‰ã€‚

ç¨€ç–å°±æ„å‘³ç€æˆ‘ä»¬ç”¨è¾ƒå°çš„çŸ©é˜µå°±å¯ä»¥æ¥åˆè¾ƒå¤§çš„ç¨€ç–çŸ©é˜µï¼Œå…¶æ•ˆæœå·®ä¸å¤šï¼Œä½†æ˜¯è®¡ç®—é‡å´å°å¾ˆå¤šã€‚å› æ­¤å°±ä¸å¦‚æŠŠQã€Kå’ŒVåˆ‡åˆ†æˆå¤šä¸ªå°æ®µï¼Œè®¡ç®—å¤šæ¬¡æ³¨æ„åŠ›çŸ©é˜µï¼Œå†å†ä»¥æŸç§æ–¹å¼æ•´åˆï¼Œè¿™æ ·ä¸€æ¥è®¡ç®—é‡å…¶å®è·Ÿç›´æ¥ ç®—å•ä¸ªæ³¨æ„åŠ›å·®ä¸å¤šï¼Œä½†è¿™æ ·æ¨¡å‹èåˆçš„æ•ˆæœåº”è¯¥è‡³å°‘ä¸å·®äºå•ä¸ªæ³¨æ„åŠ›ï¼Œç”šè‡³å¯èƒ½æ›´å¥½ï¼Œå› æ­¤æœ‰äº†å¤šå¤´æ³¨æ„åŠ›ã€‚

### 2.3 è®¡ç®—

#### è®¡ç®—æµç¨‹

å¤šå¤´æ³¨æ„åŠ›çš„è®¡ç®—æµç¨‹å°±æ˜¯æŠŠé«˜ç»´å‘é‡åˆ‡åˆ†ä¸ºè‹¥å¹²ä»½ä½ç»´å‘é‡ï¼Œåœ¨è‹¥å¹²ä½ç»´ç©ºé—´å†…åˆ†åˆ«æ±‚è§£å„è‡ªçš„Scaled Dot-Product Attentionï¼ˆç‚¹ç§¯è‡ªæ³¨æ„åŠ›ï¼‰ã€‚æ€»ä½“æµç¨‹åˆ†ä¸ºï¼šåˆ‡åˆ†ï¼Œè®¡ç®—ï¼Œæ‹¼æ¥ï¼Œèåˆå››éƒ¨åˆ†ï¼Œè¿™é‡Œæ¶‰åŠå¾ˆå¤šæ­¥éª¤å’ŒçŸ©é˜µè¿ç®—ï¼Œæˆ‘ä»¬ç”¨ä¸€å¼ å¤§å›¾æŠŠæ•´ä¸ªè¿‡ç¨‹è¡¨ç¤ºå‡ºæ¥ã€‚

*   è¾“å…¥ä¾ç„¶æ˜¯åŸå§‹çš„Qï¼ŒK å’Œ Vã€‚
*   åˆ‡åˆ†ã€‚æ¯ä¸ªæ³¨æ„å¤´éƒ½æœ‰è‡ªå·±çš„å¯å­¦ä¹ æƒé‡çŸ©é˜µ\\(W\_i^Q\\), \\(W\_i^K\\)å’Œ\\(W\_i^V\\)ã€‚è¾“å…¥çš„Qã€Kå’ŒVç»è¿‡è¿™äº›æƒé‡çŸ©é˜µè¿›è¡Œå¤šä¸ªçº¿æ€§å˜æ¢åå¾—åˆ° N ç»„Queryï¼ŒKey å’Œ Valueã€‚è¿™äº›ç»„Qã€Kå’ŒVå¯ä»¥ç†è§£ä¸ºæŠŠè¾“å…¥çš„é«˜ç»´å‘é‡çº¿æ€§æŠ•å½±åˆ°æ¯”è¾ƒä½çš„ç»´åº¦ä¸Šã€‚æ¯ä¸ªæ–°å½¢æˆçš„Qåœ¨æœ¬è´¨ä¸Šéƒ½è¦æ±‚ä¸åŒç±»å‹çš„ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå…è®¸æ³¨æ„åŠ›æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å‘é‡è®¡ç®—ä¸­å¼•å…¥æ›´å¤šä¿¡æ¯ã€‚æ­¤å¤„å¯¹äºä¸‹å›¾çš„æ ‡å·1ã€‚
*   è®¡ç®—ã€‚æ¯ä¸ªå¤´éƒ½ä½¿ç”¨ Self-Attention è®¡ç®—å¾—åˆ° N ä¸ªå‘é‡ã€‚æ¯ä¸ªå¤´å¯ä»¥ä¸“æ³¨å­¦ä¹ è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨æ›´å¤šçš„ä¿¡æ¯ã€‚æ­¤å¤„å¯¹äºä¸‹å›¾çš„æ ‡å·2ã€‚
*   æ‹¼æ¥ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªå•ä¸€çš„ä¸Šä¸‹æ–‡å‘é‡ä½œä¸ºæ³¨æ„åŠ›æ¨¡å‹çš„è¾“å‡ºã€‚å› æ­¤ï¼Œç”±å•ä¸ªæ³¨æ„å¤´äº§ç”Ÿçš„ä¸Šä¸‹æ–‡å‘é‡è¢«æ‹¼æ¥ä¸ºä¸€ä¸ªå‘é‡ã€‚æ­¤å¤„å¯¹äºä¸‹å›¾æ ‡å·3ã€‚
*   èåˆã€‚ä½¿ç”¨æƒé‡çŸ©é˜µ\\(W^O\\)ä»¥ç¡®ä¿ç”Ÿæˆçš„ä¸Šä¸‹æ–‡å‘é‡æ¢å¤ä¸ºåŸ Embedding çš„ç»´åº¦å¤§å°ã€‚è¿™å³æ˜¯é™ç»´æ“ä½œï¼Œä¹Ÿæ˜¯èåˆæ“ä½œã€‚æ­¤å¤„å¯¹äºä¸‹å›¾çš„æ ‡å·4ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125502955-1666869778.jpg)

#### è®¡ç®—å¼ºåº¦

æˆ‘ä»¬ä»¥ä¸‹å›¾ä¸ºåŸºç¡€æ¥æ€è€ƒè®¡ç®—å¼ºåº¦ï¼ŒD è¡¨ç¤º hidden sizeï¼ŒH è¡¨ç¤º Head ä¸ªæ•°ï¼ŒL è¡¨ç¤ºå½“å‰æ˜¯åœ¨åºåˆ—çš„ç¬¬ L ä¸ª Tokenã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125627239-1758300943.jpg)

*   å½“ Batch Size ä¸º 1 æ—¶ï¼Œå›¾ä¸­çº¢è‰²ã€ç´«è‰²ã€è“è‰²è™šçº¿æ¡†å¤„çš„çŸ©é˜µä¹˜æ³•å…¨éƒ¨ä¸ºçŸ©é˜µä¹˜å‘é‡ï¼Œæ˜¯ Memory Boundï¼ˆå†…å­˜å—é™æ“ä½œï¼‰ï¼Œç®—æœ¯å¼ºåº¦ä¸åˆ° 1ã€‚
*   å½“ Batch Size å¤§äº 1 æ—¶ï¼ˆæ¯”å¦‚ Continuous Batchingï¼‰ï¼š
    *   çº¢è‰²å’Œè“è‰²è™šçº¿æ¡†éƒ¨åˆ†ï¼šå› ä¸ºæ˜¯æƒé‡ä¹˜ä»¥æ¿€æ´»ï¼Œæ‰€ä»¥ä¸åŒçš„è¯·æ±‚ä¹‹é—´å¯ä»¥å…±äº« Weightã€‚è¿™é‡Œå˜æˆçŸ©é˜µä¹˜çŸ©é˜µï¼Œå¹¶ä¸” Batch Size è¶Šå¤§ï¼Œç®—æœ¯å¼ºåº¦è¶Šå¤§ï¼Œä¹Ÿå°±è¶Šè¶‹è¿‘äº Compute Boundï¼ˆFFN å±‚ä¹Ÿç±»ä¼¼ï¼‰ã€‚
    *   ç´«è‰²è™šçº¿æ¡†éƒ¨åˆ†ï¼šè¿™é‡Œ Qã€K å’Œ V çš„ Attention è®¡ç®—ï¼Œæ˜¯æ¿€æ´»ä¹˜ä»¥æ¿€æ´»ï¼Œæ‰€ä»¥ä¸åŒçš„è¯·æ±‚ä¹‹é—´æ²¡æœ‰ä»»ä½•ç›¸å…³æ€§ã€‚å³ä½¿ Batchingï¼Œè¿™é‡Œä¹Ÿæ˜¯ Batched çŸ©é˜µä¹˜å‘é‡ï¼Œå¹¶ä¸”å› ä¸ºåºåˆ—é•¿åº¦å¯èƒ½ä¸åŒï¼Œè¿™é‡Œä¸åŒè¯·æ±‚çš„çŸ©é˜µä¹˜å‘é‡æ˜¯ä¸è§„åˆ™çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™é‡Œç®—æœ¯å¼ºåº¦å§‹ç»ˆä¸åˆ° 1ï¼Œæ˜¯æ˜æ˜¾çš„ Memory Boundã€‚

ä»ä¸Šå¯ä»¥çœ‹å‡ºï¼Œé€šè¿‡ Continuous Batching å¯ä»¥å¾ˆå¥½çš„å°† Memory Bound é—®é¢˜è½¬å˜ä¸º Compute Boundï¼Œä½† Qã€K å’Œ V çš„ Attention è®¡ç®—çš„ç®—æœ¯å¼ºåº¦å´å§‹ç»ˆå°äº 1ã€‚Sequence Length è¶Šé•¿ï¼Œè¿™é‡Œçš„è®¡ç®—é‡å°±è¶Šä¸å¯å¿½ç•¥ï¼Œå› ä¸ºå…¶å±äºç³»ç»Ÿçš„çŸ­æ¿å¤„ã€‚

### 2.4 æ•ˆæœ

Transformerè®ºæ–‡æœ«å°¾ç»™å‡ºäº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­ä¸¤ä¸ªå¤´çš„attentionå¯è§†åŒ–ç»“æœï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚å›¾ä¸­ï¼Œçº¿æ¡è¶Šç²—è¡¨ç¤ºattentionçš„æƒé‡è¶Šå¤§ï¼Œå¯ä»¥çœ‹å‡ºï¼Œä¸¤ä¸ªå¤´å…³æ³¨çš„åœ°æ–¹ä¸ä¸€æ ·ï¼Œç»¿è‰²å›¾è¯´æ˜è¯¥å¤´æ›´å…³æ³¨å…¨å±€ä¿¡æ¯ï¼Œçº¢è‰²å›¾è¯´æ˜è¯¥å¤´æ›´å…³æ³¨å±€éƒ¨ä¿¡æ¯ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125539375-850574872.jpg)

è®ºæ–‡â€œWhat Does BERT Look At? An Analysis of BERTâ€™s Attentionâ€ä¹Ÿç»™å‡ºäº†ä¸åŒæ³¨æ„åŠ›å¤´çš„ç¤ºä¾‹ã€‚çº¿æ¡çš„ç²—ç»†è¡¨ç¤ºæ³¨æ„åŠ›æƒé‡çš„å¼ºåº¦ï¼ˆä¸€äº›æ³¨æ„åŠ›æƒé‡å¤ªä½ï¼Œä»¥è‡³äºçœ‹ä¸è§ï¼‰ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125526497-211225798.jpg)

### 2.5 èåˆæ–¹å¼

vanilla Transformerä¸­ï¼Œå¯¹äºä¸åŒçš„æ³¨æ„åŠ›é‡‡å–çš„æ•´åˆæ–¹å¼æ˜¯ç›´æ¥æ‹¼æ¥ã€‚è®ºæ–‡"Multi-Head Attention: Collaborate Instead of Concatenateâ€œæå‡ºäº†å…¶å®ƒæ•´åˆæ–¹å¼ã€‚è¯¥è®ºæ–‡å‘ç°æ‰€æœ‰æ³¨æ„åŠ›å¤´ä¹‹é—´æ•æ‰çš„ä¿¡æ¯è‚¯å®šæ˜¯å­˜åœ¨å†—ä½™çš„ï¼Œå¤´ä¸å¤´ä¹‹é—´å­˜åœ¨è¾ƒå¤šçš„é€šç”¨ä¿¡æ¯ã€‚æ‹¼æ¥åçš„ \\(ğ‘Š\_ğ‘„ğ‘Š\_ğ¾^ğ‘‡\\) åªéœ€è¦å¤§æ¦‚1/3çš„ç»´åº¦å°±è¶³å¤Ÿæ•æ‰ç»å¤§éƒ¨åˆ†çš„ä¿¡æ¯äº†ã€‚å› æ­¤è®ºæ–‡ä½œè€…è®¾è®¡äº†ä¸€ä¸ªæ··åˆå‘é‡æ¥æå–æ³¨æ„åŠ›å¤´ä¹‹é—´çš„é€šç”¨ä¿¡æ¯ã€‚è¿™ä¸ªå‘é‡å¯ä»¥é€šè¿‡è·Ÿæ¨¡å‹ä¸€èµ·å­¦ä¹ å¾—åˆ°ï¼Œç„¶ååº”ç”¨åˆ°åŸå§‹çš„å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ä¸­ã€‚è¿™ç§æ–¹æ¡ˆå¯ä»¥è®©æ³¨æ„åŠ›å¤´çš„è¡¨ç¤ºæ–¹å¼æ›´åŠ çµæ´»ï¼Œæ³¨æ„åŠ›å¤´çš„ç»´åº¦å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œæ”¹å˜ã€‚ä¹Ÿè®©å‚æ•°è®¡ç®—æ›´åŠ é«˜æ•ˆã€‚

ä¸‹å›¾å·¦é¢æ˜¯vanilla Transformerçš„åŸå§‹æ‹¼æ¥æ–¹å¼ï¼Œå³é¢æ˜¯è¯¥è®ºæ–‡æå‡ºçš„æ–¹æ¡ˆCollabHeadã€‚

*   (a)æ˜¯vanilla Transformerçš„åŸå§‹æ‹¼æ¥æ–¹å¼ï¼ˆç›¸å½“äºå¯¹ä¸åŒçš„headæŠ½å–ä¸åŒç»´åº¦çš„çŸ©é˜µä¿¡æ¯ï¼‰ï¼Œä¹Ÿæ˜¯CollabHeadæ–¹å¼çš„ä¸€ç§ç‰¹ä¾‹ã€‚\\(m\_i\\)æ˜¯ä¸€ä¸ªç”±1å’Œ0ä¸¤ç§å…ƒç´ ç»„æˆçš„å‘é‡ï¼Œå…¶ä¸­1çš„å…ƒç´ ä½ç½®ä¸ºå…¶å¯¹åº”æ³¨æ„åŠ›å¤´çš„æ˜ å°„çŸ©é˜µåœ¨æ‹¼æ¥åçš„æ•´ä½“çŸ©é˜µä¸­çš„ä½ç½®ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨æ•´åˆæ³¨æ„åŠ›å¤´çš„æ—¶å€™ï¼Œè®©æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¹‹é—´éƒ½äº’ç›¸ç‹¬ç«‹ã€‚
*   (b)æ˜¯è®©æ‰€æœ‰headéƒ½å…±äº«æ˜ å°„çŸ©é˜µã€‚
*   (c)åœ¨å…±äº«æ˜ å°„çŸ©é˜µçš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å‹ç¼©æœ€ç»ˆè¾“å‡ºçš„æ•´åˆçŸ©é˜µçš„ç»´åº¦ï¼Œè¾¾åˆ°å‹ç¼©ç»´åº¦çš„æ•ˆæœã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125654699-1683794546.jpg)

### 2.6 åˆ†æ

ç ”ç©¶äººå‘˜å¯¹å¤šå¤´æ³¨æ„åŠ›åšäº†æ·±å…¥çš„åˆ†æï¼ˆæ¯”å¦‚è®ºæ–‡"What Does BERT Look At? An Analysis of BERTâ€™s Attention"ï¼‰ï¼Œå…¶ä¸­ä¸€äº›æ´å¯Ÿå’Œè§‚ç‚¹å¦‚ä¸‹ï¼š

**å¤´æ•°ç›®**

*   å¤´æ•°è¶Šå°‘ï¼Œæ³¨æ„åŠ›ä¼šæ›´å€¾å‘äºå…³æ³¨tokenè‡ªå·±æœ¬èº«æˆ–è€…å…¶ä»–çš„æ¯”è¾ƒå•ä¸€çš„æ¨¡å¼ï¼Œæ¯”å¦‚éƒ½å…³æ³¨CLSã€‚
*   å·²æœ‰è®ºæ–‡è¯æ˜å¤´æ•°ç›®ä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼ˆå¤´çš„æ•°é‡å¢å¤šä¼šå¯¼è‡´å„ä¸ªå­ç©ºé—´å˜å°ï¼Œè¿™æ ·å­ç©ºé—´èƒ½è¡¨è¾¾çš„å†…å®¹å°±å‡å°‘äº†ï¼Œè€Œå½“æœ‰è¶³å¤Ÿå¤šçš„å¤´ï¼Œå·²ç»èƒ½å¤Ÿå…³æ³¨ä½ç½®ä¿¡æ¯ï¼Œè¯­æ³•ä¿¡æ¯ï¼Œå…³æ³¨ç½•è§è¯çš„èƒ½åŠ›äº†ï¼Œå†å¤šä¸€äº›å¤´ï¼Œå¯èƒ½æ˜¯å¢è¿›ä¹Ÿå¯èƒ½æ˜¯å™ªå£°ï¼‰ã€‚å¤´å¤ªå¤šå¤ªå°‘éƒ½ä¼šå˜å·®ï¼Œå…·ä½“å¤šå°‘è¦è§†æ¨¡å‹è§„æ¨¡ï¼Œä»»åŠ¡è€Œå®šã€‚ç›®å‰å¯ä»¥çœ‹åˆ°çš„è¶‹åŠ¿æ˜¯ï¼Œæ¨¡å‹è¶Šå¤§ï¼ˆä¹Ÿå°±æ˜¯hidden sizeè¶Šå¤§ï¼‰ï¼Œå¤´æ•°è¶Šå¤šï¼Œå°±è¶Šèƒ½å¸¦æ¥å¹³å‡æ•ˆæœä¸Šçš„æ”¶ç›Šã€‚

**å­¦ä¹ æ¨¡å¼**

*   å¯¹äºå¤§éƒ¨åˆ†queryï¼Œæ¯ä¸ªå¤´éƒ½å­¦ä¹ äº†æŸç§å›ºå®šçš„æ¨¡å¼ã€‚
*   æ¯ä¸ªå¤´ç¡®å®å­¦åˆ°ä¸œè¥¿æœ‰æ‰€ä¸åŒï¼Œä½†å¤§éƒ¨åˆ†å¤´ä¹‹é—´çš„å·®å¼‚æ²¡æœ‰æˆ‘ä»¬æƒ³çš„é‚£ä¹ˆå¤§ï¼ˆæ¯”å¦‚ä¸€ä¸ªå­¦å¥æ³•ï¼Œä¸€ä¸ªå­¦è¯ä¹‰è¿™æ ·æ˜æ˜¾çš„åŒºåˆ†ï¼‰ã€‚
*   å°‘éƒ¨åˆ†å¤´å¯ä»¥æ¯”è¾ƒå¥½åœ°æ•æ‰åˆ°å„ç§æ–‡æœ¬ä¿¡æ¯ï¼Œè€Œä¸ä¼šè¿‡åˆ†å…³æ³¨è‡ªèº«ä½ç½®ï¼Œä¸€å®šç¨‹åº¦ç¼“è§£äº†ä¸Šæ–‡æåˆ°çš„è®¡ç®— \\(QK^T\\)ä¹‹åå¯¹è§’çº¿å…ƒç´ è¿‡å¤§çš„é—®é¢˜ã€‚

ä¸‹å›¾ç»™å‡ºäº†æ³¨æ„åŠ›å¤´å±•ç¤ºæƒ…å†µï¼Œæœ‰çš„æ³¨æ„åŠ›å¤´å…³æ³¨æ‰€æœ‰çš„è¯ï¼ˆbroadlyï¼‰ï¼Œæœ‰çš„æ³¨æ„åŠ›å¤´å…³æ³¨ä¸‹ä¸€ä¸ªtokenï¼Œæœ‰çš„æ³¨æ„åŠ›å¤´å…³æ³¨SEPç¬¦å·ï¼Œæœ‰çš„æ³¨æ„åŠ›å¤´å…³æ³¨æ ‡ç‚¹ç¬¦å·ã€‚çº¿æ¡çš„ç²—ç»†è¡¨ç¤ºæ³¨æ„åŠ›æƒé‡çš„å¼ºåº¦ï¼ˆä¸€äº›æ³¨æ„åŠ›æƒé‡å¤ªä½ï¼Œä»¥è‡³äºçœ‹ä¸è§ï¼‰ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125709231-593240931.jpg)

**å¤´ä¸å±‚çº§çš„å…³ç³»**

*   è¶Šé è¿‘åº•å±‚çš„æ³¨æ„åŠ›ï¼Œå…¶patternç§ç±»è¶Šä¸°å¯Œï¼Œå…³æ³¨åˆ°çš„ç‚¹è¶Šå¤šã€‚
*   æ¨¡å¼éšç€å±‚æ•°çš„å¢åŠ è€Œæ…¢æ…¢å›ºå®šã€‚å¤´ä¹‹é—´çš„å·®è·ï¼ˆæˆ–è€…è¯´æ–¹å·®ï¼‰éšç€æ‰€åœ¨å±‚æ•°å˜å¤§è€Œå‡å°‘ï¼Œå³è¶Šæ¥è¶ŠåŒ…å«æ›´å¤šçš„ä½ç½®ä¿¡æ¯ã€‚
*   è¶Šåˆ°é¡¶å±‚çš„æ³¨æ„åŠ›ï¼Œå¤§éƒ¨åˆ†æ³¨æ„åŠ›å¤´çš„patternè¶‹åŒã€‚
*   æœ€åç•™ä¸‹æ¥çš„æå°‘ä¸ç›¸åŒçš„æ³¨æ„åŠ›å¤´å°±æ˜¯è¿™ä¸ªæ¨¡å‹è¡¨è¾¾è¯­ä¹‰ä¿¡æ¯çš„æ³¨æ„åŠ›å¤´ã€‚è¿™ä¹Ÿå¯ä»¥è¯´æ˜ï¼Œä¸ºä»€ä¹ˆéœ€è¦å¤šå±‚çš„Transformerå †å ï¼Œå› ä¸ºæœ‰äº›ä¿¡æ¯å¯èƒ½åœ¨æŸä¸€å±‚ä¹‹ä¸­æ— æ³•æ•æ‰åˆ°ï¼Œéœ€è¦åœ¨å…¶å®ƒå±‚æ•æ‰ã€‚

è®ºæ–‡"What Does BERT Look At? An Analysis of BERTâ€™s Attention"è¿˜åˆ†æäº†BERTå¯¹è¯è¯­ä¹‹é—´ä¾å­˜å…³ç³»çš„è¯†åˆ«æ•ˆæœã€‚ä¾å­˜å…³ç³»æ˜¯è¯è¯­å’Œè¯è¯­ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæ¯”å¦‚â€œè°“è¯­â€æ˜¯ä¸€ä¸ªå¥å­çš„ä¸­å¿ƒï¼Œå…¶ä»–æˆåˆ†ä¸åŠ¨è¯æˆ–ç›´æ¥æˆ–é—´æ¥çš„äº§ç”Ÿå…³ç³»ã€‚é€šè¿‡å¯¹è¯è¯­ä¹‹é—´ä¾å­˜å…³ç³»çš„åˆ†æï¼Œè®ºæ–‡ä½œè€…å‘ç°BERTæ— æ³•å¯¹æ‰€æœ‰çš„ä¾å­˜å…³ç³»æœ‰æ¯”è¾ƒå¥½çš„å¤„ç†ï¼Œä½†æ˜¯ç‰¹å®šçš„å±‚ä¼šå¯¹ç‰¹å®šçš„ä¾å­˜å…³ç³»è¯†åˆ«çš„æ¯”è¾ƒå¥½ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125721205-1214458753.jpg)

è®ºæ–‡â€Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Prunedâ€œå¯¹å¤šä¸ªHeadè¿›è¡Œäº†åˆ†æï¼Œå‘ç°å¤šä¸ªHeadçš„ä½œç”¨æœ‰å¤§å¤šæ•°æ˜¯å†—ä½™çš„ï¼Œå¾ˆå¤šå¯ä»¥è¢«ç æ‰ã€‚æ–‡ä¸­é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè·‘å®éªŒï¼Œå‘ç°å¤§éƒ¨åˆ†Headå¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç§ï¼š

*   Positional Headï¼šä¸»è¦å…³æ³¨é‚»å±…çš„ä½ç½®å¤´ã€‚è¿™ä¸ªHeadè®¡ç®—çš„æƒå€¼é€šå¸¸æŒ‡å‘ä¸´è¿‘çš„è¯ï¼Œè§„åˆ™æ˜¯è¿™ä¸ªHeadåœ¨90%çš„æƒ…å†µä¸‹éƒ½ä¼šæŠŠæœ€å¤§çš„æƒå€¼åˆ†é…ç»™å·¦è¾¹æˆ–è€…å³è¾¹çš„ä¸€ä¸ªè¯ã€‚
*   Syntactic Headï¼šæŒ‡å‘å…·æœ‰ç‰¹å®šè¯­æ³•å…³ç³»çš„tokençš„å¥æ³•å¤´ã€‚è¿™ä¸ªHeadè®¡ç®—çš„æƒå€¼é€šå¸¸ä¼šå°†è¯è¯­ä¹‹é—´çš„å…³ç³»è”ç³»èµ·æ¥ï¼Œæ¯”å¦‚åè¯å’ŒåŠ¨è¯çš„æŒ‡å‘å…³ç³»ã€‚
*   Rare Headï¼šæŒ‡å‘å¥å­ä¸­ç”Ÿåƒ»è¯çš„å¤´ã€‚è¿™ä¸ªHeadé€šå¸¸ä¼šæŠŠå¤§çš„æƒå€¼åˆ†é…ç»™ç¨€æœ‰è¯ã€‚

è¯æ˜å…¶å¤´éƒ¨åˆ†ç±»é‡è¦æ€§çš„æœ€å¥½æ–¹æ³•æ˜¯ä¿®å‰ªå…¶ä»–ç±»åˆ«ã€‚ä»¥ä¸‹æ˜¯ä»–ä»¬çš„ä¿®å‰ªç­–ç•¥ç¤ºä¾‹ï¼Œè¯¥ç­–ç•¥åŸºäºæ™®é€štransformerçš„ 48 ä¸ªå¤´ï¼ˆ8 ä¸ªå¤´ä¹˜ä»¥ 6 ä¸ªå—ï¼‰çš„å¤´è¿›è¡Œåˆ†ç±»ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125733266-228275211.jpg)

ä¸Šå›¾å±•ç¤ºäº†ä¿®å‰ªåä¿ç•™ç¼–ç å™¨å¤´çš„åŠŸèƒ½ã€‚æ¯åˆ—ä»£è¡¨ä¸åŒä¿®å‰ªé‡ã€‚å¯ä»¥å‘ç°ï¼Œé€šè¿‡ä¿ç•™è¢«å½’ç±»ä¸ºä¸»è¦ç±»åˆ«çš„æ³¨æ„åŠ›å¤´ï¼Œä»–ä»¬è®¾æ³•ä¿ç•™äº† 48ä¸ªå¤´ä¸­çš„ 17 ä¸ªã€‚è¯·æ³¨æ„ï¼Œè¿™å¤§çº¦ç›¸å½“äºç¼–ç å™¨æ€»å¤´æ•°çš„ 2/3ã€‚æ¯åˆ—ä¸‹é¢æ•°å­—ä»£è¡¨å‰©ä½™å¤šå°‘å¤´ã€‚

è¯¥è®ºæ–‡è¿˜åˆ†æäº†å¦‚ä½•å»ç²¾ç®€Headsï¼Œä¼˜åŒ–çš„æ–¹æ³•å¦‚ä¸‹ï¼ˆç»™å„ä¸ªHeadåŠ ä¸ªæƒå€¼ï¼Œç›¸å½“äºé—¨æ§ï¼‰ï¼š

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125745609-260008388.jpg)

### 2.7 ä¼˜ç‚¹

å¤šå¤´æ³¨æ„åŠ›çš„ä¼˜ç‚¹å¦‚ä¸‹ï¼š

*   ä¸°å¯Œä¸Šä¸‹æ–‡ç†è§£å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œå­¦ä¹ èƒ½åŠ›ï¼Œè®©æ¨¡å‹å¯ä»¥æ•æ‰åˆ°æ›´åŠ ä¸°å¯Œçš„ç‰¹å¾å’Œä¿¡æ¯ã€‚
*   æé«˜è®¡ç®—æ•ˆç‡ï¼šç”±äºæ¯ä¸ªå¤´å·¥ä½œåœ¨è¾ƒä½ç»´åº¦çš„ç©ºé—´ä¸­ï¼Œæ³¨æ„åŠ›è®¡ç®—çš„å¤æ‚åº¦é™ä½ï¼Œä»è€Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚æ³¨æ„åŠ›è®¡ç®—çš„å¤æ‚åº¦ä¸ç»´åº¦çš„å¹³æ–¹æˆæ­£æ¯”ï¼Œæ‰€ä»¥é™ç»´å¯ä»¥æ˜¾è‘—å‡å°‘è®¡ç®—é‡ã€‚
*   å¹¶è¡ŒåŒ–èƒ½åŠ›ï¼šå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´ä¸Šå¹¶è¡Œåœ°å­¦ä¹ ï¼Œè¿™æé«˜äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚
*   æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼šç”±äºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿä»å¤šä¸ªè§’åº¦åˆ†æè¾“å…¥æ•°æ®ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¾—åˆ°æå‡ã€‚åŒæ—¶ï¼Œä¹Ÿä½¿å¾—æ¨¡å‹å¯¹è¾“å…¥ä¸­çš„å™ªå£°å’Œå˜åŒ–æ›´åŠ é²æ£’ã€‚å³ä½¿æŸäº›å¤´è¢«å™ªå£°æˆ–è€…ä¸ç›¸å…³çš„ä¿¡æ¯å¹²æ‰°ï¼Œå…¶ä»–å¤´ä»ç„¶å¯ä»¥æä¾›æœ‰ç”¨çš„ä¿¡æ¯ã€‚
*   æé«˜æ¨¡å‹å®¹é‡ï¼šå³ä½¿æ¯ä¸ªå¤´å·¥ä½œåœ¨è¾ƒä½ç»´åº¦çš„å­ç©ºé—´ä¸­ï¼Œç»„åˆå¤šä¸ªå¤´çš„ç»“æœå¯ä»¥æ•æ‰åˆ°ä¸åŒå­ç©ºé—´ä¸­çš„ä¿¡æ¯ï¼Œä»è€Œå¢åŠ æ¨¡å‹çš„å®¹é‡ã€‚

0x03 å®ç°
-------

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125836299-137704544.jpg)

### 3.1 å®šä¹‰

å¤šå¤´æ³¨æ„åŠ›ç”±ç±»MultiHeadedAttentionæ¥å®ç°ï¼Œå…¶ä¸­å…³é”®å‚æ•°åŠå˜é‡å¦‚ä¸‹ã€‚

*   d\_modelæ˜¯æ¨¡å‹çš„ç»´åº¦ï¼Œä¹Ÿå°±æ˜¯å•å¤´æ³¨æ„åŠ›ä¸‹ï¼Œqueryï¼Œkeyï¼Œvalueå’Œè¯åµŒå…¥çš„å‘é‡ç»´åº¦ã€‚æˆ‘ä»¬å‡è®¾æ˜¯512ã€‚
*   hæ˜¯æ³¨æ„åŠ›å¤´æ•°ï¼Œå‡è®¾ä¸º8ã€‚
*   d\_kæ˜¯å•ä¸ªå¤´çš„æ³¨æ„åŠ›ç»´åº¦ï¼Œå¤§å°æ˜¯d\_model / hï¼Œæ‰€ä»¥512/8=64ã€‚

å¦å¤–ï¼Œæ³¨é‡Šä¸­:

*   seq\_lenæ˜¯å¥å­é•¿åº¦ï¼Œä¹Ÿå°±æ˜¯tokenä¸ªæ•°ï¼ˆå¯ä»¥è®¤ä¸ºæ˜¯å¥å­ä¸­æœ€å¤§åŒ…å«å¤šå°‘å•è¯ï¼‰ï¼Œæˆ‘ä»¬å‡è®¾æ˜¯10ä¸ªå•è¯ã€‚shapeæŒ‡çš„æ˜¯å¼ é‡å½¢çŠ¶ã€‚
*   batch\_sizeæ˜¯batch sizeã€‚

MultiHeadedAttentionçš„åˆå§‹åŒ–ä»£ç å¦‚ä¸‹ã€‚

    class MultiHeadedAttention(nn.Module):
        def __init__(self, h, d_model, dropout=0.1):
            "Take in model size and number of heads."
            super(MultiHeadedAttention, self).__init__()
            # å› ä¸ºåç»­è¦ç»™æ¯ä¸ªå¤´åˆ†é…ç­‰é‡çš„è¯ç‰¹å¾ï¼ŒæŠŠè¯åµŒå…¥æ‹†åˆ†æˆhç»„Q/K/Vï¼Œæ‰€ä»¥è¦ç¡®ä¿d_modelå¯ä»¥è¢«hæ•´é™¤ï¼Œä¿è¯ d_k = d_v = d_model/h
            assert d_model % h == 0
            # We assume d_v always equals d_k
            self.d_k = d_model // h # å•ä¸ªå¤´çš„æ³¨æ„åŠ›ç»´åº¦
            self.h = h # æ³¨æ„åŠ›å¤´æ•°é‡
            # å®šä¹‰W^Q, W^K, W^Vå’ŒW^OçŸ©é˜µï¼Œå³å››ä¸ªçº¿æ€§å±‚ï¼Œæ¯ä¸ªçº¿æ€§å±‚éƒ½å…·æœ‰d_modelçš„è¾“å…¥ç»´åº¦å’Œd_modelçš„è¾“å‡ºç»´åº¦ï¼Œå‰ä¸‰ä¸ªçº¿æ€§å±‚åˆ†åˆ«ç”¨äºå¯¹Qå‘é‡ã€Kå‘é‡ã€Vå‘é‡è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œç¬¬å››ä¸ªç”¨æ¥èåˆå¤šå¤´ç»“æœ
            self.linears = clones(nn.Linear(d_model, d_model), 4)
            self.attn = None # åˆå§‹åŒ–æ³¨æ„åŠ›æƒé‡
            self.dropout = nn.Dropout(p=dropout) # è¿›è¡Œdropoutæ“ä½œæ—¶ç½®0æ¯”ç‡ï¼Œé»˜è®¤æ˜¯0.1
    

### 3.2 è¿ç®—é€»è¾‘

ç»“åˆå“ˆä½›ä»£ç ä¸­çš„å…·ä½“å‡½æ•°ä»æ•´ä½“ä¸ŠæŠŠå¤šå¤´æ³¨æ„åŠ›çš„è®¡ç®—è¿‡ç¨‹ï¼ˆè¿™é‡Œä»ç¬¬ä¸€ä¸ªç¼–ç å±‚æ¥æ¼”ç¤ºï¼Œæ‰€ä»¥æ¶µç›–äº†è¯åµŒå…¥ï¼‰æ¢³ç†å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

æ³¨ï¼š

*   ä¸ºæ–¹ä¾¿ç†è§£ï¼Œä¸‹å›¾å»æ‰ batch\_size ç»´åº¦ï¼Œèšç„¦äºå‰©ä¸‹çš„ç»´åº¦ã€‚
*   å›¾ä¸Šé™å®šä¸º2ä¸ªå¤´ã€‚æ³¨æ„ï¼šä»£ç ä¹‹ä¸­æ²¡æœ‰åˆ‡åˆ†çº¿æ€§å±‚æƒé‡\\(W^Q,W^K,W^V\\)çš„éƒ¨åˆ†ï¼Œè€Œæ˜¯åˆç”¨ï¼Œå› æ­¤å›¾ä¸Šçœç•¥ã€‚
*   å®é™…ä¸Šä»£ç å®ç°çš„æ—¶å€™å¯ä»¥å¿½ç•¥concatï¼Œæœ€æœ´ç´ çš„å®ç°éƒ½æ˜¯åœ¨é€šé“ç»´åº¦reshapeæˆå¤šå¤´ï¼Œç„¶åè¿‡ä¸¤ä¸ªçŸ©é˜µä¹˜å°±å¯ä»¥äº†ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125852651-1423435728.jpg)

#### è¾“å…¥

ç¼–ç å™¨çš„è¾“å…¥æ˜¯è¯åµŒå…¥ï¼Œå…¶æ•°æ®ç»´åº¦ä¸ºï¼ˆbatch\_size, seq\_len, d\_model)ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè®ºæ–‡çš„æ¶æ„å›¾ä¸­ï¼ŒæŠ•å½±å’Œåˆ‡åˆ†é€šè¿‡\\(3 \\times h\\)ä¸ªå°æƒé‡çŸ©é˜µæ¥å®Œæˆã€‚

#### æŠ•å½±

æ­¤å¤„å¯¹åº”å›¾ä¸Šçš„åºå·1ã€‚

åœ¨å•å¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œè¾“å…¥ä¼šä¸ \\(W^Q,W^K,W^V\\) çŸ©é˜µç›¸ä¹˜ã€‚\\(W^Q,W^K,W^V\\) æ˜¯ä¸‰ä¸ªç‹¬ç«‹çš„çº¿æ€§å±‚ã€‚æ¯ä¸ªçº¿æ€§å±‚éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„æƒé‡ã€‚è¾“å…¥æ•°æ®ä¸ä¸‰ä¸ªçº¿æ€§å±‚åˆ†åˆ«ç›¸ä¹˜ï¼Œäº§ç”Ÿ Qã€Kã€Vã€‚è€Œå“ˆä½›ä»£ç ä¸­æ­¤å¤„ä¾ç„¶æ˜¯ç”¨ä¸‰ä¸ªå¤§çš„æƒé‡çŸ©é˜µ\\(W^Q,W^K,W^V\\) ï¼Œå¹¶éè®ºæ–‡æ‰€åˆ—å‡ºçš„\\(3 \\times h\\)ä¸ªå°æƒé‡çŸ©é˜µï¼Œç„¶è€Œï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œç‰©ç†ä¸Šçš„ä¸‰ä¸ªå¤§çš„æƒé‡çŸ©é˜µä¼šè‡ªç„¶è€Œç„¶çš„å˜æˆé€»è¾‘ä¸Šçš„\\(3 \\times h\\)ä¸ªå°æƒé‡çŸ©é˜µã€‚

#### åˆ‡åˆ†æ•°æ®

æ­¤å¤„å¯¹åº”å›¾ä¸Šçš„åºå·2ã€‚åˆ‡åˆ†å¹¶éæ˜¯ç›´æ¥åœ¨ç‰©ç†å±‚é¢ä¸Šç®€å•çš„æŠŠè¯åµŒå…¥åˆ‡åˆ†æˆhä»½ï¼Œè€Œæ˜¯è¦è¿›è¡Œé™ç»´å˜åŒ–ï¼Œå³é€šè¿‡æƒé‡çŸ©é˜µå°†å®ƒä»¬ä»åŸå§‹ç»´åº¦æ˜ å°„åˆ°è¾ƒä½çš„ç»´åº¦ï¼Œå¾—åˆ° h ä¸ªå…·æœ‰ç‹¬ç«‹è¯­ä¹‰é€»è¾‘çš„åœ¨ä¸åŒå­ç©ºé—´ä¸Šå°â€œEmbeddingâ€ã€‚

##### é€»è¾‘è§’åº¦

ç»ç”±çº¿æ€§å±‚è¾“å‡ºçš„ Qã€K å’Œ V çŸ©é˜µå°†è¢«åˆ†å‰²åˆ°å¤šä¸ªæ³¨æ„å¤´ä¸­ï¼Œä»¥ä¾¿æ¯ä¸ªæ³¨æ„å¤´èƒ½å¤Ÿç‹¬ç«‹åœ°å¤„ç†å®ƒï¼Œæ­¤å¤„ä¼šæ”¹å˜ Qã€K å’Œ V çŸ©é˜µå½¢çŠ¶ã€‚ä»é€»è¾‘ä¸Šæ¥è¯´æ˜¯åšå¦‚ä¸‹æ“ä½œã€‚

\\\[q\_i = W\_i^Q \\times q\\\\K\_i = W\_i^K \\times K\\\\V\_i = W\_i^V \\times V\\\\ \\\]

ä»å‘é‡è§’åº¦è€Œè¨€ï¼Œåˆ†å‰²æ“ä½œå°†å¼ é‡ä¸­æ¯ä¸€è¡Œ `d_model` ï¼ˆåŸå§‹è¯åµŒå…¥ï¼‰éƒ½æ‹†æˆäº†hä¸ª d\_ké•¿åº¦çš„è¡Œå‘é‡ï¼ˆå¸¦æœ‰å­è¯­ä¹‰é€»è¾‘çš„â€œå°Embeddingâ€ï¼‰ã€‚å³ï¼š(batch\_size, seq\_len, d\_model) -> (batch\_size, seq\_len, nums\_heads, d\_k)ã€‚è™½ç„¶ä» Embedding å‘é‡çš„è§’åº¦çœ‹æ˜¯ä» d\_modelç»´é™åˆ°äº†æ¯ä¸€ä¸ªå¤´çš„ d\_k ç»´ï¼Œæ¯ä¸ªå¤´æ³¨æ„åŠ›å¯¹åº”çš„ç»´åº¦å‡å°‘äº†ï¼Œä½†å®é™…ä¸Šæ¯ä¸€ä¸ªå¤´ head åŒæ ·å¯ä»¥åœ¨æŸä¸ªå­ç©ºé—´ä¸­è¡¨è¾¾æŸäº›ç»†åˆ†çš„è¯­ä¹‰é€»è¾‘ã€‚

ä»ç¥ç»ç½‘ç»œè§’åº¦è€Œè¨€ï¼šç”±äºå¯¹äºå•å±‚å…¨è¿æ¥ç½‘ç»œï¼Œè¾“å…¥å±‚ä¸éšå±‚èŠ‚ç‚¹çš„ä»»ä½•ä¸€ä¸ªå­é›†ç»“åˆï¼Œéƒ½æ˜¯ä¸€ä¸ªå®Œæ•´çš„å•éšå±‚å…¨è¿æ¥ç½‘ç»œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™ç§æ‹†åˆ†å®Œå…¨å¯ä»¥çœ‹åšå°†å‰ä¸€æ­¥`input_depth` ä¸ªèŠ‚ç‚¹åˆ° `d_model` ä¸ªèŠ‚ç‚¹çš„å…¨è¿æ¥ç½‘ç»œï¼Œæ‹†åˆ†æˆäº†hä¸ªå°çš„ `input_depth` ä¸ªèŠ‚ç‚¹åˆ°d\_kä¸ªèŠ‚ç‚¹çš„å…¨è¿æ¥ç½‘ç»œã€‚

##### ç‰©ç†è§’åº¦

å®é™…ä¸Šåœ¨ä»£ç ä¸­ä¼šé‡‡ç”¨å¤§çŸ©é˜µçš„æ–¹å¼æ¥è¿›è¡Œã€‚å…·ä½“ä¼šé€šè¿‡view(nbatches, -1, self.h, self.d\_k)æ“ä½œæŠŠæŠ•å½±è¾“å‡º Query, Key, Valueæ‹†åˆ†æˆå¤šå¤´ï¼Œå³å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œå°†æœ€åä¸€ä¸ªç»´åº¦å˜æˆd\_kã€‚æˆ–è€…è¯´ï¼ŒæŠŠåˆ†æ‹†æœ€åä¸€ä¸ªç»´åº¦åˆ° (h, d\_k)ã€‚ç°åœ¨æ¯ä¸ª "åˆ‡ç‰‡"å¯¹åº”äºæ¯ä¸ªå¤´çš„ä¸€ä¸ªçŸ©é˜µã€‚

å¦‚å‰æ‰€è¿°ï¼ŒæŠ•å½±æ˜¯é€»è¾‘æŠ•å½±ï¼Œé‚£ä¹ˆåˆ‡åˆ†ä¹Ÿåªæ˜¯é€»è¾‘ä¸Šçš„åˆ‡åˆ†ã€‚å¯¹äºå‚æ•°çŸ©é˜µ Query, Key, Value è€Œè¨€ï¼Œå¹¶æ²¡æœ‰ç‰©ç†åˆ‡åˆ†æˆå¯¹åº”äºæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç‹¬ç«‹çŸ©é˜µï¼Œä»…é€»è¾‘ä¸Šæ¯ä¸ªæ³¨æ„åŠ›å¤´å¯¹åº”äº Query, Key, Value çš„ç‹¬ç«‹ä¸€éƒ¨åˆ†ã€‚åŒæ ·ï¼Œå„æ³¨æ„åŠ›å¤´æ²¡æœ‰å•ç‹¬çš„çº¿æ€§å±‚ï¼Œè€Œæ˜¯æ‰€æœ‰çš„æ³¨æ„åŠ›å¤´å…±ç”¨çº¿æ€§å±‚ï¼Œåªæ˜¯ä¸åŒçš„æ³¨æ„åŠ›å¤´åœ¨ç‹¬å±äºå…¶çš„é€»è¾‘éƒ¨åˆ†ä¸Šè¿›è¡Œæ“ä½œã€‚è¿™ç§é€»è¾‘åˆ†å‰²ï¼Œæ˜¯é€šè¿‡å°†è¾“å…¥æ•°æ®ä»¥åŠçº¿æ€§å±‚æƒé‡ï¼Œå‡åŒ€åˆ’åˆ†åˆ°å„æ³¨æ„å¤´ä¸­æ¥å®Œæˆçš„ã€‚

åŸºäºæ­¤ï¼Œæ‰€æœ‰ Heads çš„è®¡ç®—å¯é€šè¿‡å¯¹ä¸€ä¸ªçš„çŸ©é˜µæ“ä½œæ¥å®ç°ï¼Œè€Œä¸éœ€è¦hä¸ªå•ç‹¬æ“ä½œã€‚è¿™ä½¿å¾—è®¡ç®—æ›´åŠ æœ‰æ•ˆï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„ç®€å•ï¼šæ‰€éœ€çº¿æ€§å±‚æ›´å°‘ï¼ŒåŒæ—¶è·å¾—äº†å¤šå¤´æ³¨æ„åŠ›çš„æ•ˆæœã€‚

å…¶å®ï¼Œä¹Ÿå¯é‡‡ç”¨å°çŸ©é˜µçš„æ–¹å¼è¿›è¡Œè®¡ç®—ï¼Œå³æŠŠåš Query, Key, Valueåšç‰©ç†åˆ‡åˆ†ï¼Œç„¶ååˆ©äºforå¾ªç¯ä¸€ä¸ªä¸€ä¸ªè®¡ç®—å¤´ï¼Œå†å°†ç»“æœåˆ—è¡¨è¿›è¡Œconcatï¼Œè¿™æ ·ä»£ç ä¸Šæ›´æ¸…æ™°ä¸€ç‚¹ï¼Œä½†æ˜¯æ€§èƒ½ä¸å¦‚å¤§çŸ©é˜µçš„æ–¹æ¡ˆã€‚

##### å°ç»“

è¾“å…¥çš„ç»´åº¦æ˜¯ï¼šbatch\_size, seq\_len, d\_model)ã€‚\\(W^Q,W^K,W^V\\) çº¿æ€§å±‚çš„ç»´åº¦æ˜¯ï¼ˆd\_model, d\_modelï¼‰ï¼Œå®é™…ä¸Šçº¿æ€§å±‚å¹¶æ²¡æœ‰é’ˆå¯¹å¤šå¤´åšåˆ‡åˆ†ã€‚å®é™…ä¸Šå¤šå¤´çš„ \\(W^Q,W^K,W^V\\) çŸ©é˜µä»ç„¶æ˜¯ä¸‰ä¸ªå•ä¸€çŸ©é˜µï¼Œä½†å¯ä»¥æŠŠå®ƒä»¬çœ‹ä½œæ˜¯æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„é€»è¾‘ä¸Šç‹¬ç«‹çš„\\(W^Q,W^K,W^V\\) çŸ©é˜µã€‚è¿™æ ·å¾—åˆ°çš„å•å¤´å¯¹åº”çš„ Qã€K å’Œ V é€»è¾‘çŸ©é˜µå½¢çŠ¶æ˜¯ï¼ˆbatch\_size, seq\_len, h, d\_kï¼‰ã€‚

#### è°ƒæ•´ç»´åº¦

æ­¤å¤„å¯¹åº”å›¾ä¸Šçš„åºå·3ã€‚

ä¸ºäº†æ›´å¥½çš„å¹¶è¡Œï¼Œæ¥ä¸‹æ¥ä¼šé€šè¿‡äº¤æ¢ hå’Œ seq\_len è¿™ä¸¤ä¸ªç»´åº¦æ”¹å˜ Qã€K å’Œ V çŸ©é˜µçš„å½¢çŠ¶ã€‚å›¾ç¤ºä¸­æœªè¡¨è¾¾å‡º batch ç»´åº¦ï¼Œå®é™…ä¸Šæ¯ä¸€ä¸ªæ³¨æ„åŠ›å¤´çš„ 'Q' çš„ç»´åº¦æ˜¯ï¼ˆbatch\_size, h, seq\_len, d\_kï¼‰ã€‚

#### ä¸ºæ¯ä¸ªå¤´è®¡ç®—æ³¨æ„åŠ›

å¦‚å‰æ‰€è¿°ï¼Œæœ‰ä¸¤ç§æ–¹å¼æ¥è®¡ç®—æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›ã€‚

*   å¤§çŸ©é˜µæ–¹å¼ï¼Œè¯¥ç§æ–¹å¼å°†8ä¸ªæ³¨æ„å¤´å…¨éƒ¨å¹³é“ºåœ¨ä¸‰ç»´è¾“å…¥çŸ©é˜µçš„ç¬¬0ç»´batch\_sizeä¸Šï¼Œä¸€èµ·è¿›è¡Œç‚¹ä¹˜æ“ä½œï¼Œç‚¹ä¹˜ç»“æœå†é€šè¿‡reshapeå’Œè½¬ç½®æ•´ç†ä¸º8ä¸ªå¤´åœ¨ç¬¬2ç»´ä¸Šçš„æ‹¼æ¥ï¼Œè¿™ç§æ–¹å¼è®¡ç®—å¿«ã€‚
*   forå¾ªç¯ä¸€ä¸ªä¸€ä¸ªè®¡ç®—å¤´ï¼Œå†å°†ç»“æœåˆ—è¡¨è¿›è¡Œconcatï¼Œä»£ç ä¸Šæ›´æ¸…æ™°ä¸€ç‚¹ã€‚

vanilla Transformerä½¿ç”¨å¤§çŸ©é˜µæ–¹å¼ã€‚æ­¤å¤„å¯¹åº”å›¾ä¸Šçš„åºå·4ã€‚

##### å•ç‹¬åˆ†ç»„

ç›®å‰åœ¨é€»è¾‘ä¸Šå·²ç»æŠŠæ¯ä¸ªqueryï¼Œkeyï¼ŒvalueæŒ‰ç…§å„è‡ªçš„ç»´åº¦åˆ†å‰²ä¸ºè‹¥å¹²æ®µï¼Œå½¢æˆè‹¥å¹²ç‹¬ç«‹çš„queryï¼Œkeyï¼Œvalueåˆ†ç»„ï¼Œæ¯ä¸ªåˆ†ç»„å¯¹åº”ä¸€ä¸ªæ³¨æ„åŠ›å¤´ã€‚æ¥ä¸‹æ¥æ¯ä¸ªåˆ†ç»„å†…è¿›è¡Œç‚¹ç§¯è¿ç®—å’ŒåŠ æƒå¹³å‡ï¼Œæ¯”å¦‚queryçš„ç¬¬ä¸€æ®µåªå’Œkeyçš„ç¬¬ä¸€æ®µè¿›è¡Œç‚¹ç§¯ï¼Œå…¶ç»“æœä¹Ÿåªæ˜¯valueç¬¬ä¸€æ®µçš„æƒé‡ï¼Œä»¥æ­¤ç±»æ¨ã€‚è¿™æ˜¯ç‹¬ç«‹çš„åˆ†ç»„ï¼Œåœ¨æ¯ä¸ªç»„å†…è¿›è¡Œæ³¨æ„åŠ›æ“ä½œï¼Œä¸ä¼šè·¨ç»„æ“ä½œã€‚ä»åŸç†å±‚é¢ä¸Šçœ‹ï¼Œè¿™æ˜¯æŠŠ Attention æœºåˆ¶åˆ†å‰²åœ¨ Embedding ä¸­çš„ä¸åŒç»†åˆ†é€»è¾‘å­ç©ºé—´ä¸­ï¼ˆè¯­ä¹‰é€»è¾‘ã€è¯­æ³•é€»è¾‘ã€ä¸Šä¸‹æ–‡é€»è¾‘ã€åˆ†ç±»é€»è¾‘ç­‰ï¼‰æ¥è¿ä½œäº†ï¼Œå³æŠŠåŸæ¥åœ¨ä¸€ä¸ªé«˜ç»´ç©ºé—´é‡Œè¡¡é‡ä¸€ä¸ªæ–‡æœ¬çš„ä»»æ„ä¸¤ä¸ªå­—ä¹‹é—´çš„ç›¸å…³åº¦ï¼Œå˜æˆäº†åœ¨8ç»´ç©ºé—´é‡Œå»åˆ†åˆ«è¡¡é‡ä»»æ„ä¸¤ä¸ªå­—çš„ç›¸å…³åº¦çš„å˜åŒ–ã€‚

##### å¹¶è¡Œ

æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›è®¡ç®—å…¶å®å’Œå•å¤´æ³¨æ„åŠ›æ²¡å•¥åŒºåˆ«ï¼Œä½†æ˜¯æœ‰ä¸€ä¸ªç‚¹å¯ä»¥ç•™æ„ä¸‹ï¼Œå³å•å¤´è®¡ç®—æ˜¯ä½¿ç”¨æœ€åä¸¤ä¸ªç»´åº¦ï¼ˆseq\_len, d\_kï¼‰ï¼Œè·³è¿‡å‰ä¸¤ä¸ªç»´åº¦ï¼ˆbatch\_size, hï¼‰ã€‚è€Œæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡ºå½¢çŠ¶ä¸ºï¼š(batch\_sizeï¼Œhï¼Œseq\_lenï¼Œd\_kï¼‰ã€‚ä¹‹æ‰€ä»¥è¦è¿™ä¹ˆå¤„ç†ï¼Œå®Œå…¨æ˜¯å› ä¸ºè®¡ç®—çš„éœ€è¦ã€‚å› ä¸ºQã€Kå’ŒVçš„å‰ä¸¤ä¸ªç»´åº¦ï¼ˆå¤šå¤´ä¸ batchï¼‰æ˜¯ç­‰ä»·çš„ï¼Œæœ¬è´¨ä¸Šéƒ½æ˜¯å¹¶è¡Œè®¡ç®—ã€‚æ‰€ä»¥è®¡ç®—æ—¶ä¹Ÿå¯ä»¥æŠŠå®ƒä»¬æ”¾åœ¨åŒä¸€ä¸ªç»´åº¦ä¸Šï¼šbatch\_size \* num\_headsã€‚ä¹Ÿæ­£æ˜¯å› ä¸ºè®¡ç®—çš„éœ€è¦ï¼Œæ³¨æ„åŠ›æƒé‡ ( QK^T ) çš„å½¢çŠ¶æœ‰æ—¶æ˜¯ä¸‰ç»´å¼ é‡ (batch\_size\*num\_heads, tgt\_seq\_len, src\_seq\_len)ï¼Œæœ‰æ—¶æ˜¯å››ç»´å¼ é‡ (batch\_size, num\_heads, tgt\_seq\_len, src\_seq\_len) ï¼Œä¼šæ ¹æ®éœ€è¦åœ¨äºŒè€…é—´åˆ‡æ¢ã€‚

é€šå¸¸ï¼Œç‹¬ç«‹è®¡ç®—å…·æœ‰éå¸¸ç®€å•çš„å¹¶è¡ŒåŒ–è¿‡ç¨‹ã€‚å°½ç®¡è¿™å–å†³äº GPU çº¿ç¨‹ä¸­çš„åº•å±‚ä½çº§å®ç°ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šä¸ºæ¯ä¸ªbatch å’Œæ¯ä¸ªå¤´éƒ¨åˆ†é…ä¸€ä¸ª GPU çº¿ç¨‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ batch=2 å’Œ heads=3ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ 6 ä¸ªä¸åŒçš„çº¿ç¨‹ä¸­è¿è¡Œè®¡ç®—ã€‚å³ä½¿å°ºå¯¸æ˜¯d\_k=d\_model/headsã€‚ç”±äºæ¯ä¸ªå¤´çš„è®¡ç®—æ˜¯å¹¶è¡Œè¿›è¡Œçš„ï¼ˆä¸åŒçš„å¤´æ‹¿åˆ°ç›¸åŒçš„è¾“å…¥ï¼Œè¿›è¡Œç›¸åŒçš„è®¡ç®—ï¼‰ï¼Œæ¨¡å‹å¯ä»¥é«˜æ•ˆåœ°å¤„ç†å¤§è§„æ¨¡è¾“å…¥ã€‚ç›¸æ¯”äºé¡ºåºå¤„ç†çš„ RNNï¼Œæ³¨æ„åŠ›æœºåˆ¶æœ¬èº«æ”¯æŒå¹¶è¡Œï¼Œè€Œå¤šå¤´æœºåˆ¶è¿›ä¸€æ­¥å¢å¼ºäº†è¿™ä¸€ç‚¹ã€‚

#### èåˆæ¯ä¸ªå¤´çš„Z

æˆ‘ä»¬ç°åœ¨å¯¹æ¯ä¸ªå¤´éƒ½æœ‰å•ç‹¬çš„Zï¼Œè€Œç¼–ç å™¨çš„ä¸‹ä¸€å±‚å¸Œæœ›å¾—åˆ°æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œè€Œä¸æ˜¯hä¸ªçŸ©é˜µï¼Œå› æ­¤å‰é¢æ€ä¹ˆæ‹†åˆ†ï¼Œç°åœ¨è¿˜éœ€è¦æ‹¼å›å»ã€‚å°†å¤šå¤´è¾“å‡ºçš„å¤šä¸ªZé€šè¿‡å…¨è¿æ¥åˆå¹¶ä¸ºä¸€ä¸ªè¾“å‡ºZã€‚è¿™ä¸ªåˆå¹¶æ“ä½œæœ¬è´¨ä¸Šæ˜¯ä¸åˆ†å‰²æ“ä½œç›¸åï¼Œé€šè¿‡é‡å¡‘ç»“æœçŸ©é˜µä»¥æ¶ˆé™¤ d\_k ç»´åº¦æ¥å®Œæˆçš„ã€‚å…¶æ­¥éª¤å¦‚ä¸‹ï¼š

*   ä¸ºäº†èƒ½å¤Ÿæ–¹ä¾¿åœ°å°†å¤šå¤´ç»“æœæ‹¼åˆèµ·æ¥ï¼Œé¦–å…ˆæˆ‘ä»¬å°†hè½¬ç½®åˆ°å€’æ•°ç¬¬äºŒä¸ªç»´åº¦ï¼Œå³äº¤æ¢å¤´éƒ¨å’Œåºåˆ—ç»´åº¦æ¥é‡å¡‘æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µã€‚æ¢å¥è¯è¯´ï¼ŒçŸ©é˜µçš„å½¢çŠ¶ä»ï¼ˆbatch\_sizeï¼Œhï¼Œseq\_lenï¼Œd\_kï¼‰å˜æˆï¼ˆbatch\_sizeï¼Œseq\_lenï¼Œhï¼Œd\_kï¼‰ã€‚æ­¤å¤„å¯¹åº”å›¾ä¸Šçš„åºå·5ã€‚
    
*   å°†æ„åŠ›åˆ†æ•°çŸ©é˜µæ”¾åˆ°ä¸€å—è¿ç»­çš„ç‰©ç†å†…å­˜ä¸­ï¼Œæ˜¯æ·±æ‹·è´ï¼Œä¸æ”¹å˜åŸæ•°æ®ã€‚æ­¤å¤„å¯¹åº”å›¾ä¸Šçš„åºå·6ã€‚
    
*   é€šè¿‡é‡å¡‘ (batch\_sizeï¼Œseq\_lenï¼Œd\_model)æ¥æŠ˜å å¤´éƒ¨ç»´åº¦ã€‚è¿™å°±æœ‰æ•ˆåœ°å°†æ¯ä¸ªå¤´çš„æ³¨æ„å¾—åˆ†å‘é‡è¿æ¥æˆä¸€ä¸ªåˆå¹¶çš„æ³¨æ„å¾—åˆ†ã€‚æ­¤å¤„å¯¹åº”å›¾ä¸Šçš„åºå·7ã€‚
    
*   é€šè¿‡å…¨è¿æ¥å±‚çš„çº¿æ€§å˜æ¢æŠŠæ‹¼åˆå¥½çš„è¾“å‡ºè¿›è¡Œæœ‰æœºèåˆï¼Œç»è¿‡å…¨è¿æ¥å±‚èåˆåçš„æœ€åä¸€ç»´ä»ç„¶æ˜¯ `d_model`ã€‚æ­¤å¤„å¯¹åº”å›¾ä¸Šåºå·8ã€‚
    

å¯ä»¥çœ‹åˆ° Multi-Head Attention è¾“å‡ºçš„çŸ©é˜µ**Z**ä¸å…¶è¾“å…¥çš„çŸ©é˜µ**X**çš„ç»´åº¦æ˜¯ä¸€æ ·çš„ã€‚

#### forward()å‡½æ•°

ä¸Šé¢è¿ç®—é€»è¾‘å¯¹åº”çš„æ˜¯MultiHeadedAttentionçš„forward()å‡½æ•°ï¼Œå…·ä½“å¦‚ä¸‹ã€‚

     def forward(self, query, key, value, mask=None):
         """
         æœ¬å‡½æ•°æ˜¯è®ºæ–‡ä¸­å›¾2ï¼ˆå¤šå¤´æ³¨æ„åŠ›çš„æ¶æ„å›¾ï¼‰çš„å®ç°ã€‚
         - query, key, valueï¼šå¹¶éè®ºæ–‡å…¬å¼ä¸­ç»è¿‡W^Q, W^K, W^Vè®¡ç®—åçš„Q, K, Vï¼Œè€Œæ˜¯åŸå§‹è¾“å…¥Xã€‚query, key, valueçš„ç»´åº¦æ˜¯(batch_size, seq_len, d_model)
         - maskï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­å¯èƒ½éœ€è¦çš„maskæ©ç å¼ é‡ï¼Œé»˜è®¤æ˜¯None
         """        
         if mask is not None:
             # å¯¹æ‰€æœ‰hä¸ªå¤´åº”ç”¨åŒæ ·çš„mask
             # å•å¤´æ³¨æ„åŠ›ä¸‹ï¼Œmaskå’ŒXçš„ç»´åº¦éƒ½æ˜¯3ï¼Œå³(batch_size, seq_len, d_model)ï¼Œä½†æ˜¯å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸‹ï¼Œä¼šåœ¨ç¬¬äºŒä¸ªç»´åº¦æ’å…¥headæ•°é‡ï¼Œå› æ­¤Xçš„ç»´åº¦å˜æˆ(batch_size, h,seq_len,d_model/h)ï¼Œæ‰€ä»¥maskä¹Ÿè¦ç›¸åº”çš„æŠŠè‡ªå·±æ‹“å±•æˆ4ç»´ï¼Œè¿™æ ·æ‰èƒ½å’Œåç»­çš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œå¤„ç†
             mask = mask.unsqueeze(1) # maskå¢åŠ ä¸€ä¸ªç»´åº¦
         nbatches = query.size(0) # è·å–batch_size
    
         # 1) Do all the linear projections in batch from d_model => h x d_k
         """
         1). æ‰¹é‡æ‰§è¡Œä» d_model åˆ° h x d_k çš„çº¿æ€§æŠ•å½±ï¼Œå³è®¡ç®—å¤šå¤´æ³¨æ„åŠ›çš„Q,K,Vï¼Œæ‰€ä»¥queryã€valueå’Œkeyçš„shapeä»(batch_size,seq_len,d_model)å˜åŒ–ä¸º(batch_size,h,seq_len,d_model/h)ã€‚
            zip(self.linears, (query, key, value)) æ˜¯æŠŠ(self.linears[0],self.linears[1],self.linears[2])è¿™ä¸‰ä¸ªçº¿æ€§å±‚å’Œ(query, key, value)æ”¾åˆ°ä¸€èµ·
            ç„¶ååˆ©ç”¨forå¾ªç¯å°†(query, key, value)åˆ†åˆ«ä¼ åˆ°çº¿æ€§å±‚ä¸­è¿›è¡Œéå†ï¼Œæ¯æ¬¡å¾ªç¯æ“ä½œå¦‚ä¸‹ï¼š
             1.1 é€šè¿‡W^Q,W^K,W^Vï¼ˆself.linearsçš„å‰ä¸‰é¡¹ï¼‰æ±‚å‡ºè‡ªæ³¨æ„åŠ›çš„Q,K,Vï¼Œæ­¤æ—¶Q,K,Vçš„shapeä¸º(batch_size,seq_len,d_model), å¯¹åº”ä»£ç ä¸ºlinear(x)ã€‚
             ä»¥self.linears[0](query)ä¸ºä¾‹ï¼Œself.linears[0] æ˜¯ä¸€ä¸ª (512, 512) çš„çŸ©é˜µï¼Œqueryæ˜¯(batch_size,seq_len,d_model)ï¼Œç›¸ä¹˜ä¹‹åå¾—åˆ°çš„æ–°queryè¿˜æ˜¯512(d_model)ç»´çš„å‘é‡ã€‚
             keyå’Œvalue çš„è¿ç®—å®Œå…¨ç›¸åŒã€‚
             1.2 æŠŠæŠ•å½±è¾“å‡ºæ‹†åˆ†æˆå¤šå¤´ï¼Œå³å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œå°†æœ€åä¸€ä¸ªç»´åº¦å˜æˆ(h,d_model/h)ï¼ŒæŠ•å½±è¾“å‡ºçš„shapeç”±(batch_size,seq_len,d_model)å˜ä¸º(batch_size,seq_len,h,d_model/h)ã€‚å¯¹åº”ä»£ç ä¸º`view(nbatches, -1, self.h, self.d_k)`ï¼Œå…¶ä¸­çš„-1ä»£è¡¨è‡ªé€‚åº”ç»´åº¦ï¼Œè®¡ç®—æœºä¼šæ ¹æ®è¿™ç§å˜æ¢è‡ªåŠ¨è®¡ç®—è¿™é‡Œçš„å€¼ã€‚
             å› æ­¤æˆ‘ä»¬åˆ†åˆ«å¾—åˆ°8ä¸ªå¤´çš„64ç»´çš„keyå’Œ64ç»´çš„valueã€‚è¿™æ ·å°±æ„å‘³ç€æ¯ä¸ªå¤´å¯ä»¥è·å¾—ä¸€éƒ¨åˆ†è¯ç‰¹å¾ç»„æˆçš„å¥å­ã€‚
             1.3 äº¤æ¢â€œseq_lenâ€å’Œâ€œheadæ•°â€è¿™ä¸¤ä¸ªç»´åº¦ï¼Œå°†headæ•°æ”¾åœ¨å‰é¢ï¼Œæœ€ç»ˆshapeå˜ä¸º(batch_size,h,seq_lenï¼Œd_model/h)ã€‚å¯¹åº”ä»£ç ä¸º`transpose(1, 2)`ã€‚äº¤æ¢çš„ç›®çš„æ˜¯æ–¹ä¾¿åç»­çŸ©é˜µä¹˜æ³•å’Œä¸åŒå¤´éƒ¨çš„æ³¨æ„åŠ›è®¡ç®—ã€‚ä¹Ÿæ˜¯ä¸ºäº†è®©ä»£è¡¨å¥å­é•¿åº¦ç»´åº¦å’Œè¯å‘é‡ç»´åº¦èƒ½å¤Ÿç›¸é‚»ï¼Œè¿™æ ·æ³¨æ„åŠ›æœºåˆ¶æ‰èƒ½æ‰¾åˆ°è¯ä¹‰ä¸å¥å­ä½ç½®çš„å…³ç³»ï¼Œä»attentionå‡½æ•°ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåˆ©ç”¨çš„æ˜¯åŸå§‹è¾“å…¥çš„å€’æ•°ç¬¬ä¸€å’Œç¬¬äºŒç»´.è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†æ¯ä¸ªå¤´çš„è¾“å…¥ã€‚
             å¤šå¤´ä¸batchæœ¬è´¨ä¸Šéƒ½æ˜¯å¹¶è¡Œè®¡ç®—ã€‚æ‰€ä»¥è®¡ç®—æ—¶æŠŠå®ƒä»¬æ”¾åœ¨åŒä¸€ä¸ªç»´åº¦ä¸Šï¼Œåœ¨ç”¨GPUè®¡ç®—æ—¶ï¼Œå¤§å¤šä¾æ®batch_size * headæ•°æ¥å¹¶è¡Œåˆ’åˆ†ã€‚å°±æ˜¯å¤šä¸ªæ ·æœ¬å¹¶è¡Œè®¡ç®—ï¼Œå…·ä½“åˆ°æŸä¸€ä¸ªtokenä¸Šï¼Œå¯ä»¥ç†è§£ä¸ºnä¸ªheadä¸€èµ·å¹¶è¡Œè®¡ç®—ã€‚
         """          
         query, key, value = [
             lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) # å¯¹åº”å›¾ä¸Šçš„åºå·2ï¼Œ3
             for lin, x in zip(self.linears, (query, key, value)) # å¯¹åº”å›¾ä¸Šçš„åºå·1
         ]
    
         # 2) Apply attention on all the projected vectors in batch.
         """
         2) åœ¨æŠ•å½±çš„å‘é‡ä¸Šæ‰¹é‡åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…·ä½“å°±æ˜¯æ±‚å‡ºQ,K,Våï¼Œé€šè¿‡attentionå‡½æ•°è®¡ç®—å‡ºAttentionç»“æœã€‚å› ä¸ºheadæ•°é‡å·²ç»æ”¾åˆ°äº†ç¬¬äºŒç»´åº¦ï¼Œæ‰€ä»¥å°±æ˜¯Qã€Kã€Vçš„æ¯ä¸ªå¤´è¿›è¡Œä¸€ä¸€å¯¹åº”çš„ç‚¹ç§¯ã€‚åˆ™ï¼š     
            xçš„shapeä¸º(batch_size,h,seq_len,d_model/h)ã€‚
            self.attnçš„shapeä¸º(batch_size,h,seq_len,seq_len)
         """          
         x, self.attn = attention( # å¯¹åº”å›¾ä¸Šçš„åºå·4
             query, key, value, mask=mask, dropout=self.dropout
         )
    
         # 3) "Concat" using a view and apply a final linear.
         """
         3) æŠŠå¤šä¸ªå¤´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ï¼Œå˜æˆå’Œè¾“å…¥å½¢çŠ¶ç›¸åŒã€‚
            é€šè¿‡å¤šå¤´æ³¨æ„åŠ›è®¡ç®—åï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†æ¯ä¸ªå¤´è®¡ç®—ç»“æœç»„æˆçš„4ç»´å¼ é‡ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºè¾“å…¥çš„å½¢çŠ¶ä»¥æ–¹ä¾¿åç»­çš„è®¡ç®—ï¼Œå³å°†å¤šä¸ªå¤´å†åˆå¹¶èµ·æ¥ï¼Œè¿›è¡Œç¬¬ä¸€æ­¥å¤„ç†ç¯èŠ‚çš„é€†æ“ä½œï¼Œå…ˆå¯¹ç¬¬äºŒå’Œç¬¬ä¸‰ç»´è¿›è¡Œè½¬ç½®ï¼Œå°†xçš„shapeç”±(batch_size,h,seq_len,d_model/h)è½¬æ¢ä¸º (batch_size,seq_len,d_model)ã€‚
            3.1 äº¤æ¢â€œheadæ•°â€å’Œâ€œseq_lenâ€è¿™ä¸¤ä¸ªç»´åº¦ï¼Œç»“æœä¸º(batch_size,seq_len,h,d_model/h)ï¼Œå¯¹åº”ä»£ç ä¸ºï¼š`x.transpose(1, 2).contiguous()`ã€‚`contiguous()`æ–¹æ³•å°†å˜é‡æ”¾åˆ°ä¸€å—è¿ç»­çš„ç‰©ç†å†…å­˜ä¸­ï¼Œæ˜¯æ·±æ‹·è´ï¼Œä¸æ”¹å˜åŸæ•°æ®ï¼Œè¿™æ ·èƒ½å¤Ÿè®©è½¬ç½®åçš„å¼ é‡åº”ç”¨viewæ–¹æ³•ï¼Œå¦åˆ™å°†æ— æ³•ç›´æ¥ä½¿ç”¨ã€‚
            3.2 ç„¶åå°†â€œheadæ•°â€å’Œâ€œd_model/headæ•°â€è¿™ä¸¤ä¸ªç»´åº¦åˆå¹¶ï¼Œç»“æœä¸º(batch_size,seq_len,d_model)ï¼Œä»£ç æ˜¯view(nbatches, -1, self.h * self.d_k)ã€‚
            æ¯”å¦‚ï¼ŒæŠŠ8ä¸ªheadçš„64ç»´å‘é‡æ‹¼æ¥æˆä¸€ä¸ª512çš„å‘é‡ã€‚ç„¶åå†ä½¿ç”¨ä¸€ä¸ªçº¿æ€§å˜æ¢(512,512)ï¼Œshapeä¸å˜ã€‚å› ä¸ºæœ‰æ®‹å·®è¿æ¥çš„å­˜åœ¨ä½¿å¾—è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦è‡³å°‘æ˜¯ä¸€æ ·çš„ã€‚
            å³(5, 8, 10, 64)  ==> (5, 10, 512)
         """            
         x = (
             x.transpose(1, 2) # å¯¹åº”å›¾ä¸Šçš„åºå·5
             .contiguous() # å¯¹åº”å›¾ä¸Šçš„åºå·6
             .view(nbatches, -1, self.h * self.d_k) # å¯¹åº”å›¾ä¸Šçš„åºå·7
         )
         del query
         del key
         del value
         # å½“å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—å®Œæˆåï¼Œå°†ä¼šå¾—åˆ°ä¸€ä¸ªå½¢çŠ¶ä¸º[src_len,d_model]çš„çŸ©é˜µï¼Œä¹Ÿå°±æ˜¯å¤šä¸ªz_iæ°´å¹³å †å åçš„ç»“æœã€‚å› æ­¤ä¼šåˆå§‹åŒ–ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆW^OçŸ©é˜µï¼‰æ¥å¯¹è¿™ä¸€ç»“æœè¿›è¡Œä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆç»“æœï¼Œå¹¶ä¸”ä½œä¸ºå¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºæ¥è¿”å›ã€‚
         # self.linears[-1]å½¢çŠ¶æ˜¯(512, 512)ï¼Œå› æ­¤æœ€ç»ˆè¾“å‡ºè¿˜æ˜¯(batch_size, seq_len, d_model)ã€‚
         return self.linears[-1](x) # å¯¹åº”å›¾ä¸Šçš„åºå·8
    

### 3.3 è°ƒç”¨

æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹çœ‹å¦‚ä½•è°ƒç”¨ã€‚åœ¨ Transformer é‡Œï¼Œæœ‰ 3 ä¸ªåœ°æ–¹ç”¨åˆ°äº† MultiHeadedAttentionï¼ŒEncoderå±‚ç”¨åˆ°ä¸€å¤„ï¼ŒDecoderå±‚ç”¨åˆ°ä¸¤å¤„ã€‚

#### ç¼–ç å™¨

Encoderä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„ç›®çš„æ˜¯ï¼šæ‰¾åˆ°è‡ªèº«çš„å…³ç³»ï¼Œå› æ­¤å¯¹äºå…¶å†…éƒ¨çš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Attentyionï¼‰æœºåˆ¶æ¥è¯´ï¼Œè°ƒç”¨MultiHeadedAttention.forward(query, key, value, mask)æ—¶å€™ï¼Œqueryï¼Œkey å’Œ value éƒ½æ˜¯ç›¸åŒçš„è¾“å…¥å€¼Xæˆ–è€…ä¸‹å±‚ï¼ˆå¯¹åº”Transformeræ¶æ„å›¾ï¼‰çš„è¾“å‡ºã€‚åœ¨ä»£ç ä¹‹ä¸­ï¼Œå¯¹åº”å¦‚ä¸‹ï¼š

    class EncoderLayer(nn.Module):
        "Encoder is made up of self-attn and feed forward (defined below)"
        def forward(self, x, mask):
            # è¿™é‡Œè°ƒç”¨MultiHeadedAttention.forward(query, key, value, mask)
            x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) 
            return self.sublayer[1](x, self.feed_forward)
    

#### è§£ç å™¨

Decoderçš„ç›®çš„æ˜¯ï¼š

*   ä½¿ç”¨è‡ªæ³¨æ„åŠ›æ‰¾åˆ°è¾“å‡ºåºåˆ—è‡ªèº«å†…éƒ¨çš„è¯­ä¹‰å…³ç³»ã€‚è®©ç›®æ ‡åºåˆ—ä¹‹ä¸­ï¼Œæ¯ä¸ªtokenéƒ½æœé›†åˆ°æœ¬å­—å’Œç›®æ ‡åºåˆ—ä¹‹ä¸­å…¶ä»–å“ªå‡ ä¸ªå­—æ¯”è¾ƒç›¸å…³ã€‚
*   ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›è®©æºåºåˆ—ä¸ç›®æ ‡åºåˆ—å¯¹é½ã€‚

å› æ­¤ï¼Œ

*   å¯¹äºDecoderæœ€å‰é¢çš„æ©ç å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMasked Multi-Head Attentyionï¼‰æ¥è¯´ï¼Œè°ƒç”¨MultiHeadedAttention.forward(query, key, value, mask)æ—¶å€™ï¼Œqueryï¼Œkey å’Œ value éƒ½æ˜¯ç›¸åŒçš„å€¼Xï¼ˆDecoderçš„è¾“å…¥ï¼‰ã€‚ä½†æ˜¯ Mask ä½¿å¾—å®ƒä¸èƒ½è®¿é—®æœªæ¥çš„è¾“å…¥ï¼Œå³ä¸ºäº†å¹¶è¡Œä¸€æ¬¡å–‚å…¥æ‰€æœ‰è§£ç éƒ¨åˆ†çš„è¾“å…¥ï¼Œæ‰€ä»¥è¦ç”¨maskæ¥è¿›è¡Œæ©ç›–å½“å‰æ—¶åˆ»ä¹‹åçš„ä½ç½®ä¿¡æ¯ã€‚
*   å¯¹äºDecoderä¸­é—´çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentyionï¼‰æ¥è¯´ï¼Œä¼šå°†Encoderçš„è¾“å‡ºmemory  
    ä½œä¸ºkeyå’Œvalueï¼Œå°†ä¸‹å±‚çš„è¾“å‡ºä½œä¸ºæœ¬å±‚çš„queryã€‚

ä»£ç å¦‚ä¸‹ï¼š

    class DecoderLayer(nn.Module):
        "Decoder is made of self-attn, src-attn, and feed forward (defined below)"
        def forward(self, x, memory, src_mask, tgt_mask):
            m = memory
            x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
            x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
            return self.sublayer[2](x, self.feed_forward)
    

0x04 æ”¹è¿›
-------

äººä»¬ä¹Ÿå¯¹å¤šå¤´æ³¨æ„åŠ›è¿›è¡Œäº†ä¸€äº›æ”¹è¿›ã€‚ä¸‹å›¾ç»™å‡ºäº†æ³¨æ„åŠ›å¤´åˆå¹¶æ–¹å¼çš„ä¸€äº›æ–¹æ¡ˆï¼ˆhead composition approachesï¼‰çš„æ¯”è¾ƒã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125916149-1069400832.jpg)

### 4.1 MOHSA

Transformeræ¨¡å‹æˆåŠŸçš„ä¸»è¦åŸå› æ˜¯ä¸åŒ Token ä¹‹é—´çš„æœ‰æ•ˆä¿¡æ¯äº¤æ¢ï¼Œä»è€Œä½¿æ¯ä¸ª Token éƒ½èƒ½è·å¾—ä¸Šä¸‹æ–‡çš„å…¨å±€è§†å›¾ã€‚ç„¶è€Œï¼Œæ¯ä¸ªHeadä¸­çš„ Query ã€ Keyå’ŒValue æ˜¯åˆ†å¼€çš„ï¼Œæ²¡æœ‰é‡å ï¼Œå½“åœ¨å„ä¸ªHeadä¸­è®¡ç®—æ³¨æ„åŠ›æ—¶ä¹Ÿæ²¡æœ‰ä¿¡æ¯äº¤æ¢ã€‚æ¢å¥è¯è¯´ï¼Œåœ¨è®¡ç®—å½“å‰Headçš„æ³¨æ„åŠ›æ—¶ï¼Œå®ƒæ²¡æœ‰å…¶ä»–Headä¸­çš„ä¿¡æ¯ã€‚å°½ç®¡ Token åœ¨æ³¨æ„åŠ›ä¹‹åä¼šé€šè¿‡çº¿æ€§æŠ•å½±è¿›è¡Œå¤„ç†ï¼Œä½†é‚£æ—¶çš„ä¿¡æ¯äº¤æ¢ä»…é™äºæ¯ä¸ª Tokenã€‚

è®ºæ–‡â€œImproving Vision Transformers by Overlapping Heads in Multi-Head Self-Attentionâ€å°±å¯¹æ­¤è¿›è¡Œäº†ç ”ç©¶ã€‚ä½œè€…æå‡ºä¿¡æ¯äº¤æ¢åœ¨è§†è§‰ Transformer ï¼ˆVision Transformersï¼‰çš„æ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹ä¸­å¯ä»¥æé«˜æ€§èƒ½ã€‚è¿™å¯ä»¥é€šè¿‡å°†æ¯ä¸ªHeadçš„ queriesã€keyså’Œvaluesä¸ç›¸é‚»Headçš„ queriesã€keyså’Œvaluesé‡å æ¥å®ç°ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºMOHSAï¼ˆMulti-Overlapped-Head Self-Attention/å¤šé‡å å¤´è‡ªæ³¨æ„åŠ›ï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡é‡å Headæ¥æ”¹è¿›å¤šHeadè‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰æœºåˆ¶ï¼Œä½¿å¾—åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œæ¯ä¸ªHeadä¸­çš„ Qã€ Kå’Œ Vä¹Ÿå¯ä»¥è¢«å…¶ç›¸é‚»Headçš„ Qã€ Kå’Œ Væ‰€å½±å“ï¼ŒHeadé—´ä¿¡æ¯äº¤æµå¯ä»¥ä¸ºè§†è§‰ Transformer å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚å¦‚å›¾æ‰€ç¤ºã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125925617-1578097396.jpg)

ä¸ºäº†å®ç°Headä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ï¼Œä½œè€…åœ¨Qã€Kå’ŒVè¢«åˆ’åˆ†ä¸ºä¸åŒHeadæ—¶ï¼Œä½¿ç”¨é‡å ï¼ˆSoftï¼‰é™¤è€Œä¸æ˜¯ç›´æ¥é™¤ã€‚é€šè¿‡é‡å ç›¸é‚»Headï¼Œå…¶ä»–Headä¸­çš„ä¿¡æ¯ä¹Ÿå¯ä»¥å‚ä¸å½“å‰Headçš„æ³¨æ„åŠ›è®¡ç®—ã€‚ç”±äºå°†ä¸åŒHeadçš„ Token è¿æ¥åï¼Œé‡å ä¼šä½¿ Token ç»´åº¦å¢åŠ ï¼Œå› æ­¤çº¿æ€§æŠ•å½±ä¼šå°†å…¶å‡å°å›åŸå§‹å¤§å°ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125934945-848221917.jpg)

### 4.2 MoH

è®ºæ–‡â€œMoH: Multi-Head Attention as Mixture-of-Head Attentionâ€å€Ÿé‰´å¹¶éæ‰€æœ‰æ³¨æ„åŠ›å¤´éƒ½å…·æœ‰åŒç­‰é‡è¦æ€§çš„è§‚ç‚¹ï¼Œæå‡ºäº†æ··åˆå¤´æ³¨æ„åŠ›ï¼ˆMixture-of-Headï¼ŒMoHï¼‰çš„æ–°æ¶æ„ï¼Œå°†æ³¨æ„åŠ›å¤´è§†ä¸ºæ··åˆä¸“å®¶æœºåˆ¶ï¼ˆMixture-of-Expertsï¼ŒMoEï¼‰ä¸­çš„ä¸“å®¶ï¼Œè¿™æ ·å°±å‡çº§äº†Transformeræ¨¡å‹çš„æ ¸å¿ƒâ€”â€”å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚MoHå…·æœ‰ä¸¤ä¸ªæ˜¾è‘—ä¼˜ç‚¹ï¼š

*   ä½¿æ¯ä¸ªè¯å…ƒèƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„æ³¨æ„åŠ›å¤´ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡è€Œä¸ç‰ºç‰²å‡†ç¡®ç‡æˆ–å¢åŠ å‚æ•°æ•°é‡ï¼›
*   ç”¨åŠ æƒæ±‚å’Œå–ä»£äº†å¤šå¤´æ³¨æ„åŠ›çš„æ ‡å‡†æ±‚å’Œï¼Œä¸ºæ³¨æ„åŠ›æœºåˆ¶å¼•å…¥äº†çµæ´»æ€§ï¼Œæ— éœ€å¢åŠ å‚æ•°æ•°é‡ï¼Œå¹¶é‡Šæ”¾äº†é¢å¤–çš„æ€§èƒ½æ½œåŠ›ã€‚

MoHæ€»ä½“æ¶æ„å¦‚ä¸‹å›¾å³ä¾§æ‰€ç¤ºï¼ŒåŒ…æ‹¬å¤šä¸ªæ³¨æ„åŠ›å¤´å’Œä¸€ä¸ªè·¯ç”±å™¨ï¼ˆæ¿€æ´»Top-Kä¸ªå¤´ï¼‰ã€‚MoHçš„è¾“å‡ºæ˜¯Kä¸ªé€‰å®šå¤´çš„è¾“å‡ºçš„åŠ æƒå’Œã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125944615-660226191.jpg)

MoHä¸»è¦æ”¹è¿›å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

*   å…±äº«å¤´ï¼šæŒ‡å®šä¸€éƒ¨åˆ†å¤´ä¸ºå§‹ç»ˆä¿æŒæ¿€æ´»çš„å…±äº«å¤´ï¼Œåœ¨å…±äº«å¤´ä¸­å·©å›ºå…±åŒçŸ¥è¯†,å‡å°‘å…¶ä»–åŠ¨æ€è·¯ç”±å¤´ä¹‹é—´çš„å†—ä½™ã€‚
*   ä¸¤é˜¶æ®µè·¯ç”±ï¼šè·¯ç”±åˆ†æ•°ç”±æ¯ä¸ªå•ç‹¬å¤´çš„åˆ†æ•°å’Œä¸å¤´ç±»å‹ç›¸å…³çš„åˆ†æ•°å…±åŒå†³å®šã€‚ç›¸å…³è·¯ç”±åˆ†æ•°å…¬å¼å¦‚ä¸‹å›¾æ ‡å·1ã€‚
*   è´Ÿè½½å¹³è¡¡æŸå¤±ï¼šä¸ºé¿å…ä¸å¹³è¡¡è´Ÿè½½ï¼Œåº”ç”¨äº†è´Ÿè½½å¹³è¡¡æŸå¤±ã€‚å…¬å¼å¦‚ä¸‹å›¾æ ‡å·2ã€‚
*   æ€»è®­ç»ƒç›®æ ‡ï¼šæ€»è®­ç»ƒæŸå¤±æ˜¯ä»»åŠ¡ç‰¹å®šæŸå¤±å’Œè´Ÿè½½å¹³è¡¡æŸå¤±çš„åŠ æƒå’Œï¼Œå…¬å¼å¦‚ä¸‹å›¾æ ‡å·3ã€‚å…¶ä¸­Î²æ˜¯æƒè¡¡è¶…å‚æ•°,é»˜è®¤è®¾ç½®ä¸º0.01ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308125953178-1964108161.jpg)

### 4.3 DCMHA

è®ºæ–‡â€œImproving Transformers with Dynamically Composable Multi-Head Attentionâ€æå‡ºç”¨å¯åŠ¨æ€ç»„åˆçš„å¤šå¤´æ³¨æ„åŠ›ï¼ˆDCMHAï¼ŒDynamically Composable Multi-Head Attentionï¼‰æ¥æ›¿æ¢Transformeræ ¸å¿ƒç»„ä»¶å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼ˆMHAï¼‰ï¼Œä»è€Œè§£é™¤äº†MHAæ³¨æ„åŠ›å¤´çš„æŸ¥æ‰¾é€‰æ‹©å›è·¯å’Œå˜æ¢å›è·¯çš„å›ºå®šç»‘å®šï¼Œè®©å®ƒä»¬å¯ä»¥æ ¹æ®è¾“å…¥åŠ¨æ€ç»„åˆï¼Œä»æ ¹æœ¬ä¸Šæå‡äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

å¯ä»¥æŠŠDCMHAè¿‘ä¼¼ç†è§£ä¸ºï¼ŒåŸæ¥æ¯å±‚æœ‰å›ºå®šçš„Hä¸ªæ³¨æ„åŠ›å¤´ï¼Œç°åœ¨ç”¨å‡ ä¹åŒæ ·çš„å‚æ•°é‡å’Œç®—åŠ›ï¼Œå¯æŒ‰éœ€åŠ¨æ€ç»„åˆå‡ºå¤šè‡³HxHä¸ªæ³¨æ„åŠ›å¤´ã€‚è¿™æ ·å³æ’å³ç”¨ï¼Œå¯åœ¨ä»»ä½•Transformeræ¶æ„ä¸­æ›¿æ¢MHAï¼Œå¾—åˆ°é€šç”¨ã€é«˜æ•ˆå’Œå¯æ‰©å±•çš„æ–°æ¶æ„DCFormerã€‚

#### ç ”ç©¶èƒŒæ™¯

åœ¨Transformerçš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼ˆMHAï¼‰ä¸­ï¼Œå„ä¸ªæ³¨æ„åŠ›å¤´å½¼æ­¤å®Œå…¨ç‹¬ç«‹çš„å·¥ä½œã€‚è¿™ä¸ªè®¾è®¡å› å…¶ç®€å•æ˜“å®ç°çš„ä¼˜ç‚¹å·²åœ¨å®è·µä¸­å¤§è·æˆåŠŸï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µçš„ä½ç§©åŒ–å‰Šå¼±äº†è¡¨è¾¾èƒ½åŠ›ã€æ³¨æ„åŠ›å¤´åŠŸèƒ½çš„é‡å¤å†—ä½™æµªè´¹äº†å‚æ•°å’Œè®¡ç®—èµ„æºç­‰ä¸€äº›å¼Šç«¯ã€‚åŸºäºæ­¤ï¼Œè¿‘å¹´æ¥æœ‰ä¸€äº›ç ”ç©¶å·¥ä½œè¯•å›¾å¼•å…¥æŸç§å½¢å¼çš„æ³¨æ„åŠ›å¤´é—´çš„äº¤äº’ã€‚

#### åŠ¨æœº

æ ¹æ®Transformerå›è·¯ç†è®ºï¼Œåœ¨MHAä¸­ ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„è¡Œä¸ºç”±\\(W^Q\\)ã€\\(W^K\\)ã€\\(W^V\\)ã€\\(W^O\\)å››ä¸ªæƒé‡çŸ©é˜µåˆ»ç”»ï¼ˆå…¶ä¸­\\(W^O\\)ç”±MHAçš„è¾“å‡ºæŠ•å½±çŸ©é˜µåˆ‡åˆ†å¾—åˆ°ï¼‰ï¼Œå…¶ä¸­ï¼š

*   \\(W^QW^K\\)å«åšQKå›è·¯ï¼ˆæˆ–å«æŸ¥æ‰¾é€‰æ‹©å›è·¯ï¼‰ï¼Œå†³å®šä»å½“å‰tokenå…³æ³¨ä¸Šä¸‹æ–‡ä¸­çš„å“ªä¸ªï¼ˆäº›ï¼‰token
*   \\(W^OW^V\\)å«åšOVå›è·¯ï¼ˆæˆ–å«æŠ•å½±å˜æ¢å›è·¯ï¼‰ï¼Œå†³å®šä»å…³æ³¨åˆ°çš„tokenå–å›ä»€ä¹ˆä¿¡æ¯ï¼ˆæˆ–æŠ•å½±ä»€ä¹ˆå±æ€§ï¼‰å†™å…¥å½“å‰ä½ç½®çš„æ®‹å·®æµï¼Œè¿›è€Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚

ç ”ç©¶äººå‘˜æ³¨æ„åˆ°ï¼ŒæŸ¥æ‰¾ï¼ˆä»å“ªæ‹¿ï¼‰å’Œå˜æ¢ï¼ˆæ‹¿ä»€ä¹ˆï¼‰æœ¬æ¥æ˜¯ç‹¬ç«‹çš„ä¸¤ä»¶äº‹ï¼Œç†åº”å¯ä»¥åˆ†åˆ«æŒ‡å®šå¹¶æŒ‰éœ€è‡ªç”±ç»„åˆï¼ŒMHAç¡¬æŠŠå®ƒä»¬æ”¾åˆ°ä¸€ä¸ªæ³¨æ„åŠ›å¤´çš„QKOVé‡Œâ€œæ†ç»‘é”€å”®â€ï¼Œé™åˆ¶äº†çµæ´»æ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

#### æ€è·¯

ä»¥æ­¤ä¸ºå‡ºå‘ç‚¹ï¼Œæœ¬æ–‡ç ”ç©¶å›¢é˜Ÿåœ¨MHAä¸­å¼•å…¥composeæ“ä½œï¼Œä»è€Œå¾—åˆ°DCMHAå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308130004209-2065016310.jpg)

ä¸ºäº†æœ€å¤§é™åº¦çš„å¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†åŠ¨æ€å†³å®šæ³¨æ„åŠ›å¤´æ€æ ·ç»„åˆï¼Œå³æ˜ å°„çŸ©é˜µç”±è¾“å…¥**åŠ¨æ€ç”Ÿæˆ**ã€‚ä¸ºäº†é™ä½è®¡ç®—å¼€é”€å’Œæ˜¾å­˜å ç”¨ï¼Œä»–ä»¬è¿›ä¸€æ­¥å°†æ˜ å°„çŸ©é˜µåˆ†è§£ä¸ºä¸€ä¸ªè¾“å…¥æ— å…³çš„é™æ€çŸ©é˜µ\\(W\_b\\)ã€ä¸¤ä¸ªä½ç§©çŸ©é˜µ\\(w\_1,w\_2\\)å’Œä¸€ä¸ªå¯¹è§’çŸ©é˜µ\\(Diag(w\_g)\\)ä¹‹å’Œï¼Œåˆ†åˆ«è´Ÿè´£åŸºç¡€ç»„åˆã€æ³¨æ„åŠ›å¤´é—´çš„æœ‰é™æ–¹å¼ï¼ˆå³ç§©R<=2ï¼‰çš„åŠ¨æ€ç»„åˆå’Œå¤´è‡ªèº«çš„åŠ¨æ€é—¨æ§ã€‚å…¶ä¸­åä¸¤ä¸ªçŸ©é˜µç”±QçŸ©é˜µå’ŒKçŸ©é˜µåŠ¨æ€ç”Ÿæˆã€‚å…·ä½“å…¬å¼å¦‚ä¸‹å›¾ï¼š

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308130011645-332074750.jpg)

ä¸‹å›¾ç»™å‡ºäº†composeçš„è®¡ç®—æ–¹å¼ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250308130019924-1421828846.jpg)

0xFF å‚è€ƒ
-------

[On the Role of Attention Masks and LayerNorm in Transformers](https://arxiv.org/pdf/2405.18781)

[MOH: MULTI-HEAD ATTENTION AS MIXTURE-OFHEAD ATTENTION](https://arxiv.org/pdf/2410.11842)

[Improving Transformers with Dynamically Composable Multi-Head Attention](https://arxiv.org/abs/2405.08553)

[ICML2024é«˜åˆ†ï¼é­”æ”¹æ³¨æ„åŠ›ï¼Œè®©å°æ¨¡å‹èƒ½æ‰“ä¸¤å€å¤§çš„æ¨¡å‹](https://baijiahao.baidu.com/s?id=1800820627950803347&wfr=spider&for=pc)

[é‡å­ä½](https://author.baidu.com/home?from=bjh_article&app_id=1556018077895386)

[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)

[bertæ€§èƒ½ä¼˜åŒ–ä¹‹â€”â€”ç”¨å¦ä¸€ç§æ–¹å¼æ•´åˆå¤šå¤´æ³¨æ„åŠ›](https://zhuanlan.zhihu.com/p/156820477/) [é‚±éœ‡å®‡](https://www.zhihu.com/people/qiu-zhen-yu-87)

[Multi-Head-Attentionçš„ä½œç”¨åˆ°åº•æ˜¯ä»€ä¹ˆ](https://zhuanlan.zhihu.com/p/626820422) [MECH](https://www.zhihu.com/people/sharkalwayslovebunny)

[å½¢è€Œä¸Šå­¦Transformer](https://zhuanlan.zhihu.com/p/689999953) [æ¢…æ°æ³¢å°”å¦çš„å†¬å¤©](https://www.zhihu.com/people/0xffff)

[ç†è§£Attention:ä»èµ·æºåˆ°MHA,MQAå’ŒGQA](https://zhuanlan.zhihu.com/p/686149289)

[\[ç¡¬æ ¸\]å½»åº•ææ‡‚å¤šå¤´æ³¨æ„åŠ›ï¼šå…¨é¢è§£è¯»Andrej Karpathy MHAä»£ç ](https://zhuanlan.zhihu.com/p/694052268) å–ä¸ªå¥½åå­—çœŸéš¾

[Transformerè‡ªä¸‹è€Œä¸Šç†è§£(5) ä»Attentionå±‚åˆ°Transformerç½‘ç»œ](https://zhuanlan.zhihu.com/p/375073534)

[Multiscale Visualization of Attention in the Transformer Model](https://arxiv.org/pdf/1906.05714.pdf)

[What Does BERT Look At? An Analysis of BERTâ€™s Attention](https://arxiv.org/pdf/1906.04341v1.pdf)

[Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention](https://arxiv.org/pdf/1908.11365.pdf)

[Adaptively Sparse Transformers](https://arxiv.org/pdf/1909.00015.pdf)

[Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/pdf/1905.09418.pdf)

[ã€ŠAre Sixteen Heads Really Better than One?ã€‹](https://arxiv.org/pdf/1905.10650.pdf)

[Transformerå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æœ¬è´¨æ´å¯Ÿ](https://mp.weixin.qq.com/s?__biz=MzI2MjU4MDYwOA==&mid=2247485052&idx=1&sn=d7fda61c3e4422fa50c63295c3912e63&chksm=ea49b693dd3e3f8568be7f0055596a98d75ffe1e95e7677f187a3c19b813e6b07af4449d0abe&cur_album_id=2884802760154791940&scene=189#wechat_redirect) ä½œè€…ï¼šNikolas Adaloglou ç¼–è¯‘ï¼šç‹åº†æ³•

[Transformerç³»åˆ—ï¼šMulti-Head Attentionç½‘ç»œç»“æ„å’Œä»£ç è§£æ](https://www.jianshu.com/p/45c3a0c93366) xiaogp

[Transformerç³»åˆ—ï¼šæ®‹å·®è¿æ¥åŸç†è¯¦ç»†è§£æå’Œä»£ç è®ºè¯](https://www.jianshu.com/p/c2f32b8fc90e) xiaogp

[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)

[MHA -> GQAï¼šæå‡ LLM æ¨ç†æ•ˆç‡](https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==&mid=2247488906&idx=1&sn=e2038e8b907c9b703354481ed0193af9&chksm=c2437164308b699ffe83a81842f17e611351c867b5b51fc3ee58bd6e7628a5b0c7716e52c26e&mpshare=1&scene=1&srcid=0115XLnq4kZjRAdZ8tI4DzYD&sharer_shareinfo=6f5890ca41e9b97d037f34b4c9518848&sharer_shareinfo_first=6f5890ca41e9b97d037f34b4c9518848#rd) AIé—²è°ˆ \[AIé—²è°ˆ\](javascript:void(0)ğŸ˜‰

[Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA](https://arxiv.org/abs/2412.20677)

[ç”±Karpathyå¯¹Transformeræ¶æ„çš„è®¨è®ºå¼•å‘çš„æ€è€ƒ](https://mp.weixin.qq.com/s?__biz=MzkzNTM5NDI2MQ==&mid=2247485096&idx=1&sn=31728a835c021db27ac1c8d224ce05ab&chksm=c34f4b63a859e58accc562f064cc89ce4f55b1b8bd1c920e5b62d47b5b36dff475418bcacd5a&mpshare=1&scene=1&srcid=0306mdETQAF0S1yERQevbhVk&sharer_shareinfo=f5df28637e76587857eaabd0412b2b8c&sharer_shareinfo_first=f5df28637e76587857eaabd0412b2b8c#rd) é™åŸŸAI