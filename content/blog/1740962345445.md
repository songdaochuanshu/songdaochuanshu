---
layout: post
title: 'deepseek-llamafactory模型微调并转为gguf'
date: "2025-03-03T00:39:05Z"
---
deepseek-llamafactory模型微调并转为gguf
================================

模型微调测试
------

### 基础设施配置

使用云计算平台  
![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228100745816-838601986.png)

使用vscode进行配置  
![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228101753199-585245222.png)

打开系统盘文件夹  
![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228102224910-309952675.png)

### llamafactory基础配置

    git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
    root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# cd LLaMA-Factory/
    CITATION.cff  MANIFEST.in  README.md     assets  docker      examples        requirements.txt  setup.py  tests
    LICENSE       Makefile     README_zh.md  data    evaluation  pyproject.toml  scripts           src
    root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp/LLaMA-Factory
    # - 创建虚拟环境对应目录，数据盘可以被保留
    mkdir -p /root/autodl-tmp/conda/pkgs 
    conda config --add pkgs_dirs /root/autodl-tmp/conda/pkgs 
    mkdir -p /root/autodl-tmp/conda/envs 
    conda config --add envs_dirs /root/autodl-tmp/conda/envs
    
    # - 创建 conda 虚拟环境(一定要 3.10 的 python 版本，不然和 LLaMA-Factory 不兼容)
    conda create -n llama-factory python=3.10
    root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp/LLaMA-Factory# conda env list
    # conda environments:
    #
    llama-factory            /root/autodl-tmp/conda/envs/llama-factory
    base                     /root/miniconda3
    
    root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp/LLaMA-Factory# 
    conda init
    source /root/.bashrc 
    conda activate llama-factory
    pip install -e ".[torch,metrics]"
    (llama-factory) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp/LLaMA-Factory# llamafactory-cli version
    
    ----------------------------------------------------------
    | Welcome to LLaMA Factory, version 0.9.2.dev0           |
    |                                                        |
    | Project page: https://github.com/hiyouga/LLaMA-Factory |
    ----------------------------------------------------------
    
    (llama-factory) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp/LLaMA-Factory# 
    llamafactory-cli webui
    

![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228115822979-749253138.png)  
也没端口转发就这样打开了，这个vscode还挺方便，我也不知道啥原理

再开一个终端

    (base) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# conda activate llama-factoy ry
    (llama-factory) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# 
    (llama-factory) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# mkdir Hugging-Face
    (llama-factory) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# export HF_ENDPOINT=https://hf-mirror.com
    (llama-factory) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# export HF_HOME=/root/autodl-tmp/Hugging-Face
    (llama-factory) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# 
    (llama-factory) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# pip install -U huggingface_hub
    (llama-factory) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# huggingface-cli download --resume-download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
    
    下载好大模型
    (base) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# du -sh Hugging-Face/
    3.4G    Hugging-Face/
    (base) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# pwd
    /root/autodl-tmp
    (base) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# ls
    Hugging-Face  LLaMA-Factory  conda
    (base) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# 
    

### 聊天测试

![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228130218004-153990093.png)

![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228130342872-1663619645.png)

### 编写数据集

数据集定义  
llama\_factory - data - README\_zh.md  
![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228133403585-1258731123.png)  
![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228133622332-246183884.png)

### 训练模型

![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228135657806-1715710898.png)  
学习率：开车的油门，一脚油门会过终点，太少就很慢才达到终点(最优解)  
训练轮数:太少会欠拟合(没有学到足够知识)太多会过拟合模型泛化能力下降，在训练数据表现很好但是新数据会很差  
最大梯度范数:当梯度的值超过这个范围时会被截断，防止梯度爆炸现象，当成保险丝  
最大样本数:每轮训练中最多使用的样本数。不设置，则拿整个数据集都去训练，但是十几万的时候就会特别多  
计算类型:常见float32 float16 用float16会减少内存占用会速度快，但导致精度损失  
截断长度:处理长文本时如果太长超过这个阈值的部分会被截断掉，避免内存溢出  
批处理大小:由于内存限制，分批次处理  
梯度累计:默认情况下模型会在每个 batch 处理完后进行一次更新一个参数，但你可以通过设置这个梯度累计，让他直到处理完多个小批次的数据后才进行一次更新  
验证集比例:训练时数据会被分成训练集和验证集，80与20  
学习率调节器:动态调节率，列如开车一开始加速，快到地方则降速

![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228135812595-1155673270.png)  
这些参数会导致过拟合

![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228140135650-981968209.png)  
损失从2.7降低为0  
损失曲线:降低太慢就加大学习率，如果学习结束还是下降趋势，还没到底就为欠拟合。降低太快降低学习率  
检查点路径:保存的是模型在训练过程中的一个中间状态，包含了模型权重、训练过程中使用的配置（如学习率、批次大小）等信息，对LoRA来说，检查点包含了**训练得到的 B 和 A 这两个低秩矩阵的权重**

![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228141007920-916427747.png)

*   若微调效果不理想，你可以：
    *   使用更强的预训练模型
    *   增加数据量
    *   优化数据质量（数据清洗、数据增强等，可学习相关论文如何实现）
    *   调整训练参数，如学习率、训练轮数、优化器、批次大小等等

导出合并后的模型

*   为什么要合并：因为 LoRA 只是通过**低秩矩阵**调整原始模型的部分权重，而**不直接修改原模型的权重**。合并步骤将 LoRA 权重与原始模型权重融合生成一个完整的模型
*   先创建目录，用于存放导出后的模型

    (base) root@autodl-container-10a44fbcf4-b07c334b:~# cd /root/autodl-tmp/
    (base) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# mkdir -p Models/deepseek-r1-1.5b-merged
    (base) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# 
    

![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228141550942-839730086.png)

### 部署模型

    conda create -n fastApi python=3.10
    (base) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp/Models/deepseek-r1-1.5b-merged# conda activate fastApi
    (fastApi) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp/Models/deepseek-r1-1.5b-merged# 
    conda install -c conda-forge fastapi uvicorn transformers pytorch
    pip install safetensors sentencepiece protobuf
    
    (fastApi) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# mkdir App
    (fastApi) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp# cd App/
    (fastApi) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp/App# touch main.py
    (fastApi) root@autodl-container-10a44fbcf4-b07c334b:~/autodl-tmp/App# 
    

    from fastapi import FastAPI
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    
    app = FastAPI()
    
    # 模型路径
    model_path = "/root/autodl-tmp/Models/deepseek-r1-1.5b-merged"
    
    # 加载 tokenizer （分词器）
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    ### 分词器十分重要自动加载模型匹配分词器
    # 加载模型并移动到可用设备（GPU/CPU）
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)
    
    @app.get("/generate")
    async def generate_text(prompt: str):
        # 使用 tokenizer 编码输入的 prompt
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # 使用模型生成文本
        outputs = model.generate(inputs["input_ids"], max_length=150)
        
        # 解码生成的输出
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return {"generated_text": generated_text}
    
    

    python -m uvicorn main:app --reload --host 0.0.0.0
    

[http://localhost:8000/docs](http://localhost:8000/docs)  
![](https://img2024.cnblogs.com/blog/2415164/202502/2415164-20250228145557427-1237897583.png)  
训练的过拟合了

    python3 /root/llama.cpp/convert_hf_to_gguf.py    /mnt/c/Users/DK/Desktop/投喂的资料/deepseek-r1-1.5b-merged --outfile /root/test.gguf --outtype f16
    

    modelfile文件已经被llamafactory定义
    PS C:\Users\DK\Desktop\投喂的资料\deepseek-r1-1.5b-merged> ollama create haimianbb
    gathering model components
    copying file sha256:a998574673a76c152a2aabce1ab7eaf0c307990dc6c0badd964617afb9d79652 100%
    copying file sha256:8dd737d110cceb8396782d1b0c9196655a4a78fee4de35f056b22d1f9e96bf96 100%
    copying file sha256:59cda48bbe8bab9d61ffb410e6e3c07b6d98bff73cee7c88ff8b51f95f21ab1c 100%
    copying file sha256:e20ddafc659ba90242154b55275402edeca0715e5dbb30f56815a4ce081f4893 100%
    copying file sha256:6ce236e90057bbc36feee740e52666fc58103659d82063fc5ddb1355551e8148 100%
    copying file sha256:b0e8dce267611e5e03ebf2cd16a8c3821bbfcad441415dd6875173788f518a56 100%
    converting model
    creating new layer sha256:ac67fabf626c9acf8fbfbc2b6391a8e1d9d1f4ed7f93e340627df1eb421ba7df
    creating new layer sha256:741e943dbd4c7642ec2e10b275b4cc1a154d97550dce8a6288e1edf56bec5e8f
    creating new layer sha256:b2ad9c47ff5fee622d61048a4fe3ba330b91d26bc04578fad9d10d143dc86322
    writing manifest
    success
    PS C:\Users\DK\Desktop\投喂的资料\deepseek-r1-1.5b-merged>
    

![](https://img2024.cnblogs.com/blog/2415164/202503/2415164-20250301225359049-578237723.png)