---
layout: post
title: '大模型 Token 究竟是啥：图解大模型Token'
date: "2025-04-02T00:39:30Z"
---
大模型 Token 究竟是啥：图解大模型Token
=========================

前几天，一个朋友问我：“**大模型**中的 **Token** 究竟是什么？”

这确实是一个很有代表性的问题。许多人听说过 Token 这个概念，但未必真正理解它的作用和意义。思考之后，我决定写篇文章，详细解释这个话题。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329221904262-1613828067.gif)

我说：像 **DeepSeek** 和 **ChatGPT** 这样的超大语言模型，都有一个“刀法精湛”的小弟——**分词器（**Tokenizer**）**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329223612477-343640341.png)

当**大模型**接**收到一段文字**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329224705879-1626860868.png)

会让**分词器**把它**切成很多个小块**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329224911567-619863587.png)

这切出来的每一个小块就叫做一个 **Token**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329231936736-1030805454.png)

比如这段话（**我喜欢唱、跳、Rap和篮球**），在大模型里可能会被切成这个样子。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329213514131-1707276093.png)

像**单个汉字**，可能是一个 **Token**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329213949184-1060535411.png)

**两个汉字**构成的**词语**，也可能是一个 **Token**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329214101749-234936468.png)

**三个字**构成的**常见短语**，也可能是一个 **Token**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329214320969-662623668.png)

**一个标点符号**，也可能是一个 **Token**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329214407497-1732726574.png)

**一个单词**，或者是**几个字母**组成的一个**词缀**，也可能是一个 **Token**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329214633732-384853561.png)

大模型在输出文字的时候，也是一个 Token 一个 Token 的往外蹦，所以看起来可能有点像在打字一样。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250329232612837-845003352.gif)

朋友听完以后，好像更疑惑了：

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330013618237-828530496.png)

于是，我决定换一个方式，给他通俗解释一下。

大模型的Token究竟是啥，以及为什么会是这样。

首先，请大家快速读一下这几个字：

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330014203560-1662959825.png)

是不是有点没有认出来，或者是需要愣两秒才可以认出来？

但是如果这些字出现在**词语**或者**成语**里，你**瞬间**就可以念出来。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330015143896-721304174.png)

那之所以会这样，是因为我们的**大脑在日常生活中**，**喜欢**把这些有含义的**词语**或者**短语**，优先作为**一个整体**来对待。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330015710305-238696458.png)

不到万不得已，不会去一个字一个字的抠。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330021011370-790893575.png)

这就导致我们对这些**词语还挺熟悉**，**单看**这些字（旯妁圳侈邯）的时候，反而会觉得**有点陌生**。

而大脑🧠之所以要这么做，是因为这样可以节省脑力，咱们的大脑还是非常懂得偷懒的。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330030726315-1791083719.png)

比如 “**今天天气不错**” 这句话，如果一个字一个字的去处理，一共需要有**6个部分**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330030617520-1888029110.png)

但是如果划分成**3个**、**常见**且**有意义的词**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330030928066-100510559.png)

就只需要处理**3个**部分**之间的关系**，从而**提高效率**，**节省脑力**。

既然人脑可以这么做，那人工智能也可以这么做。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330034728676-1692578228.png)

所以就有了**分词器**，专门**帮大模型**把大段的文字，**拆解成大小合适**的一个个 **Token**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330034901880-907658531.png)

不同的分词器，它的分词方法和结果不一样。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330035057914-495700775.png)

分得越合理，大模型就越轻松。这就好比餐厅里负责切菜的切配工，它的刀功越好，主厨做起菜来当然就越省事。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330035305316-676264201.png)

分词器究竟是怎么分的词呢？

其中一种方法大概是这样，分词器统计了大量文字以后，发现 **“苹果”** 这两个字，**经常一起出现**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330035442025-1702036198.png)

就把它们打包成一个 **Token**，给它一个**数字编号**，比如 **19416**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330035535969-1587747105.png)

然后丢到一个大的**词汇表**里。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330035623537-190919188.png)

这样下次再看到 **“苹果”** 这两个字的时候，就可以直接认出这个组合就可以了。

然后它可能又发现 **“鸡”** 这个字**经常出现**，并且**可以搭配不同的其他字**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330035752625-549436850.png)

于是它就把 **“鸡”** 这个字，打包成一个 **Token**，给它**配一个数字编号**，比如 **76074**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330035936186-392337978.png)

并且丢到**词汇表**里。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330040346612-1147326693.png)

它又发现 **“ing”** 这三个字母**经常一起出现**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330040444164-1266108088.png)

于是又把 **“ing”** 这**三个字母**打包成一个 **Token**，给它**配一个数字编号**，比如 **288**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330040552603-1400920552.png)

并且收录到**词汇表**里。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330040646815-983421702.png)

它又发现 **“逗号”** 经常出现。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330040739749-2145278878.png)

于是又把 **“逗号”** 也打包作为一个 **Token**，给它**配一个数字编号**，比如 **14**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330040759029-1687533560.png)

收录到**词汇表**里。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330040909080-929892084.png)

经过**大量统计**和**收集**，分词器就可以得到**一个庞大的Token表**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330041027313-2070868030.png)

可能有**5万个**、**10万个**，甚至**更多Token**，可以**囊括**我们日常见到的各种**字**、**词**、**符号**等等。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330041202714-406489753.png)

这样一来，大模型在**输入**和**输出**的时候，都只需要**面对一堆数字编号**就可以了。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330041315336-1323830099.png)

再由分词器**按照Token表**，转换成**人类可以看懂**的**文字**和**符号**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330041403085-1030412812.png)

这样一分工，工作效率就非常高。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330041513486-954314015.png)

有这么一个网站 **Tiktokenizer**：[https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app)

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330041733913-1477351521.png)

输入一段话，它就可以告诉你，这段话是**由几个Token构成**的，分别是什么，以及这几个**Token的编号分别是多少**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330124658377-412060905.png)

我来演示一下，这个网站有很多模型可以选择，像 **GPT-4o**、**DeepSeek**、**LLaMA** 等等。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330124841908-1086689995.png)

我选的是 **DeepSeek**，我输入 **“哈哈”**，显示是**一个 Token**，编号是 **11433**：

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330124940576-1066826376.png)

**“哈哈哈”**，也是**一个 Token**，编号是 **40886**：

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330042254101-1720127855.png)

**4**个 **“哈”**，还是**一个 Token**，编号是 **59327**：

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330125649628-134511117.png)

但是**5**个 **“哈”**，就变成了**两个Token**，编号分别是 **11433**, **40886**：

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330130458163-500229089.png)

说明大家平常用两个 **“哈”** 或者**三个**的更多。

再来，“一心一意” 是三个 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330130627407-799656843.png)

“鸡蛋” 是一个 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330130705809-627990355.png)

但是 “鸭蛋” 是两个 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330130734684-1820601027.png)

“关羽” 是一个 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330130814275-959895115.png)

“张飞” 是两个 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330130843670-107093440.png)

“孙悟空” 是一个 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330130915668-780281331.png)

“沙悟净” 是三个 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330130955222-154375341.png)

另外，正如前面提到的，不同模型的分词器可能会有不同的切分结果。比如，“**苹果**” 中的 “**苹**” 字，在 **DeepSeek** 中被拆分成两个 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330131239995-368194495.png)

但是在 `Qwen` 模型里却是一个 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330131342692-847984391.png)

所以回过头来看，**Token** 到底是什么？

它就是构建大模型世界的一块块积木。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330133139211-359063061.png)

大模型之所以能理解和生成文本，就是通过计算这些 Token 之间的关系，来预测下一个最可能出现的 Token。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330133212151-159956032.png)

这就是为什么几乎所有大模型公司都按照 **Token** 数量计费，因为 Token 数量直接对应背后的计算成本。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330133338300-964232955.png)

“**Token**” 这个词不仅用于**人工智能**领域，在其他领域也经常出现。其实，它们只是**恰好**都叫这个名字而已。![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330133413332-588777855.png)

就像同样都是 **“车模”**，**汽车模型**和**车展模特**，虽然用词相同，但含义却**截然不同**。

![](https://img2023.cnblogs.com/blog/2105804/202503/2105804-20250330131918921-488085824.png)

FAQ
===

1\. 苹为啥会是2个?
------------

因为“苹” 字单独出现的概率太低，无法独立成为一个 Token。

2\. 为什么张飞算两个 Token?
-------------------

“张” 和 “飞” 一起出现的频率不够高，或者“ 张” 字和 “飞” 字的搭配不够稳定，经常与其他字组合，因此被拆分为两个 Token。

* * *

Token 在大模型方面最好的翻译是 '词元' 非常的信雅达。

欢迎关注我的微信公众号【程序员 NEO】！

这里有丰富的技术分享、实用的编程技巧、深度解析微服务架构，还有更多精彩内容等你探索！🚀