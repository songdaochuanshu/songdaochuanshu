---
layout: post
title: 'å·ç§¯ç¥ç»ç½‘ç»œçš„å¼•å…¥4 â€”â€” å±€éƒ¨æ‰°åŠ¨ä¸ç©ºé—´ç»“æ„ç ´åä¸‹çš„é²æ£’æ€§éªŒè¯'
date: "2025-11-27T00:41:57Z"
---
å·ç§¯ç¥ç»ç½‘ç»œçš„å¼•å…¥4 â€”â€” å±€éƒ¨æ‰°åŠ¨ä¸ç©ºé—´ç»“æ„ç ´åä¸‹çš„é²æ£’æ€§éªŒè¯
================================

åœ¨å‰ä¸‰ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¾æ¬¡éªŒè¯äº†ï¼š

1.  **CNN å¯¹å¹³ç§»ç­‰ç©ºé—´æ‰°åŠ¨å…·æœ‰å¤©ç„¶ä¼˜åŠ¿**
2.  **åœ¨ä½ç»´ç°åº¦å›¾ï¼ˆFashion-MNISTï¼‰ä¸Šï¼ŒCNN ä¸ MLP å·®è·æœ‰é™**
3.  **åœ¨ä¸­ç­‰å¤æ‚åº¦æ•°æ®é›†ï¼ˆCIFAR-10ï¼‰ä¸Šï¼Œå·®è·è¿…é€Ÿæ‹‰å¤§**

åˆ°è¿™é‡Œï¼Œä¸€ä¸ªé‡è¦é—®é¢˜æµ®ç°ï¼š

> **CNN çš„ä¼˜åŠ¿åˆ°åº•æ¥è‡ªâ€œæ›´å¤§çš„æ•°æ®é›†â€è¿˜æ˜¯æ¥è‡ªâ€œå›¾åƒçš„ç©ºé—´ç»“æ„â€ï¼Ÿ  
> æ¢å¥è¯è¯´ï¼šæ˜¯å¦å³ä¾¿ä¸æ¢æ›´å¤§çš„æ•°æ®é›†ï¼Œåªè¦ç ´åç©ºé—´ç»“æ„ï¼ŒMLP å°±ä¼šæ›´åƒäºï¼Ÿ**

æœ¬ç¯‡å°†ä»è¿™ä¸€å…³é”®è§†è§’å±•å¼€å®éªŒã€‚

* * *

ä¸€ã€å®éªŒç›®æ ‡
======

æœ¬ç¯‡å¸Œæœ›è¿›ä¸€æ­¥å›ç­”ï¼š

### ğŸ§ª 1. å¦‚æœæˆ‘ä»¬â€œç ´åå›¾åƒçš„å±€éƒ¨ç»“æ„â€ï¼ŒMLP ä¸ CNN è°æ›´ç¨³å¥ï¼Ÿ

### ğŸ§ª 2. å½“å›¾åƒé­é‡â€œå±€éƒ¨é®æŒ¡â€â€œéšæœºå™ªå£°â€â€œéšæœºæ“¦é™¤â€ç­‰æ‰°åŠ¨ï¼ŒCNN æ˜¯å¦ä»èƒ½ä¿æŒè¾ƒå¼ºæ³›åŒ–èƒ½åŠ›ï¼Ÿ

### ğŸ§ª 3. æ—¢ç„¶ MLP å®Œå…¨ä¾èµ–å…¨å±€å¹³é“ºè¾“å…¥ï¼Œç©ºé—´ç ´åæ˜¯å¦ä¼šå¯¹ MLP é€ æˆæ¯ç­æ€§å½±å“ï¼Ÿ

* * *

äºŒã€å®éªŒç­–ç•¥ï¼ˆä¸å†æ›´æ¢æ•°æ®é›†ï¼Œè€Œæ˜¯æ›´æ¢æ‰°åŠ¨æ–¹å¼ï¼‰
========================

æˆ‘ä»¬ç»§ç»­ä½¿ç”¨ **CIFAR-10** â€”â€” æ•°æ®é›†æœ¬èº«ä¸å˜ã€‚

ä½†é‡ç‚¹ä¿®æ”¹â€œè¾“å…¥å›¾åƒâ€ï¼Œé€šè¿‡æ³¨å…¥ä¸åŒç±»åˆ«çš„ç©ºé—´æ‰°åŠ¨ï¼Œè®©æ¨¡å‹é¢ä¸´çœŸæ­£çš„å¯¹æŠ—æ€§æŒ‘æˆ˜ã€‚

### æœ¬æ–‡é‡‡ç”¨ä¸€ç±»å…¸å‹æ‰°åŠ¨ï¼š

#### ğŸ”¶ 1. Random Erasingï¼ˆéšæœºæ“¦é™¤å±€éƒ¨åŒºåŸŸï¼‰

æ¨¡æ‹Ÿé®æŒ¡ï¼š

*   éšæœºæŒ–æ‰å›¾ç‰‡ä¸­çš„ä¸€å°å—ï¼ˆå¦‚ 16Ã—16ï¼‰
*   å¹¿æ³›ç”¨äºè®­ç»ƒ CNN çš„å¢å¼ºç­–ç•¥

> CNN å¯é€šè¿‡å‘¨è¾¹åŒºåŸŸè¡¥å¿  
> MLP å› å®Œå…¨ flattenï¼Œä¼šå¤±å»ç©ºé—´ç»“æ„ â†’ ç‰¹å¾è¢«ç ´å

ä¸‰ã€å®éªŒæ­¥éª¤
======

1.  ä½¿ç”¨ CIFAR-10
2.  è®­ç»ƒ MLP ä¸ CNNï¼ˆç»“æ„ä¸ä¸Šä¸€ç« ä¸€è‡´ï¼‰
3.  æ¯ç§æ‰°åŠ¨åˆ†åˆ«è®­ç»ƒ 10 epochï¼Œè®°å½•ï¼š
    *   è®­ç»ƒé›†ç²¾åº¦
    *   éªŒè¯é›†ç²¾åº¦
    *   æ”¶æ•›é€Ÿåº¦
4.  ä½¿ç”¨æŠ˜çº¿å›¾å±•ç¤º MLP vs CNN çš„ç²¾åº¦å·®å¼‚

* * *

å››ã€æ ¸å¿ƒå®éªŒä»£ç ï¼ˆåŠ å…¥å›¾åƒæ‰°åŠ¨å¢å¼ºï¼‰
==================

ä»¥ä¸‹ä¸ºç»“æ„åŒ– Demoï¼Œæ–¹ä¾¿ç›´æ¥å¤ç°å®éªŒã€‚

    # -*- coding: utf-8 -*-
    # å·ç§¯ç¥ç»ç½‘ç»œçš„å¼•å…¥4 â€”â€” ç©ºé—´æ‰°åŠ¨ä¸‹çš„ MLP ä¸ CNN é²æ£’æ€§å¯¹æ¯”å®éªŒ
    # Author: zhchoice
    # Date: 2025-11-XX
    
    import torch
    import torchvision
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader
    from torchvision import datasets, transforms
    import matplotlib.pyplot as plt
    import random
    
    device = 'mps' if torch.backends.mps.is_available() else 'cpu'
    
    # =============================
    # 1ï¸âƒ£ æ‰°åŠ¨ç±»å‹é€‰æ‹©ï¼ˆé‡ç‚¹ï¼‰
    # =============================
    # å¯é€‰ï¼š none / erasing / gaussian / cutout
    AUG_TYPE = 'erasing'
    
    print(f"===> ä½¿ç”¨æ‰°åŠ¨æ–¹å¼ï¼š{AUG_TYPE}")
    
    # =============================
    # 2ï¸âƒ£ å®šä¹‰æ‰°åŠ¨å‡½æ•°ï¼ˆæ ¸å¿ƒéƒ¨åˆ†ï¼‰
    # =============================
    
    def add_gaussian_noise(img, mean=0.0, std=0.1):
        noise = torch.randn_like(img) * std + mean
        return torch.clamp(img + noise, -1, 1)
    
    def cutout(img, size=16):
        _, h, w = img.size()
        y = random.randint(size, h - size)
        x = random.randint(size, w - size)
        img[:, y-size:y+size, x-size:x+size] = 0
        return img
    
    
    class CutoutTransform:
        """å¯åœ¨ transforms ä¸­ä½¿ç”¨çš„ cutout å°è£…"""
        def __call__(self, img):
            return cutout(img)
    
    
    class GaussianNoiseTransform:
        """é«˜æ–¯å™ªå£°å°è£…"""
        def __call__(self, img):
            return add_gaussian_noise(img)
    
    
    # =============================
    # 3ï¸âƒ£ å›¾åƒå¢å¼º pipeline è®¾ç½®
    # =============================
    train_transforms = [
        transforms.ToTensor(),
        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),
    ]
    
    if AUG_TYPE == 'erasing':
        train_transforms.append(transforms.RandomErasing(p=1.0, scale=(0.1,0.2)))
    elif AUG_TYPE == 'gaussian':
        train_transforms.append(GaussianNoiseTransform())
    elif AUG_TYPE == 'cutout':
        train_transforms.append(CutoutTransform())
    
    train_transform = transforms.Compose(train_transforms)
    
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))
    ])
    
    # =============================
    # 4ï¸âƒ£ CIFAR10 æ•°æ®åŠ è½½
    # =============================
    trainset = datasets.CIFAR10('./data', train=True, download=True, transform=train_transform)
    testset = datasets.CIFAR10('./data', train=False, download=True, transform=test_transform)
    
    train_loader = DataLoader(trainset, batch_size=64, shuffle=True)
    test_loader = DataLoader(testset, batch_size=256)
    
    # =============================
    # 5ï¸âƒ£ å®šä¹‰æ¨¡å‹ï¼ˆæ²¿ç”¨ä¸Šä¸€ç« ï¼‰
    # =============================
    class MLP(nn.Module):
        def __init__(self, input_dim=32*32*3, hidden=1024):
            super().__init__()
            self.net = nn.Sequential(
                nn.Flatten(),
                nn.Linear(input_dim, hidden),
                nn.ReLU(),
                nn.Linear(hidden, 10)
            )
        def forward(self, x):
            return self.net(x)
    
    class CNN(nn.Module):
        def __init__(self, in_ch=3):
            super().__init__()
            self.net = nn.Sequential(
                nn.Conv2d(in_ch, 32, 3, padding=1),
                nn.BatchNorm2d(32),
                nn.ReLU(),
                nn.MaxPool2d(2),
    
                nn.Conv2d(32, 64, 3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(2),
    
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d(1),
    
                nn.Flatten(),
                nn.Linear(128, 10)
            )
        def forward(self, x):
            return self.net(x)
    
    # =============================
    # 6ï¸âƒ£ è®­ç»ƒ & éªŒè¯
    # =============================
    loss_fn = nn.CrossEntropyLoss()
    
    def train_one_epoch(model, loader, opt):
        model.train()
        tot_loss, tot_correct = 0, 0
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            loss = loss_fn(out, y)
    
            opt.zero_grad()
            loss.backward()
            opt.step()
    
            tot_loss += loss.item()
            tot_correct += (out.argmax(1) == y).sum().item()
    
        return tot_loss / len(loader), tot_correct / len(loader.dataset)
    
    
    def evaluate(model, loader):
        model.eval()
        tot_correct = 0
        with torch.no_grad():
            for x, y in loader:
                x, y = x.to(device), y.to(device)
                tot_correct += (model(x).argmax(1) == y).sum().item()
        return tot_correct / len(loader.dataset)
    
    # =============================
    # 7ï¸âƒ£ å®éªŒæ‰§è¡Œ
    # =============================
    mlp = MLP().to(device)
    cnn = CNN().to(device)
    
    opt_mlp = torch.optim.Adam(mlp.parameters(), lr=1e-3)
    opt_cnn = torch.optim.Adam(cnn.parameters(), lr=1e-3)
    
    epochs = 10
    
    mlp_train, mlp_test = [], []
    cnn_train, cnn_test = [], []
    
    for ep in range(epochs):
        _, acc_m = train_one_epoch(mlp, train_loader, opt_mlp)
        _, acc_c = train_one_epoch(cnn, train_loader, opt_cnn)
    
        val_m = evaluate(mlp, test_loader)
        val_c = evaluate(cnn, test_loader)
    
        mlp_train.append(acc_m)
        cnn_train.append(acc_c)
        mlp_test.append(val_m)
        cnn_test.append(val_c)
    
        print(f"[{ep+1}/{epochs}] MLP val={val_m:.3f} | CNN val={val_c:.3f}")
    
    # =============================
    # 8ï¸âƒ£ ç”»ç²¾åº¦æ›²çº¿
    # =============================
    plt.figure(figsize=(10,6))
    plt.plot(range(1,epochs+1), mlp_train, 'r--o', label='MLP Train')
    plt.plot(range(1,epochs+1), mlp_test, 'r-', label='MLP Val')
    plt.plot(range(1,epochs+1), cnn_train, 'b--o', label='CNN Train')
    plt.plot(range(1,epochs+1), cnn_test, 'b-', label='CNN Val')
    
    plt.title(f"Accuracy Curve under Perturbation: {AUG_TYPE}")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()
    

* * *

äº”ã€è®­ç»ƒç»“æœè¡¨ç°
========

ä»æœ¬æ¬¡ â€œRandom Erasingï¼ˆéšæœºæ“¦é™¤ï¼‰â€ çš„å®éªŒç»“æœä¸­ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè§‚å¯Ÿåˆ°ä»¥ä¸‹å‡ ä¸ªæ¸…æ™°ç»“è®ºï¼š

1.  CNN åœ¨å±€éƒ¨é®æŒ¡ä¸‹çš„é²æ£’æ€§æ˜¾è‘—ä¼˜äº MLP

ä»ç¬¬ 1 ä¸ª epoch èµ·ï¼š

CNN éªŒè¯é›†ç²¾åº¦ï¼š0.426

MLP éªŒè¯é›†ç²¾åº¦ï¼š0.437

ä¸¤è€…ç›¸å·®ä¸å¤§ã€‚ä½†éšç€è®­ç»ƒç»§ç»­è¿›è¡Œï¼ŒCNN çš„è¡¨ç°å¼€å§‹å¿«é€Ÿæå‡ï¼š

æœ€ç»ˆ CNN val â‰ˆ 0.660

è€Œ MLP val â‰ˆ 0.505

CNN çš„éªŒè¯ç²¾åº¦å§‹ç»ˆä¿æŒ 10%~15% çš„ç¨³å®šä¼˜åŠ¿ï¼Œå¹¶ä¸”åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ›²çº¿å¹³æ»‘ã€ç¨³æ­¥ä¸Šå‡ã€‚

ç»“è®ºï¼šCNN å¯¹äºéšæœºæ“¦é™¤é€ æˆçš„å±€éƒ¨ç»“æ„ç ´åå…·æœ‰å¤©ç„¶æŠ—æ€§ï¼Œè€Œ MLP æ›²çº¿æå‡ç¼“æ…¢ä¸”ä¸Šé™æ›´ä½ã€‚

2.  éšæœºæ“¦é™¤å¯¹ MLP çš„å½±å“è¿œå¤§äºå¯¹ CNN çš„å½±å“

ä»æ›²çº¿å¯ä»¥çœ‹åˆ°ï¼š

MLP çš„è®­ç»ƒæ›²çº¿ä¸éªŒè¯æ›²çº¿å§‹ç»ˆå­˜åœ¨æ˜æ˜¾ gap

ä¸”éªŒè¯é›†ç²¾åº¦å¢é•¿ç¼“æ…¢ï¼ŒåæœŸå‡ ä¹è¿›å…¥åœæ»

éšæœºæ“¦é™¤ç ´åäº† MLP flatten åçš„å…¨å±€å‘é‡ï¼Œä½¿æ¨¡å‹éš¾ä»¥æ¢å¤æœ‰æ•ˆç‰¹å¾

è€Œ CNNï¼š

å³ä¾¿è¢«é®æŒ¡ä¸€éƒ¨åˆ†åŒºåŸŸ

ä»èƒ½é€šè¿‡å±€éƒ¨å·ç§¯æ ¸ä»æœªè¢«é®æŒ¡çš„åŒºåŸŸæå–è¶³å¤Ÿä¿¡æ¯

éªŒè¯æ›²çº¿å¹³æ»‘ä¸”ç¨³å®šä¸Šå‡

è¿™è¡¨æ˜ï¼šMLP ç¼ºä¹ç©ºé—´è¡¥å¿æœºåˆ¶ï¼Œä¸€æ—¦å±€éƒ¨åƒç´ è¢«ç ´åï¼Œæ•´ä½“ç‰¹å¾éƒ½ä¼šè¢«æ‰°ä¹±ï¼›è€Œ CNN åˆ™å…·å¤‡â€œå±€éƒ¨å®¹é”™â€èƒ½åŠ›ã€‚

3.  CNN çš„æ”¶æ•›é€Ÿåº¦æ˜æ˜¾å¿«äº MLP

è§‚å¯Ÿå‰ 3 ä¸ª epochï¼š

CNN valï¼šä» 0.426 â†’ 0.562 â†’ 0.590

MLP valï¼šä» 0.437 â†’ 0.465 â†’ 0.475

CNN çš„å¢é•¿é€Ÿåº¦æ›´å¿«ï¼Œæ›²çº¿ä¹Ÿæ›´é™¡å³­ã€‚

CNN åœ¨æ—©æœŸå°±æŒæ¡äº†ç¨³å¥çš„å±€éƒ¨çº¹ç†ç‰¹å¾ï¼Œè€Œ MLP åˆ™éœ€è¦æ›´é•¿æ—¶é—´æ‰å¼€å§‹å­¦ä¹ åˆ°æœ‰æ•ˆç»“æ„ã€‚

4.  éšæœºæ“¦é™¤åè€Œè¿›ä¸€æ­¥å‡¸æ˜¾äº† CNN çš„ä¼˜åŠ¿

ä¸ä¸Šä¸€ç« ï¼ˆæ— æ‰°åŠ¨ï¼‰ç›¸æ¯”ï¼š

CNN åœ¨ erasing ä¸‹ç²¾åº¦ç•¥é™ï¼Œä½†ä»ç»´æŒé«˜ç¨³å®šæ€§ä¸é«˜ä¸Šé™

MLP åœ¨ erasing ä¸‹æŸå¤±æ˜æ˜¾åŠ å‰§ï¼Œæœ€ç»ˆéªŒè¯ç²¾åº¦ä¹Ÿæ›´ä½

è¿™è¯´æ˜ï¼š

æ‰°åŠ¨è¶Šå¼ºï¼ŒMLP è¶Šè„†å¼±ï¼›  
æ‰°åŠ¨è¶Šå¼ºï¼ŒCNN è¶Šä½“ç°å…¶è¾¨è¯†å±€éƒ¨ç»“æ„çš„èƒ½åŠ›ã€‚

ğŸ¯ æœ€ç»ˆæ€»ç»“ï¼ˆå·4æ ¸å¿ƒç»“è®ºï¼‰

åŸºäºæœ¬æ¬¡å®éªŒæ•°æ®å¯ä»¥æ˜ç¡®å¾—å‡ºï¼š

CNN çš„ä¼˜åŠ¿ä¸ä¾èµ–äºæ›´å¤§çš„æ•°æ®é›†ï¼Œè€Œæ˜¯æ¥æºäºâ€œç©ºé—´ç»“æ„æ•æ„Ÿæ€§ + å±€éƒ¨ç‰¹å¾è¡¥å¿èƒ½åŠ›â€ã€‚

éšæœºæ“¦é™¤æœ¬è´¨ä¸Šç ´åäº†ï¼š

å±€éƒ¨çº¹ç†

éƒ¨åˆ†è¯­ä¹‰åŒºåŸŸ

MLP ç”±äº flatten ç‰¹æ€§ï¼š

å®Œå…¨æ²¡æœ‰ç©ºé—´æ„è¯†

å±€éƒ¨æŸåä¼šæ‰°ä¹±æ•´ä¸ªè¾“å…¥å‘é‡

å› æ­¤éªŒè¯ç²¾åº¦ä¸¥é‡å—æŸ

CNN åˆ™å¯ä»¥ï¼š

é å‘¨è¾¹ç‰¹å¾è¡¥å¿ç¼ºå¤±çš„ä¿¡æ¯

ä¿æŒç¨³å®šå­¦ä¹ 

åœ¨é®æŒ¡ç¯å¢ƒä¸‹ä»èƒ½æ­£ç¡®åˆ†ç±»

å› æ­¤ï¼š

åœ¨ä»»ä½•æ¶‰åŠå›¾åƒâ€œé®æŒ¡ã€å™ªå£°ã€å±€éƒ¨æŸåâ€çš„ä»»åŠ¡ä¸­ï¼ŒCNN éƒ½æ˜¾è‘—ä¼˜äº MLPã€‚