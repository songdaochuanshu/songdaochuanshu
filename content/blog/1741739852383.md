---
layout: post
title: 'LayerSkip: ä½¿ç”¨è‡ªæŽ¨æµ‹è§£ç åŠ é€Ÿå¤§æ¨¡åž‹æŽ¨ç†'
date: "2025-03-12T00:37:32Z"
---
LayerSkip: ä½¿ç”¨è‡ªæŽ¨æµ‹è§£ç åŠ é€Ÿå¤§æ¨¡åž‹æŽ¨ç†
=========================

è‡ªæŽ¨æµ‹è§£ç æ˜¯ä¸€ç§æ–°é¢–çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒç»“åˆäº†æŽ¨æµ‹è§£ç  (Speculative Decoding) çš„ä¼˜åŠ¿å’Œå¤§è¯­è¨€æ¨¡åž‹ (LLM) çš„æå‰é€€å‡º (Early Exit) æœºåˆ¶ã€‚è¯¥æ–¹æ³•å‡ºè‡ªè®ºæ–‡ [LayerSkip: Enabling Early-Exit Inference and Self-Speculative Decoding](https://arxiv.org/abs/2404.16710)ã€‚å®ƒé€šè¿‡ä½¿ç”¨ _åŒä¸€ä¸ªæ¨¡åž‹_ çš„æ—©æœŸå±‚æ¥ç”Ÿæˆå€™é€‰è¯å…ƒ (token)ï¼Œå¹¶ä½¿ç”¨åŽæœŸå±‚è¿›è¡ŒéªŒè¯ï¼Œä»Žè€Œå®žçŽ°é«˜æ•ˆç”Ÿæˆã€‚

è¿™é¡¹æŠ€æœ¯ä¸ä»…åŠ å¿«äº†æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ï¼Œè¿˜æ˜¾è‘—èŠ‚çœäº†å†…å­˜å¹¶é™ä½Žäº†è®¡ç®—å»¶è¿Ÿã€‚ä¸ºäº†å®žçŽ°ç«¯åˆ°ç«¯çš„åŠ é€Ÿï¼Œæ—©æœŸå±‚çš„è¾“å‡ºéœ€è¦ä¸Žæœ€ç»ˆå±‚çš„è¾“å‡ºè¶³å¤ŸæŽ¥è¿‘ã€‚æ­£å¦‚è®ºæ–‡ä¸­æ‰€è¿°ï¼Œè¿™å¯ä»¥é€šè¿‡ä¸€ç§è®­ç»ƒæ–¹æ³•æ¥å®žçŽ°ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨é¢„è®­ç»ƒæœŸé—´åº”ç”¨ï¼Œä¹Ÿå¯ä»¥åœ¨ç‰¹å®šé¢†åŸŸè¿›è¡Œå¾®è°ƒæ—¶åº”ç”¨ã€‚è‡ªæŽ¨æµ‹è§£ç å¯¹äºŽå®žé™…åº”ç”¨ç‰¹åˆ«é«˜æ•ˆï¼Œå®ƒå¯ä»¥åœ¨è¾ƒå°çš„ GPU ä¸Šéƒ¨ç½²ï¼Œå¹¶é™ä½Ž **å¤§è§„æ¨¡æŽ¨ç†** æ‰€éœ€çš„æ•´ä½“ç¡¬ä»¶èµ„æºã€‚

åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨è‡ªæŽ¨æµ‹è§£ç çš„æ¦‚å¿µã€å…¶å®žçŽ°æ–¹å¼ä»¥åŠåœ¨ ðŸ¤— transformers åº“ä¸­çš„å®žé™…åº”ç”¨ã€‚æ‚¨å°†äº†è§£åˆ°å…¶æŠ€æœ¯åŽŸç†ï¼ŒåŒ…æ‹¬ **æå‰é€€å‡ºå±‚ (Early-Exit Layers)** ã€ **ååµŒå…¥ (Unembedding)** å’Œ **è®­ç»ƒä¿®æ”¹ (Training Modifications)**ã€‚ä¸ºäº†å°†è¿™äº›æ¦‚å¿µä»˜è¯¸å®žè·µï¼Œæˆ‘ä»¬æä¾›äº†ä»£ç ç¤ºä¾‹ã€ä¸Žä¼ ç»ŸæŽ¨æµ‹è§£ç çš„åŸºå‡†æ¯”è¾ƒï¼Œä»¥åŠå¯¹æ€§èƒ½æƒè¡¡çš„è§è§£ã€‚

æ‚¨è¿˜å¯ä»¥ç›´æŽ¥æŸ¥çœ‹ä»¥ä¸‹ Hugging Face èµ„æºï¼Œäº†è§£æ›´å¤šå…³äºŽè¯¥æ–¹æ³•çš„ä¿¡æ¯å¹¶äº²è‡ªå°è¯•:

1.  [Hugging Face è®ºæ–‡è®¨è®ºè®ºå›](https://huggingface.co/papers/2404.16710)
2.  [LayerSkip æ¨¡åž‹é›†åˆ](https://huggingface.co/collections/facebook/layerskip-666b25c50c8ae90e1965727a)
3.  [å±•ç¤ºè‡ªæŽ¨æµ‹è§£ç æ·±å…¥å·¥ä½œåŽŸç†çš„ Colab ç¬”è®°æœ¬](https://huggingface.co/datasets/ariG23498/layer-skip-assets/blob/main/early_exit_self_speculative_decoding.ipynb)

æŽ¨æµ‹è§£ç ä¸Žè‡ªæŽ¨æµ‹è§£ç 
----------

![LayerSkip æ¼”ç¤º GIF](https://img-s2.andfun.cn/devrel/posts/2025/03/f9219dd8e44eb.gif)

_åœ¨ [`facebook/layerskip-llama2-7B`](https://huggingface.co/facebook/layerskip-llama2-7B) ä¸Šçš„ LayerSkip æŽ¨ç†æ¼”ç¤º (ä½¿ç”¨ LayerSkip æ–¹æ³•æŒç»­é¢„è®­ç»ƒçš„ Llama2 7B)ã€‚_

[ä¼ ç»Ÿçš„æŽ¨æµ‹è§£ç ](https://huggingface.co/blog/assisted-generation) ä½¿ç”¨ **ä¸¤ä¸ª** æ¨¡åž‹: ä¸€ä¸ªè¾ƒå°çš„æ¨¡åž‹ (è‰ç¨¿æ¨¡åž‹) ç”¨äºŽç”Ÿæˆä¸€ç³»åˆ—å€™é€‰è¯å…ƒï¼Œä¸€ä¸ªè¾ƒå¤§çš„æ¨¡åž‹ (éªŒè¯æ¨¡åž‹) ç”¨äºŽéªŒè¯è‰ç¨¿çš„å‡†ç¡®æ€§ã€‚è¾ƒå°çš„æ¨¡åž‹æ‰§è¡Œå¤§éƒ¨åˆ†ç”Ÿæˆå·¥ä½œï¼Œè€Œè¾ƒå¤§çš„æ¨¡åž‹åˆ™è´Ÿè´£æ”¹è¿›ç»“æžœã€‚è¿™æé«˜äº†æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ï¼Œå› ä¸ºè¾ƒå¤§çš„æ¨¡åž‹ä¸€æ¬¡æ€§éªŒè¯å®Œæ•´åºåˆ—ï¼Œè€Œä¸æ˜¯é€ä¸ªç”Ÿæˆè¯å…ƒã€‚

åœ¨è‡ªæŽ¨æµ‹è§£ç ä¸­ï¼Œä½œè€…åœ¨æ­¤æ¦‚å¿µçš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨å¤§æ¨¡åž‹çš„æ—©æœŸå±‚æ¥ç”Ÿæˆè‰ç¨¿è¯å…ƒï¼Œç„¶åŽç”±æ¨¡åž‹çš„æ›´æ·±å±‚è¿›è¡ŒéªŒè¯ã€‚è¿™ç§æŽ¨æµ‹è§£ç çš„â€œè‡ªæ´½â€ç‰¹æ€§éœ€è¦ç‰¹å®šçš„è®­ç»ƒï¼Œä½¿æ¨¡åž‹èƒ½å¤ŸåŒæ—¶æ‰§è¡Œè‰ç¨¿ç”Ÿæˆå’ŒéªŒè¯ã€‚è¿™åè¿‡æ¥åˆæ¯”ä¼ ç»Ÿçš„æŽ¨æµ‹è§£ç æé«˜äº†é€Ÿåº¦å¹¶é™ä½Žäº†è®¡ç®—æˆæœ¬ã€‚

åœ¨ `transformers` ä¸­çš„ä½¿ç”¨
---------------------

ä¸ºäº†åœ¨ ðŸ¤— transformers åº“ä¸­å¯ç”¨æå‰é€€å‡ºè‡ªæŽ¨æµ‹è§£ç ï¼Œæˆ‘ä»¬åªéœ€åœ¨ `generate()` å‡½æ•°ä¸­æ·»åŠ  `assistant_early_exit` å‚æ•°ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä»£ç ç‰‡æ®µï¼Œå±•ç¤ºäº†è¯¥åŠŸèƒ½:

    pip install transformers
    
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    early_exit_layer = 4
    prompt = "Alice and Bob"
    checkpoint = "facebook/layerskip-llama2-7B"
    
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    model = AutoModelForCausalLM.from_pretrained(checkpoint).to("cuda")
    outputs = model.generate(**inputs, assistant_early_exit=early_exit_layer)
    

æ³¨æ„: è™½ç„¶ `assistant_early_exit` å‚æ•°å¯ä»¥ä¸ºä»»ä½•ä»…è§£ç å™¨çš„ transformer å¯ç”¨æå‰é€€å‡ºè‡ªæŽ¨æµ‹è§£ç ï¼Œä½†é™¤éžæ¨¡åž‹ç»è¿‡ä¸“é—¨è®­ç»ƒï¼Œå¦åˆ™æ— æ³•ååµŒå…¥ (é€šè¿‡ LM å¤´è¿›è¡Œè§£ç çš„è¿‡ç¨‹ï¼Œåœ¨åšå®¢æ–‡ç« åŽé¢æœ‰æè¿°) ä¸­é—´å±‚çš„ logitsã€‚åªæœ‰å¯¹æ£€æŸ¥ç‚¹è¿›è¡Œè¿™æ ·çš„è®­ç»ƒï¼Œä»¥æé«˜æ—©æœŸå±‚çš„å‡†ç¡®æ€§ï¼Œæ‚¨æ‰èƒ½èŽ·å¾—åŠ é€Ÿã€‚LayerSkip è®ºæ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒæ–¹æ³•æ¥å®žçŽ°è¿™ä¸€ç‚¹ (å³åº”ç”¨æå‰é€€å‡ºæŸå¤±ï¼Œå¹¶é€æ­¥å¢žåŠ å±‚ä¸¢å¼ƒçŽ‡)ã€‚[è¿™é‡Œ](https://huggingface.co/collections/facebook/layerskip-666b25c50c8ae90e1965727a) æä¾›äº†ä½¿ç”¨ LayerSkip è®­ç»ƒæ–¹æ³•æŒç»­é¢„è®­ç»ƒçš„ Llama2ã€Llama3 å’Œ Code Llama æ£€æŸ¥ç‚¹çš„é›†åˆã€‚

åŸºå‡†æµ‹è¯•
----

æˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥è¡¡é‡ LayerSkip çš„è‡ªæŽ¨æµ‹è§£ç ç›¸å¯¹äºŽè‡ªå›žå½’è§£ç åœ¨å„ç§æ¨¡åž‹ä¸Šçš„åŠ é€Ÿæƒ…å†µã€‚æˆ‘ä»¬è¿˜å°†è‡ªæŽ¨æµ‹è§£ç  (åŸºäºŽæå‰é€€å‡º) ä¸Žæ ‡å‡†æŽ¨æµ‹è§£ç æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚è¦å¤çŽ°è¿™äº›ç»“æžœï¼Œæ‚¨å¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/aritra24rg/LayerSkip-Benchmarking) æ‰¾åˆ°ä»£ç ï¼Œå¹¶åœ¨ [æ­¤ç”µå­è¡¨æ ¼](https://docs.google.com/spreadsheets/d/15poLaR_7tG_5xZo-LzLMFd4dzz-dHl_h/edit#gid=1155443081) ä¸­æ‰¾åˆ°è¿è¡Œæ¯ä¸ªå®žéªŒçš„å‘½ä»¤ã€‚æ‰€æœ‰å®žéªŒå‡åœ¨å•ä¸ª 80GB A100 GPU ä¸Šè¿è¡Œï¼Œé™¤äº† Llama2 70B å®žéªŒåœ¨ 8 ä¸ª A100 GPU çš„èŠ‚ç‚¹ä¸Šè¿è¡Œã€‚

#### Llama3.2 1B

Model Variant (æ¨¡åž‹å˜ä½“)

Layers (å±‚æ•°)

Assistant Model (è¾…åŠ©æ¨¡åž‹)

Assistant Layers (è¾…åŠ©å±‚æ•°)

Task (ä»»åŠ¡)

Total Layers (æ€»å±‚æ•°)

FLOPs/Input (G) (è¾“å…¥ FLOPs)

Time/Input (s) (è¾“å…¥æ—¶é—´)

FLOPs/Output (G) (è¾“å‡º FLOPs)

Time/Output (s) (è¾“å‡ºæ—¶é—´)

Efficiency (æ•ˆçŽ‡)

facebook/layerskip-llama3.2-1B

1

Early Exit @ Layer 4

summarization

1

1195.28

9.96

2147.7

17.9

1.80

#### Llama3 8B

Model Variant (æ¨¡åž‹å˜ä½“)

Layers (å±‚æ•°)

Assistant Model (è¾…åŠ©æ¨¡åž‹)

Assistant Layers (è¾…åŠ©å±‚æ•°)

Task (ä»»åŠ¡)

Total Layers (æ€»å±‚æ•°)

FLOPs/Input (G) (è¾“å…¥ FLOPs)

Time/Input (s) (è¾“å…¥æ—¶é—´)

FLOPs/Output (G) (è¾“å‡º FLOPs)

Time/Output (s) (è¾“å‡ºæ—¶é—´)

Efficiency (æ•ˆçŽ‡)

meta-llama/Meta-Llama-3-8B

8

meta-llama/Llama-3.2-1B

1

summarization

9

1872.46

19.04

2859.35

29.08

1.53

meta-llama/Meta-Llama-3-8B

8

meta-llama/Llama-3.2-3B

3

summarization

11

2814.82

28.63

2825.36

28.73

1.00

facebook/layerskip-llama3-8B

8

Early Exit @ Layer 4

summarization

8

1949.02

15.75

3571.81

28.87

1.83

#### Llama2 70B

Model Variant (æ¨¡åž‹å˜ä½“)

Layers (å±‚æ•°)

Assistant Model (è¾…åŠ©æ¨¡åž‹)

Assistant Layers (è¾…åŠ©å±‚æ•°)

Task (ä»»åŠ¡)

Total Layers (æ€»å±‚æ•°)

FLOPs/Input (G) (è¾“å…¥ FLOPs)

Time/Input (s) (è¾“å…¥æ—¶é—´)

FLOPs/Output (G) (è¾“å‡º FLOPs)

Time/Output (s) (è¾“å‡ºæ—¶é—´)

Efficiency (æ•ˆçŽ‡)

meta-llama/Llama-2-70b-hf

70

meta-llama/Llama-2-13b-hf

13

summarization

83

5036.54

46.3

12289.01

112.97

2.44

meta-llama/Llama-2-70b-hf

70

meta-llama/Llama-2-7b-hf

7

summarization

77

4357.55

40.06

12324.19

113.3

2.83

meta-llama/Llama-2-70b-hf

70

TinyLlama/TinyLlama\_v1.1

1

summarization

71

4356.21

40.05

12363.22

113.66

2.84

**facebook/layerskip-llama2-70B**

70

Early Exit @ Layer 10

summarization

70

6012.04

54.96

1283.34

113.2

2.06

#### Llama2 13B

Model Variant (æ¨¡åž‹å˜ä½“)

Layers (å±‚æ•°)

Assistant Model (è¾…åŠ©æ¨¡åž‹)

Assistant Layers (è¾…åŠ©å±‚æ•°)

Task (ä»»åŠ¡)

Total Layers (æ€»å±‚æ•°)

FLOPs/Input (G) (è¾“å…¥ FLOPs)

Time/Input (s) (è¾“å…¥æ—¶é—´)

FLOPs/Output (G) (è¾“å‡º FLOPs)

Time/Output (s) (è¾“å‡ºæ—¶é—´)

Efficiency (æ•ˆçŽ‡)

meta-llama/Llama-2-13b-hf

13

meta-llama/Llama-2-7b-hf

7

summarization

20

3557.07

27.79

4088.48

31.94

1.15

meta-llama/Llama-2-13b-hf

13

TinyLlama/TinyLlama\_v1.1

1

summarization

14

2901.92

22.67

4190.42

32.74

1.44

meta-llama/Llama-2-13b-hf

13

apple/OpenELM-270M

0.27

summarization

13.27

2883.33

22.53

4521.12

35.32

1.57

meta-llama/Llama-2-13b-hf

13

apple/OpenELM-450M

0.45

summarization

13.45

3267.69

25.53

4321.75

33.76

1.32

**facebook/layerskip-llama2-13B**

**13**

**Early Exit @ Layer 4**

**summarization**

**13**

**4238.45**

**33.11**

**4217.78**

**32.95**

**0.995**

**facebook/layerskip-llama2-13B**

13

Early Exit @ Layer 8

summarization

13

2459.61

19.22

4294.98

33.55

1.746

#### Llama2 7B

Model Variant (æ¨¡åž‹å˜ä½“)

Layers (å±‚æ•°)

Assistant Model (è¾…åŠ©æ¨¡åž‹)

Assistant Layers (è¾…åŠ©å±‚æ•°)

Task (ä»»åŠ¡)

Total Layers (æ€»å±‚æ•°)

FLOPs/Input (G) (è¾“å…¥ FLOPs)

Time/Input (s) (è¾“å…¥æ—¶é—´)

FLOPs/Output (G) (è¾“å‡º FLOPs)

Time/Output (s) (è¾“å‡ºæ—¶é—´)

Efficiency (æ•ˆçŽ‡)

meta-llama/Llama-2-7b-hf

7

TinyLlama/TinyLlama\_v1.1

1

summarization

8

2771.54

21.65

3368.48

26.32

1.22

meta-llama/Llama-2-7b-hf

7

apple/OpenELM-270M

0.27

summarization

7.27

2607.82

20.37

4221.14

32.98

1.62

meta-llama/Llama-2-7b-hf

7

apple/OpenELM-450M

0.45

summarization

7.45

3324.68

25.97

4178.66

32.65

1.26

**facebook/layerskip-llama2-7B**

7

Early Exit @ Layer 4

summarization

7

2548.4

19.91

3306.73

25.83

1.297

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ä»¥ä¸‹å‡ ç‚¹:

*   ä»Žâ€œ **æ€»å‚æ•°æ•°é‡**â€åˆ—å¯ä»¥çœ‹å‡ºï¼Œè‡ªæŽ¨æµ‹è§£ç æ¶ˆè€—çš„å†…å­˜æ›´å°‘ï¼Œå› ä¸ºå®ƒä¸éœ€è¦å•ç‹¬çš„è‰ç¨¿æ¨¡åž‹ï¼Œå¹¶ä¸”è‰ç¨¿é˜¶æ®µå±‚çš„æƒé‡è¢«é‡ç”¨ã€‚
*   å¯¹äºŽé™¤ Llama2 70B ä¹‹å¤–çš„æ‰€æœ‰æ¨¡åž‹å¤§å°å’Œç”Ÿæˆï¼Œæå‰é€€å‡ºè‡ªæŽ¨æµ‹è§£ç æ¯”å¸¸è§„çš„ä¸¤æ¨¡åž‹æŽ¨æµ‹è§£ç æ›´å¿«ã€‚
*   ä¸Žå…¶å®ƒæ¨¡åž‹ç›¸æ¯”ï¼ŒLlama2 70B çš„è‡ªæŽ¨æµ‹è§£ç é€Ÿåº¦æå‡ç›¸å¯¹æœ‰é™ï¼Œå¯èƒ½æœ‰ä¸åŒçš„åŽŸå› ï¼Œä¾‹å¦‚ï¼ŒLlama2 70B çš„ LayerSkip æ£€æŸ¥ç‚¹æŒç»­é¢„è®­ç»ƒçš„ token è¾ƒå°‘ (Llama2 70B ä¸º 328M tokenï¼Œè€Œ Llama2 7B ä¸º 52B token)ã€‚ä½†è¿™æ˜¯æœªæ¥ç ”ç©¶éœ€è¦æ”¹è¿›çš„ä¸€ä¸ªæ–¹é¢ã€‚å°½ç®¡å¦‚æ­¤ï¼Œ70B çš„è‡ªæŽ¨æµ‹è§£ç æ˜Žæ˜¾å¿«äºŽè‡ªå›žå½’è§£ç ã€‚

**è‡ªç”Ÿæˆå’Œè‡ªéªŒè¯**
-----------

è‡ªæŽ¨æµ‹è§£ç è¿‡ç¨‹ä»Žè‡ªç”Ÿæˆå¼€å§‹ï¼Œå…¶ä¸­è¯å…ƒæ˜¯é€šè¿‡ä»ŽæŸä¸ªä¸­é—´å±‚æå‰é€€å‡ºæ¥ç”Ÿæˆçš„ã€‚æŽ¨æµ‹è¯å…ƒçš„æ•°é‡å®šä¹‰äº†åœ¨æ­¤é˜¶æ®µç”Ÿæˆå¤šå°‘è‰ç¨¿è¯å…ƒï¼Œè€Œæˆ‘ä»¬é€€å‡ºçš„å±‚å®šä¹‰äº†è‰ç¨¿é˜¶æ®µçš„è§„æ¨¡å’Œå‡†ç¡®æ€§ã€‚è¿™ä¸¤ä¸ªå‚æ•°éƒ½å¯ä»¥åœ¨æŽ¨ç†æ—¶æ ¹æ®è‰ç¨¿é˜¶æ®µçš„é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡æ¥æŒ‡å®šã€‚

ä¸‹ä¸€æ­¥æ˜¯è‡ªéªŒè¯ï¼Œå…¶ä¸­ä½¿ç”¨å®Œæ•´æ¨¡åž‹æ¥éªŒè¯è‰ç¨¿è¯å…ƒã€‚éªŒè¯æ¨¡åž‹é‡ç”¨è‰ç¨¿æ¨¡åž‹ä¸­çš„ç¼“å­˜éƒ¨åˆ†ã€‚å¦‚æžœè‰ç¨¿è¯å…ƒä¸ŽéªŒè¯çš„è¯å…ƒä¸€è‡´ï¼Œåˆ™å°†å®ƒä»¬æ·»åŠ åˆ°æœ€ç»ˆè¾“å‡ºä¸­ï¼Œä»Žè€Œæ›´å¥½åœ°åˆ©ç”¨æˆ‘ä»¬ç³»ç»Ÿä¸­çš„å†…å­˜å¸¦å®½ï¼Œå› ä¸ºä½¿ç”¨å®Œæ•´æ¨¡åž‹ç”Ÿæˆä¸€ç³»åˆ—è¯å…ƒæ¯”éªŒè¯è‰ç¨¿è¦æ˜‚è´µå¾—å¤šï¼Œåªè¦æœ‰å‡ ä¸ªè¯å…ƒåŒ¹é…å³å¯ã€‚

åœ¨è‡ªéªŒè¯é˜¶æ®µï¼Œåªæœ‰å‰©ä½™çš„å±‚æ‰ä¼šè¢«è®¡ç®—ä»¥è¿›è¡ŒéªŒè¯ï¼Œå› ä¸ºæ—©æœŸå±‚çš„ç»“æžœåœ¨è‰ç¨¿é˜¶æ®µå·²è¢«ç¼“å­˜ã€‚

**æå‰é€€å‡ºå’ŒååµŒå…¥**
------------

è‡ªæŽ¨æµ‹è§£ç ä¸­çš„ä¸€é¡¹å…³é”®æŠ€æœ¯æ˜¯æå‰é€€å‡ºï¼Œå³ç”Ÿæˆè¿‡ç¨‹å¯ä»¥åœ¨é¢„å…ˆæŒ‡å®šçš„å±‚åœæ­¢ã€‚ä¸ºäº†å®žçŽ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é€šè¿‡å°†è¿™äº›å±‚çš„ logits æŠ•å½±åˆ°è¯­è¨€æ¨¡åž‹ (LM) å¤´ä¸Šæ¥ååµŒå…¥å®ƒä»¬ï¼Œä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒã€‚è¿™å…è®¸æ¨¡åž‹è·³è¿‡åŽç»­å±‚å¹¶æé«˜æŽ¨ç†æ—¶é—´ã€‚

å¯ä»¥åœ¨ä»»ä½• transformer å±‚æ‰§è¡ŒååµŒå…¥ï¼Œå°†æå‰é€€å‡ºè½¬å˜ä¸ºä¸€ç§é«˜æ•ˆçš„è¯å…ƒé¢„æµ‹æœºåˆ¶ã€‚ä¸€ä¸ªè‡ªç„¶è€Œç„¶çš„é—®é¢˜å‡ºçŽ°äº†: å½“ LM å¤´æœ€åˆè¢«è®­ç»ƒä¸ºä»…ä¸Žæœ€ç»ˆå±‚ä¸€èµ·å·¥ä½œæ—¶ï¼Œå¦‚ä½•ä½¿å…¶é€‚åº”ååµŒå…¥è¾ƒæ—©å±‚çš„ logitsï¼Ÿè¿™å°±æ˜¯è®­ç»ƒä¿®æ”¹å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

**è®­ç»ƒä¿®æ”¹**
--------

åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†å±‚ä¸¢å¼ƒï¼Œå®ƒå…è®¸æ¨¡åž‹åœ¨è®­ç»ƒæœŸé—´è·³è¿‡æŸäº›å±‚ã€‚ä¸¢å¼ƒçŽ‡åœ¨è¾ƒæ·±çš„å±‚ä¸­é€æ¸å¢žåŠ ï¼Œä½¿æ¨¡åž‹ä¸å¤ªä¾èµ–å…¶åŽé¢çš„å±‚ï¼Œå¹¶å¢žå¼ºæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

é™¤äº†å±‚ä¸¢å¼ƒä¹‹å¤–ï¼Œè¿˜åº”ç”¨äº†æå‰é€€å‡ºæŸå¤±ï¼Œä»¥ç¡®ä¿ LM å¤´å­¦ä¹ ååµŒå…¥ä¸åŒçš„å±‚ã€‚ä½¿ç”¨æ¯ä¸ªå‡ºå£ (ä¸­é—´å±‚) çš„å½’ä¸€åŒ–æŸå¤±çš„æ€»å’Œæ¥ç»™å‡ºä½¿ç”¨æå‰å‡ºå£è®­ç»ƒæ¨¡åž‹çš„æ€»æŸå¤±å‡½æ•°ã€‚è¿™ç§æŠ€æœ¯é€šè¿‡åœ¨æ‰€æœ‰å±‚ä¹‹é—´åˆ†é…å­¦ä¹ ä»»åŠ¡æ¥å®žçŽ°é«˜æ•ˆè®­ç»ƒã€‚

ä¼˜åŒ–: å…±äº«æƒé‡ã€å…±äº« KV ç¼“å­˜å’Œå…±äº«è®¡ç®—
----------------------

è‡ªæŽ¨æµ‹è§£ç æ˜¾è‘—å—ç›ŠäºŽç¼“å­˜é‡ç”¨ï¼Œç‰¹åˆ«æ˜¯ KV ç¼“å­˜ï¼Œå®ƒå­˜å‚¨åœ¨è‰ç¨¿é˜¶æ®µè®¡ç®—çš„é”®å€¼å¯¹ã€‚æ­¤ç¼“å­˜å…è®¸æ¨¡åž‹è·³è¿‡å†—ä½™è®¡ç®—ï¼Œå› ä¸ºè‰ç¨¿å’ŒéªŒè¯é˜¶æ®µéƒ½ä½¿ç”¨ç›¸åŒçš„æ—©æœŸå±‚ã€‚æ­¤å¤–ï¼Œé€€å‡ºæŸ¥è¯¢ç¼“å­˜å­˜å‚¨æ¥è‡ªé€€å‡ºå±‚çš„æŸ¥è¯¢å‘é‡ï¼Œå…è®¸éªŒè¯ä»Žè‰ç¨¿é˜¶æ®µæ— ç¼ç»§ç»­ã€‚

ä¸Žä¼ ç»Ÿçš„åŒæ¨¡åž‹æŽ¨æµ‹è§£ç ç›¸æ¯”ï¼Œæå‰é€€å‡ºè‡ªæŽ¨æµ‹è§£ç å¯ä»¥ä»Žä»¥ä¸‹èŠ‚çœä¸­å—ç›Š:

*   **å…±äº«æƒé‡**: ä¸ºè‰ç¨¿å’ŒéªŒè¯é‡ç”¨å‰ E å±‚ çš„æƒé‡ã€‚
*   **å…±äº« KV ç¼“å­˜**: ä¸ºè‰ç¨¿å’ŒéªŒè¯é‡ç”¨å‰ E å±‚çš„é”®å€¼å¯¹
*   **å…±äº«è®¡ç®—**: é€šè¿‡ä½¿ç”¨ä»…ä¿å­˜é€€å‡ºå±‚ E-1 çš„æŸ¥è¯¢å‘é‡çš„é€€å‡ºæŸ¥è¯¢ç¼“å­˜æ¥é‡ç”¨å‰ E å±‚çš„è®¡ç®—ï¼Œä»¥ä¾¿éªŒè¯è¿‡ç¨‹æ— éœ€è®¡ç®—å±‚ 0 åˆ° E-1ã€‚

KV å’Œé€€å‡ºæŸ¥è¯¢ç¼“å­˜çš„ç»„åˆç§°ä¸º KVQ ç¼“å­˜ï¼Œå¯å‡å°‘å†…å­˜å¼€é”€å¹¶æé«˜æŽ¨ç†å»¶è¿Ÿã€‚

åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒðŸ¤— transformers åº“å·²åœ¨æ­¤ [pull request](https://github.com/huggingface/transformers/pull/30890) ä¸­å®žçŽ°äº†ç¬¬ä¸€ä¸ªä¼˜åŒ– (å…±äº«æƒé‡)ã€‚éšç€ä½¿ç”¨æ­¤æ–¹æ³•çš„æ¨¡åž‹æ•°é‡å¢žåŠ ï¼Œæˆ‘ä»¬å°†è€ƒè™‘å…¶ä»–ä¼˜åŒ–ã€‚å¦‚æžœæ‚¨æœ‰å…´è¶£ï¼Œè¯·éšæ—¶æå‡º PRï¼

æå‰é€€å‡ºå±‚çš„é€‰æ‹©ç­–ç•¥
----------

è‰ç¨¿é˜¶æ®µçš„æå‰é€€å‡ºå±‚æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æŽ¨ç†æœŸé—´è°ƒæ•´æˆ–ä¿®æ”¹:

*   æˆ‘ä»¬è¶Šæ—©é€€å‡ºï¼Œç”Ÿæˆè‰ç¨¿è¯å…ƒçš„é€Ÿåº¦å°±è¶Šå¿«ï¼Œä½†å®ƒä»¬çš„å‡†ç¡®æ€§å°±è¶Šä½Žã€‚
*   æˆ‘ä»¬è¶Šæ™šé€€å‡ºï¼Œç”Ÿæˆçš„è‰ç¨¿è¯å…ƒå°±è¶Šå‡†ç¡®ï¼Œä½†å®ƒä»¬çš„é€Ÿåº¦å°±è¶Šæ…¢ã€‚

æˆ‘ä»¬ç¼–å†™äº†ä¸€ä¸ªè„šæœ¬æ¥éåŽ†ä¸åŒçš„æå‰é€€å‡ºå±‚å¹¶æµ‹é‡ A100 GPU ä¸Šçš„æ¯ç§’è¯å…ƒæ•°ã€‚åœ¨ä¸‹é¢çš„è¡¨æ ¼ä¸­ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†é’ˆå¯¹ä¸åŒ Llama æ¨¡åž‹çš„ LayerSkip å’ŒåŸºçº¿æ£€æŸ¥ç‚¹çš„æ¯ç§’è¯å…ƒæ•°ä¸Žæå‰é€€å‡ºå±‚çš„å…³ç³»å›¾ (æ‚¨å¯ä»¥åœ¨ [æ­¤å¤„](https://docs.google.com/spreadsheets/d/15poLaR_7tG_5xZo-LzLMFd4dzz-dHl_h/edit#gid=1155443081) æŸ¥çœ‹å®Œæ•´æ—¥å¿—)ã€‚

#### Llama3.2 1B

Normal (å¸¸è§„æ¨¡åž‹)

LayerSkip (LayerSkip æ¨¡åž‹)

![llama 3.2 1b](https://img-s2.andfun.cn/devrel/posts/2025/03/d240200a5a4ae.png)

![layer skip llama 3.2 1b](https://img-s2.andfun.cn/devrel/posts/2025/03/7986ed1d443bf.png)

#### Llama3 8B

Normal (å¸¸è§„æ¨¡åž‹)

LayerSkip (LayerSkip æ¨¡åž‹)

![llama 3 8b](https://img-s2.andfun.cn/devrel/posts/2025/03/57f23cc6903cd.png)

![layer skip llama 3 8b](https://img-s2.andfun.cn/devrel/posts/2025/03/4ce0a860909aa.png)

#### Code Llama3 34B

Normal (å¸¸è§„æ¨¡åž‹)

LayerSkip (LayerSkip æ¨¡åž‹)

![code llama 3 34b](https://img-s2.andfun.cn/devrel/posts/2025/03/37e3deef9439f.png)

![code layer skip llama 3 34b](https://img-s2.andfun.cn/devrel/posts/2025/03/cbd40b5a62436.png)

#### Code Llama3 7B

Normal (å¸¸è§„æ¨¡åž‹)

LayerSkip (LayerSkip æ¨¡åž‹)

![code llama 3 7b](https://img-s2.andfun.cn/devrel/posts/2025/03/686542c0ca421.png)

![code layer skip llama 3 7b](https://img-s2.andfun.cn/devrel/posts/2025/03/1633ddb1a1603.png)

#### Llama2 70B

Normal (å¸¸è§„æ¨¡åž‹)

LayerSkip (LayerSkip æ¨¡åž‹)

![llama 2 70b](https://img-s2.andfun.cn/devrel/posts/2025/03/66e5c6631175d.png)

![layer skip llama 2 70b](https://img-s2.andfun.cn/devrel/posts/2025/03/bde6acabc54ba.png)

#### Llama2 13B

Normal (å¸¸è§„æ¨¡åž‹)

LayerSkip (LayerSkip æ¨¡åž‹)

![llama 2 13b](https://img-s2.andfun.cn/devrel/posts/2025/03/57400c5b2afc6.png)

![layer skip llama 2 13b](https://img-s2.andfun.cn/devrel/posts/2025/03/892351300bada.png)

#### Llama2 7B

Normal (å¸¸è§„æ¨¡åž‹)

LayerSkip (LayerSkip æ¨¡åž‹)

![llama 2 7b](https://img-s2.andfun.cn/devrel/posts/2025/03/ee27edcc592a5.png)

![layer skip llama 2 7b](https://img-s2.andfun.cn/devrel/posts/2025/03/9de975ee945c4.png)

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ä»¥ä¸‹å‡ ç‚¹:

*   å¯¹äºŽæ²¡æœ‰ä½¿ç”¨ LayerSkip è®­ç»ƒæ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒæˆ–æŒç»­é¢„è®­ç»ƒçš„åŸºçº¿æ£€æŸ¥ç‚¹ï¼Œæå‰é€€å‡ºè‡ªæŽ¨æµ‹è§£ç æ¯”è‡ªå›žå½’è§£ç æ›´æ…¢ã€‚è¿™æ˜¯å› ä¸ºåœ¨å¤§å¤šæ•° LLM çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ—©æœŸå±‚å¹¶æ²¡æœ‰è¢«æ¿€åŠ±åŽ»å­¦ä¹ é¢„æµ‹è¾“å‡ºï¼Œå› æ­¤ä½¿ç”¨æ—©æœŸå±‚ç”Ÿæˆè¯å…ƒçš„æŽ¥å—çŽ‡ä¼šéžå¸¸ä½Žã€‚
*   å¦ä¸€æ–¹é¢ï¼Œå¯¹äºŽä½¿ç”¨ LayerSkip è®­ç»ƒæ–¹æ³•æŒç»­é¢„è®­ç»ƒçš„ Llama æ£€æŸ¥ç‚¹ï¼Œæå‰é€€å‡ºè‡ªæŽ¨æµ‹è§£ç åœ¨è‡³å°‘ä¸€éƒ¨åˆ†å±‚ä¸­æ¯”è‡ªå›žå½’è§£ç å…·æœ‰æ›´é«˜çš„åŠ é€Ÿæ¯”ã€‚
    *   å¯¹äºŽå¤§å¤šæ•°æ¨¡åž‹ (é™¤äº† Llama3.2 1B)ï¼Œå½“æˆ‘ä»¬éåŽ†å„å±‚æ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ä¸€ä¸ªè§„å¾‹æ¨¡å¼: åŠ é€Ÿæ¯”åœ¨å‰å‡ å±‚è¾ƒä½Žï¼Œé€æ¸å¢žåŠ åˆ°ä¸€ä¸ªæœ€ä½³ç‚¹ï¼Œç„¶åŽå†æ¬¡ä¸‹é™ã€‚
    *   æå‰é€€å‡ºå±‚çš„æœ€ä½³ç‚¹æ˜¯åœ¨é¢„æµ‹çš„é«˜å‡†ç¡®æ€§å’Œç”Ÿæˆè¯å…ƒçš„ä½Žå¼€é”€ä¹‹é—´è¾¾åˆ°æœ€ä½³æƒè¡¡æ—¶ã€‚è¿™ä¸ªæœ€ä½³ç‚¹å–å†³äºŽæ¯ä¸ªæ¨¡åž‹ï¼Œä¹Ÿå¯èƒ½å–å†³äºŽæç¤ºæˆ–æç¤ºçš„é¢†åŸŸã€‚

è¿™äº›è§‚å¯Ÿä¸ºè¿›ä¸€æ­¥çš„å®žéªŒå’ŒæŽ¢ç´¢æä¾›äº†æœ‰è¶£çš„æœºä¼šã€‚æˆ‘ä»¬é¼“åŠ±è¯»è€…åœ¨è¿™äº›æƒ³æ³•çš„åŸºç¡€ä¸Šè¿›è¡Œæž„å»ºï¼Œæµ‹è¯•å˜ä½“ï¼Œå¹¶è¿›è¡Œè‡ªå·±çš„ç ”ç©¶ã€‚è¿™äº›åŠªåŠ›å¯ä»¥å¸¦æ¥æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸåšå‡ºæœ‰æ„ä¹‰çš„è´¡çŒ®ã€‚

ç»“è®º
--

LayerSkip åˆ©ç”¨æå‰é€€å‡ºã€å±‚ä¸¢å¼ƒå’Œç¼“å­˜é‡ç”¨ä¹‹é—´çš„ååŒä½œç”¨ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¿«é€Ÿé«˜æ•ˆçš„æ–‡æœ¬ç”Ÿæˆæµç¨‹ã€‚é€šè¿‡è®­ç»ƒæ¨¡åž‹ä»Žä¸åŒå±‚ååµŒå…¥è¾“å‡ºï¼Œå¹¶ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–éªŒè¯è¿‡ç¨‹ï¼Œè¿™ç§æ–¹æ³•åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚å› æ­¤ï¼Œå®ƒæ˜¾è‘—æ”¹å–„äº†å¤§è¯­è¨€æ¨¡åž‹çš„æŽ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è¾“å‡ºã€‚ç”±äºŽä½¿ç”¨å•ä¸ªæ¨¡åž‹ä½œä¸ºè‰ç¨¿å’ŒéªŒè¯æ¨¡åž‹ï¼Œå®ƒè¿˜æ¯”ä¼ ç»Ÿçš„æŽ¨æµ‹è§£ç æŠ€æœ¯å‡å°‘äº†å†…å­˜ä½¿ç”¨ã€‚

è‡ªæŽ¨æµ‹æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹çš„é¢†åŸŸï¼ŒåŒä¸€ä¸ª LLM å¯ä»¥åˆ›å»ºè‰ç¨¿è¯å…ƒå¹¶è‡ªæˆ‘ä¿®æ­£ã€‚å…¶ä»–è‡ªæŽ¨æµ‹æ–¹æ³•åŒ…æ‹¬:

*   [Draft & Verify](https://aclanthology.org/2024.acl-long.607/): å…¶ä¸­è‰ç¨¿é˜¶æ®µæ¶‰åŠè·³è¿‡é¢„å®šçš„æ³¨æ„åŠ›å’Œå‰é¦ˆå±‚ã€‚
*   [MagicDec](https://arxiv.org/abs/2408.11049): å…¶ä¸­è‰ç¨¿é˜¶æ®µä½¿ç”¨ KV ç¼“å­˜çš„å­é›†ï¼Œè¿™å¯¹é•¿ä¸Šä¸‹æ–‡è¾“å…¥å¾ˆæœ‰ç”¨ã€‚
*   [Jacobi Decoding](https://arxiv.org/abs/2305.10427) å’Œ [Lookahead Decoding](https://arxiv.org/abs/2402.02057): å…¶ä¸­è‰ç¨¿é˜¶æ®µæ˜¯ä¸€ç³»åˆ—â€œçŒœæµ‹è¯å…ƒâ€ï¼Œå¯ä»¥æ˜¯éšæœºçš„æˆ–ä»Ž n-gram æŸ¥æ‰¾è¡¨ä¸­èŽ·å¾—çš„ã€‚

* * *

> è‹±æ–‡åŽŸæ–‡: [https://huggingface.co/blog/layerskip](https://huggingface.co/blog/layerskip)
> 
> åŽŸæ–‡ä½œè€…: Aritra Roy Gosthipaty, Mostafa Elhoushi, Pedro Cuenca, Vaibhav Srivastav
> 
> è¯‘è€…: smartisan