---
layout: post
title: '附043.KubeEdge边缘云部署实施方案'
date: "2025-04-10T00:38:53Z"
---
附043.KubeEdge边缘云部署实施方案
======================

目录

*   [KubeEdge介绍](#kubeedge介绍)
    *   [KubeEdge概述](#kubeedge概述)
    *   [KubeEdge优势](#kubeedge优势)
    *   [KubeEdge架构](#kubeedge架构)
*   [KubeEdge部署](#kubeedge部署)
    *   [部署依赖](#部署依赖)
    *   [部署规划](#部署规划)
    *   [主机名配置](#主机名配置)
    *   [变量准备](#变量准备)
    *   [互信配置](#互信配置)
    *   [环境预配置](#环境预配置)
    *   [安装keadm](#安装keadm)
    *   [设置标签](#设置标签)
    *   [安装CNI](#安装cni)
    *   [设置云端](#设置云端)
    *   [设置调度](#设置调度)
    *   [设置边缘端](#设置边缘端)
    *   [边缘端优化](#边缘端优化)
    *   [确认验证](#确认验证)
*   [KubeEdge测试验证](#kubeedge测试验证)
    *   [调度测试](#调度测试)

KubeEdge介绍
----------

### KubeEdge概述

KubeEdge 是一个开源系统，将原生的容器化的业务流程和设备管理功能扩展到边缘节点。KubeEdge 是基于 Kubernetes 构建的，并为云，边缘之间的网络通信，应用程序部署以及元数据同步提供核心基础架构支持。同时 KubeEdge 还支持 MQTT，并允许开发人员编写自定义逻辑并在 Edge 上启用一定资源的设备进行通信。

KubeEdge 由云端和边缘端组成。

### KubeEdge优势

KubeEdge 的优势主要包括：

*   边缘计算

借助在 Edge 上运行业务应用，可以在产生数据的边端存储和处理大量数据。这样可以减少边缘和云之间的网络带宽需求和消耗，提高响应速度，降低成本并保护客户的数据隐私。

*   简化开发

开发人员可以编写基于 HTTP 或 MQTT 的常规应用程序，对其进行容器化，然后在 Edge 或 Cloud 中的任何一个更合适的位置运行应用程序。

*   Kubernetes 原生支持

借助 KubeEdge，用户可以像在传统的 Kubernetes 集群一样，在 Edge 节点上编排应用程序，管理设备并监视应用程序和设备状态。

*   丰富的应用

可以轻松地将现有的复杂机器学习，图像识别，事件处理等其他高级应用程序部署到 Edge。

*   云边可靠协作  
    在不稳定的云边网络上，可以保证消息传递的可靠性，不会丢失。
    
*   边缘自治  
    当云边之间的网络不稳定或者边缘端离线或重启时，确保边缘节点可以自主运行，同时确保边缘端的应用正常运行。
    
*   边缘设备管理  
    通过 Kubernetes 的原生API，并由CRD来管理边缘设备。
    
*   极致轻量的边缘代理  
    在资源有限的边缘端上运行的非常轻量级的边缘代理(EdgeCore)。
    

### KubeEdge架构

KubeEdge 由以下组件组成：

云上（CloudCore）部分：  
CloudHub: Web 套接字服务器，负责在云端缓存信息、监视变更，并向 EdgeHub 端发送消息。  
EdgeController: kubernetes 的扩展控制器，用于管理边缘节点和 pod 的元数据，以便可以将数据定位到对应的边缘节点。  
DeviceController: 一个扩展的 kubernetes 控制器，用于管理边缘 IoT 设备，以便设备元数据/状态数据可以在 edge 和 cloud 之间同步。

边缘（EdgeCore）部分：  
EdgeHub: Web 套接字客户端，负责与 Cloud Service 进行交互以进行边缘计算（例如 KubeEdge 体系结构中的 Edge Controller）。这包括将云侧资源更新同步到边缘，并将边缘侧主机和设备状态变更报告给云。  
Edged: 在边缘节点上运行并管理容器化应用程序的代理。  
EventBus: 一个与 MQTT 服务器（mosquitto）进行交互的 MQTT 客户端，为其他组件提供发布和订阅功能。  
ServiceBus: 用于与 HTTP 服务器 （REST） 交互的 HTTP 客户端，为云组件提供 HTTP 客户端功能，以访问在边缘运行的 HTTP 服务器。  
DeviceTwin: 负责存储设备状态并将设备状态同步到云端。它还为应用程序提供查询接口。  
MetaManager: Edged 端和 Edgehub 端之间的消息处理器。它还负责将元数据存储到轻量级数据库（SQLite）或从轻量级数据库（SQLite）检索元数据。

KubeEdge架构图如下：  
![001](https://release-1-20.docs.kubeedge.io/zh/assets/images/kubeedge_arch-a0fa6324bc543a933d766e45d5f00f77.png)

提示：更多介绍参考官方文档：[为什么选择KubeEdge](https://release-1-20.docs.kubeedge.io/zh/docs/)  

KubeEdge部署
----------

### 部署依赖

KubeEdge基于原生Kubernetes基础上构建，将Kubernetes能力从云端延伸到边缘，并使其适配边缘计算的要求，比如网络不稳定性以及资源受限场景等，因此在部署KubeEdge前，请先准备好一个Kubernetes集群。有关 Kubernetes 的部署可参考： [附042.Kubernetes\_v1.32.3生成环境高可用部署](https://blog.csdn.net/tagaochen1276/article/details/146888946)

提示：Kubernetes 和 KubeEdge 版本匹配和兼容可查看： [Kubernetes compatibility](https://github.com/kubeedge/kubeedge?tab=readme-ov-file#kubernetes-compatibility/) 。  

对于容器运行时，可支持如下：

*   docker
*   containerd
*   cri-o
*   virtlet

### 部署规划

本方案基于 Keadm 部署工具实现完整生产环境可用的 KubeEdge 边缘云部署。  
其主要信息如下：

*   版本：KubeEdge v1.20.0 版本；
*   Keadm：采用 Keadm 部署 KubeEdge ；
*   OS：Ubuntu Server 24.04 LTS；
*   containerd：容器运行时。

节点主机名

IP

类型

备注

master01

172.24.8.181

Kubernetes master节点

KubeEdge Cloud节点

master02

172.24.8.182

Kubernetes master节点

KubeEdge Cloud节点

master03

172.24.8.183

Kubernetes master节点

KubeEdge Cloud节点

worker01

172.24.8.184

Kubernetes worker节点

worker02

172.24.8.185

Kubernetes worker节点

worker03

172.24.8.186

Kubernetes worker节点

worker03

172.24.8.186

Kubernetes worker节点

edgenode01

172.24.8.187

KubeEdge worker节点

edgenode02

172.24.8.188

KubeEdge worker节点

提示：Kubernetes 集群部署不在本方案讨论内，但本方案基于快速部署目的，会使用 Kubernetes 的 master01 充当部署节点，从而快速批量完成对edgenode节点的相关部署。  

### 主机名配置

需要对所有节点主机名进行相应配置。

    root@localhost:~# hostnamectl set-hostname edgenode01	    #其他节点依次修改
    

提示：如上需要在所有节点修改对应的主机名。

生产环境通常建议在内网部署dns服务器，使用dns服务器进行解析，本指南采用本地hosts文件名进行解析。  
如下hosts文件修改仅需在master01执行，后续使用批量分发至其他所有节点。

    root@master01:~# cat >> /etc/hosts << EOF
    172.24.8.187 edgenode01
    172.24.8.188 edgenode02
    EOF
    
    root@master01:~# cat /etc/hosts
    #……
    172.24.8.181 master01
    172.24.8.182 master02
    172.24.8.183 master03
    172.24.8.184 worker01
    172.24.8.185 worker02
    172.24.8.186 worker03
    172.24.8.187 edgenode01
    172.24.8.188 edgenode02
    

提示：如上仅需在master01节点上操作。

### 变量准备

为实现自动化部署，自动化分发相关文件，提前定义相关主机名、IP组、变量等。

    root@master01:~# wget http://down.linuxsb.com/mydeploy/kubeedge/v1.20.0/kubeedgenodeenv.sh
    
    root@master01:~# vi kubeedgenodeenv.sh            #确认相关主机名和IP
    #!/bin/bash
    #***************************************************************#
    # ScriptName: kubeedgenodeenv.sh
    # Author: xhy
    # Create Date: 2025-04-01 16:12
    # Modify Author: xhy
    # Modify Date: 2025-04-01 16:15
    # Version: v1
    #***************************************************************#
    
    # Kubernetes 集群 MASTER 机器 IP 数组
    export K8S_MASTER_IPS=(172.24.8.181 172.24.8.182 172.24.8.183)
    
    # Kubernetes 集群 MASTER IP 对应的主机名数组
    export K8S_MASTER_NAMES=(master01 master02 master03)
    
    # Kubernetes 集群 NODE 机器 IP 数组
    export K8S_NODE_IPS=(172.24.8.184 172.24.8.185 172.24.8.186)
    
    # Kubernetes 集群 NODE IP 对应的主机名数组
    export K8S_NODE_NAMES=(worker01 worker02 worker03)
    
    # Kubernetes 集群 所有机器 IP 数组
    export K8S_ALL_IPS=(172.24.8.181 172.24.8.182 172.24.8.183 172.24.8.184 172.24.8.185 172.24.8.186)
    
    # Kubernetes 集群 所有IP 对应的主机名数组
    export K8S_ALL_NAMES=(master01 master02 master03 worker01 worker02 worker03)
    
    # Kubeedge 集群 所有机器 IP 数组
    export EDGE_ALL_IPS=(172.24.8.187 172.24.8.188)
    
    # Kubeedge 集群 所有IP 对应的主机名数组
    export EDGE_ALL_NAMES=(edgenode01 edgenode02)
    

提示：如上仅需在master01节点上操作。

### 互信配置

为了方便远程分发文件和执行命令，本方案配置master01节点到其它节点的 ssh信任关系，即免秘钥管理所有其他节点。

    root@master01:~# source kubeedgenodeenv.sh                                #载入变量
        
    root@master01:~# for edge_ip in ${EDGE_ALL_IPS[@]}
      do
        echo -e "\n\n\033[33m[INFO] >>> ${edge_ip}...\033[0m"
        ssh-copy-id -i ~/.ssh/id_rsa.pub root@${edge_ip}
      done
      
    root@master01:~# for edge_name in ${EDGE_ALL_NAMES[@]}
      do
        echo -e "\n\n\033[33m[INFO] >>> ${edge_name}...\033[0m"
        ssh-copy-id -i ~/.ssh/id_rsa.pub root@${edge_name}
      done
    
    root@master01:~# for all_ip in ${K8S_ALL_IPS[@]}
      do
        echo -e "\n\n\033[33m[INFO] >>> ${all_ip}...\033[0m"
        sleep 2
        scp -rp /etc/hosts root@${all_ip}:/etc/hosts
      done
    

提示：如上仅需在master01节点上操作。

### 环境预配置

需要在每个边缘节点上安装一个容器运行时，以使边缘引擎 EdgeCore 能够成功安装，并且边缘 Pod 可以运行在边缘节点上。  
Keadm 用于部署 KubeEdge ，在正式使用 Keadm 部署 KubeEdge 之前需要对操作系统环境进行准备，即环境预配置。  
环境的预配置本方案使用脚本自动完成。  
使用如下脚本对基础环境进行初始化，主要功能包括：

*   安装 containerd ，边缘引擎 EdgeCore 依赖的容器组件
*   关闭 SELinux 及防火墙
*   优化相关内核参数，针对生产环境的基础系统调优配置
*   关闭 swap
*   设置相关模块，主要为转发模块
*   配置相关基础软件，系统部分基础依赖包
*   创建 container 所使用的独立目录
*   配置 crictl 和运行时的连接，便于后期使用 crictl 命令

提示：后续ctr命令下载镜像的时候，若需要使用containerd的加速，必须带上--hosts-dir，ctr 当前环境所管理的镜像都属于 k8s.io ，因此创建一个别名。

    root@master01:~# wget http://down.linuxsb.com/mydeploy/k8s/v1.32.3/uk8spreconfig.sh
    
    root@master01:~# vim uk8spreconfig.sh
    #!/bin/bash
    #***************************************************************#
    # ScriptName: uk8spreconfig.sh
    # Author: xhy
    # Create Date: 2025-03-29 12:30
    # Modify Author: xhy
    # Modify Date: 2025-04-01 16:43
    # Version: v1
    #***************************************************************#
    
    CONTAINERD_VERSION=1.7.26-1
    
    # Initialize the machine. This needs to be executed on every machine.
    apt update && apt upgrade -y && apt autoremove -y
    
    # Install package
    sudo apt -y install conntrack ntpdate ntp ipvsadm ipset jq iptables sysstat wget
    
    # Add Docker's official GPG key
    sudo apt -y install ca-certificates curl gnupg
    sudo install -m 0755 -d /etc/apt/keyrings
    sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
    sudo chmod a+r /etc/apt/keyrings/docker.gpg
    
    # Add the repository to Apt sources:
    echo \
      "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \
      "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
      sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    sudo apt update
    
    # Install and config containerd
    apt-cache madison containerd
    sudo apt -y install containerd.io=${CONTAINERD_VERSION}
    sleep 3s
    
    mkdir -p /etc/containerd/certs.d/docker.io
    mkdir -p /data/containerd
    
    cat > /etc/containerd/config.toml <<EOF
    disabled_plugins = ["io.containerd.internal.v1.restart"]
    root = "/data/containerd"
    version = 2
    
    [plugins]
    
      [plugins."io.containerd.grpc.v1.cri"]
    #    sandbox_image = "registry.k8s.io/pause:3.10"
        sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.10"
    
        [plugins."io.containerd.grpc.v1.cri".containerd]
    
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
    
            [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
              runtime_type = "io.containerd.runc.v2"
    
              [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                SystemdCgroup = true
    
        [plugins."io.containerd.grpc.v1.cri".registry]
          config_path = "/etc/containerd/certs.d"
    
      [plugins."io.containerd.runtime.v1.linux"]
        shim_debug = true
    EOF
    
    cat > /etc/containerd/certs.d/docker.io/hosts.toml <<EOF
    server = "https://registry-1.docker.io"
    
    [host."https://docker.1ms.run"]
      capabilities = ["pull", "resolve", "push"]
    
    [host."https://dbzucv6w.mirror.aliyuncs.com"]
      capabilities = ["pull", "resolve", "push"]
    
    [host."https://docker.xuanyuan.me"]
      capabilities = ["pull", "resolve", "push"]
    EOF
    
    # config crictl & containerd
    cat > /etc/crictl.yaml <<EOF
    runtime-endpoint: unix:///run/containerd/containerd.sock
    image-endpoint: unix:///run/containerd/containerd.sock
    timeout: 10
    debug: false
    EOF
    
    echo 'alias ctrpull="ctr -n k8s.io images pull --hosts-dir /etc/containerd/certs.d"' >> /etc/profile.d/custom_bash.sh
    
    systemctl restart containerd
    systemctl enable containerd --now
    
    # Turn off and disable the firewalld.
    systemctl disable ufw --now || true
    
    # Modify related kernel parameters & Disable the swap.
    cat > /etc/sysctl.d/k8s.conf << EOF
    net.ipv4.ip_forward = 1
    net.bridge.bridge-nf-call-ip6tables = 1
    net.bridge.bridge-nf-call-iptables = 1
    net.ipv4.tcp_tw_recycle = 0
    vm.swappiness = 0
    vm.overcommit_memory = 1
    vm.panic_on_oom = 0
    net.ipv6.conf.all.disable_ipv6 = 1
    EOF
    sysctl -p /etc/sysctl.d/k8s.conf >&/dev/null
    swapoff -a
    sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
    modprobe br_netfilter
    modprobe overlay
    
    sysctl --system
    
    # Add ipvs modules
    cat > /etc/modules-load.d/ipvs.conf <<EOF
    ip_vs
    ip_vs_rr
    ip_vs_wrr
    ip_vs_sh
    nf_conntrack
    br_netfilter
    overlay
    EOF
    
    systemctl restart systemd-modules-load.service
    

提示：containerd 镜像加速配置更多可参考：

[containerd配置镜像加速器](https://www.cnblogs.com/fsdstudy/p/18355827) [containerd官方加速配置](https://github.com/containerd/containerd/blob/main/docs/hosts.md)  

提示：如上仅需在master01节点上操作，建议初始化完后进行重启。

    root@master01:~# source kubeedgenodeenv.sh
    root@master01:~# chmod +x *.sh
    
    root@master01:~# for edge_ip in ${EDGE_ALL_IPS[@]}
      do
        echo -e "\n\n\033[33m[INFO] >>> ${edge_ip}...\033[0m"
        sleep 2
        scp -rp /etc/hosts root@${edge_ip}:/etc/hosts
        scp -rp uk8spreconfig.sh root@${edge_ip}:/root/
        ssh root@${edge_ip} "bash /root/uk8spreconfig.sh"
      done
    

提示：如上仅需在master01节点上操作。

### 安装keadm

云端（CloudCore）部分需要设置云端节点，即 KubeEdge 主节点，边缘（EdgeCore）部分需要设置边缘端节点，即 KubeEdge 工作节点。  
因此建议所有节点安装 keadm 工具。

    root@master01:~# mkdir keadm
    root@master01:~# cd keadm/
    root@master01:~/keadm# KEADMVERSION=v1.20.0
    root@master01:~/keadm# wget https://github.com/kubeedge/kubeedge/releases/download/${KEADMVERSION}/keadm-${KEADMVERSION}-linux-amd64.tar.gz
    root@master01:~/keadm# tar -zxvf keadm-${KEADMVERSION}-linux-amd64.tar.gz
    root@master01:~/keadm# cp keadm-${KEADMVERSION}-linux-amd64/keadm/keadm /usr/local/bin/keadm
    root@master01:~/keadm# keadm version
    version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.0", GitCommit:"bae61505d4919c404665f70347ad0646aaa98958", GitTreeState:"clean", BuildDate:"2025-01-21T12:47:49Z", GoVersion:"go1.22.9", Compiler:"gc", Platform:"linux/amd64"}
    

分发keadm工具。

    root@master01:~/keadm# source /root/kubeedgenodeenv.sh
    
    root@master01:~/keadm# for edge_ip in ${EDGE_ALL_IPS[@]}
      do
        echo -e "\n\n\033[33m[INFO] >>> ${edge_ip}...\033[0m"
        sleep 2
        scp -rp /usr/local/bin/keadm root@${edge_ip}:/usr/local/bin/keadm
      done
    

### 设置标签

本实验直接复用 Kubernetes master 的高可用，采用 VIP 对外暴露，因此规划将 CloudCore 部署在 master 节点。  
keadm 底层基于 Kubernetes helm chart 进行部署，因此也支持通过 helm values 来创建自定义参数来控制部署情况。

    root@master01:~/keadm# kubectl label nodes master0{1,2,3} cloudcore=enabled
    

### 安装CNI

默认边缘节点安装containerd并不会安装cni，需要手动额外安装。  
同时需要配置calico网络以及手动分发相关calico证书，本实验直接从当前已存在的master01节点快速分发。

    root@master01:~/keadm# source /root/kubeedgenodeenv.sh
    
    root@master01:~/keadm# for edge_ip in ${EDGE_ALL_IPS[@]}
      do
        echo -e "\n\n\033[33m[INFO] >>> ${edge_ip}...\033[0m"
        sleep 2
        scp -rp /opt/cni/bin/* root@${edge_ip}:/opt/cni/bin/
        scp -rp /etc/cni/net.d/* root@${edge_ip}:/etc/cni/net.d/
      done
    

提示：也可参考如下手动安装cni：

root@master01:~/keadm# wget https://github.com/containernetworking/plugins/releases/download/v1.6.2/cni-plugins-linux-amd64-v1.6.2.tgz
    root@master01:~/keadm# tar zxvf cni-plugins-linux-amd64-v1.6.2.tgz -C /opt/cni/bin

### 设置云端

使用 keadm init 设置 KubeEdge 主节点，即设置 CloudCore 端。  
默认情况下边缘节点需要访问 cloudcore 中 10000 ，10002 端口。

*   自定义部署  
    创建自定义相关配置，然后进行部署。

    root@master01:~/keadm# vim myvalues.yaml
    cloudCore:
      replicaCount: 3
      nodeSelector:
        cloudcore: enabled
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          effect: NoSchedule
    
      modules:
        cloudHub:
          advertiseAddress:
            - "172.24.8.180"
    
        dynamicController:
          enable: true 
    
    iptablesManager:
      nodeSelector:
        cloudcore: enabled
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          effect: NoSchedule
    
    mosquitto:
      nodeSelector:
        cloudcore: enabled
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          effect: NoSchedule
    
    root@master01:~/keadm# keadm init \
      --profile=myvalues.yaml \
      --kubeedge-version=v1.20.0 \
      --kube-config=/root/.kube/config
    
    

提示：keadm init 将安装并运行 cloudcore，生成证书并安装 CRD，同时提供了--set key=value 命令行参数，通过它可以设置特定的配置。也支持 helm 方式配置参数，官方默认的 helm chart 参数参考：[cloudcore-values.yaml](https://github.com/kubeedge/kubeedge/blob/master/manifests/charts/cloudcore/values.yaml)  

*   查看结果  
    查看云端部署结果。

    root@master01:~/keadm# kubectl -n kubeedge get all -owide
    

![002](https://tp.linuxsb.com/study/kubernetes/f043/002.png)

### 设置调度

在边缘节点添加至集群之前，建议设置调度策略，避免系统自带的 DaemonSet 调度到边缘节点。

主要需要设置的相关Pod有 calico-node ，kube-proxy 。

    root@master01:~/keadm# kubectl get daemonset -n kube-system | grep -v NAME | awk '{print $1}' | xargs -n 1 kubectl patch daemonset -n kube-system --type='json' -p='[{"op": "replace","path": "/spec/template/spec/affinity","value":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"node-role.kubernetes.io/edge","operator":"DoesNotExist"}]}]}}}}]'
    
    root@master01:~/keadm# kubectl -n kube-system rollout restart daemonset kube-proxy calico-node
    
    root@master01:~/keadm# kubectl -n kube-system get daemonset kube-proxy -o yaml | grep -A5 affinity
    root@master01:~/keadm# kubectl -n kube-system get daemonset calico-node -o yaml | grep -A5 affinity
    

本实验持久存储 Longhorn 也存在 DaemonSet ，且 Longhorn 是基于 helm 部署，需要做如下调整：

    root@master01:~/longhorn# vim myvalues.yaml
    longhornManager:
      nodeSelector:
        longhorn-storage: enabled
    
      extraVolumeMounts:
        - name: timeconfig
          mountPath: /etc/localtime
          readOnly: true
      extraVolumes:
        - name: timeconfig
          hostPath:
            path: /etc/localtime
    
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/edge
                operator: DoesNotExist
    
    longhornDriver:
      nodeSelector:
        longhorn-storage: enabled
    
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/edge
                operator: DoesNotExist
    
    csi:
      plugin:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/edge
                  operator: DoesNotExist
      attacher:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/edge
                  operator: DoesNotExist
      provisioner:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/edge
                  operator: DoesNotExist
      resizer:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/edge
                  operator: DoesNotExist
    
    longhornUI:
      replicas: 3
      nodeSelector:
        longhorn-ui: enabled
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          effect: NoSchedule
    
      extraVolumeMounts:
        - name: timeconfig
          mountPath: /etc/localtime
          readOnly: true
      extraVolumes:
        - name: timeconfig
          hostPath:
            path: /etc/localtime
    
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/edge
                operator: DoesNotExist
    ingress:
      enabled: true
      ingressClassName: "nginx"
      host: "longhorn.linuxsb.com"
      path: /
      pathType: Prefix
      annotations:
        nginx.ingress.kubernetes.io/auth-type: basic
        nginx.ingress.kubernetes.io/auth-secret: longhorn-basic-auth
        nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required'
        nginx.ingress.kubernetes.io/proxy-body-size: 50m
        nginx.ingress.kubernetes.io/ssl-redirect: "false"
    
    root@master01:~/longhorn# helm upgrade --install longhorn longhorn/longhorn --create-namespace --namespace longhorn-system -f myvalues.yaml
    root@master01:~/longhorn# kubectl -n longhorn-system rollout restart  daemonsets.apps engine-image-ei-db6c2b6f longhorn-csi-plugin longhorn-manager
    

### 设置边缘端

使用 keadm joini 设置 KubeEdge 边缘节点，即设置 EdgeCore 端。  
默认情况下边缘节点需要访问 cloudcore 中 10000 ，10002 端口。

提示：

1.  \--cloudcore-ipport 是必填参数。
2.  加上 --token 会自动为边缘节点生成证书。
3.  需要保证云和边缘端使用的 KubeEdge 版本相同。

*   获取加入的token  
    在云端获取token，然后在边缘端加入。

    root@master01:~/keadm# keadm gettoken
    2b299ac235ea1ae400a02dfd2442bafa57f5e7b1a0239a6551e7d5c8e6366ecd.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NDQwNDAzNjV9.v8fv9295l4OaqwISKpIDkRrA8FT-0y45VJRmSwgSJaY
    

*   边缘节点join

    root@edgenode01:~# keadm join --cloudcore-ipport=172.24.8.180:10000 \
     --token=2b299ac235ea1ae400a02dfd2442bafa57f5e7b1a0239a6551e7d5c8e6366ecd.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NDQwNDAzNjV9.v8fv9295l4OaqwISKpIDkRrA8FT-0y45VJRmSwgSJaY \
     --kubeedge-version=v1.20.0 --cgroupdriver=systemd --edgenode-name=edgenode01
    
    root@edgenode02:~# keadm join --cloudcore-ipport=172.24.8.180:10000 \
     --token=2b299ac235ea1ae400a02dfd2442bafa57f5e7b1a0239a6551e7d5c8e6366ecd.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NDQwNDAzNjV9.v8fv9295l4OaqwISKpIDkRrA8FT-0y45VJRmSwgSJaY \
     --kubeedge-version=v1.20.0 --cgroupdriver=systemd --edgenode-name=edgenode02
    

### 边缘端优化

edgeStream用于边缘节点与云端的通信，建议开启。

    root@edgenode01:~# vim /etc/kubeedge/config/edgecore.yaml
    apiVersion: edgecore.config.kubeedge.io/v1alpha2
    #……
    modules:
    #……
      edgeStream:
        enable: true
    root@edgenode01:~# systemctl restart edgecore.service
    
    root@edgenode02:~# vim /etc/kubeedge/config/edgecore.yaml
    apiVersion: edgecore.config.kubeedge.io/v1alpha2
    #……
    modules:
    #……
      edgeStream:
        enable: true
        
    root@edgenode02:~# systemctl restart edgecore.service
    

### 确认验证

查看当前部署情况。

    root@master01:~/keadm# kubectl get nodes -o wide
    NAME         STATUS   ROLES           AGE   VERSION                    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
    edgenode01   Ready    agent,edge      62m   v1.30.7-kubeedge-v1.20.0   172.24.8.187   <none>        Ubuntu 24.04.2 LTS   6.8.0-57-generic   containerd://1.7.27
    edgenode02   Ready    agent,edge      62m   v1.30.7-kubeedge-v1.20.0   172.24.8.188   <none>        Ubuntu 24.04.2 LTS   6.8.0-57-generic   containerd://1.7.27
    master01     Ready    control-plane   8d    v1.32.3                    172.24.8.181   <none>        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
    master02     Ready    control-plane   8d    v1.32.3                    172.24.8.182   <none>        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
    master03     Ready    control-plane   8d    v1.32.3                    172.24.8.183   <none>        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
    worker01     Ready    <none>          8d    v1.32.3                    172.24.8.184   <none>        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
    worker02     Ready    <none>          8d    v1.32.3                    172.24.8.185   <none>        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
    worker03     Ready    <none>          8d    v1.32.3                    172.24.8.186   <none>        Ubuntu 24.04.2 LTS   6.8.0-56-generic   containerd://1.7.26
    

查看当前KubeEdge的Pod。

    root@master01:~/keadm# kubectl get pods -o wide -n kubeedge
    NAME                           READY   STATUS    RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES
    cloud-iptables-manager-klqkb   1/1     Running   0          92m   172.24.8.182   master02     <none>           <none>
    cloud-iptables-manager-v4np5   1/1     Running   0          92m   172.24.8.181   master01     <none>           <none>
    cloud-iptables-manager-vknlw   1/1     Running   0          92m   172.24.8.183   master03     <none>           <none>
    cloudcore-5cc47bd5c7-bfk9f     1/1     Running   0          92m   172.24.8.181   master01     <none>           <none>
    cloudcore-5cc47bd5c7-x822x     1/1     Running   0          92m   172.24.8.182   master02     <none>           <none>
    cloudcore-5cc47bd5c7-zh9z2     1/1     Running   0          92m   172.24.8.183   master03     <none>           <none>
    edge-eclipse-mosquitto-hfl88   1/1     Running   0          64m   172.24.8.188   edgenode02   <none>           <none>
    edge-eclipse-mosquitto-j8ft7   1/1     Running   0          64m   172.24.8.187   edgenode01   <none>           <none>
    

KubeEdge测试验证
------------

### 调度测试

KubeEdge部署好之后，可以使用定向调度测试应用运行。

    root@master01:~/keadm# cat > edgetest.yaml <<EOF
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-edgename
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          nodeName: edgenode01
          hostNetwork: true
          containers:
          - name: nginx
            image: nginx
            ports:
            - containerPort: 80
    EOF
    
    root@master01:~/keadm# kubectl apply -f edgetest.yaml
    
    root@master01:~/keadm# kubectl get pods -o wide
    NAME                              READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES
    nginx-edgename-75c465f8b9-hcxj7   1/1     Running   0          3m38s   172.24.8.187   edgenode01   <none>           <none>
    

访问： [http://172.24.8.187](http://172.24.8.187) 验证部署情况。

![003](https://tp.linuxsb.com/study/kubernetes/f043/003.png)

作者：[木二](http://www.linuxsb.com/)

出处：[http://www.cnblogs.com/itzgr/](http://www.cnblogs.com/itzgr/)

关于作者：云计算、虚拟化，Linux，多多交流！

本文版权归作者所有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出[原文链接](#)!如有其他问题，可邮件（xhy@itzgr.com）咨询。