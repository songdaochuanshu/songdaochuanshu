---
layout: post
title: 'AgentFounder浅析——Agent的演化历程与目标'
date: "2025-10-16T00:40:39Z"
---
AgentFounder浅析——Agent的演化历程与目标
=============================

理论上（浅显）分析Agent与传统llm、RAG的不同以及演进历程，根据Agent的目标解读[AgentFounder](https://www.arxiv.org/abs/2509.13310)论文的训练策略和数据集构建

Agent的目标以及对应的技术方案
=================

Agent的推理目标
----------

形式化的表达:

咱们首先来分析一下最开始大模型的功能，即仅根据\\(\\pi\\)的内部知识和问题\\(q\\)采样出答案\\(o\\)

\\\[o \\sim \\pi(\\cdot|q) \\\]

然而，模型\\(\\pi\\)原有的内部知识可能不足以支撑回答\\(q\\)问题（没有训练过相关领域的数据），那么就需要引入外部知识\\(R\\)，也就是常见的RAG架构。RA 先根据\\(q\\)调用外部tool（向量数据库）得到一定的response（\\(R\\)），再一同输入到\\(\\pi\\)中进行答案的生成。有效地缓解\\(\\pi\\)在问题\\(q\\)上自身知识不足的问题，人为地注入了外部知识\\(R\\)；  
此方法有效建立在 \\(\\pi\\)是否在\\(R\\)上有泛化性，**即\\(\\pi\\)是否会使用外部知识\\(R\\)**。

\\\[R\\leftarrow f(q)\\\\ o \\sim \\pi(\\cdot|q,R) \\\]

那好，新的问题又出现了，即使\\(\\pi\\)会使用\\(R\\)，但是\\(R\\)是否真正能够帮助\\(\\pi\\)解决\\(q\\)也是一个问题，  
这个\\(R\\)仅依赖于问题\\(q\\)和事先设定好的工具\\(f(\\cdot)\\)，也就是说在生成\\(R\\)时，并没有考虑到是否能帮助到后续的模型\\(\\pi\\)  
因此**应该在生成\\(R\\)时，也要依赖于\\(\\pi\\)**。

\\\[\\begin{aligned} 【初始化】tool\\\_type, tool\\\_args &\\sim \\pi(\\cdot|q) \\\\ 【获得工具输出】 R\_i &\\leftarrow f(tool\\\_args; tool\\\_type) \\\\ 【迭代生成】o\\ |\\ tool\\\_type, tool\\\_args &\\sim \\pi(\\cdot|q, R\_1, ..., R\_i) \\end{aligned} \\\]

因此，可以发现的是，\\(\\pi\\)不仅仅要生成答案\\(o\\)，还需要学会工具调用来获取\\(R\\)以更好的执行后面的任务流程。

所以Agent的**目标**（需要的推理能力）分为三类

*   利用模型内部知识根据q生成第一步的planning。
    
    *   \\(tool\\\_type, tool\\\_args \\sim \\pi(\\cdot|q)\\)
    *   学会如何仅根据q构建完整的**planning**
*   利用模型内部知识+外部知识（R） 根据q生成下一步的工具调用/答案 的能力 （step-wise）
    
    *   \\(o\\ |\\ tool\\\_type, tool\\\_args \\sim \\pi(\\cdot|q, R)\\)
    *   学会 single-step下，如何根据需要的信息 **选取工具**
*   学会连续调用工具，理解工具间的调用关系，（traj-wise）
    
    *   \\(o\\sim \\pi(\\cdot|q, R\_1, R\_2, ...)\\)
    *   学会在整体的traj维度下，**协调多个工具之间**的使用关系

对应来说：

*   局部
    
    *   初始化
        
    *   step-wise的单个工具point-wise的使用
        
*   整体
    
    *   traj-wise的工具之间的协调调度

Agent的训练方案
----------

训练目标需要和推理模型对齐。而训练目标体现在（1）数据集的构建方案（2）训练策略（loss）

那么Agent训练方案的是**数据集构建+模型训练**方式两个难点

*   数据构建
    *   以上三类能力对应的数据集的构建
    *   输入输出的pair对，参考公式即可
    *   数据需要可扩展/高质量（因此优先在Web Brower 领域进行研究）
*   训练策略
    *   使用sft教会模型前两种能力（planning生成、学会根据需要的信息选取工具）
    *   使用rl教会模型最后一种能力（工具间的协调调用，因为此任务比较难学习，需要大量的探索以及较高的泛化性要求）

相较于传统的single-step的数据及其sft RL的训练方式

Agent的关键区别是给予了llm自主获取外界知识、与外界交互的能力。

因此，Agent的数据和训练目标 均服务于 如何使Agent学会更好的使用工具与外界交互，从而利用外界的信息更好地完成任务

* * *

思路借鉴：AgentFounder：[https://www.arxiv.org/abs/2509.13310](https://www.arxiv.org/abs/2509.13310)