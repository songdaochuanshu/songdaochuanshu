---
layout: post
title: '在windows主机本地快速部署使用deepseek-r1大模型'
date: "2025-02-17T00:38:28Z"
---
在windows主机本地快速部署使用deepseek-r1大模型
================================

一台配备 Windows 操作系统、12GB 或以上显存的英伟达显卡、8GB 或以上内存，并能连接互联网的电脑可以继续阅读以下内容。

### 简介

* * *

### Ollama（用于下载和启动大模型）

Ollama 专注于本地大型语言模型（LLM）的快速、极简安装和使用，例如 LLaMA 3.3 和 DeepSeek-R1。它提供开箱即用的体验，适合个人开发者和小规模实验研究，但不适用于高可用性和高并发的生产环境。地址：[https://ollama.com/](https://ollama.com/)

#### Open WebUI（用于提供一个交互友好的界面，向指定的大模型提问）

Open WebUI 是一个可扩展、功能丰富且用户友好的自托管 AI 平台，旨在完全离线运行。它支持多种 LLM 运行器（如 Ollama）和与 OpenAI 兼容的 API，并内置 RAG 推理引擎，使其成为强大的 AI 部署解决方案。所谓“自托管”是指用户可以在自己的服务器或本地环境中运行和管理该平台。地址：[https://openwebui.com/](https://openwebui.com/)

#### DeepSeek-R1-Distill-Qwen-14B（大模型介绍）

这里提到的大模型是 DeepSeek-R1-Distill-Qwen-14B，它是基于 Qwen2.5-14B 模型进行蒸馏微调后得到的，使用了 DeepSeek-R1 生成的样本。尽管蒸馏过程减小了模型大小，但运行此模型仍需要至少 30GB 的显存，这超出了大部分人的硬件条件。而 Ollama 提供了进一步量化的版本，将参数规模降至 14.8 亿，模型大小降至 9GB。

### 开始

#### 下载安装Ollama(windows)

浏览器打开地址：[https://ollama.com/](https://ollama.com/) ，点击下载按钮

![图片](https://img2024.cnblogs.com/blog/706195/202502/706195-20250216151814832-1178389683.png)

下载后之后，双击安装即可，没有看到有什么可以自定义的配置。

#### 使用Ollama下载指定的大模型

由于Ollama默认存储路径在C盘，而C盘空间有限，所以建议将模型文件保存在其他盘符或目录下。可以通过设置环境变量OLLAMA\_MODELS来更改模型的下载路径:

1.首先，右键点击Windows桌面上的“此电脑”图标，选择“属性”以打开系统属性窗口。

2.在系统属性窗口右侧找到并点击“高级系统设置”，在弹出窗口上点击“高级-环境变量”

3.在“环境变量”窗口中，可以在“系统变量”新建一个名为OLLAMA\_MODELS的环境变量，并在“变量值”文本框中输入你自己希望保存模型文件的新路径。由于大模型的文件都非常大，比如我们将要部署的模型文件大小就为9G左右，所以建议选择的路径所在的磁盘空间要比较大一些。

![图片](https://img2024.cnblogs.com/blog/706195/202502/706195-20250216151814807-968599514.png)

4.通过任务管理器结束Ollama进程，然后重新运行Ollama来使修改的环境变量生效。

5.上述过程完成后，打开powershell命令行工具，执行如下命令下载deepseek-r1:14b大模型，模型文件较大，需要等待一定时间。

    ollama pull deepseek-r1:14b
    

#### 查看Ollama下载好的模型列表

下载完成后，可通过如下命令检查是否能看到下载好的模型

    ollama list
    

使用Ollama启动模型

    ollama run deepseek-r1:14b    
    

该命令会启动 DeepSeek-R1 模型，并启动一个 REPL（交互式终端），你可以接着在命令行直接输入问题，模型会根据问题生成回答。

#### 安装Open WebUI

    pip install open-webui
    

运行Open WebUI

    open-webui serve
    

Open WebUI页面访问

    http://localhost:8080
    

查看Open WebUI的大模型配置

*   点击右上角的用户头像图标，选择Settings;
    
*   在弹出的界面左侧菜单选择Admin Settings;
    
*   在打开的界面的导航栏选择Settings;
    
*   在左侧界面选择Models，如果你的ollama和open webui在同一台机器上，那么此时会看到已经自动识别到的deepseek-r1:14b模型。（可能是因为本地提前启动了ollama,毕竟open webui以前叫ollama webui嘛~）
    

#### 使用Open WebUI 向指定大模型提问

笔者是24g显存的显卡，所以部署的是32b的deepseek-r1模型，此模型大约需要20g显存占用。

![图片](https://img2024.cnblogs.com/blog/706195/202502/706195-20250216151814844-1600971560.png)

直接在输入框输入你的问题，点击发送即可，可能会等待10到15秒时间给出回复。

#### 注意事项

如果在chrome上使用open webui 出现打字机效果相关的问题，导致内容被更改或显示不全，可以更换浏览器试试。此现象应该极少遇到，但是笔者遇到了，不知什么原因，更换为edge浏览器尝试是正常的。

本文来自博客园，作者：[AI粉嫩特攻队](https://www.cnblogs.com/anai/)，转载请注明原文链接：[https://www.cnblogs.com/anai/p/18718130](https://www.cnblogs.com/anai/p/18718130)