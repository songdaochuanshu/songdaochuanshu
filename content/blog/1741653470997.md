---
layout: post
title: '超详细：普通电脑也行Windows部署deepseek R1训练数据并当服务器共享给他人'
date: "2025-03-11T00:37:50Z"
---
超详细：普通电脑也行Windows部署deepseek R1训练数据并当服务器共享给他人
============================================

### 一、**Windows 版 DeepSeek-R1、Ollama 与 AnythingLLM 介绍及核心使用场景**‌

#### ‌**一、组件功能与定位**‌

1.  ‌**DeepSeek-R1**‌
    
    *   ‌**模型特性**‌：支持 ‌**FP16 计算**‌ 和 ‌**CUDA 加速**‌，提供 1.5B 至 671B 参数量级版本，适用于本地部署的逻辑推理、文本生成、数据分析等场景‌。
    *   ‌**优势**‌：开源免费、响应速度快（本地低延迟），支持中文复杂任务处理‌。
2.  ‌**Ollama**‌
    
    *   ‌**核心功能**‌：简化大语言模型本地部署流程，支持一键下载、运行和管理模型（如 DeepSeek-R1），提供命令行界面和灵活的环境变量配置‌。
    *   ‌**特性**‌：支持自定义安装路径（需修改系统变量）、多模型切换、离线运行‌。
3.  ‌**AnythingLLM**‌
    
    *   ‌**定位**‌：全栈 AI 应用，集成本地大模型（通过 Ollama）与知识库，支持文档/音视频/网页内容转换为上下文数据，供 LLM 调用‌。
    *   ‌**功能**‌：私有化知识库构建、多工作区管理、模型与数据关联式问答‌。

* * *

#### ‌**二、核心使用场景**‌

1.  ‌**企业内部知识库与智能客服**‌
    
    *   ‌**场景**‌：通过 AnythingLLM 上传企业文档（如产品手册、合同），结合 DeepSeek-R1 实现精准问答，替代传统人工客服‌。
    *   ‌**优势**‌：数据本地化存储（避免云端泄露）、支持多格式文件解析‌。
2.  ‌**专业领域研究与分析**‌
    
    *   ‌**场景**‌：科研人员使用 DeepSeek-R1 处理长文本（如论文、报告），生成摘要或提取核心结论；结合 AnythingLLM 训练领域专属模型‌。
    *   ‌**案例**‌：法律条文分析、医学文献结构化处理‌。
3.  ‌**个人效率工具**‌
    
    *   ‌**场景**‌：
        *   ‌**周报生成**‌：输入工作记录，由 DeepSeek-R1 自动整理成结构化周报‌。
        *   ‌**实时翻译**‌：本地部署模型实现无网络环境下的多语言互译‌。
        *   **个性化知识助手**‌:导入个人笔记、电子书等资料，构建专属知识库，辅助日常学习决策‌。
4.  ‌**开发与测试环境**‌
    
    *   ‌**场景**‌：开发者通过 Ollama 快速切换不同参数量级的 DeepSeek-R1 版本，测试模型性能或调试应用兼容性‌。

### 二、本地下载安装部署

#### **1、安装 CUDA Toolkit 步骤如下**：[https://developer.nvidia.com/cuda-downloads?target\_os=Windows&target\_arch=x86\_64](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64 "CUDA Toolkit ")

1.  不用显卡跳过此步骤
2.  访问指定链接下载 CUDA Toolkit。请务必留意针对 Windows 系统的版本选项，如 Windows 10、Windows 11 等，并选择 “local” 本地安装类型，以获取最佳安装体验。
3.  下载过程需登录 NVIDIA 账号。若您尚无账号，请提前完成注册流程，确保下载顺利进行。
4.  完成 CUDA Toolkit 安装后，请重启计算机，以使所有配置更改生效，从而确保 CUDA Toolkit 能够正常运行。![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307141342085-733882947.png)

#### 2、Ollama 安装：[https://ollama.com/](https://ollama.com/)

1.  **下载与安装**：前往 Ollama 官方网站，获取 Ollama 安装程序。整个下载与安装流程十分简易，按照系统提示逐步操作即可轻松完成，在此便不展开详述。
2.  **后台运行检查**：安装完成并启动 Ollama 后，该程序将在后台持续运行。您可通过查看电脑右下角的系统托盘区域，确认是否出现 Ollama 的 Logo 图标，以此判断 Ollama 是否已成功在后台启动并正常运行 。

![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307140959674-1816146356.png)

#### 3、 **通过Ollama安装模型**：[https://ollama.com/search](https://ollama.com/search)

1.  **选择模型参数与获取安装命令**：Ollama 的模型库涵盖了丰富多样的大语言模型，通过 Ollama 安装 DeepSeek - R1 模型的操作十分便捷。其模型库中的可用模型包括但不限于 DeepSeek - R1、Lamma3.3、qwq 等，用户可按需选择安装 。进入 DeepSeek - R1 模型集合，依照下方图示的顺序，选择合适的参数数量，然后复制对应的安装命令。![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307141038398-108665489.png)
2.  **考虑硬件限制**：需特别注意，GPU 显存大小（若仅使用 CPU，则考虑本地主机的内存）会限制可使用的模型大小。例如，拥有 16GB 显存的 GPU 能够运行 14B 的模型，而 24GB 显存的 GPU 则可以运行 32B 的模型。![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307141132308-667036559.png)
3.  **执行安装命令**：普通电脑运行1.5b就可以，不用显卡。以本人为例，我选择了 14B 的模型。复制好命令后，打开 Powershell 并执行该命令，Ollama 会自动开始安装相应版本的 DeepSeek - R1 模型。你只需耐心等待命令运行结束，安装完成后即可使用该模型。

    ollama run deepseek-r1:1.5b

![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307142938444-90790737.png)

 Powershell 的终端中直接运行了，但是存在诸多不便之处。比如，难以对对话记录进行保存、搜索与管理，无法读取附件，并且无法集成本地知识库来实现检索增强生成等功能，极大地影响了用户体验。因此，强烈推荐大家安装一款本地 AI 应用用户界面，本文以 AnythingLLM 为例进行示范。

#### 4、AnythingLLM 安装与配置：[https://anythingllm.com/desktop](https://anythingllm.com/desktop)

1.  **下载与安装**：前往 AnythingLLM 官方网站获取安装程序并进行安装。![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307145501376-844204615.png)
2.  **启动与设置**：安装完成并启动 AnythingLLM 后，在设置中选择 “Ollama”（注意不是 “DeepSeek”）。![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307150034268-1329073474.png)
3.  在设置中选择 “Ollama”（注意不是 “DeepSeek”）作为 LLM Provider。此时，AnythingLLM 会自动检测本地部署的大语言模型，从中选择 “deepseek - r1:14b”。然后一直点击右键。![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307150343865-973690151.png)
4.  **创建工作区并开始对话**：完成上述步骤后，创建一个 Workspace，随后即可开启与模型的对话之旅。![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307161437501-74548815.png)
5.  进行设置
    
    ![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307161402183-679398563.png) 
    
6.  保存设置
    
    ![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307161637053-1151304714.png)
    
    #### 5、下载 embedding 模型
    
    #### 步骤一：下载模型
    
    1.  打开命令行工具。
    2.  在命令行中输入指令 `ollama pull nomic-embed-text`，然后回车执行，等待模型下载完成。
    3.  ![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307163933729-979564789.png)
    
    #### 步骤二：切换并保存模型设置
    
    1.  找到系统左下角的相关操作入口（扳手）。
    2.  ![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307163157650-2033287569.png)
    3.  切换到新下载的 `nomic-embed-text` embedding 模型。
    4.  ![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307163435483-41473718.png)
    5.  切换完成后，点击 “保存更改” 按钮，确保设置生效。
    6.  ![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307164024284-499338744.png)
    

#### 6\. 上传本地资料构建本地数据库

1.  在工作空间页面中，找到并点击 “上传” 按钮。
2.  ![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307164350752-1283461136.png)
3.  依据实际需求选择上传方式：你既可以直接上传本地文件，也能够选择连接数据库以获取所需资料。![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307164717941-1248439679.png)
4.  资料上传或数据库连接完成后，点击 “保存” 按钮，系统将自动对上传的资料进行向量化处理。
5.  然后新建对话进行测试。![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250307164959542-542797908.png)

### 三、Windows 系统 Ollama 当服务器开放共享给其他人

* * *

#### ‌**一、配置环境变量开放访问权限**‌

1.  ‌**设置 `OLLAMA_HOST` 环境变量**‌
    
    *   右键点击「此电脑」→「属性」→「高级系统设置」→「环境变量」→「系统变量」→「新建」：
        *   ‌**变量名**‌：`OLLAMA_HOST`
        *   ‌**变量值**‌：`0.0.0.0`（允许所有网络接口监听请求）‌
    *   若需同时解决跨域问题，可添加变量 `OLLAMA_ORIGINS`，值为 `*` ‌。
    *   ![](https://img2024.cnblogs.com/blog/785716/202503/785716-20250310092719366-1188089114.png)
2.  ‌**重启 Ollama 服务**‌
    
    *   退出任务栏的 Ollama 程序（右键图标→「Quit Ollama」），重新启动 Ollama ‌。
    *   若配置未生效，建议重启系统 ‌。

* * *

#### ‌**二、开放防火墙端口**‌

1.  ‌**通过命令行添加防火墙规则**‌
    
    *   以管理员身份运行命令提示符，执行以下命令：
        
        `netsh advfirewall firewall add rule name="Ollama" dir=in action=allow protocol=TCP localport=11434`
        
        _（放行 Ollama 默认端口 `11434`）‌_
2.  ‌**验证防火墙规则**‌
    
    *   检查端口是否放行成功：
        
        `netsh advfirewall firewall show rule name="Ollama"`
        
        _（输出需包含 `Enabled=Yes` 和 `Action=Allow`）‌_

* * *

#### ‌**三、验证非本机访问**‌

1.  ‌**局域网设备测试**‌
    
    *   执行命令 `ipconfig` 查看本机 IPv4 地址（如 `192.168.1.100`）。
    *   在其他设备的浏览器或命令行中访问：
        
        `curl http://[Windows主机IP]:11434/api/tags`
        
        _（若返回模型列表，则配置成功）‌_
2.  ‌**可视化工具验证（可选）**‌
    
    *   使用 Open WebUI 或 LobeChat 等工具，输入 `http://[Windows主机IP]:11434` 作为 Ollama 服务地址进行连接 ‌。

* * *

#### ‌**四、常见问题解决**‌

*   ‌**报错 `Server connection failed`**‌：  
    检查环境变量 `OLLAMA_HOST` 和 `OLLAMA_ORIGINS` 是否配置正确，并重启服务 ‌。
*   ‌**端口占用或冲突**‌：  
    使用 `netstat -ano | findstr 11434` 确认端口未被其他进程占用 ‌。

* * *

#### **五、公网共享（可选）**‌

1.  ‌**内网穿透工具部署**‌
    
    *   使用 `ngrok` 将本地服务映射至公网：
        
        `ngrok http 11434 # 映射 Ollama 服务`
        
    *   生成公网链接（如 `https://xxx.ngrok.io`）共享给外部用户。
2.  ‌**域名绑定（高级）**‌
    
    *   若已备案域名，可通过路由器或云服务商配置端口转发，将域名指向服务器 IP。

‌**注意事项**‌

*   开放 `0.0.0.0` 会暴露服务至公网，建议内网使用时结合 IP 白名单或 VPN 提升安全性 ‌。
*   若修改了模型存储路径（`OLLAMA_MODELS`），需确保目录权限允许网络访问 ‌。
*   内网穿透需谨慎暴露公网端口，建议配置 HTTPS 加密和 IP 白名单‌。
*   定期清理 AnythingLLM 的无效文档，避免存储空间占用过高‌