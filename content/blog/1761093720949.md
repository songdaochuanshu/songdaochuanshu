---
layout: post
title: '刚刚，DeepSeek又一重大突破，小身材大智慧玩出新高度'
date: "2025-10-22T00:42:00Z"
---
刚刚，DeepSeek又一重大突破，小身材大智慧玩出新高度
=============================

原文:[https://mp.weixin.qq.com/s/RWmTAk-SMadqi5BZEy9pqA](https://mp.weixin.qq.com/s/RWmTAk-SMadqi5BZEy9pqA)

全文摘要
====

DeepSeek-OCR是由DeepSeek-AI提出的、用于探索通过光学2D映射压缩长上下文可行性的视觉语言模型（VLM），核心包含**DeepEncoder**（编码器）和**DeepSeek3B-MoE-A570M**（解码器）两大组件。其中DeepEncoder能在高分辨率输入下保持低激活值并实现高压缩比，实验显示当文本token数量为视觉token的10倍以内（压缩比<10×）时，模型OCR精度达**97%**，压缩比20×时精度仍约**60%**；在实用性能上，它在OmniDocBench基准测试中，仅用100个视觉token就超越需256个token的GOT-OCR2.0，用少于800个视觉token超越平均需6000+个token的MinerU2.0，且单A100-40G显卡日生成20万+页LLM/VLM训练数据，代码和模型权重已开源（[http://github.com/deepseek-ai/DeepSeek-OCR），为LLM长上下文压缩、记忆遗忘机制研究及实际OCR任务提供重要价值。](http://github.com/deepseek-ai/DeepSeek-OCR%EF%BC%89%EF%BC%8C%E4%B8%BALLM%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8E%8B%E7%BC%A9%E3%80%81%E8%AE%B0%E5%BF%86%E9%81%97%E5%BF%98%E6%9C%BA%E5%88%B6%E7%A0%94%E7%A9%B6%E5%8F%8A%E5%AE%9E%E9%99%85OCR%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BE%9B%E9%87%8D%E8%A6%81%E4%BB%B7%E5%80%BC%E3%80%82)

论文方法
====

DeepSeek-OCR概述
--------------

*   **基本定位**：由DeepSeek-AI提出的视觉语言模型（VLM），核心目标是探索通过**光学2D映射**压缩长上下文的可行性，为LLM处理长文本的计算挑战提供解决方案（利用视觉模态作为文本信息的高效压缩媒介）。
*   **核心组件**：包含编码器（DeepEncoder）和解码器（DeepSeek3B-MoE-A570M），代码与模型权重已开源（地址：[http://github.com/deepseek-ai/DeepSeek-OCR）。](http://github.com/deepseek-ai/DeepSeek-OCR%EF%BC%89%E3%80%82)
*   **核心优势**：兼顾高压缩比与高OCR精度，同时具备强实用性能，可大规模生成LLM/VLM训练数据。

核心组件设计
------

### DeepEncoder（编码器）

*   **设计目标**：满足高分辨率处理、高分辨率下低激活、少视觉token、多分辨率支持、参数适中5大需求，解决现有VLM视觉编码器的缺陷（如token过多、激活量大等）。
*   **架构细节**：
    *   总参数约380M，由**SAM-base（80M，窗口注意力主导）**、**16×卷积压缩器**、**CLIP-large（300M，密集全局注意力）** 串联组成。
    *   卷积压缩器：2层卷积（核3×3、步长2、填充1），通道从256→1024，实现视觉token16倍下采样（如1024×1024图像输入，token从4096→256）。
*   **分辨率支持**：通过位置编码动态插值实现多分辨率，具体模式如下表：

分辨率模式

子模式

原生分辨率

视觉token数

处理方式

原生分辨率

Tiny

512×512

64

直接resize

Small

640×640

100

直接resize

Base

1024×1024

256

padding（保留宽高比）

Large

1280×1280

400

padding（保留宽高比）

动态分辨率

Gundam

640×640+1024×1024

n×100+256（n∈\[2:9\]）

分块+resize+padding

Gundam-M

1024×1024+1280×1280

n×256+400（n∈\[2:9\]）

分块+resize+padding

> 注：动态分辨率主要用于超高清输入（如报纸），避免图像过度碎片化；Gundam-M需在预训练模型基础上继续训练，平衡训练速度。

### 解码器（DeepSeek3B-MoE-A570M）

*   **架构特点**：基于DeepSeek3B-MoE，推理时激活**64个路由专家中的6个+2个共享专家**，激活参数约570M，兼顾3B模型的表达能力与500M小模型的推理效率
*   **核心功能**：通过非线性映射（\\(f\_{dec}\\)）从DeepEncoder输出的压缩视觉token重构文本表示。

训练流程与数据引擎
---------

### 数据引擎（多样化训练数据）

数据类型

内容细节

占比/规模

作用

OCR 1.0数据

30M页多语言PDF（中/英25M+其他5M，含粗/细标注）、3M页Word、10M页中/英自然场景图

占总数据70%

训练传统OCR能力（文档/场景文本识别）

OCR 2.0数据

10M页图表（线图/柱状图等，转HTML表格）、5M页化学公式（SMILES格式）、1M页平面几何图

含于OCR数据70%内

训练复杂图像解析能力

通用视觉数据

图像描述、目标检测、接地等任务数据（参考DeepSeek-VL2）

占总数据20%

保留通用视觉接口

纯文本数据

内部数据，统一处理为8192token长度

占总数据10%

保障模型语言能力

> 注：OCR 1.0细标注含2M页中/英数据，用PP-DocLayout（布局）、MinerU2.0/GOT-OCR2.0（识别）构建；小语种数据通过“模型飞轮”生成600K样本。

### 训练流程

*   **阶段1：独立训练DeepEncoder**
    *   数据：所有OCR 1.0/2.0数据+100M采样自LAION的通用数据
    *   配置：AdamW优化器，余弦退火调度器，学习率5e-5，批大小1280，训练2轮，序列长度4096
*   **阶段2：训练DeepSeek-OCR**
    *   平台：HAI-LLM平台
    *   并行策略：4段管道并行（DeepEncoder占2段，解码器占2段），20节点（每节点8张A100-40G），数据并行40，全局批大小640
    *   配置：AdamW优化器，步长调度器，初始学习率3e-5；纯文本数据训练速度90B token/天，多模态数据70B token/天

论文实验
====

核心实验性能
------

### Fox基准测试（文本token600-1300，英文文档，验证压缩-解压缩能力）

文本token范围

视觉token=64（Tiny模式）

视觉token=100（Small模式）

测试页数

精度

压缩比

精度

压缩比

600-700

96.5%

10.5×

98.5%

6.7×

7

700-800

93.8%

11.8×

97.3%

7.5×

28

800-900

83.8%

13.2×

96.8%

8.5×

28

900-1000

85.9%

15.1×

96.8%

9.7×

14

1000-1100

79.3%

16.5×

91.5%

10.6×

11

1100-1200

76.4%

17.7×

89.8%

11.3×

8

1200-1300

59.1%

19.7×

87.1%

12.6×

4

*   **关键结论**：**压缩比<10×时，精度≈97%**；压缩比20×时，精度≈60%；实际精度因输出与标注格式差异会更高。

OmniDocBench基准测试（真实文档解析，指标为编辑距离，越小越好）
-------------------------------------

模型/模式

视觉token数（有效token）

整体编辑距离

关键对比结论

GOT-OCR2.0

256

\-

DeepSeek-OCR（100token）超越它

MinerU2.0

6000+（平均）

\-

DeepSeek-OCR（<800token）超越它

DeepSeek-OCR（Small）

100

0.205

\-

DeepSeek-OCR（Base）

256（182）

0.156

\-

DeepSeek-OCR（Gundam）

795

0.083

接近SOTA性能

实用价值
----

1.  **大规模训练数据生成**：单张A100-40G显卡每日可生成**20万+页**LLM/VLM训练数据；20节点（每节点8张A100-40G）每日可生成**3300万+页**。
2.  **多场景OCR能力**：
    *   语言支持：可处理近100种语言，小语种文档支持布局/非布局输出。
    *   深度解析：支持图表（转HTML表格）、化学公式（转SMILES）、平面几何图（结构化输出）、自然图像（密集描述）的深度解析。
3.  **通用视觉理解**：保留图像描述、目标检测、接地等通用视觉能力，可通过提示激活。

总结和展望
-----

*   **总结**：
    *   为LLM**长上下文压缩**提供新范式（光学压缩，7-20×token reduction）；
    *   为LLM**记忆遗忘机制**研究提供思路（模拟人类记忆衰减，通过逐步缩小图像分辨率实现多级别压缩）；
    *   为VLM**token分配优化**提供实证指导。
*   **未来方向**：
    *   开展数字-光学文本交错预训练；
    *   进行“大海捞针”（needle-in-a-haystack）测试，验证长上下文处理能力；
    *   进一步优化光学上下文压缩的精度与效率。