---
layout: post
title: 'NLLB 与 ChatGPT 双向优化：探索翻译模型与语言模型在小语种应用的融合策略'
date: "2025-02-15T00:35:48Z"
---
NLLB 与 ChatGPT 双向优化：探索翻译模型与语言模型在小语种应用的融合策略
==========================================

本文探讨了 NLLB 翻译模型与 ChatGPT 在小语种应用中的双向优化策略。首先介绍了 NLLB-200 的背景、数据、分词器和模型，以及其与 LLM（Large Language Model）的异同和协同关系。接着列举了实战与应用的案例，包括使用 ChatGPT 生成的样本微调 NLLB-200 和使用 NLLB-200 的翻译结果作为 LLM 的 prompt 等。通过本文的研究和实践，可以为小语种翻译模型和语言模型的融合提供一定的参考和借鉴。

> 作者：来自 vivo 互联网算法团队- Huang Minghui

本文探讨了 NLLB 翻译模型与 ChatGPT 在小语种应用中的双向优化策略。首先介绍了 NLLB-200 的背景、数据、分词器和模型，以及其与 LLM（Large Language Model）的异同和协同关系。接着列举了实战与应用的案例，包括使用 ChatGPT 生成的样本微调 NLLB-200 和使用 NLLB-200 的翻译结果作为 LLM 的 prompt 等。通过本文的研究和实践，可以为小语种翻译模型和语言模型的融合提供一定的参考和借鉴。

一、NLLB 背景介绍
===========

NLLB(No Language Left Behind) \[1\]  是Meta发起的一个项目，目标是打破语言障碍。不论每个人说的是什么语言，都能使他们平等地获取信息和交流。官方对外宣称，这是全球第一个以单一模型对应多数语言翻译的设计，希望借此能够帮助更多人在社群平台上进行跨语言互动。

NLLB 计划的核心是开发一个既准确又能适应大量语言的翻译模型。这不仅包括像英语、中文和西班牙语这样的广泛使用的语言，还扩展到那些使用人数较少的语言，比如土著和地区性语言，它们通常缺乏用于高质量机器翻译的数据资源。NLLB 计划的更广泛愿景是促进不同语言障碍背后的沟通与知识获取，进而支持全球社区的教育、经济机会和社会包容性。随着这些语言的AI模型和数据集的改进，它们可以被集成到各种工具和平台中，以提供实时翻译服务、内容理解和其他与语言相关的功能，为全球用户带来好处。

NLLB 项目包含多种语言的翻译数据集，也包含基于机器学习技术的翻译模型，这些技术和模型能够更有效地从有限的数据中学习，使得那些可能几乎没有翻译数据的语言也能被翻译成其他语言。为了实现这些目标，NLLB 聚集了研究人员、语言学家和工程师，共同致力于先进的机器学习模型、数据收集和增强策略，以及改进的评估方法，以确保翻译不仅仅是字面上的，而且在文化和语境上也是准确的。

在本文中，我们主要要介绍的是 NLLB 项目中的主角，也就是 NLLB-200 这个预训练模型。这是 Meta AI 开源的一个机器翻译的大模型，支持200+语言之前的互相翻译。

1.1 NLLB-200 数据
---------------

目前，谷歌翻译能够覆盖大约130种语言的翻译，而微软则大约能够翻译110种语言。Meta AI 的目标是通过 NLLB 计划实现覆盖200多种语言的翻译。因此，除了考虑模型本身，Meta AI 还需要关注翻译模型层面之外的方面，包括数据标注与数据清洗等。

NLLB-200 项目使用的数据集名为 Flores-200，它是 Flores-101 数据集的扩展，其中包含来自世界各地的101种语言。而 Flores-200 数据集不仅覆盖了之前的语言集合，还增加了更多语言，使总数扩大到了200种。

除了收集公开可用数据和使用网络爬虫爬取的数据外，研究团队还采用回译策略来进一步扩充训练数据集。回译是一种独特的数据增强技术，涉及使用已有的机器翻译模型将目标语言的文本翻译回源语言，以创造新的、丰富的训练样本，有助于提高模型在各种语言上的表现和准确性。

由于这些数据用于机器翻译任务，因此其格式都是句子对格式，即包含第一种语言及其相应的表达，同时包含对应翻译的第二种语言的表达。数据示例如下：

    {
      "translation": 
      {
         "ace_Latn": "Gobnyan hana geupeukeucewa gata atawa geutinggai meunan mantong gata.",
         "ban_Latn": "Ida nenten jaga manggayang wiadin ngutang semeton."
      },
         "laser_score": 1.2499876022338867,
         "source_sentence_lid": 1.0000100135803223,
         "target_sentence_lid": 0.9991400241851807,
         "source_sentence_source": "paracrawl9_hieu",
         "source_sentence_url": "_",
         "target_sentence_source": "crawl-data/CC-MAIN-2020-10/segments/1581875144165.4/wet/CC-MAIN-20200219153707-20200219183707-00232.warc.wet.gz",
         "target_sentence_url": "https://alkitab.mobi/tb/Ula/31/6/\n"
    }

1.2 NLLB-200 分词器
----------------

在收集并清理了大量数据之后，NLLB-200 模型使用了 一些方法来促进模型学习到语言不可知（language-agnostic）的表示，从而能够用于多种语言之间的转换，而不仅仅是针对特定的单一语言。其中，最关键的要素之一是设计了一个能够处理所有语言的分词器。在这个分词器中，使用了一个共享的词汇表，采用共享的 SentencePiece 词汇表来编码所有涉及的语言的输入和输出序列。

这意味着相同的子词单元可以在不同的语言之间共用，有助于模型学习到跨语言共通的词汇表示。该 SentencePiece 模型是在采样的1亿个句子上进行训练的。为了确保低资源语言能够得到充分的表示，NLLB 团队对高资源语言进行了下采样，同时对低资源语言进行了上采样，最终训练出一个词表大小高达256206的模型（相比之下，Llama/Llama2/Llama3的词表大小为32000）。

要使用 NLLB 的分词器，我们可以直接调用 Transformers，里面有一个 NllbTokenizer，接收的参数为：

      - vocab_file (str)
      - bos_token (str, optional, defaults to "<s>")
      - eos_token (str, optional, defaults to "</s>")
      - sep_token (str, optional, defaults to "</s>")
      - unk_token (str, optional, defaults to "<unk>")
      - pad_token (str, optional, defaults to "<pad>")
      - mask_token (str, optional, defaults to "<mask>")
      - tokenizer_file (str, optional)
      - src_lang (str, optional)
      - tgt_lang (str, optional)
      - sp_model_kwargs (Dict[str, str])

    from transformers import NllbTokenizer
    
    tokenizer = NllbTokenizer.from_pretrained(
      "nllb/nllb-200-distilled-1.3B", src_lang="eng_Latn", tgt_lang="fra_Latn"
    )
    example_english_phrase = " UN Chief Says There Is No Military Solution in Syria"
    expected_translation_french = (
      "Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie."
    )
    inputs = tokenizer(
      example_english_phrase, text_target=expected_translation_french, return_tensors="pt")

1.3 NLLB-200 模型
---------------

在大型分词器准备好之后，每一个句子都能够合理地被拆分成若干个 token，并且找到对应的 token id。那接下来就是进入模型训练的部分。NLLB-200 模型使用了 Transformer 架构来学习不同语言之间的表示。

*   **共享的模型参数**：在一个统一的神经网络架构中对所有语言使用共享的参数。这强制模型学习到不特定于任何单个语言的特征表示。
    
*   **交叉语言迁移学习**：通过在相似的语言对训练时共享知识，可以使模型在不同语言间转移学到的模式，以利于语言不可知特征的发展。
    
*   **多任务学习**：在训练中包含任务不局限于翻译，比如语言建模、降噪自编码器或其他任务，这也有助于模型捕捉语言共通性。
    

多语种机器翻译模型能够学习不同语言之间的转换，而不必为每一个语言对分别建立独立的模型。这样的方法减少了模型的数量和维护成本，并允许模型在不同语言之间分享知识，通常有助于改进低资源语言的翻译性能。

1.4 NLLB-200 蒸馏
---------------

![](https://static001.geekbang.org/infoq/fe/fee8f138c69f48fba005e09763e44115.png)

【表1】：  NLLB-200及蒸馏版本的参数量

开源的 NLLB-200 的 MoE (Mixture of Experts) 版本具备高达545亿个参数，要在我们的设备上运行如此规模的模型确实存在一定难度，对计算资源的要求较高。然而，开发者同时也提供了一个经过蒸馏的精简版本。这个经蒸馏处理的模型仅含13亿参数，相较于原版更为轻量，降低了对硬件资源的依赖，使得在资源受限的环境中运行变得可行。

二、NLLB 与 LLM
============

NLLB 作为一个翻译任务，和 LLM 一样，都可以表述为序列到序列任务，都属于生成式任务。那么，NLLB 和 LLM 有哪些异同点呢？二者是不是相互冲突？二者能不能有协同合作的机会？

2.1 NLLB 与 LLM 模型异同
-------------------

NLLB 和 LLM 都使用 Transformer 结构作为基础。不同之处在于，NLLB 采用标准的编码器-解码器架构，而基于 GPT 的模型则仅使用了解码器（而 BERT 使用的是编码器）。

在 Transformer 模型中，编码器负责理解和提取输入文本中的相关信息。这个过程通常包括处理文本序列化形式（如单词或字符），并利用自注意力机制来理解文本中的上下文关系。编码器的输出是输入文本的连续表示，通常称为嵌入。这种嵌入包含了编码器从文本中提取的所有有用信息，并以模型可处理的高维向量格式表示。然后，这个嵌入会传递给解码器。解码器的任务是根据从编码器接收到的嵌入生成翻译后的文本（目标语言）。解码器同样使用自注意力机制和编码器-解码器注意力机制来生成翻译文本。

LLMs 中有的是只有编码器（encoder-only），有的是只有解码器（decoder-only），而有的是两者混合（encoder-decoder hybrid）。这三种类型都属于 Seq2Seq（序列对序列）模型。尽管名义上是只有编码器，但实际上 LLMs 仍能够解码一些文本和标记，因此也可以看作是解码器。然而，与 decoder-only 和编码器-解码器类型不同，encoder-only 类型的 LLMs 更注重理解输入内容，并生成针对特定任务的输出，而不像前两种类型那样具有自回归特性。自回归表示输出内容是基于先前生成的标记，逐个标记生成上下文理解。总的来说，encoder-only 类型更擅长分类任务；编码器-解码器类型擅长生成强烈依赖于输入的输出，如翻译和文本总结；而 decoder-only 类型则适用于各种Q&A等任务。

2.2 NLLB 与 LLM 目标不同
-------------------

现在的大规模语言模型也拥有很强的翻译能力，但是 LLM，比如 ChatGPT，擅长翻译更高资源的语言，而 NLLB-200 则是对低资源语言的翻译做了很多的改善。NLLB-200 和 LLM 在翻译能力上的差异主要体现在它们的专注点和训练目标上：

1.专业度：

> *   NLLB-200 被设计和优化用来进行机器翻译，支持200多种语言，特别包括很多低资源语言。这意味着它在机器翻译方面应当展现出较高的专业度和准确率，特别是在那些通常没有足够数据支持的语言对之间的翻译。
>     
> *   LLM 虽然也能进行一定的翻译任务，但它的主要目标是进行自然语言理解和生成，为了创建流畅和自然的对话。因此，它的翻译能力可能不如明确为翻译目的而训练的模型，如 NLLB-200。
>     

2.覆盖范围：

> *   NLLB-200 专门致力于包括广泛的语言，确保即使是低资源语言也有良好的翻译支持。
>     
> *   LLM 支持多语言处理，但它的训练数据集可能不如 NLLB-200 那样全面覆盖多种语言，尤其是对于低资源语言。
>     

3.适用性：

> *   NLLB-200 专为翻译而设计，适用于需要精确翻译的场景。
>     
> *   LLM 虽然可以完成简单的翻译任务，但它更适用于需要理解上下文、生成人性化回答的对话场景。
>     

总之，NLLB-200 基于其训练和设计被期望在机器翻译方面表现优异，而 LLM 则更侧重于创建高质量的对话体验。如果需要进行复杂或专业的翻译工作，特别是跨越稀有语言的翻译， NLLB-200 可能是更好的选择；而在需要与人类用户进行自然交流的场合，则可能会偏向使用 LLM。

2.3 NLLB 与 LLM 如何协同
-------------------

NLLB-200 和 LLM 在翻译上的擅长点不同，然而，LLM 的强大自然语言理解和生成能力可以与 NLLB-200 独特的翻译技术互惠互利：

> *   通过利用 LLM 所特有的生成能力，可以创造更精确的领域特定术语数据集 \[2\] 。这种针对性强的数据集能够进一步用于对 NLLB-200 进行细致的调整，从而在特定的技术或专业领域提升其翻译效果。
>     
> *   当下的大型语言模型（LLM）主要以主流语言为核心，这一现象得益于英语、中文等语言的语料在获取便利性、数据质量和量级方面的明显优势。针对语料资源较为稀缺的小语种，比如55种非洲语言，这种中心化趋势意味着LLM在这些语种上不如在主流语种上表现得那么出色，因此可以依赖 NLLB-200 的翻译能力，将小语种 prompt 翻译成主流语言，再去使用 LLM \[3\]。
>     

这两种结合方案我们在后面将用实战的形式来展开讲述。

三、实战与应用
=======

3.1 使用 LLM 生成针对性数据微调 NLLB-200
-----------------------------

目前，外销应用商店主要运营国家包括印度、印尼、泰国、马来西亚、菲律宾和越南。此外，还有几十个用户量较少的国家和地区也在使用。由于用户输入的查询语种繁多，2023年初的评测结果显示，各个国家都存在着本地语言查询导致的 badcase。由于少数国家的用户行为数据较少，基于用户行为的模型无法很好地发挥作用。另外，许多应用在国外都采用英文名称，因此本地语言的查询与英文应用在仅靠文本匹配时会出现不匹配的情况，导致无法返回用户想要的结果。为了解决这一问题，我们需要引入翻译功能，将各运营国家缺少召回的本地语言 query 统一翻译成英语，然后进行结果召回。

因为 NLLB-200 模型原始的训练数据来源于各种文章，其中可能包含大量修饰性词语，可能导致文本召回抓取不到关键信息。因此，为了在外销商店搜索中使用 NLLB-200 模型，我们需要对模型进行简单的微调。

3.2 LoRA 微调：方案介绍
----------------

大型语言模型参数庞大（例如，GPT-3 的参数量为 1750 亿），在应用到下游任务时，需要相当高的算力来微调全部参数。为了节省成本，一些参数高效（Parameter Efficient）的微调方法被提出，旨在仅训练少量参数使模型适应到下游任务。

> LoRA（Label-Oriented Attentive Fine-tuning）微调是一种被广泛使用的方法，其通过引入标签相关的关注机制，帮助模型更好地专注于与所预测标签相关的信息，从而提高了模型对关键信息的学习和提取能力。这种方法在提升模型性能和推理能力方面取得了显著效果。因此，将 LoRA 微调引入到模型训练中可以帮助模型更好地理解和学习标签相关的文本信息，从而提高模型的预测准确性和泛化能力。目前，对于计算机视觉领域的稳定扩散（stable diffusion）和自然语言处理领域的  Llama，LoRA 已成为主流的微调方法。

LoRA（Label-Oriented Attentive Fine-tuning）提出了一种独特的微调方法：固定预训练模型参数并在原本的权重矩阵旁路添加低秩矩阵的乘积作为可训练参数，用以模拟参数的变化量。这一方法旨在通过细微的调整来适应下游任务的需求。

![](https://static001.geekbang.org/infoq/22/22255fe40a773c207332899c2848f7ee.png)

【图1】：LoRA微调方法

具体而言，左侧的W代表了 LLM 的预训练参数，在常见的完成 fine-tuning 当中，这个W参数会随着训练而更新为W+ΔWW+ΔW，其中ΔWΔW可以被拆解成ΔW=BAΔW=BA。

因此h=W+ΔW=W+BAh=W+ΔW=W+BA。

在前向计算当中，hx=(W+ΔW)x=(W+BA)xhx=(W+ΔW)x=(W+BA)x。

> 也就是说，左侧的 WW 代表了 LLM 的预训练参数，被固定不变。而右侧则包括两个矩阵：AA 和 BB，它们根据下游任务进行微调。在获得结果之后，将 WW 和 BABA 合并，输出新的模型权重。另一种方法是左右侧的参数不进行合并，而是在推理时同时载入两个模型同步进行计算，然后将计算结果相加。这一方法的创新之处在于其独特的参数调整方式，目标是实现对模型权重的微调而不需要调整大量的参数，从而提高了模型在特定任务上的适应性和性能表现。对于深度学习领域而言，LoRA 的应用已显示出广阔的应用前景和潜力。

此外，现在我们不再需要自行开发微调算法了，因为 Hugging Face 已经开源了一个 PEFT 库 \[4\] ，其中包括了许多高效微调的方法可供直接调用。从代码上看，PEFT 支持多种高效微调方式，而且 Hugging Face 已经为常用的参数微调方法提供了封装，直接调用即可使用。PEFT 库中封装好的方法有如下：

    class PeftType(str, enum.Enum):
      """
      Enum class for the different types of adapters in PEFT.
    
      Supported PEFT types:
      - PROMPT_TUNING
      - MULTITASK_PROMPT_TUNING
      - P_TUNING
      - PREFIX_TUNING
      - LORA
      - ADALORA
      - BOFT
      - ADAPTION_PROMPT
      - IA3
      - LOHA
      - LOKR
      - OFT
      - POLY
      - LN_TUNING
      """
      PROMPT_TUNING = "PROMPT_TUNING"
      MULTITASK_PROMPT_TUNING = "MULTITASK_PROMPT_TUNING"
      P_TUNING = "P_TUNING"
      PREFIX_TUNING = "PREFIX_TUNING"
      LORA = "LORA"
      ADALORA = "ADALORA"
      BOFT = "BOFT"
      ADAPTION_PROMPT = "ADAPTION_PROMPT"
      IA3 = "IA3"
      LOHA = "LOHA"
      LOKR = "LOKR"
      OFT = "OFT"
      POLY = "POLY"
      LN_TUNING = "LN_TUNING"
      VERA = "VERA"

根据 PEFT 官方提供的指南，可以看出 LoRA 已经覆盖了所有模态和模型，因此属于一种通用的技巧，掌握了这个方法就可以应用于多个场景。正是因为如此，我们选择使用这种方法来进行外销商店搜索的翻译微调。

从源码来看，我们不需要自己编写代码，只需调用 LoRA 就可以直接用于以下几大类任务：

    class TaskType(str, enum.Enum):
        """
        Enum class for the different types of tasks supported by PEFT.
        
        Overview of the supported task types:
        - SEQ_CLS: Text classification.
        - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling.
        - CAUSAL_LM: Causal language modeling.
        - TOKEN_CLS: Token classification.
        - QUESTION_ANS: Question answering.
        - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features
            for downstream tasks.
        """
    
        SEQ_CLS = "SEQ_CLS"
        SEQ_2_SEQ_LM = "SEQ_2_SEQ_LM"
        CAUSAL_LM = "CAUSAL_LM"
        TOKEN_CLS = "TOKEN_CLS" 
        QUESTION_ANS = "QUESTION_ANS"
        FEATURE_EXTRACTION = "FEATURE_EXTRACTION"

3.3 NLLB 模型微调：数据
----------------

因为我们希望将 query 翻译成英语后，能够通过文本匹配与英语 APP 进行匹配，因此，我们希望翻译的结果也可以尽量接近英语APP的命名方式。比如：

尽量少连词、修饰词等非实体，比如，“The digital wallet.“就有“the”这种无用的连词，可能引入噪音，而翻译成“digital wallet”，则更便于文本召回。

在一词多义中，尽量选择 APP 常用的词汇，比如同样表示“无网状态“，就应该翻译成“offline”而不是”netless”。

为了实现这个效果，我们需要首先针对性的准备一批数据用于微调。数据生成的工作我们采用了 ChatGPT（基于GPT 3.5的版本）：

> 1.选择海外运营国家中曝光 top100 的 APP；
> 
> 2.输入 APP 主标签名字进行翻译，让 ChatGPT 将 APP 名称翻译成印地语、泰语、越南语（与英语差别较大的语言），如【表2】的示例。构建成“小语种-英语”的翻译对，进行模型微调。

![](https://static001.geekbang.org/infoq/be/beb2ec15cb24691095abd1443ca3f875.png)

【表2】：训练数据示例

3.4 NLLB模型微调：训练
---------------

### 3.4.1 初始化一个预训练的模型

    from transformers import AutoModelForSeq2SeqLM
    model = AutoModelForSeq2SeqLM.from_pretrained("nllb/nllb-200-distilled-1.3B")

### 3.4.2 设置 lora 参数，通过 get\_peft\_model() 获取需要训练的 PeftModel

    from peft import get_peft_model, LoraConfig, TaskType
    
    peft_config = LoraConfig(
      task_type=TaskType.SEQ_2_SEQ_LM,
      inference_mode=False,
      r=8,
      lora_alpha=16,
      lora_dropout=0.1,
      target_modules=["q_proj", "k_proj", "v_proj", "out_proj", "fc_in", "fc_out", "wte"],
      )
    lora_model = get_peft_model(model, peft_config)
    lora_model.print_trainable_parameters()
    # trainable params: 4718592 || all params: 1375356928 || trainable%: 0.3430812688646303

### 3.4.3 按照平常的训练方式进行训练

训练参数量降至原先的0.34%，原本需要几块 GPU 并行训练的参数，现在可以降低至单卡完成，甚至可以在 CPU 机器上完成。训练完毕，将 PeftModel 保存下来。

    lora_model.save_pretrained('nllb_lora_model')

3.5 NLLB 模型微调：推理
----------------

因为前面我们仅仅保存了 PeftModel 模型，因此，在推理过程中，我们需要将 base 模型和 lora 参数同时载入进行推理。

### 3.5.1 初始化前面的 base 模型

    from transformers import AutoModelForSeq2SeqLM
    model = AutoModelForSeq2SeqLM.from_pretrained("nllb/nllb-200-distilled-1.3B")

### 3.5.2 通过获取保存的 lora 参数，获取 PeftModel 结果

    from peft import PeftConfig, PeftModel
    config = PeftConfig.from_pretrained("nllb_lora_model")
    lora_model = PeftModel.from_pretrained(model, "nllb_lora_model")

### 3.5.3 按照平常的推理方式输出结果

从自身实验结果来看，LoRA 微调速度很快，需要的计算资源小，同时产生的结果也很不错，大多数 case 基本上可以达到和全量参数微调类似的效果。

    lora_model.generate(
      **encoded_text, forced_bos_token_id=tokenizer.lang_code_to_id["eng_Latn"]
    )

从开源的测试来看，LoRA 基本上都是可以训练不足1%的参数，完成接近全量参数微调的效果。

3.6 NLLB 模型微调前后效果对比
-------------------

**翻译同时能够做源语言的简单纠错与补全**

比如说，用户的输入为免的游戏，实际表述应该是免费的游戏，经过 NLLB-200 模型翻译后，可以得到正确的英语free games。从而可以方便进行后续的文本、标签、语义等召回。

**无用的连词减少，能够得到更适合应用搜索的简洁 query**

原始版本的翻译是基于文章的，所以会有很多连词等等，让整个句子读起来更通顺更优美（更长）。这种方式并不利于应用搜索，因为应用名称本身就很短，普通的 query 也是一两个短词，所以翻译出一整个长句子，反而不利于召回到正确的结果。比如下面的 case：

![](https://static001.geekbang.org/infoq/60/602bd1805c18d0ca29ae0a4370885791.png)

拿第一个结果举例，如果翻译出来 Playing games of chat，那就会有很多 Party Games 的召回，和用户的需求比较不符。原因是 query 里面包含比较多词语，因此就会带来一定的噪音，使得文本匹配的时候，匹配到一些并不关键的词语。

![](https://static001.geekbang.org/infoq/ea/eaf067bb5daebc6074094483b3099d5b.png)

因此需要模型尽量翻译出靠近 APP 的名字，如果是翻译出 chat game ，那整个 query 就简洁了不少，因此噪音减少之后，召回的结果相关度也明显增加，甚至可以通过类似标签找到 Prank Master 一类的结果。

![](https://static001.geekbang.org/infoq/b4/b4d75169008decfd35678fb9395907b0.png)

**让 query 更符合 APP 命名**某类应用应该是有一类应用常用的命名方式的，比如仿真模拟类游戏，一般就叫做什么 simulation 或者是 simulator，词典的话，一般就是 dictionary 而不是 word，教发音的 APP 名字一般是 spell 而不是 syllable，因此，在下游数据 finetune 之后，可以让翻译的 query 更加靠近 APP 本身命名。

![](https://static001.geekbang.org/infoq/e6/e699edc2680d4f12deafc55f07dd0d97.png)

比如，翻译成 A mocking game，事实上很少有应用或者游戏的名字带有 mocking 的，因此文本召回使用的主要关键词的是 game，那当然就很难满足用户的需求。

![](https://static001.geekbang.org/infoq/10/10feeccf915e5f6cc46db91fbb4c2f7d.png)

但是如果能够翻译出来 simulation game，那就能找到学校模拟、史莱姆模拟等一系列模拟游戏。

![](https://static001.geekbang.org/infoq/fc/fcaa8f7f1a1adf4e3b736c53db4ca8f6.png)

四、使用 NLLB-200 翻译结果作为 LLM 的 prompt
=================================

毫无疑问，无论是 GPT3.5 还是 GPT4，都是以英文为中心的大规模语言模型。而文心一言、通义千问以及豆包和 kimi，则是以中文为中心。

大规模语言模型的训练是极其耗费资源的，所以像是非洲贫穷国家，很难出现一款以自己的语言为中心的大规模语言模型。那是不是这些国家的人，如果不会中文和英文，就不能享受到大规模语言模型的便利了呢？答案当然不是的，现在的一部分研究表明，将小语种的 prompt 翻译到英文之后，ChatGPT 或者 Llama 就可以迅速给出对应的准确结果。

4.1 直接 prompt 和翻译 prompt 的实践
----------------------------

我们尝试一个很小众的语种--毛利语（mri）的问题。一般而言，我们会根据语种统计，比如 Common Crawl，把高于1%占比的语言看作高资源语种，其他看作低资源语种。根据统计，毛利语的语言占比为0.0009%，是当之无愧的小语种。接下来，我们就用这个低资源语种来做prompt的实验。

输入：

> Ko te hanga i te koroka me 2 raka o te kakahu muka kahurangi me te haurua o te nui o te kakahu muka ma. E hia katoa nga momo kakahu ka whakamahia kia rite ki te whakatakotoranga: "Whakautu:".

**ChatGPT** 输出：

> Kia taea te hanga i te koroka me 2 raka o te kakahu muka kahurangi me te haurua o te nui o te kakahu muka ma, ka taea te whakamahia i nga momo kakahu katoa ka rite ki te whakatakotoranga "Whakautu: ". Ka taea te whakamahi i nga momo kakahu penei i te kakahu muka, te kakahu pukapuka, te kakahu rarau, me etahi atu. Ko te take he kia rite te whakamahi me te whakarite i te whakatakotoranga ki te hiahia o te tangata.

但是，这个回答从格式上看应该就不对。我们把输出翻译回中文来做理解，可以看到，这个回答的意思是:

> 为了能够用2锁蓝色纤维布和一半数量的白色纤维布制作一件斗篷，所有类型的布料都可以按照“答案：”的格式使用。可用于纤维布、书布、鞋布等不同类型的衣服。原因就是要根据用户的需求来使用。

这个回答是完全驴唇不对马嘴的回答。

同时，我们把这个问题输入到两个以中文语料为中心的模型–通义千问和 ChatGLM2 里面看一下。可以看到，以中文语料为中心的两个模型更加没办法理解和处理这个问题。

**通义千问 qwen-max-1201输出**：

> Ko te whakautu i to kōrero, heoi anō tēnei: "Tau Arapi" kua whakamahia hei tauira mō te whakatakotoranga o ngā momo kakahu. Ko te koroka me 2 raka o te kakahu muka kahurangi me te haurua o te nui o te kakahu muka ma, ka whakamahia kia rite ki te whakatakotoranga o "Whakautu: ". Nā reira, ko te whakatakotoranga o ngā momo kakahu nei e rua ake atu ana ki te tauira o "Whakautu: ".

**ChatGLM2-6B** 输出：

> I'm sorry, but it appears that the text you provided doesn't have a clear question or request for me to respond to. Could you please provide more context or clarify what you would like to know or talk about?

但是如果我们将它翻译成中文或者英语呢？通过翻译软件，我们可以得到这个 prompt 直接翻译成中文是:

> 制作一件袍子需要2 匹蓝色纤维布料和这个数量一半的白色纤维布料。它一共需要用掉多少匹布料，您的最终答案的格式应为："答案: <阿拉伯数字>".

对应的输出 **ChatGPT** 输出：

> 製作一件袍子需要 2 匹藍色纖維布料和這個數量一半的白色纖維布料。它一共需要用掉多少匹布料？答案：3。

虽然输出是繁体字，但是不影响我们理解，这个回答结果是正确的。同理，我们也把这个prompt输入到通义千问和 ChatGLM2 里面看一下。可以看到，此时通义千问已经可以回答正确了。

**通义千问 qwen-max-1201** 输出：

> 答案: 3 匹

**ChatGLM2-6B** 输出：

> 答案:4.5

翻译成英语，则是:

> To make a robe, two pieces of blue fiber and half of that amount of white fiber are needed. How many pieces of fabric does it take to make? You should format your final answer as "Answer:".

**ChatGPT** 输出是：

> To make the robe, it takes a total of 3 pieces of fabric.Answer: 3

同理，我们也把这个 prompt 输入到通义千问和 ChatGLM2 里面看一下。可以看到，此时通义千问依旧是可以正确回答的。ChatGLM 的回答错误可能是因为6B模型在推理上的不足。

**通义千问 qwen-max-1201** 输出：

> To make a robe, we need 2 pieces of blue fiber and half of that amount of white fiber. Half of 2 is 1, so we need 1 piece of white fiber.In total, we need 2 pieces of blue fiber + 1 piece of white fiber = 3 pieces of fabric.Therefore, the answer is: Answer: 3.

**ChatGLM2-6B** 输出：

> Answer: 7

我们可以看到，一旦毛利语的 prompt 先通过翻译，变成了高资源语种，比如中文或者英语，模型就能给出正确的答案，同时格式也能够满足我们的要求。

4.2 直接 prompt 和翻译 prompt 的效果
----------------------------

![](https://static001.geekbang.org/infoq/a2/a2b3584907e12e487f2b7e1b53d431ee.png)

【图2】：各语种直接 prompt &统一翻译成英语 prompt 在 ChatGPT 和 Llama-2-70B-Chat 的表现

在论文中，研究人员提到，高资源语种翻译成英语之后再输入大模型，可能并不会有很大的提升，甚至还可能带来噪音，影响效果。但是，低资源语种，如果能够先翻译成高资源语种，然后再输入现在以高资源语种为中心的大模型，往往就能够取得不错的效果。

我们从【图2】也可以看出来，对于中文这种高资源语言，使用翻译之后，在 ChatGPT 中效果还不如直接用中文 prompt；而在 Llama 中，将中文翻译成英语，然后再输入 Llama，表现则要优于中文 prompt，这可能是因为 Llama 训练语料中的中文语料数量要远远 ChatGPT 中的中文语料数量。

五、总结与展望
=======

使用 LLM 生成的语料来微调特定任务的 NLLB 模型是一个常见的做法，许多场景都在尝试利用 LLM 的知识来更好地适应自己的任务。然而，LLM 生成的知识可能存在误导性，有时甚至可能与事实不符，因此在使用这些语料时，需要进行机器筛选或人工判断。

虽然机器翻译能提升以英语或中文等高资源语言为中心的 LLM 在多语言 NLP 任务中的性能，但并非适用于所有情境。对于需要深度语言理解及考虑文化差异的任务，使用母语提示常常更具有潜力，因为母语能更准确地捕捉文化和语言的微妙差别。因此，开发更多种类的多语言 LLM 是非常必要的。

**【参考资料】**

\[1\] Costa-jussà M R, Cross J, Çelebi O, et al. No language left behind: Scaling human-centered machine translation\[J\]. arXiv preprint arXiv:2207.04672, 2022.

\[2\] Moslem Y, Romani G, Molaei M, et al. Domain terminology integration into machine translation: Leveraging large language models\[C\]//Proceedings of the Eighth Conference on Machine Translation. 2023: 902-911.

\[3\] Liu C, Zhang W, Zhao Y, et al. Is translation all you need? a study on solving multilingual tasks with large language models\[J\]. arXiv preprint arXiv:2403.10258, 2024.

\[4\] PEFT(https://huggingface.co/docs/peft/main/en/index)

分享 vivo 互联网技术干货与沙龙活动，推荐最新行业动态与热门会议。