---
layout: post
title: '手把手教你如何用yolo算法进行运动监测'
date: "2025-09-11T00:39:41Z"
---
手把手教你如何用yolo算法进行运动监测
====================

![手把手教你如何用yolo算法进行运动监测](https://img2024.cnblogs.com/blog/3687401/202509/3687401-20250910110941438-1171511522.png) 本文介绍了一个基于YOLO算法的运动监测项目，通过识别人体17个关键关节节点，实现对俯卧撑、深蹲等动作的自动计数。项目使用Python+OpenCV+Ultralytics解决方案，核心原理是通过分析关节弯曲角度变化（如俯卧撑监测肩膀、肘部、手腕的角度）来识别动作状态并计数。文章详细解析了代码实现流程，包括视频读取、AIGym对象初始化、帧处理循环等，并展示了不同动作（深蹲、压腿、高抬腿）的监测示例。该项目适合计算机视觉和体育健康领域的爱好者，可根据实际需求调整关键点、模型和角度阈值参数。

​

视频演示
----

[https://www.bilibili.com/video/BV1wnHyzPEvM](https://www.bilibili.com/video/BV1wnHyzPEvM "手把手教你如何用yolo算法进行运动监测")

大家好，我是Coding茶水间。

今天分享一个基于YOLO算法的运动监测项目，通过识别人体关键关节节点，实现对俯卧撑、深蹲、压腿和高抬腿等动作的自动计数。

视频演示中，我们可以看到实时标注关节点、计算弯曲角度，并统计动作次数。这是一个简单实用的AI应用，适合计算机视觉和体育健康领域的爱好者。

项目效果演示
------

在视频开头，我们看到一个俯卧撑监测的示例：程序用红色圆圈标注肩膀、肘部和手腕，并显示弯曲角度。随着角度变化，动作次数实时更新，最终计数为4次。

核心原理：人体关键关节节点识别
---------------

YOLO算法将人体关节分为17个关键节点，包括眼睛、鼻子、耳朵、肩膀、肘部、手腕、髋关节、膝盖和脚踝等。通过识别这些节点的位置，分析关节间的弯曲角度，来判断动作姿态和计数。

对于俯卧撑，我们重点识别左臂的肩膀（节点5）、肘部（节点7）和手腕（节点9）。

弯曲角度如152度时，表示伸展状态；

角度变化用于计数。

类似地，其他动作选择相应节点：

*   深蹲：肩膀（6）、髋关节（12）、膝盖（14）
*   压腿：髋关节（11）、膝盖（13）、脚踝（15）
*   高抬腿：髋关节（12）、膝盖（14）、脚踝（16）

原理是通过三个核心节点的弯曲情况，监测伸展（up）和收缩（down）状态。

一旦完成一个周期，计数加1。

代码实现
----

我们使用Python、OpenCV和Ultralytics的solutions包。核心是AIGym类，用于运动监测。以下是完整代码（以俯卧撑为例）：

    import cv2
    
    from ultralytics import solutions
    
    # 读取视频
    cap = cv2.VideoCapture("Pushups.demo.video.mp4")
    # 判断视频是否读取成功
    assert cap.isOpened(), "Error reading video file"
    
    # 初始化AIGym
    gym = solutions.AIGym(
        show=True,  # 展示效果
        kpts=[5, 7, 9],  # 处理的关键点
        model="yolo11x-pose.pt",  # 模型路径
        # up_angle = 145,
        down_angle = 100
    )
    
    # 处理视频
    while cap.isOpened():
        # 读取视频帧数
        success, im0 = cap.read()
    
        # 判断是否读取成功
        if not success:
            print("Video frame is empty or processing is complete.")
            break
        
        # 处理视频帧数
        results = gym(im0)
    
    cap.release()
    cv2.waitKey(0)
    cv2.destroyAllWindows()  # destroy all opened windows

整个代码是用Python实现的YOLO运动监测程序，主要用于处理视频帧，识别人体关键点，并计数动作（如俯卧撑）。

它依赖OpenCV（cv2）和Ultralytics的solutions模块。

#### 导入模块

    import cv2
    
    from ultralytics import solutions

*   **解释**：
    *   import cv2：导入OpenCV库，用于视频读取、处理和显示。
    *   from ultralytics import solutions：从Ultralytics库导入solutions模块，其中包含AIGym类，用于AI运动监测（基于YOLO姿态估计）。 这部分是代码的基础，确保所需工具可用。

#### 读取视频并检查

    #读取视频
    cap = cv2.VideoCapture("Pushups.demo.video.mp4")
    #判断视频是否读取成功
    assert cap.isOpened(), "Error reading video file"

*   **解释**：
    *   cap = cv2.VideoCapture("Pushups.demo.video.mp4")：打开指定视频文件（这里是俯卧撑演示视频）。cap是一个视频捕获对象，用于逐帧读取。
    *   assert cap.isOpened(), "Error reading video file"：检查视频是否成功打开。如果失败，抛出错误信息并停止程序。这是一个简单的错误处理，确保视频文件存在且可读。

#### 初始化AIGym对象

    # 初始化AIGym
    gym = solutions.AIGym(
        show=True,  # 展示效果
        kpts=[5, 7, 9],  # 处理的关键点
        model="yolo11x-pose.pt",  # 模型路径
        # up_angle = 145,
        down_angle = 100
    )

*   **解释**：
    *   创建AIGym实例，用于运动监测。
    *   show=True：启用实时显示处理后的视频帧（包括标注关键点和角度）。
    *   kpts=\[5, 7, 9\]：指定人体关键点（基于YOLO的17个姿态节点）。这里是左侧肩膀(5)、肘部(7)、手腕(9)，适合俯卧撑监测。
    *   model="yolo11x-pose.pt"：加载YOLOv11x姿态模型（x版精度高，但计算密集；可换成n/m版以加速）。模型文件需提前下载或通过Ultralytics自动获取。
    *   up\_angle = 145：伸展角度阈值（默认145度，表示动作伸展状态）。
    *   down\_angle = 100：设置收缩角度阈值（小于此值视为收缩状态）。用于判断动作周期（up到down再回up计数一次）。这里调整为100度，可能适应特定动作如压腿。 这部分是核心配置，定义了监测的关节和阈值。

#### 处理视频帧的循环

    # 处理视频
    while cap.isOpened():
        # 读取视频帧数
        success, im0 = cap.read()
    
        # 判断是否读取成功
        if not success:
            print("Video frame is empty or processing is complete.")
            break
        
        # 处理视频帧数
        results = gym(im0)

*   **解释**：
    *   while cap.isOpened()：循环处理视频，直到结束或关闭。
    *   success, im0 = cap.read()：读取一帧视频。success是布尔值（True表示读取成功），im0是当前帧图像。
    *   if not success: ... break：如果读取失败（视频结束），打印消息并退出循环。
    *   results = gym(im0)：将当前帧传入AIGym处理。它会使用YOLO检测关键点、计算角度、更新计数，并返回结果（可能包括标注后的图像）。由于show=True，会自动显示处理效果。 这是一个主循环，实现实时视频处理。

####  释放资源和等待

    cap.release()
    cv2.waitKey(0)
    cv2.destroyAllWindows()  # destroy all opened windows

*   **解释**：
    *   cap.release()：释放视频捕获对象，关闭视频文件。
    *   cv2.waitKey(0)：等待按键输入（0表示无限等待），防止窗口立即关闭。用户可按任意键退出。
    *   cv2.destroyAllWindows()：关闭所有OpenCV窗口。 这部分是清理操作，确保程序优雅结束。

### 整体运行逻辑

*   程序读取视频，初始化YOLO监测对象，逐帧处理（检测关键点、计算角度、计数），实时显示结果。
*   适用于俯卧撑（通过肘部弯曲计数），可修改kpts、model和角度阈值扩展到其他动作（如深蹲）。
*   注意：需确保YOLO模型文件存在；如果电脑配置低，大模型如x版可能卡顿。

不同动作示例
------

### 深蹲监测

更换kpts=\[6,12,14\]，视频为深蹲文件。效果：完美识别角度变化，计数4次。

### 压腿监测

kpts=\[11,13,15\]，初始用"yolo11n-pose.pt"可能识别不准，换"yolo11x-pose.pt"。调整down\_angle=100

### 高抬腿监测

kpts=\[12,14,16\]，用"yolo11m-pose.pt"。视频中动作9次，程序准确计数。

注意：根据动作实际情况，动态调整节点、模型和角度阈值。

电脑配置低时，用小模型；精度不足时，用大模型。

有问题随时交流。更多AI教程，关注Coding茶水间。

​