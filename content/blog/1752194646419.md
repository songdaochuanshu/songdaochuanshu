---
layout: post
title: '通过 .NET Aspire 使用本地 AI 模型'
date: "2025-07-11T00:44:06Z"
---
通过 .NET Aspire 使用本地 AI 模型
=========================

引言
--

在当今快速发展的 AI 领域，开发人员经常需要在本地环境中实验和测试 AI 模型，然后再将其部署到云端。使用本地 AI 模型不仅能够节省云资源成本，还能提供更快的迭代速度和更好的隐私保护。本文将介绍如何利用 .NET Aspire 框架结合 Ollama 在本地运行 AI 模型，并通过 Microsoft.Extensions.AI 抽象层实现从本地开发到云部署的无缝过渡。

正文内容
----

### 在 .NET Aspire 中设置 Ollama

Ollama 是一个出色的工具，它允许开发者在本地运行大型语言模型。要在 .NET Aspire 应用程序中使用 Ollama，最简便的方法是使用 .NET Aspire 社区工具包中的 Ollama 托管集成。

首先，我们需要通过 NuGet 安装 Ollama 托管集成包。可以通过以下命令行将其添加到应用程序主机项目中：

    dotnet add package CommunityToolkit.Aspire.Hosting.Ollama
    

安装完成后，我们可以在 Program.cs 文件中配置 Ollama 托管集成。以下是一个典型的配置示例：

    var ollama = builder.AddOllama("ollama")
                       .WithDataVolume()
                       .WithOpenWebUI();
    

这段代码使用了 `AddOllama` 扩展方法将容器添加到应用程序主机。`WithDataVolume()` 方法确保了模型数据在容器重启后仍然保留，避免了每次启动时重新下载大量数据的麻烦。而 `WithOpenWebUI()` 则添加了一个网页界面，让我们可以在应用程序之外与模型进行交互。

### 运行本地 AI 模型

配置好 Ollama 服务器后，我们需要为其添加具体的 AI 模型。Ollama 提供了 `AddModel` 方法来实现这一功能。例如，要添加 Llama 3.2 模型，可以使用以下代码：

    var chat = ollama.AddModel("chat", "llama3.2");
    

如果需要指定模型的特定版本或标签，可以在方法中添加相应参数。例如，`ollama.AddModel("chat", "llama3.2:1b")` 将选择 Llama 3.2 模型的 1b 版本。如果所需模型不在 Ollama 库中，还可以使用 `AddHuggingFaceModel` 方法从 Hugging Face 模型中心添加模型。

添加模型后，我们需要将其作为资源关联到应用程序中的其他服务：

    builder.AddProject<Projects.MyApi>("api")
           .WithReference(chat);
    

运行应用程序主机项目时，Ollama 服务器会自动启动并下载指定的模型。需要注意的是，模型下载可能需要较长时间，在此期间不应停止应用程序主机。如果希望确保依赖模型的资源等待下载完成后再启动，可以使用 `WaitFor` 方法：

    builder.AddProject<Projects.MyApi>("api")
           .WithReference(chat)
           .WaitFor(chat);
    

在控制面板中，我们可以看到模型下载的状态。Ollama 服务器会显示为运行中但非正常状态，直到模型下载完成。同时，依赖该模型的 API 资源也会保持等待状态。

![Image downloading models](https://devblogs.microsoft.com/dotnet-ch/wp-content/uploads/sites/75/2025/01/downloading-models-300x118.png)

### 在应用程序中使用模型

配置好 API 项目与 chat 模型的关联后，我们可以使用 OllamaSharp 库与 Ollama 服务器交互。首先需要安装 .NET Aspire 社区工具包中的 OllamaSharp 集成：

    dotnet add package CommunityToolkit.Aspire.OllamaSharp
    

这个集成允许我们将 OllamaSharp 客户端注册为 Microsoft.Extensions.AI 包中的 `IChatClient` 或 `IEmbeddingsGenerator` 服务。这种抽象设计使得我们可以轻松地从本地 Ollama 服务器切换到云服务（如 Azure OpenAI）而无需修改客户端代码：

    builder.AddOllamaSharpChatClient("chat");
    

对于需要使用嵌入模型的场景，可以使用 `AddOllamaSharpEmbeddingsGenerator` 方法注册 `IEmbeddingsGenerator` 服务。

为了充分利用 Microsoft.Extensions.AI 的功能管道，我们可以将服务提供给 `ChatClientBuilder`：

    builder.AddKeyedOllamaSharpChatClient("chat");
    builder.Services.AddChatClient(sp => sp.GetRequiredKeyedService("chat"))
                    .UseFunctionInvocation()
                    .UseOpenTelemetry(configure: t => t.EnableSensitiveData = true)
                    .UseLogging();
    

最后，我们可以将 `IChatClient` 注入到路由处理程序中，实现与模型的交互：

    app.MapPost("/chat", async (IChatClient chatClient, string question) =>
    {
        var response = await chatClient.CompleteAsync(question);
        return response.Message;
    });
    

### 云托管模型支持

虽然 Ollama 非常适合本地开发，但在生产环境中，我们可能需要使用云端的 AI 服务，如 Azure OpenAI。为此，我们可以修改 API 项目的配置，使其在云端运行时自动切换服务实现：

    if (builder.Environment.IsDevelopment())
    {
        builder.AddKeyedOllamaSharpChatClient("chat");
    }
    else
    {
        builder.AddKeyedAzureOpenAIClient("chat");
    }
    
    builder.Services.AddChatClient(sp => sp.GetRequiredKeyedService("chat"))
                    .UseFunctionInvocation()
                    .UseOpenTelemetry(configure: t => t.EnableSensitiveData = true)
                    .UseLogging();
    

这种设计模式充分体现了 Microsoft.Extensions.AI 抽象层的价值，它允许我们在不改变业务逻辑代码的情况下，灵活切换底层 AI 服务的实现方式。

结论
--

本文详细介绍了如何利用 .NET Aspire 框架在本地环境中设置和使用 AI 模型。通过简单的几行代码，我们就能配置 Ollama 服务器，指定所需的 AI 模型，并将其集成到应用程序中。更重要的是，借助 Microsoft.Extensions.AI 的抽象能力，我们可以轻松实现从本地开发到云部署的平滑过渡。

这种开发模式为 AI 应用开发提供了极大的灵活性和便利性。开发者可以在本地环境中快速迭代和测试模型，待功能成熟后再迁移到云端，既保证了开发效率，又兼顾了生产环境的性能和稳定性。对于 .NET 开发者而言，.NET Aspire 结合 Ollama 的方案提供了一条高效、便捷的 AI 应用开发路径。

[.NET AI 模板](https://www.cnblogs.com/powertoolsteam/p/18970200)

* * *

  

本文是由葡萄城技术开发团队发布，转载请注明出处：[葡萄城官网](https://www.grapecity.com.cn/)