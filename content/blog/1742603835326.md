---
layout: post
title: 'æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ16ï¼‰--- èµ„æºå ç”¨'
date: "2025-03-22T00:37:15Z"
---
æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ16ï¼‰--- èµ„æºå ç”¨
============================

ä»é›¶å¼€å§‹è§£æTransformerï¼Œç›®æ ‡æ˜¯ï¼š(1) è§£æTransformerå¦‚ä½•è¿ä½œï¼Œä»¥åŠä¸ºä½•å¦‚æ­¤è¿ä½œï¼Œè®©æ–°åŒå­¦å¯ä»¥å…¥é—¨ï¼›(2) åŠ›äº‰èå…¥ä¸€äº›æ¯”è¾ƒæ–°çš„æˆ–è€…æœ‰ç‰¹è‰²çš„è®ºæ–‡æˆ–è€…ç†å¿µï¼Œè®©è€é¸Ÿä¹Ÿå¯ä»¥æœ‰æ‰€æ”¶è·ã€‚

æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ16ï¼‰--- èµ„æºå ç”¨
============================

ç›®å½•

*   [æ¢ç§˜Transformerç³»åˆ—ä¹‹ï¼ˆ16ï¼‰--- èµ„æºå ç”¨](#æ¢ç§˜transformerç³»åˆ—ä¹‹16----èµ„æºå ç”¨)
    *   [æ–‡ç« æ€»è¡¨](#æ–‡ç« æ€»è¡¨)
    *   [0x00 æ¦‚è¿°](#0x00-æ¦‚è¿°)
    *   [0x01 èƒŒæ™¯çŸ¥è¯†](#0x01-èƒŒæ™¯çŸ¥è¯†)
        *   [1.1 æ•°æ®ç±»å‹](#11-æ•°æ®ç±»å‹)
        *   [1.2 è¿›åˆ¶&æ¢ç®—](#12-è¿›åˆ¶æ¢ç®—)
            *   [æ•°å­—è¿›åˆ¶](#æ•°å­—è¿›åˆ¶)
            *   [å­˜å‚¨åº¦é‡](#å­˜å‚¨åº¦é‡)
            *   [æ¢ç®—](#æ¢ç®—)
        *   [1.3 å‚æ•°æ˜¾å­˜å ç”¨](#13-å‚æ•°æ˜¾å­˜å ç”¨)
            *   [æœ‰å‚æ•°çš„å±‚](#æœ‰å‚æ•°çš„å±‚)
            *   [æ— å‚æ•°çš„å±‚](#æ— å‚æ•°çš„å±‚)
            *   [æ‰€éœ€èµ„æº](#æ‰€éœ€èµ„æº)
        *   [1.4 è®¡ç®—é‡](#14-è®¡ç®—é‡)
    *   [0x02 Transformerå‚æ•°é‡](#0x02-transformerå‚æ•°é‡)
        *   [2.1 æœ¯è¯­](#21-æœ¯è¯­)
        *   [2.2 embeddingå±‚](#22-embeddingå±‚)
        *   [2.3 Transformerå±‚](#23-transformerå±‚)
            *   [MHA](#mha)
            *   [FFN](#ffn)
            *   [LayerNorm](#layernorm)
            *   [å°ç»“](#å°ç»“)
        *   [2.4 lm\_head](#24-lm_head)
        *   [2.5 æœ€ç»ˆå‚æ•°é‡](#25-æœ€ç»ˆå‚æ•°é‡)
        *   [2.6 LLaMA3](#26-llama3)
            *   [SwiGLU](#swiglu)
            *   [GQA](#gqa)
    *   [0x03 Transformeræ˜¾å­˜å ç”¨](#0x03-transformeræ˜¾å­˜å ç”¨)
        *   [3.1 è®­ç»ƒ](#31-è®­ç»ƒ)
        *   [3.2 æ¨ç†](#32-æ¨ç†)
        *   [3.3 æ¿€æ´»](#33-æ¿€æ´»)
            *   [æ¶æ„](#æ¶æ„)
            *   [æœ¯è¯­è¯´æ˜](#æœ¯è¯­è¯´æ˜)
            *   [æ•°æ®é‡](#æ•°æ®é‡)
                *   [æ³¨æ„åŠ›å—](#æ³¨æ„åŠ›å—)
                *   [MLP](#mlp)
                *   [LayerNorm](#layernorm-1)
                *   [æ€»ç»“](#æ€»ç»“)
                *   [å¹¶è¡Œ](#å¹¶è¡Œ)
    *   [0x04 Transformerè®¡ç®—é‡](#0x04-transformerè®¡ç®—é‡)
        *   [4.1 çŸ©é˜µä¹˜æ³•](#41-çŸ©é˜µä¹˜æ³•)
        *   [4.2 å‰å‘ä¼ æ’­è®¡ç®—é‡](#42-å‰å‘ä¼ æ’­è®¡ç®—é‡)
            *   [Embedding](#embedding)
            *   [MHA](#mha-1)
                *   [è®¡ç®—Qã€Kã€V](#è®¡ç®—qkv)
                *   [QK^T](#qkt)
                *   [ä¹˜ä»¥V](#ä¹˜ä»¥v)
                *   [çº¿æ€§æ˜ å°„](#çº¿æ€§æ˜ å°„)
            *   [MLP](#mlp-1)
            *   [LayerNorm](#layernorm-2)
            *   [å•å±‚layer](#å•å±‚layer)
        *   [4.3 ç»¼åˆæ€è€ƒ](#43-ç»¼åˆæ€è€ƒ)
            *   [åå‘ä¼ æ’­](#åå‘ä¼ æ’­)
                *   [å•å±‚](#å•å±‚)
                *   [logits](#logits)
            *   [æ€»ä½“è®¡ç®—é‡](#æ€»ä½“è®¡ç®—é‡)
        *   [4.4 è®¡ç®—ç‰¹ç‚¹](#44-è®¡ç®—ç‰¹ç‚¹)
            *   [ä¸å‚æ•°é‡çš„å…³ç³»](#ä¸å‚æ•°é‡çš„å…³ç³»)
                *   [å•æ¬¡æ¨ç†](#å•æ¬¡æ¨ç†)
                *   [å•æ¬¡è®­ç»ƒ](#å•æ¬¡è®­ç»ƒ)
            *   [å¸¦å®½å—é™](#å¸¦å®½å—é™)
                *   [æ³¨æ„åŠ›è®¡ç®—](#æ³¨æ„åŠ›è®¡ç®—)
                *   [FFNè®¡ç®—](#ffnè®¡ç®—)
            *   [KV Cacheçš„å½±å“](#kv-cacheçš„å½±å“)
                *   [prefill](#prefill)
                *   [decode](#decode)
                *   [æ€»ä½“](#æ€»ä½“)
                *   [kv cache èŠ‚çœäº†å¤šå°‘è®¡ç®—é‡](#kv-cache-èŠ‚çœäº†å¤šå°‘è®¡ç®—é‡)
    *   [0x05 ä¼˜åŒ–æ–¹å‘](#0x05-ä¼˜åŒ–æ–¹å‘)
        *   [5.1 åŸºäºæ³¨æ„åŠ›æœºåˆ¶æ¥ä¿®æ”¹å¤–æ¨æŠ€æœ¯](#51-åŸºäºæ³¨æ„åŠ›æœºåˆ¶æ¥ä¿®æ”¹å¤–æ¨æŠ€æœ¯)
        *   [5.2 åŸºäºMemoryæœºåˆ¶å¤–æ¨æŠ€æœ¯](#52-åŸºäºmemoryæœºåˆ¶å¤–æ¨æŠ€æœ¯)
    *   [0xFF å‚è€ƒ](#0xff-å‚è€ƒ)

æ–‡ç« æ€»è¡¨
----

å…¨éƒ¨æ–‡ç« åˆ—è¡¨åœ¨è¿™é‡Œ [æ¢ç§˜Transformerç³»åˆ—ä¹‹æ–‡ç« åˆ—è¡¨](https://www.cnblogs.com/rossiXYZ/p/18785601)ï¼Œåç»­æ¯å‘ä¸€ç¯‡æ–‡ç« ï¼Œä¼šä¿®æ”¹è¿™é‡Œã€‚

0x00 æ¦‚è¿°
-------

å¯¹äºæ ‡å‡† Transformer æ¨¡å‹ï¼Œä¸ç®¡æ˜¯ Encoder Only çš„ Bert ç³»åˆ—æ¨¡å‹ï¼Œè¿˜æ˜¯ Decoder Only çš„ GPT ç³»åˆ—æ¨¡å‹ï¼ŒåŒé…ç½®ä¸‹å‚æ•°é‡å’Œè®¡ç®—é‡éƒ½æ˜¯ç±»ä¼¼çš„ã€‚å…¶ä¸­çš„ä¸€ä¸ªå…³é”®ç‚¹æ˜¯ï¼šæ ‡å‡† Transformer blockï¼ˆå±‚ï¼‰è¾“å…¥ã€è¾“å‡ºä»¥åŠä¸­é—´ Hidden Dim ä¿æŒä¸å˜ï¼Œå§‹ç»ˆæ˜¯ Token Embedding çš„ Hidden Dimï¼Œæ‰€æœ‰çš„ Transformer Block éƒ½éå¸¸è§„æ•´ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒEncoderä¸»è¦å‚æ•°éƒ½æ¥è‡ªå‡ ä¸ªçŸ©é˜µä¹˜çš„ Weight çŸ©é˜µï¼Œå…¶ä¸­ d è¡¨ç¤º Token Embedding çš„ Hidden Dimï¼Œl è¡¨ç¤º Token æ•°ï¼Œh è¡¨ç¤º MHA ä¸­çš„ Head ä¸ªæ•°ï¼Œ\\(d\_{FFN}\\) è¡¨ç¤º FFN å±‚ä¸­é—´å‡ç»´åçš„ Dimã€‚å…¶ä¸»è¦å‡ ä¸ªæ¨¡å—çš„å‚æ•°é‡å¦‚ä¸‹ã€‚

*   MHAï¼š\\(W\_Qï¼ŒW\_Kï¼ŒW\_V\\) çš„å¤§å°éƒ½æ˜¯ d x dã€‚å½“ç„¶è¿™é‡Œä¹Ÿå¯ä»¥ä» h ä¸ª Head çš„è§’åº¦å»çœ‹ï¼Œåˆ™æ¯ä¸ª Head çš„ \\(W\_Qï¼ŒW\_Kï¼ŒW\_V\\) ä¸º d x d/hã€‚åœ¨ MHA çš„æœ€åè¿˜æœ‰ä¸€ä¸ªçŸ©é˜µä¹˜æ“ä½œï¼Œå¯¹åº”çš„ \\(W\_{out}\\) ç»´åº¦ä¾ç„¶ä¸º d x dã€‚æ‰€ä»¥MHAå¤„æƒé‡çŸ©é˜µçš„å‚æ•°é‡æ˜¯ \\(3d \\times d + d \\times d\\)ã€‚
*   FFNï¼šæ ‡å‡† Transformer çš„ FFN ä¸­æœ‰ä¸¤ä¸ª Linear å±‚ï¼ˆå…ˆå‡ç»´å†é™ç»´ï¼‰ï¼Œå¯¹åº”æƒé‡çŸ©é˜µ \\(W\_1\\) å’Œ$ W\_2$ çš„å¤§å°éƒ½æ˜¯ \\(d\_{FFN}\\) x dï¼Œå¹¶ä¸”æ ‡å‡†çš„ \\(d\_{FFN}\\) ä¸º 4dï¼Œä¹Ÿå°±æ˜¯è¯´ FFN å¤„ä¸¤ä¸ªæƒé‡çŸ©é˜µçš„å‚æ•°é‡ä¸º 8d x dã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181117131-778754780.jpg)

ç»¼ä¸Šï¼Œåœ¨æ ‡å‡†çš„ Transformer æ¨¡å‹æˆ–è€… LLaMA ç³»åˆ—ï¼ˆMHAï¼‰ä¸­ï¼Œå¦‚æœå¿½ç•¥è¯è¡¨ã€Embeddingã€LayerNorm ç­‰å‚æ•°åï¼Œæ€»å‚æ•°é‡ä¸ºï¼ˆæ‰€æœ‰ Transformer Blockï¼‰ï¼š \\(N = n\_{layer} \\times (n\_{mha}+ n\_{ffn}) = n\_{layer} \\times (3d \\times d + d \\times d + 8d \\times d) = 12 \\times n\_{layer} \\times d \\times d\\)

æ³¨æ„ï¼šæœ¬ç« å‚è€ƒäº†å¤šç¯‡è®ºæ–‡ï¼Œå…¶ä¸­å¯¹æœ¯è¯­çš„å®šä¹‰å„ä¸ç›¸åŒï¼Œå› ä¸ºæ¨¡å‹ç»“æ„ä¹Ÿä¸åŒï¼Œæ‰€ä»¥è®¡ç®—ç»“æœä¸å…¶å®ƒèµ„æ–™å¯èƒ½ä¹Ÿæœ‰å·®å¼‚ã€‚

0x01 èƒŒæ™¯çŸ¥è¯†
---------

### 1.1 æ•°æ®ç±»å‹

æ·±åº¦å­¦ä¹ ä¸­ç”¨çš„æ•°å€¼ç±»å‹å‘½åè§„èŒƒä¸€èˆ¬ä¸ºTypeNumï¼Œæ¯”å¦‚Int64ã€Float32ã€Double64ã€‚

*   Typeï¼šæœ‰Intï¼ŒFloatï¼ŒDoubleç­‰ã€‚
*   Num: ä¸€èˆ¬æ˜¯ 8ï¼Œ16ï¼Œ32ï¼Œ64ï¼Œ128ï¼Œè¡¨ç¤ºè¯¥ç±»å‹æ‰€å æ®çš„æ¯”ç‰¹æ•°ç›®ã€‚

å¸¸ç”¨çš„æ•°å€¼ç±»å‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

ç±»å‹

å¤§å°ï¼ˆå­—èŠ‚æ•°ï¼‰

int4

0.5

int8

1

int16

2

int32

4

int64

8

float32

4

float16

2

### 1.2 è¿›åˆ¶&æ¢ç®—

æˆ‘ä»¬å…ˆæŠ›å‡ºä¸€ä¸ªé—®é¢˜:1Bå‚æ•°å¯¹åº”å¤šå°‘Gæ˜¾å­˜ï¼ŸBå’ŒGéƒ½ä»£è¡¨åäº¿ï¼ˆ1000Mæˆ–1024Mï¼‰ï¼Œä½†è¿™æ˜¯ä¸¤ä¸ªä¸åŒçš„åº¦é‡ç»´åº¦ã€‚

#### æ•°å­—è¿›åˆ¶

Bæ˜¯è‹±ç¾å¸¸ç”¨çš„è¿›åˆ¶å•ä½ï¼Œæ¯”å¦‚ï¼š

*   1K = 1000ï¼Œä¸€åƒï¼›
    
*   1M = 1000 Kï¼Œç™¾ä¸‡ï¼›
    
*   1B = 1000 Mï¼Œåäº¿ï¼›
    

å¯ä»¥çœ‹å‡ºæ¥ï¼Œè¿™ä¸ªè¿›åˆ¶å•ä½ä»¥ 1000 ä¸ºè¿›åˆ¶ã€‚ä»¥ Qwen-7B ä¸ºä¾‹ï¼Œ7B çš„æ„æ€å°±æ˜¯ è¿™ä¸ª LLM çš„ æ¨¡å‹å‚æ•°æœ‰ 70äº¿ ä¸ª å‚æ•°ã€‚

#### å­˜å‚¨åº¦é‡

Gæ˜¯è®¡ç®—æœºå†…å­˜/ç£ç›˜å­˜å‚¨çš„åº¦é‡ï¼ŒåŸºæœ¬å•ä½æ˜¯å­—èŠ‚ï¼Œè¿›åˆ¶æ˜¯ 1024ã€‚å•ä½ä¾æ¬¡æ˜¯ï¼šKB / MB / GB / TBã€‚å¹³æ—¶è¯´æ˜¾å­˜æœ‰å¤šå°‘G/Mæ˜¯è¯´æœ‰å¤šå°‘G/Mä¸ªå­—èŠ‚ï¼ˆbyteï¼‰ï¼Œ1ä¸ªå­—èŠ‚=8æ¯”ç‰¹ï¼ˆbitï¼‰ã€‚ä¸¾ä¾‹æ¥è¯´ï¼šæœ‰ä¸€ä¸ª1000x1000çš„ çŸ©é˜µï¼Œfloat32ï¼Œé‚£ä¹ˆå ç”¨çš„æ˜¾å­˜å·®ä¸å¤šå°±æ˜¯1000x1000x4 Byte = 4MBã€‚

#### æ¢ç®—

å¯ä»¥çœ‹å‡ºæ¥ï¼Œ\\(1B=10^9 byte \\approx 1GB\\)ï¼Œ1Bå’Œ1Gçš„å¤§å°åŸºæœ¬ä¸€è‡´ï¼Œæ‰€ä»¥æˆ‘ä»¬è®°ä½œBå’ŒGç›¸ç­‰ã€‚ä½†æ˜¯ï¼Œ1Bæ¨¡å‹å‚æ•°å¯¹åº”å¤šå°‘Gå†…å­˜å’Œå‚æ•°çš„ç²¾åº¦æœ‰å…³ã€‚å¦‚æœæ˜¯å…¨ç²¾åº¦è®­ç»ƒï¼ˆfp32ï¼‰ï¼Œä¸€ä¸ªå‚æ•°å¯¹åº”32æ¯”ç‰¹ï¼Œä¹Ÿå°±æ˜¯4ä¸ªå­—èŠ‚ï¼Œå‚æ•°æ¢ç®—åˆ°æ˜¾å­˜çš„æ—¶å€™è¦ä¹˜4ï¼Œä¹Ÿå°±æ˜¯1Bæ¨¡å‹å‚æ•°å¯¹åº”4Gæ˜¾å­˜ã€‚å¦‚æœæ˜¯fp16æˆ–è€…bf16å°±æ˜¯ä¹˜2ï¼Œ1Bæ¨¡å‹å‚æ•°å¯¹åº”2Gæ˜¾å­˜ã€‚å…·ä½“å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚

æ•°æ®ç±»å‹

æ¯1Bå‚æ•°éœ€è¦å ç”¨å†…å­˜

fp32

4G

fp16/bf16

2G

int8

1G

int4

0.5G

### 1.3 å‚æ•°æ˜¾å­˜å ç”¨

æœ‰å‚æ•°çš„æ¨¡å—æ‰ä¼šå ç”¨æ˜¾å­˜ã€‚è¿™éƒ¨ä»½çš„æ˜¾å­˜å ç”¨å’Œè¾“å…¥æ— å…³ï¼Œæ¨¡å‹åŠ è½½å®Œæˆä¹‹åå°±ä¼šå ç”¨ã€‚ä¸€èˆ¬çš„å·ç§¯å±‚éƒ½ä¼šå ç”¨æ˜¾å­˜ï¼Œè€Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨çš„æ¿€æ´»å±‚Reluæ²¡æœ‰å‚æ•°ï¼Œæ‰€ä»¥ä¸ä¼šå ç”¨ç¼“å­˜ã€‚

#### æœ‰å‚æ•°çš„å±‚

å¸¸è§çš„æœ‰å‚æ•°çš„æ¨¡å—ä¸»è¦åŒ…æ‹¬ï¼š

*   å·ç§¯å±‚ï¼Œé€šå¸¸çš„conv2dã€‚
*   å…¨è¿æ¥å±‚ï¼Œä¹Ÿå°±æ˜¯Linearå±‚ã€‚
*   BatchNormå±‚ã€‚
*   Embeddingå±‚ã€‚

#### æ— å‚æ•°çš„å±‚

å¸¸è§çš„æ— å‚æ•°çš„æ¨¡å—ä¸»è¦åŒ…æ‹¬ï¼š

*   å¤šæ•°çš„æ¿€æ´»å±‚ï¼Œæ¯”å¦‚Sigmoid/ReLUã€‚
*   æ± åŒ–å±‚ã€‚
*   Dropoutã€‚

#### æ‰€éœ€èµ„æº

æˆ‘ä»¬å¯ä»¥ç”¨å¦‚ä¸‹å…¬å¼æ¥è®¡ç®—ç¥ç»ç½‘ç»œçš„æ˜¾å­˜å ç”¨ï¼šæ˜¾å­˜å ç”¨ = æ¨¡å‹æ˜¾å­˜å ç”¨ + è¾“å…¥è¾“å‡ºç›¸å…³çš„æ˜¾å­˜

æ¨¡å‹æ˜¾å­˜å ç”¨æ˜¯æ¨¡å‹ä¸­ä¸è¾“å…¥æ— å…³çš„æ˜¾å­˜å ç”¨ï¼Œä¸»è¦åŒ…æ‹¬ï¼š

*   æ¨¡å‹æƒé‡å‚æ•°ã€‚
*   æ¢¯åº¦ï¼ˆä¸€èˆ¬æ˜¯å‚æ•°é‡çš„1å€ï¼‰ã€‚
*   ä¼˜åŒ–å™¨çš„åŠ¨é‡ï¼ˆå’Œå…·ä½“ä¼˜åŒ–å™¨å¯†åˆ‡ç›¸å…³ï¼Œæ¯”å¦‚æ™®é€šSGDæ²¡æœ‰åŠ¨é‡ï¼Œmomentum-SGDåŠ¨é‡ä¸æ¢¯åº¦ä¸€æ ·ï¼ŒAdamä¼˜åŒ–å™¨åŠ¨é‡æ•°é‡æ˜¯æ¢¯åº¦çš„ä¸¤å€ï¼‰ã€‚

è¾“å…¥è¾“å‡ºç›¸å…³çš„æ˜¾å­˜å ç”¨ä¸»è¦å¦‚ä¸‹ï¼š

*   batch\_size Ã— æ¯ä¸ªæ ·æœ¬çš„æ˜¾å­˜å ç”¨ã€‚
*   æ¯ä¸€å±‚çš„feature mapï¼Œéœ€è¦ä¿å­˜æ¿€æ´»æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚

å› ä¸º åå‘ä¼ æ’­ / Adam-ä¼˜åŒ– / Transformeræ¶æ„ ç­‰å› ç´ ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œè®­ç»ƒéœ€è¦çš„æ˜¾å­˜ï¼Œæ˜¯ åŒæ ·è§„æ¨¡æ¨ç† çš„ 3-4å€ã€‚

### 1.4 è®¡ç®—é‡

ä¸Šæ–‡æåˆ°Transformerçš„è®¡ç®—å¤æ‚åº¦æ˜¯ $O(dN^2) $ã€‚å¤§ O è¡¨ç¤ºæ³•å…³æ³¨çš„æ˜¯è®¡ç®—é‡çº§ä¸è¾“å…¥è§„æ¨¡ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸æ˜¯å…·ä½“çš„è®¡ç®—é‡ã€‚å…·ä½“è®¡ç®—é‡é€šå¸¸ç”¨FLOPsä½“ç°ã€‚è¿™é‡Œç®€å•åˆ—ä¸¾ä¸€äº›æ¯”è¾ƒå¸¸è§çš„å•ä½ï¼š

*   FLOPs ï¼šfloating point of operationsçš„ç¼©å†™ï¼Œæ˜¯æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼Œä¸€èˆ¬ç‰¹æŒ‡ä¹˜åŠ è¿ç®—æ¬¡æ•°ï¼Œç†è§£ä¸ºè®¡ç®—é‡ï¼Œå¯ä»¥ç”¨æ¥è¡¡é‡ç®—æ³•/æ¨¡å‹å¤æ‚åº¦ã€‚
*   ä¸€ä¸ªGFLOPSï¼ˆgigaFLOPSï¼‰= æ¯ç§’åäº¿ï¼ˆ=10^9ï¼‰æ¬¡çš„æµ®ç‚¹è¿ç®—
*   ä¸€ä¸ªTFLOPSï¼ˆteraFLOPSï¼‰ = æ¯ç§’ä¸€ä¸‡äº¿ï¼ˆ=10^12ï¼‰æ¬¡çš„æµ®ç‚¹è¿ç®—

0x02 Transformerå‚æ•°é‡
-------------------

ä»¥Decoder onlyæ¨¡å‹ä¸ºä¾‹ï¼Œå…¶ä¸»è¦åŒ…æ‹¬ 3 ä¸ªéƒ¨åˆ†ï¼šembeddingï¼Œdecoderï¼Œheadã€‚æœ€ä¸»è¦éƒ¨åˆ†æ˜¯decoderï¼Œå…¶ç”±è‹¥å¹²ä¸ªdecoder-layerç»„æˆï¼Œæ¯ä¸ªdecoder-layeråˆåˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šMHAå’ŒFFNã€‚æˆ‘ä»¬æ¥ä¸‹æ¥é€ä¸€çœ‹çœ‹è¿™äº›æ¨¡å—çš„å‚æ•°é‡ã€‚

### 2.1 æœ¯è¯­

æˆ‘ä»¬å…ˆç»™å‡ºæœ¬èŠ‚ä½¿ç”¨çš„æœ¯è¯­ã€‚

Symbol

Meaning

\\(d\\)

æ¨¡å‹çš„è¯åµŒå…¥å¤§å°ï¼ˆThe model size / hidden state dimension / positional encoding sizeï¼‰

\\(h\\)

æ³¨æ„åŠ›å¤´ä¸ªæ•°

\\(s\\)

æ–‡æœ¬æ€»é•¿åº¦ï¼ˆprompt+è§£ç å™¨è¾“å‡ºï¼‰

\\(b\\)

æ•°æ®batch sizeï¼ˆæ‰¹å¤§å°ï¼‰

\\(l\\)

Transformerå±‚æ•°

\\(v\\)

è¯è¡¨å¤§å°

### 2.2 embeddingå±‚

embeddingå±‚çš„è¾“å…¥å½¢çŠ¶æ˜¯\[b,s,v\]ï¼Œè¾“å‡ºå½¢çŠ¶æ˜¯\[b,s,d\]ï¼Œå‚æ•°é‡ä¸º\\(v \\times d\\)ã€‚å¦‚æœé‡‡ç”¨å¯è®­ç»ƒå¼çš„ä½ç½®ç¼–ç ï¼Œä¼šæœ‰ä¸€äº›å¯è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œä½†æ˜¯å…¶æ•°é‡æ¯”è¾ƒå°‘ã€‚å¦‚æœé‡‡ç”¨ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œä¾‹å¦‚RoPEå’ŒALiBiï¼Œåˆ™ä¸åŒ…å«å¯è®­ç»ƒçš„æ¨¡å‹å‚æ•°ã€‚å› æ­¤æˆ‘ä»¬å¿½ç•¥ä½ç½®ç¼–ç çš„å‚æ•°ã€‚

### 2.3 Transformerå±‚

Transformeræ¨¡å‹ç”± l ä¸ªç›¸åŒçš„å±‚ç»„æˆï¼Œæ¯ä¸ªå±‚ä¸»è¦åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šMHAå’ŒFFNã€‚å› ä¸ºå¤šå¤´åªæ˜¯é€»è¾‘ä¸Šåˆ‡åˆ†ï¼Œç‰©ç†ä¸Šæ²¡æœ‰å¢åŠ æ¨¡å—ï¼Œå› æ­¤åç»­è®¨è®ºä¸­çœç•¥å¤šå¤´ï¼ˆæŸäº›è®ºæ–‡ä¸­å¦‚æœè®¨è®ºå¤šå¤´ç›¸å…³ï¼Œæˆ‘ä»¬ä¼šä»¥è®ºæ–‡ä¸ºå‡†ï¼‰ï¼Œè€Œåˆå› ä¸ºDecoder onlyæ¨¡å‹ä½¿ç”¨çš„æ˜¯è‡ªæ³¨æ„åŠ›ï¼Œå› æ­¤æ¥ä¸‹æ¥æˆ‘ä»¬è®¤ä¸º Qã€Kã€Vã€Oçš„ç»´åº¦ç›¸ç­‰ã€‚

#### MHA

MHAä¸­åŒ…å«å››ä¸ªæƒé‡çŸ©é˜µ\\(W^Q,W^K,W^V,W^O\\)ä»¥åŠåç½®ï¼ˆæŸäº›æ¨¡å‹å¯èƒ½æ²¡æœ‰åç½®ï¼‰ã€‚4ä¸ªæƒé‡çŸ©é˜µçš„å½¢çŠ¶ä¸º \[\\(d\\),\\(d\\)\]ï¼Œ4ä¸ªåç½®çš„å½¢çŠ¶ä¸º \[\\(d\\)\]ï¼Œå…¶ä¸­ \\(d = h \\times d\_{head}\\)ã€‚å› æ­¤ï¼Œå¤šå¤´æ³¨æ„åŠ›å±‚å‚æ•°é‡ä¸ºï¼š\\(4\\times (d \\times d + d) = 4d^2 + 4d\\)ã€‚

#### FFN

FFNåŒ…æ‹¬ä¸¤ä¸ªçº¿æ€§å±‚ã€‚

*   ç¬¬ä¸€å±‚å°†åŸæœ‰çš„ç»´åº¦æ˜ å°„åˆ°4å€åŸç»´åº¦å¤§å°ï¼Œå³ä»\\(d\\)æ˜ å°„åˆ°4\\(d\\)ã€‚æƒé‡çŸ©é˜µå½¢çŠ¶æ˜¯\[d, 4d\]ï¼Œåç½®å½¢çŠ¶æ˜¯\[4d\]ã€‚å‚æ•°é‡ä¸ºï¼š\\(d\\times 4d + 4d\\)
*   ç¬¬äºŒå±‚ä»4å€ç»´åº¦é™ç»´å›åŸå§‹ç»´åº¦ã€‚å³ä»4\\(d\\)æ˜ å°„åˆ°\\(d\\)ã€‚æƒé‡çŸ©é˜µå½¢çŠ¶æ˜¯\[4d, d\]ï¼Œåç½®å½¢çŠ¶æ˜¯\[d\]ã€‚å‚æ•°é‡ä¸ºï¼š \\(4d\\times d + d\\)

æœ€ç»ˆFFNçš„å‚æ•°æ˜¯ï¼š\\(8d^2 + 5d\\)ã€‚

#### LayerNorm

å¯¹äºLayer Normæ¥è¯´ï¼Œå…¶ç¼©æ”¾å‚æ•° \\(\\gamma\\)ä¸å¹³ç§»å‚æ•° \\(beta\\) ç»´åº¦éƒ½ä¸º \\(d\\)ï¼Œå› æ­¤å‚æ•°é‡æ˜¯ \\(2 \\times d\\)ã€‚å› ä¸ºMHAå’ŒFFNéƒ½æœ‰LayerNormï¼Œå› æ­¤æ€»å‚æ•°é‡æ˜¯\\(4 \\times d\\)ã€‚

#### å°ç»“

ç»¼ä¸Šï¼Œå•ä¸ªTransformerå±‚çš„å‚æ•°é‡æ˜¯ï¼š\\(12d^2 + 13d\\)ã€‚

### 2.4 lm\_head

lm\_headæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ä¸­çš„ä¸€ä¸ªç»„ä»¶ï¼Œä¸»è¦ä½œç”¨æ˜¯å°†æ¨¡å‹çš„è¾“å‡ºï¼ˆé€šå¸¸æ˜¯ç»è¿‡Transformerç¼–ç å™¨å¤„ç†åçš„éšè—çŠ¶æ€ï¼‰è½¬æ¢æˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚

Headä¸embeddingçš„å‚æ•°é‡ç›¸åŒã€‚å¦‚æœæ˜¯tied embeddingï¼ˆå³ï¼Œheadæƒé‡çŸ©é˜µä¸è¯åµŒå…¥çŸ©é˜µæ˜¯å‚æ•°å…±äº«çš„ï¼‰ï¼Œåˆ™ä¸¤è€…å…¬ç”¨ä¸€ä¸ªå‚æ•°ã€‚

### 2.5 æœ€ç»ˆå‚æ•°é‡

æœ€ç»ˆï¼Œl å±‚transformeræ¨¡å‹çš„å¯è®­ç»ƒæ¨¡å‹å‚æ•°é‡ä¸º\\(l(12d^2 + 13d) + 2vd\\) ã€‚å½“dè¾ƒå¤§æ—¶ï¼Œå¯ä»¥å¿½ç•¥ä¸€æ¬¡é¡¹ï¼Œæ¨¡å‹å‚æ•°é‡è¿‘ä¼¼ä¸º\\(12ld^2\\) ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181134848-1724419527.jpg)

### 2.6 LLaMA3

æˆ‘ä»¬å†ç”¨LLaMA3æ¥çœ‹çœ‹åœ¨å·¥ä¸šç•Œè½åœ°ä¸­çš„ä¸€äº›ç‰¹æ®Šä¹‹å¤„ã€‚

#### SwiGLU

LLaMA ç­‰æ¨¡å‹åœ¨ FFN ä¸­ä¼šä½¿ç”¨ SwiGLU æ¿€æ´»ï¼Œè¿™ä¹Ÿå°±å¯¼è‡´å…¶ä¼šé¢å¤–å¤šäº†ä¸€ä¸ªæƒé‡çŸ©é˜µã€‚LLaMAè®ºæ–‡ä¸­æåˆ°ï¼Œä½¿ç”¨ SwiGLU åå°† dFFN ä» 4d é™ä½åˆ°äº† 8d/3ã€‚è¿™æ · 3 ä¸ªæƒé‡çŸ©é˜µçš„å‚æ•°é‡è¿˜æ˜¯ 8dï¼Œæ€»çš„å‚æ•°é‡ä¾ç„¶å¯ä»¥ä½¿ç”¨ \\(12 \\times n\_{layer}\\times d\\times d\\)æ¥ é¢„ä¼°ã€‚

#### GQA

å‰é¢å…¬å¼å¯¹åº”çš„æ˜¯ MHAï¼ˆMulti Head Attentionï¼‰ï¼Œè¿™ä¹Ÿæ˜¯ LLaMA-1 ç³»åˆ—æ¨¡å‹çš„æ ‡å‡†å®ç°ã€‚ä¸è¿‡ï¼ŒLLaMA-2 çš„ 30B å’Œ 70B æ¨¡å‹ä»¥åŠ LLaMA-3 çš„å…¨éƒ¨æ¨¡å‹éƒ½å¼€å§‹ä½¿ç”¨ GQAï¼ˆGrouped Query Attentionï¼‰ã€‚ä½¿ç”¨ GQA æ—¶ï¼Œå¤šä¸ª æ³¨æ„åŠ›å¤´ä¼šå…±äº«ä¸€ä¸ª Key å’Œ Valueï¼Œæ­¤æ—¶\\(W^K,W^V\\)çš„å¤§å°ä¼šå˜ä¸º d x d/gï¼Œå…¶ä¸­ g è¡¨ç¤ºæ¯ g ä¸ª Head å…±äº«ç›¸åŒçš„ Key å’Œ Valueã€‚LLaMA 2è®ºæ–‡æåˆ°ï¼Œä¸ºäº†ä¿æŒä½¿ç”¨ GQA å’Œä½¿ç”¨ MHA çš„æ€»å‚æ•°é‡ä¿æŒä¸å˜ï¼Œå¯¹äº GQA æ¨¡å‹ï¼ŒLLaMA 2ä¼šå°† FFN Dim ç»´åº¦ä¹˜ä»¥ 1.3ã€‚

ç»è¿‡ä¸Šè¿°è°ƒæ•´ä¹‹åï¼ŒLLaMA 3 ä¸å†æ˜¯æ ‡å‡†çš„ Transformer Blockï¼Œæ­¤æ—¶ä½¿ç”¨ \\(N=12d^2\\) æ¥é¢„ä¼°å‚æ•°é‡å·²ç»ä¸å¤ªå‡†ç¡®ã€‚ä½†ä¾æ—§å¯ä»¥å°†å…¶æŒ‰ç…§ï¼ˆ\\(W^Q,W^O\\)ï¼‰ï¼ˆ\\(W^K,W^V\\)ï¼‰ï¼Œ$W\_{FFN} $å’Œ \\(W\_{emb}\\) 4 ä¸ªéƒ¨åˆ†æ¥ç»Ÿè®¡ã€‚æ¯”å¦‚ï¼Œå¯¹äº LLaMA 3 æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä¸‹è¿°æ–¹å¼ä¼°è®¡å…¶å‚æ•°é‡ï¼š\\(N = n\_{layer} \\times (2d^2 + 2d \\times d \\times kv/h + 3d \\times d\_{FFN})+2 \\times Vocab \\times d\\)ã€‚

0x03 Transformeræ˜¾å­˜å ç”¨
--------------------

### 3.1 è®­ç»ƒ

åœ¨è®­ç»ƒç¥ç»ç½‘ç»œçš„è¿‡ç¨‹ä¸­ï¼Œå ç”¨æ˜¾å­˜çš„å¤§å¤´ä¸»è¦åˆ†ä¸ºå››éƒ¨åˆ†ï¼šæ¨¡å‹å‚æ•°ã€å‰å‘è®¡ç®—è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸­é—´æ¿€æ´»ã€åå‘ä¼ æ’­è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€‚åé¢å‡ ä¸ªçš„æ•°é‡å¯èƒ½æ¯”æ¨¡å‹å‚æ•°æ›´å¤§ï¼Œå› æ­¤å¯¹æ¨¡å‹å†…å­˜çš„éœ€æ±‚é‡ä¹Ÿæ›´å¤§ã€‚

è®­ç»ƒå¤§æ¨¡å‹æ—¶ç»å¸¸é‡‡ç”¨AdamWä¼˜åŒ–å™¨ï¼Œå¹¶ç”¨æ··åˆç²¾åº¦è®­ç»ƒæ¥åŠ é€Ÿè®­ç»ƒï¼Œæˆ‘ä»¬åŸºäºè¿™ä¸ªå‰æåˆ†ææ˜¾å­˜å ç”¨ã€‚åœ¨ä¸€æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œæ¯ä¸ªå¯è®­ç»ƒæ¨¡å‹å‚æ•°éœ€è¦ä¿å­˜è¿™ä¸ªå‚æ•°æœ¬èº«ã€å‚æ•°å¯¹åº”çš„æ¢¯åº¦ä»¥åŠä¼˜åŒ–å™¨å¯¹è¿™ä¸ªå‚æ•°çš„ä¸¤ä¸ªçŠ¶æ€ï¼ˆAdamä¸­çš„ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡ï¼‰ã€‚è®¾æ¨¡å‹å‚æ•°é‡ä¸º Î¦ ï¼Œé‚£ä¹ˆæ¢¯åº¦çš„å…ƒç´ æ•°é‡ä¸º Î¦ ï¼ŒAdamWä¼˜åŒ–å™¨çš„å…ƒç´ æ•°é‡ä¸º 2Î¦ ã€‚åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼Œä¼šä½¿ç”¨åŠç²¾åº¦æ¥è¿›è¡Œå‰å‘ä¸åå‘ä¼ æ’­è®¡ç®—ï¼Œä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ä¼šä½¿ç”¨å•ç²¾åº¦è¿›è¡ŒçŠ¶æ€ã€æ¢¯åº¦ä»¥åŠå‚æ•°çš„æ›´æ–°ã€‚æ‰€ä»¥ä¸€ä¸ªå‚æ•°åœ¨è®­ç»ƒæ—¶å ç”¨çš„ç©ºé—´ä¸ºæ­£å‘ä¼ æ’­æ—¶ä½¿ç”¨åŠç²¾åº¦å’Œåå‘ä¼ æ’­æ—¶ä½¿ç”¨å•ç²¾åº¦æ‰€å ç”¨çš„ç©ºé—´ä¹‹å’Œã€‚å› æ­¤ï¼Œä½¿ç”¨AdamWä¼˜åŒ–å™¨å’Œæ··åˆç²¾åº¦è®­ç»ƒæ¥è®­ç»ƒæ—¶å€™ï¼Œé’ˆå¯¹æ¯ä¸ªå¯è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œè®­ç»ƒé˜¶æ®µä¼šå ç”¨ (2+4)+(2+4)+(4+4)=20bytes ã€‚å‚æ•°é‡ä¸º Î¦ çš„å¤§æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€å ç”¨çš„æ˜¾å­˜å¤§å°ä¸º 20Î¦ bytes ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181151065-1203545758.jpg)

æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ä¸ä¼˜åŒ–å™¨çŠ¶æ€çš„ç©ºé—´å ç”¨å·²ç»è®¡ç®—å®Œäº†ï¼Œæ¥ä¸‹æ¥å°±æ˜¯åœ¨å‰å‘ä¼ æ’­æ—¶çš„ä¸­é—´æ¿€æ´»éƒ¨åˆ†çš„ç©ºé—´å ç”¨ã€‚æˆ‘ä»¬å°†åœ¨åç»­å°èŠ‚è¿›è¡Œåˆ†æã€‚

æ¨¡å‹çš„è®­ç»ƒåŒ…å« Forward å’Œ Backward è¿‡ç¨‹ã€‚Backward è¿‡ç¨‹å®é™…ä¸ŠåŒ…å«ä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ˜¯å¯¹è¾“å…¥çš„æ¢¯åº¦ï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼Œä¸€éƒ¨åˆ†æ˜¯å¯¹æƒé‡çš„æ¢¯åº¦ã€‚å…¶å®è¿™ä¸¤éƒ¨åˆ†ä¸»è¦çš„è®¡ç®—é‡éƒ½æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œå¹¶ä¸”å¤§å°ä¸ Forwardä¸­çš„å¤§å°ä¸€è‡´ï¼Œå› æ­¤å¾€å¾€ä¼šç›´æ¥è¿‘ä¼¼ Backward çš„è®¡ç®—é‡ä¸º Forward çš„ 2 å€ã€‚

### 3.2 æ¨ç†

æ¨ç†é˜¶æ®µé€šå¸¸æ¯”è®­ç»ƒé˜¶æ®µè¦æ±‚æ›´ä½çš„æ˜¾å­˜ï¼Œå› ä¸ºä¸æ¶‰åŠæ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°ç­‰å¤§é‡è®¡ç®—ã€‚å°‘äº†æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œä¸­é—´æ¿€æ´»ï¼Œæ¨¡å‹æ¨ç†é˜¶æ®µå ç”¨çš„æ˜¾å­˜è¦è¿œå°äºè®­ç»ƒé˜¶æ®µã€‚

å¦‚æœä½¿ç”¨KV cacheæ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼ŒKV cacheä¹Ÿéœ€è¦å ç”¨æ˜¾å­˜ï¼ŒKV cacheå ç”¨çš„æ˜¾å­˜ä¸‹æ–‡ä¼šè¯¦ç»†ä»‹ç»ï¼Œæ­¤å¤„å¿½ç•¥ã€‚æ­¤å¤–ï¼Œè¾“å…¥æ•°æ®ä¹Ÿéœ€è¦æ”¾åˆ°GPUä¸Šï¼Œè¿˜æœ‰ä¸€äº›ä¸­é—´ç»“æœï¼ˆæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸­é—´ç»“æœç”¨å®Œä¼šå°½å¿«é‡Šæ”¾æ‰ï¼‰ï¼Œä¸è¿‡è¿™éƒ¨åˆ†å ç”¨çš„æ˜¾å­˜æ˜¯å¾ˆå°çš„ï¼Œä¹Ÿå¯ä»¥å¿½ç•¥ã€‚

æœ€ç»ˆï¼Œæ¨ç†é˜¶æ®µçš„ä¸»è¦æ˜¾å­˜å ç”¨ä¸ºæ¨¡å‹çš„å‚æ•°ï¼Œæ¨¡å‹å‚æ•°å†…å­˜ = n Ã— pã€‚næ˜¯æ¨¡å‹å‚æ•°æ€»é‡ï¼Œpæ˜¯æ¯ä¸ªå‚æ•°å ç”¨çš„å­—èŠ‚æ•°ã€‚å¦‚æœä½¿ç”¨åŠç²¾åº¦è¿›è¡Œæ¨ç†çš„è¯ï¼Œä¸€ä¸ªå‚æ•°å ç”¨2bytesç©ºé—´ï¼Œé‚£ä¹ˆæ¨¡å‹åœ¨æ¨ç†æ—¶çš„æ˜¾å­˜å ç”¨çº¦ä¸ºï¼š

\\\[mem\_{inference} = 2 \\times n\_{params} \\\]

ä»¥ä¸‹æ˜¯è®¡ç®—æ¨¡å‹æ¨ç†æ—¶æ‰€éœ€æ˜¾å­˜çš„ä¸€äº›å…³é”®å› ç´ ï¼š

*   æ¨¡å‹ç»“æ„ï¼š æ¨¡å‹çš„ç»“æ„åŒ…æ‹¬å±‚æ•°ã€æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡ã€å·ç§¯æ ¸å¤§å°ç­‰ã€‚è¾ƒæ·±çš„æ¨¡å‹é€šå¸¸éœ€è¦æ›´å¤šçš„æ˜¾å­˜ï¼Œå› ä¸ºæ¯ä¸€å±‚éƒ½ä¼šäº§ç”Ÿä¸­é—´è®¡ç®—ç»“æœã€‚
    
*   è¾“å…¥æ•°æ®ï¼š æ¨ç†æ—¶æ‰€éœ€çš„æ˜¾å­˜ä¸è¾“å…¥æ•°æ®çš„å°ºå¯¸æœ‰å…³ã€‚æ›´å¤§å°ºå¯¸çš„è¾“å…¥æ•°æ®ä¼šå ç”¨æ›´å¤šçš„æ˜¾å­˜ã€‚
    
*   æ‰¹å¤„ç†å¤§å° BatchSizeï¼š æ‰¹å¤„ç†å¤§å°æ˜¯æŒ‡ä¸€æ¬¡æ¨ç†ä¸­å¤„ç†çš„æ ·æœ¬æ•°é‡ã€‚è¾ƒå¤§çš„æ‰¹å¤„ç†å¤§å°å¯èƒ½ä¼šå¢åŠ æ˜¾å­˜ä½¿ç”¨ï¼Œå› ä¸ºéœ€è¦åŒæ—¶å­˜å‚¨å¤šä¸ªæ ·æœ¬çš„è®¡ç®—ç»“æœã€‚
    
*   æ•°æ®ç±»å‹ï¼š ä½¿ç”¨çš„æ•°æ®ç±»å‹ï¼ˆå¦‚å•ç²¾åº¦æµ®ç‚¹æ•°ã€åŠç²¾åº¦æµ®ç‚¹æ•°ï¼‰ä¹Ÿä¼šå½±å“æ˜¾å­˜éœ€æ±‚ã€‚è¾ƒä½ç²¾åº¦çš„æ•°æ®ç±»å‹é€šå¸¸ä¼šå‡å°‘æ˜¾å­˜éœ€æ±‚ã€‚
    
*   ä¸­é—´è®¡ç®—ï¼š åœ¨æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¼šäº§ç”Ÿä¸€äº›ä¸­é—´è®¡ç®—ç»“æœï¼Œè¿™äº›ä¸­é—´ç»“æœä¹Ÿä¼šå ç”¨ä¸€å®šçš„æ˜¾å­˜ã€‚
    

### 3.3 æ¿€æ´»

è®­ç»ƒä¸­çš„æ¿€æ´»ï¼ˆactivationsï¼‰æŒ‡çš„æ˜¯ï¼šå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—å¾—åˆ°çš„ï¼Œå¹¶åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­éœ€è¦ç”¨åˆ°çš„æ‰€æœ‰å¼ é‡ã€‚è¿™é‡Œçš„æ¿€æ´»ä¸åŒ…å«æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä½†åŒ…å«äº†dropoutæ“ä½œéœ€è¦ç”¨åˆ°çš„maskçŸ©é˜µã€‚

åœ¨ä¸€æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œæ¨¡å‹å‚æ•°ï¼ˆæˆ–æ¢¯åº¦ï¼‰å ç”¨çš„æ˜¾å­˜å¤§å°åªä¸æ¨¡å‹å‚æ•°é‡å’Œå‚æ•°æ•°æ®ç±»å‹æœ‰å…³ï¼Œä¸è¾“å…¥æ•°æ®çš„å¤§å°æ˜¯æ²¡æœ‰å…³ç³»çš„ã€‚ä¼˜åŒ–å™¨çŠ¶æ€å ç”¨çš„æ˜¾å­˜å¤§å°ä¹Ÿæ˜¯ä¸€æ ·ï¼Œä¸ä¼˜åŒ–å™¨ç±»å‹æœ‰å…³ï¼Œä¸æ¨¡å‹å‚æ•°é‡æœ‰å…³ï¼Œä½†ä¸è¾“å…¥æ•°æ®çš„å¤§å°æ— å…³ã€‚è€Œä¸­é—´æ¿€æ´»å€¼ä¸è¾“å…¥æ•°æ®çš„å¤§å°ï¼ˆæ‰¹æ¬¡å¤§å° b å’Œåºåˆ—é•¿åº¦ s ï¼‰æ˜¯æˆæ­£ç›¸å…³çš„ï¼Œéšç€æ‰¹æ¬¡å¤§å° b å’Œåºåˆ—é•¿åº¦ s çš„å¢å¤§ï¼Œä¸­é—´æ¿€æ´»å ç”¨çš„æ˜¾å­˜ä¼šåŒæ­¥å¢å¤§ã€‚å½“æˆ‘ä»¬è®­ç»ƒç¥ç»ç½‘ç»œé‡åˆ°æ˜¾å­˜ä¸è¶³OOMï¼ˆOut Of Memoryï¼‰é—®é¢˜æ—¶ï¼Œé€šå¸¸ä¼šå°è¯•å‡å°æ‰¹æ¬¡å¤§å°æ¥é¿å…æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œè¿™ç§æ–¹å¼å‡å°‘çš„å…¶å®æ˜¯ä¸­é—´æ¿€æ´»å ç”¨çš„æ˜¾å­˜ï¼Œè€Œä¸æ˜¯æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çš„æ˜¾å­˜ã€‚

æˆ‘ä»¬æ¥ä¸‹æ¥ä»¥è®ºæ–‡â€œReducing Activation Recomputation in Large Transformer Modelsâ€ä¸­çš„Megatronä¸ºä¾‹ï¼Œåˆ†æ­¥æ¥è®¡ç®—ä¸€ä¸‹ä¸­é—´æ¿€æ´»çš„æ˜¾å­˜å ç”¨ã€‚

#### æ¶æ„

ä¸‹å›¾å°±æ˜¯Megatronçš„æ¶æ„ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181208917-1585611197.jpg)

å…¶ä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚å…¶ä¸­æŒ‡å®šäº†core\_attentionå°±æ˜¯submodules.core\_attentionï¼Œlinear\_projå°±æ˜¯submodules.linear\_projã€‚

    class Attention(MegatronModule, ABC):
        """Attention layer abstract class.
        This layer only contains common modules required for the "self attn" and
        "cross attn" specializations.
        """
        def __init__(
            self,
            config: TransformerConfig,
            submodules: Union[SelfAttentionSubmodules, CrossAttentionSubmodules],
            layer_number: int,
            attn_mask_type: AttnMaskType,
            attention_type: str,
        ):
            super().__init__(config=config)
    
            self.config = config
            self.layer_number = layer_number
            self.attn_mask_type = attn_mask_type
            self.attention_type = attention_type
    
            # For normal attention without groups, num_query_groups == num_attention_heads,
            # so these two will be the same
            self.query_projection_size = self.config.kv_channels * self.config.num_attention_heads
            self.kv_projection_size = self.config.kv_channels * self.config.num_query_groups
    
            # Per attention head and per partition values.
            world_size = parallel_state.get_tensor_model_parallel_world_size()
            self.hidden_size_per_attention_head = divide(
                self.query_projection_size, self.config.num_attention_heads
            )
            self.num_attention_heads_per_partition = divide(self.config.num_attention_heads, world_size)
            self.num_query_groups_per_partition = divide(self.config.num_query_groups, world_size)
    
            self.core_attention = build_module(
                submodules.core_attention,
                config=self.config,
                layer_number=self.layer_number,
                attn_mask_type=self.attn_mask_type,
                attention_type=self.attention_type,
            )
    
            self.checkpoint_core_attention = self.config.recompute_granularity == 'selective'
    
            # Output.
            self.linear_proj = build_module(
                submodules.linear_proj,
                self.query_projection_size,
                self.config.hidden_size,
                config=self.config,
                init_method=self.config.output_layer_init_method,
                bias=self.config.add_bias_linear,
                input_is_parallel=True,
                skip_bias_add=True,
                is_expert=False,
                tp_comm_buffer_name='proj',
            )
            
            
        def forward(
            self,
            hidden_states,
            attention_mask,
            key_value_states=None,
            inference_params=None,
            rotary_pos_emb=None,
            packed_seq_params=None,
        ):
            # hidden_states: [sq, b, h]
    
            # For self attention we just duplicate the rotary_pos_emb if it isn't already
            if rotary_pos_emb is not None and not isinstance(rotary_pos_emb, tuple):
                rotary_pos_emb = (rotary_pos_emb,) * 2
    
            # =====================
            # Query, Key, and Value
            # =====================
            # Get the query, key and value tensors based on the type of attention -
            # self or cross attn.
            query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)
    
            # ===================================================
            # Adjust key, value, and rotary_pos_emb for inference
            # ===================================================
            key, value, rotary_pos_emb, attn_mask_type = self._adjust_key_value_for_inference(
                inference_params, key, value, rotary_pos_emb
            )
    
            if packed_seq_params is not None:
                query = query.squeeze(1)
                key = key.squeeze(1)
                value = value.squeeze(1)
    
            # ================================================
            # relative positional embedding (rotary embedding)
            # ================================================
            if rotary_pos_emb is not None:
                q_pos_emb, k_pos_emb = rotary_pos_emb
    
                if packed_seq_params is not None:
                    cu_seqlens_q = packed_seq_params.cu_seqlens_q
                    cu_seqlens_kv = packed_seq_params.cu_seqlens_kv
                else:
                    cu_seqlens_q = cu_seqlens_kv = None
                query = apply_rotary_pos_emb(
                    query, q_pos_emb, config=self.config, cu_seqlens=cu_seqlens_q
                )
                key = apply_rotary_pos_emb(key, k_pos_emb, config=self.config, cu_seqlens=cu_seqlens_kv)
    
                # TODO, can apply positional embedding to value_layer so it has
                # absolute positional embedding.
                # otherwise, only relative positional embedding takes effect
                # value_layer = apply_rotary_pos_emb(value_layer, k_pos_emb)
    
            # ==================================
            # core attention computation
            # ==================================
    
            if self.checkpoint_core_attention and self.training:
                core_attn_out = self._checkpointed_attention_forward(
                    query,
                    key,
                    value,
                    attention_mask,
                    attn_mask_type=attn_mask_type,
                    packed_seq_params=packed_seq_params,
                )
            else:
                core_attn_out = self.core_attention(
                    query,
                    key,
                    value,
                    attention_mask,
                    attn_mask_type=attn_mask_type,
                    packed_seq_params=packed_seq_params,
                )
    
            if packed_seq_params is not None:
                # reshape to same output shape as unpacked case
                # (t, np, hn) -> (t, b=1, h=np*hn)
                # t is the pack size = sum (sq_i)
                # note that batch is a dummy dimension in the packed case
                core_attn_out = core_attn_out.reshape(core_attn_out.size(0), 1, -1)
    
            # =================
            # Output. [sq, b, h]
            # =================
    
            output, bias = self.linear_proj(core_attn_out) # è¿™é‡Œæ˜¯çº¿æ€§å±‚
    
            return output, bias
    

æœ€ç»ˆæ³¨æ„åŠ›ä»£ç æ˜¯ï¼š

    class DotProductAttention(MegatronModule):
        """
        Region where selective activation recomputation is applied.
        This region is memory intensive but less compute intensive which
        makes activation checkpointing more efficient for LLMs (20B+).
        See Reducing Activation Recomputation in Large Transformer Models:
        https://arxiv.org/abs/2205.05198 for more details.
    
        We use the following notation:
         h: hidden size
         n: number of attention heads
         p: number of tensor model parallel partitions
         b: batch size
         s: sequence length
        """
    
        def __init__(
            self,
            config: TransformerConfig,
            layer_number: int,
            attn_mask_type: AttnMaskType,
            attention_type: str,
            attention_dropout: float = None,
        ):
            super().__init__(config=config)
    
            self.config: TransformerConfig = config
    
            assert (
                self.config.context_parallel_size == 1
            ), "Context parallelism is only supported by TEDotProductAttention!"
    
            assert (
                self.config.window_size is None
            ), "Sliding Window Attention is only supported by TEDotProductAttention!"
    
            self.layer_number = max(1, layer_number)
            self.attn_mask_type = attn_mask_type
            self.attention_type = attention_type  # unused for now
    
            projection_size = self.config.kv_channels * self.config.num_attention_heads
    
            # Per attention head and per partition values.
            world_size = parallel_state.get_tensor_model_parallel_world_size()
            self.hidden_size_per_partition = divide(projection_size, world_size)
            self.hidden_size_per_attention_head = divide(projection_size, config.num_attention_heads)
            self.num_attention_heads_per_partition = divide(self.config.num_attention_heads, world_size)
            self.num_query_groups_per_partition = divide(self.config.num_query_groups, world_size)
    
            coeff = None
            self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)
            if self.config.apply_query_key_layer_scaling:
                coeff = self.layer_number
                self.norm_factor *= coeff
    
            self.scale_mask_softmax = FusedScaleMaskSoftmax(
                input_in_fp16=self.config.fp16,
                input_in_bf16=self.config.bf16,
                attn_mask_type=self.attn_mask_type,
                scaled_masked_softmax_fusion=self.config.masked_softmax_fusion,
                mask_func=attention_mask_func,
                softmax_in_fp32=self.config.attention_softmax_in_fp32,
                scale=coeff,
            )
    
            # Dropout. Note that for a single iteration, this layer will generate
            # different outputs on different number of parallel partitions but
            # on average it should not be partition dependent.
            self.attention_dropout = torch.nn.Dropout(
                self.config.attention_dropout if attention_dropout is None else attention_dropout
            )
    
        def forward(
            self,
            query: Tensor,
            key: Tensor,
            value: Tensor,
            attention_mask: Tensor,
            attn_mask_type: AttnMaskType = None,
            packed_seq_params: Optional[PackedSeqParams] = None,
        ):
            assert packed_seq_params is None, (
                "Packed sequence is not supported by DotProductAttention."
                "Please use TEDotProductAttention instead."
            )
    
            # ===================================
            # Raw attention scores. [b, n/p, s, s]
            # ===================================
    
            # expand the key and value [sk, b, ng, hn] -> [sk, b, np, hn]
            # This is a noop for normal attention where ng == np. When using group query attention this
            # creates a view that has the keys and values virtually repeated along their dimension to
            # match the number of queries.
    
            # attn_mask_type is not used.
            if self.num_attention_heads_per_partition // self.num_query_groups_per_partition > 1:
                key = key.repeat_interleave(
                    self.num_attention_heads_per_partition // self.num_query_groups_per_partition, dim=2
                )
                value = value.repeat_interleave(
                    self.num_attention_heads_per_partition // self.num_query_groups_per_partition, dim=2
                )
    
            # [b, np, sq, sk]
            output_size = (query.size(1), query.size(2), query.size(0), key.size(0))
    
            # [sq, b, np, hn] -> [sq, b * np, hn]
            # This will be a simple view when doing normal attention, but in group query attention
            # the key and value tensors are repeated to match the queries so you can't use
            # simple strides to extract the queries.
            query = query.reshape(output_size[2], output_size[0] * output_size[1], -1)
            # [sk, b, np, hn] -> [sk, b * np, hn]
            key = key.view(output_size[3], output_size[0] * output_size[1], -1)
    
            # preallocting input tensor: [b * np, sq, sk]
            matmul_input_buffer = parallel_state.get_global_memory_buffer().get_tensor(
                (output_size[0] * output_size[1], output_size[2], output_size[3]), query.dtype, "mpu"
            )
    
            # Raw attention scores. [b * np, sq, sk]
            matmul_result = torch.baddbmm(
                matmul_input_buffer,
                query.transpose(0, 1),  # [b * np, sq, hn]
                key.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]
                beta=0.0,
                alpha=(1.0 / self.norm_factor),
            )
    
            # change view to [b, np, sq, sk]
            attention_scores = matmul_result.view(*output_size)
    
            # ===========================
            # Attention probs and dropout ----------------- åœ¨è¿™é‡Œæœ‰softmaxçš„dropout
            # ===========================
    
            # attention scores and attention mask [b, np, sq, sk]
            attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)
    
            # This is actually dropping out entire tokens to attend to, which might
            # seem a bit unusual, but is taken from the original Transformer paper.
    
            if not self.config.sequence_parallel:
                with tensor_parallel.get_cuda_rng_tracker().fork():
                    attention_probs = self.attention_dropout(attention_probs)
            else:
                attention_probs = self.attention_dropout(attention_probs)
    
            # =========================
            # Context layer. [sq, b, hp]
            # =========================
    
            # value -> context layer.
            # [sk, b, np, hn] --> [b, np, sq, hn]
    
            # context layer shape: [b, np, sq, hn]
            output_size = (value.size(1), value.size(2), query.size(0), value.size(3))
    
            # change view [sk, b * np, hn]
            value = value.view(value.size(0), output_size[0] * output_size[1], -1)
    
            # change view [b * np, sq, sk]
            attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)
    
            # matmul: [b * np, sq, hn]
            context = torch.bmm(attention_probs, value.transpose(0, 1))
    
            # change view [b, np, sq, hn]
            context = context.view(*output_size)
    
            # [b, np, sq, hn] --> [sq, b, np, hn]
            context = context.permute(2, 0, 1, 3).contiguous()
    
            # [sq, b, np, hn] --> [sq, b, hp]
            new_context_shape = context.size()[:-2] + (self.hidden_size_per_partition,)
            context = context.view(*new_context_shape)
    
            return context
    

#### æœ¯è¯­è¯´æ˜

æˆ‘ä»¬é¦–å…ˆçœ‹çœ‹è®ºæ–‡ä¸­çš„æœ¯è¯­ã€‚

*   aæ˜¯ transformer æ¨¡å‹ä¸­æ³¨æ„åŠ›å¤´ (attention heads) çš„ä¸ªæ•°ã€‚
*   bä¸ºæ¯ä¸ªGPUçš„batch sizeï¼›
*   hæ˜¯æ¯ä¸ª transformer å±‚çš„éšå«ç»´åº¦
*   Lä¸ºTransformerçš„å±‚æ•°ï¼›
*   pä¸ºæµæ°´çº¿å¹¶è¡Œçš„å¹¶è¡Œæœºå™¨æ•°ï¼›
*   sä¸ºå¥å­çš„é•¿åº¦ï¼Œå³åºåˆ—ä¸­è¯å…ƒçš„ä¸ªæ•°
*   tä¸ºå¼ é‡å¹¶è¡Œçš„å¹¶è¡Œæœºå™¨æ•°ï¼›
*   vä¸ºè¯å…¸çš„å¤§å°ï¼›

æˆ‘ä»¬å‡è®¾æ¿€æ´»æ•°æ®ç±»å‹ä¸º fp16ã€‚

#### æ•°æ®é‡

æ¯ä¸ªTransformerå±‚ç”±ä¸€ä¸ªæ³¨æ„åŠ›å’Œä¸€ä¸ªMLPæ„æˆï¼Œä¸­é—´è¿˜æœ‰ä¸¤ä¸ªLayerNormã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬æ¥æ¨å¯¼å­˜å‚¨æ¯ä¸ªå…ƒç´ çš„æ¿€æ´»æ‰€éœ€çš„å†…å­˜ã€‚åœ¨ä¸‹é¢çš„åˆ†æä¸­éœ€è¦æ³¨æ„å‡ ç‚¹ï¼š

*   å•ä½æ˜¯bytesï¼Œè€Œä¸æ˜¯å…ƒç´ ä¸ªæ•°ã€‚
*   å¤§æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šå¸¸é‡‡ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå› æ­¤ï¼Œåœ¨åˆ†æä¸­é—´æ¿€æ´»çš„æ˜¾å­˜å ç”¨æ—¶ï¼Œæˆ‘ä»¬å‡è®¾ä¸­é—´æ¿€æ´»å€¼æ˜¯ä»¥float16æˆ–bfloat16æ•°æ®æ ¼å¼æ¥ä¿å­˜çš„ï¼Œæ¯ä¸ªå…ƒç´ å äº†2ä¸ªbytesã€‚å”¯ä¸€ä¾‹å¤–çš„æ˜¯ï¼Œdropoutæ“ä½œçš„maskçŸ©é˜µï¼Œæ¯ä¸ªå…ƒç´ åªå 1ä¸ªbytesã€‚
*   åœ¨åˆ†æä¸­é—´æ¿€æ´»çš„æ˜¾å­˜å ç”¨æ—¶ï¼Œåªè€ƒè™‘æ¿€æ´»å ç”¨æ˜¾å­˜çš„å¤§å¤´ï¼Œå¿½ç•¥æ‰ä¸€äº›å°çš„buffersã€‚æ¯”å¦‚ï¼Œå¯¹äºlayer normalizationï¼Œè®¡ç®—æ¢¯åº¦æ—¶éœ€è¦ç”¨åˆ°å±‚çš„è¾“å…¥ã€è¾“å…¥çš„å‡å€¼ å’Œæ–¹å·® ã€‚è¾“å…¥åŒ…å«äº† bsâ„ ä¸ªå…ƒç´ ï¼Œè€Œè¾“å…¥çš„å‡å€¼å’Œæ–¹å·®åˆ†åˆ«åŒ…å«äº† bs ä¸ªå…ƒç´ ã€‚ç”±äº â„ é€šå¸¸æ˜¯æ¯”è¾ƒå¤§çš„ï¼ˆåƒæ•°é‡çº§ï¼‰ï¼Œæœ‰ bsâ„â‰«bs ã€‚å› æ­¤ï¼Œå¯¹äºlayer normalizationï¼Œä¸­é—´æ¿€æ´»è¿‘ä¼¼ä¼°è®¡ä¸º bsâ„ ï¼Œè€Œä¸æ˜¯ bsâ„+2bs ã€‚

##### æ³¨æ„åŠ›å—

æ³¨æ„åŠ›å—çš„æ¿€æ´»å¦‚ä¸‹ã€‚

ä¿å­˜å†…å®¹

æ“ä½œ

æ¿€æ´»å¤§å°

æ‰€å±æ¨¡å—

ä¿å­˜åŸå› 

X

Query (Q), Key (K), Value (V) ç›¸å…³çš„çŸ©é˜µä¹˜æ³•

2bsh

self attention

ä¿å­˜Q/K/Vå…±åŒçš„è¾“å…¥X

Qã€K

\\(QK^T\\) çŸ©é˜µä¹˜æ³•

4bsh

self attention

ä¿å­˜ \\(QK^T\\) çŸ©é˜µä¹˜æ³•çš„è¾“å…¥

\\(QK^T\\)

Softmax

\\(2 bas^2\\)

self attention

ä¿å­˜Softmax çš„è¾“å…¥ï¼Œå½¢çŠ¶æ˜¯ \[b, a, s, s\]

Mask

Softmax dropout

\\(bas^2\\)

self attention

ä¿å­˜Softmax dropout çš„maskï¼Œå½¢çŠ¶å’Œ\\(QK^T\\)ç›¸åŒï¼Œä¸€ä¸ªbyteå³å¯

V

æ³¨æ„åŠ›è®¡ç®—

2bsh

self attention

ä¿å­˜\\(softmax(\\frac{QK^T}{\\sqrt d})V\\)çš„è¾“å…¥V

Score

æ³¨æ„åŠ›è®¡ç®—

\\(2 bas^2\\)

self attention

ä¿å­˜\\(softmax(\\frac{QK^T}{\\sqrt d})V\\)çš„è¾“å…¥\\(softmax(\\frac{QK^T}{\\sqrt d})\\)

Linear

è®¡ç®—è¾“å‡ºæ˜ å°„

2bsh

linear projection

è¾“å…¥æ˜ å°„éœ€è¦ä¿å­˜å…¶è¾“å…¥

Mask

attention dropout

bsh

attention dropout

24å†…dropoutéœ€è¦ä¿å­˜maskçŸ©é˜µï¼Œä¸€ä¸ªbyteå³å¯

æ€»è®¡

\\(11bsh + 5bas^2\\)

æˆ‘ä»¬å›é¡¾ä¸€ä¸‹MHAçš„è®¡ç®—é€»è¾‘å¦‚ä¸‹ï¼š

\\\[MultiHead(Q,K,V)=Concat(head\_1,head\_2,...,head\_{n\_{heads}})W\_O \\\\where\\ head\_i = Attention(QW^Q\_i, KW^K\_i, VW^V\_i) \\\\=softmax(\\frac{QW^Q\_i(KW\_i^K)^T}{\\sqrt d\_{head}}) VW^V\_i \\\]

ä¸Šè¿°è¡¨æ ¼ä¸­çš„å„ä¸ªè®¡ç®—è§£é‡Šå¦‚ä¸‹ã€‚

*   è¾“å…¥Xã€‚Xè¢«ç”¨æ¥è®¡ç®—Qã€Kã€Vã€‚Xçš„å½¢çŠ¶æ˜¯\[b,s,h\]ï¼Œå…ƒç´ ä¸ªæ•°æ˜¯bshï¼ŒFP16å æ®ä¸¤ä¸ªbyteï¼Œæ‰€ä»¥æ˜¾å­˜ä¸º2bshã€‚
    
*   ä¸­é—´æ¿€æ´» Qã€Kã€‚è¿™ä¸¤è€…è¢«ç”¨æ¥è®¡ç®—\\(QK^T\\)ã€‚Qã€Kçš„å½¢çŠ¶éƒ½æ˜¯\[b,s,h\]ï¼Œå…ƒç´ ç±»å‹æ˜¯FP16ï¼Œä¸¤è€…å æ®æ˜¾å­˜å¤§å°æ˜¯4bshã€‚
    
*   ä¸­é—´æ¿€æ´»\\(QK^T\\)ã€‚\\(QK^T\\)æ˜¯softmaxçš„è¾“å…¥ï¼Œå…ƒç´ ç±»å‹æ˜¯FP16ï¼Œå æ®æ˜¾å­˜å¤§å°æ˜¯\\(2bs^2a\\)ã€‚aæ˜¯æ³¨æ„åŠ›å¤´æ•°ç›®ã€‚
    
    Qçš„å½¢çŠ¶æ˜¯\[b,a,s,h/a\]ï¼Œ\\(K^T\\)å½¢çŠ¶æ˜¯\[b,a,h/a,s\]ã€‚\\(QK^T\\)å½¢çŠ¶æ˜¯\[b,a,s,s\]ã€‚è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š\\(score=softmax(QK^T/\\sqrt d\_k)\\)
    
*   dropoutç”¨åˆ°çš„maskçŸ©é˜µã€‚softmaxæ“ä½œå®Œæˆä¹‹åï¼Œä¼šè¿›è¡Œdropoutæ“ä½œã€‚éœ€è¦ä¿å­˜ä¸€ä¸ªmaskçŸ©é˜µï¼ŒmaskçŸ©é˜µçš„å½¢çŠ¶ä¸\\(QK^T\\)ç›¸åŒï¼Œç±»å‹æ˜¯intï¼Œå æ®æ˜¾å­˜æ˜¯\\(bs^2a\\)ã€‚
    
*   scoreæƒé‡çŸ©é˜µå’ŒVã€‚è¿™ä¸¤è€…è¢«ç”¨æ¥è®¡ç®—Zã€‚
    
    *   softmaxå’Œdropoutç»“æŸä¹‹åï¼Œå¾—åˆ°äº†scoreæƒé‡çŸ©é˜µï¼Œå¤§å°æ˜¯2\\(bs^2a\\)ã€‚
    *   Vçš„å½¢çŠ¶éƒ½æ˜¯\[b,s,h\]ï¼Œå…ƒç´ ç±»å‹æ˜¯FP16ï¼Œå æ®æ˜¾å­˜å¤§å°æ˜¯2bshã€‚
*   è®¡ç®—è¾“å‡ºæ˜ å°„ä»¥åŠä¸€ä¸ªdropoutæ“ä½œã€‚è¾“å…¥æ˜ å°„éœ€è¦ä¿å­˜å…¶è¾“å…¥ï¼Œå¤§å°ä¸º 2bsh ï¼›dropoutéœ€è¦ä¿å­˜maskçŸ©é˜µï¼Œå¤§å°ä¸º bsh ã€‚äºŒè€…å ç”¨æ˜¾å­˜å¤§å°åˆè®¡ä¸º 3bshã€‚
    

å› æ­¤ï¼Œå°†ä¸Šè¿°ä¸­é—´æ¿€æ´»ç›¸åŠ å¾—åˆ°self-attentionå—çš„ä¸­é—´æ¿€æ´»å ç”¨æ˜¾å­˜å¤§å°ä¸º \\(11bsh + 5bas^2\\)

##### MLP

FFNçš„ä¸¤ä¸ªçº¿æ€§å±‚ä»¥2_sbh_å’Œ8_sbh_çš„å¤§å°å­˜å‚¨å®ƒä»¬çš„è¾“å…¥ã€‚GeLUéçº¿æ€§è¿˜éœ€è¦å…¶å¤§å°ä¸º8_sbh_çš„è¾“å…¥ç”¨äºåå‘ä¼ æ’­ã€‚æœ€åï¼Œdropoutå°†å…¶æ©ç å­˜å‚¨ä¸º_sbh_å¤§å°ã€‚æ€»çš„æ¥è¯´ï¼ŒMLPå—éœ€è¦19_sbh_å­—èŠ‚çš„å­˜å‚¨ç©ºé—´ã€‚

æ¨¡å—

åŠ¨ä½œ

æ¿€æ´»å¤§å°

linear 1

ç¬¬ä¸€ä¸ªçº¿æ€§å±‚éœ€è¦ä¿å­˜å…¶è¾“å…¥

2 bsh

GeLU

æ¿€æ´»å‡½æ•°éœ€è¦ä¿å­˜å…¶è¾“å…¥

8 bsh

linear 2

ç¬¬äºŒä¸ªçº¿æ€§å±‚éœ€è¦ä¿å­˜å…¶è¾“å…¥

8 bsh

dropout

æœ€åæœ‰ä¸€ä¸ªdropoutæ“ä½œï¼Œéœ€è¦ä¿å­˜maskçŸ©é˜µ

bsh

æ€»è®¡

19_sbh_

æˆ‘ä»¬å›é¡¾ä¸€ä¸‹MHAçš„è®¡ç®—é€»è¾‘å¦‚ä¸‹ï¼š

\\\[FFN(x) = f\_{gelu}(xW\_1+b\_1)W\_2 + b\_2 \\\]

ä¸Šè¿°çš„å„ä¸ªè®¡ç®—å¦‚ä¸‹ã€‚

*   ç¬¬ä¸€ä¸ªçº¿æ€§å±‚éœ€è¦ä¿å­˜å…¶è¾“å…¥ï¼Œå ç”¨æ˜¾å­˜å¤§å°ä¸º 2bsh ã€‚
    
*   æ¿€æ´»å‡½æ•°éœ€è¦ä¿å­˜å…¶è¾“å…¥ï¼Œå ç”¨æ˜¾å­˜å¤§å°ä¸º 8bsh ã€‚
    
*   ç¬¬äºŒä¸ªçº¿æ€§å±‚éœ€è¦ä¿å­˜å…¶è¾“å…¥ï¼Œå ç”¨æ˜¾å­˜å¤§å°ä¸º 8bshã€‚
    
*   æœ€åæœ‰ä¸€ä¸ªdropoutæ“ä½œï¼Œéœ€è¦ä¿å­˜maskçŸ©é˜µï¼Œå ç”¨æ˜¾å­˜å¤§å°ä¸ºbsh ã€‚
    

å› æ­¤ï¼Œå¯¹äºMLPå—ï¼Œéœ€è¦ä¿å­˜çš„ä¸­é—´æ¿€æ´»å€¼ä¸º 19bsh ã€‚

##### LayerNorm

å¦å¤–ï¼Œself-attentionå—å’ŒMLPå—åˆ†åˆ«å¯¹åº”äº†ä¸€ä¸ªlayer normalizationã€‚æ¯ä¸ªlayer norméœ€è¦ä¿å­˜å…¶è¾“å…¥ï¼Œå¤§å°ä¸º 2_sbh_ã€‚2ä¸ªlayer norméœ€è¦ä¿å­˜çš„ä¸­é—´æ¿€æ´»ä¸º 4_sbh_ã€‚

##### æ€»ç»“

ç»¼ä¸Šï¼Œæ¯ä¸ªtransformerå±‚éœ€è¦ä¿å­˜çš„ä¸­é—´æ¿€æ´»å ç”¨æ˜¾å­˜å¤§å°ä¸º\\(34bsh + 5bas^2\\)ã€‚å¯¹äº l å±‚transformeræ¨¡å‹ï¼Œè¿˜æœ‰embeddingå±‚ã€æœ€åçš„LayerNormå’Œè¾“å‡ºå±‚ã€‚å½“éšè—ç»´åº¦ â„ æ¯”è¾ƒå¤§ï¼Œå±‚æ•°l è¾ƒæ·±æ—¶ï¼Œè¿™éƒ¨åˆ†çš„ä¸­é—´æ¿€æ´»æ˜¯å¾ˆå°‘çš„ï¼Œå¯ä»¥å¿½ç•¥ã€‚å› æ­¤ï¼Œå¯¹äº l å±‚transformeræ¨¡å‹ï¼Œä¸­é—´æ¿€æ´»å ç”¨çš„æ˜¾å­˜å¤§å°å¯ä»¥è¿‘ä¼¼ä¸º \\((34bsh + 5bas^2)\\times l\\)ã€‚

ä½œä¸ºå¯¹æ¯”ï¼Œä¸‹å›¾æ˜¯å“ˆä½›ä»£ç ä¸­è§£ç å™¨å¯¹åº”çš„æ¿€æ´»æƒ…å†µï¼Œé‡Œé¢æœ‰å„ä¸ªå¼ é‡çš„å½¢çŠ¶ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181220298-723321234.jpg)

æœ‰ç ”ç©¶æŒ‡å‡ºï¼Œ`13B` çš„ `LLM` æ¨ç†æ—¶ï¼Œæ¯ä¸ª `token` å¤§çº¦æ¶ˆè€— `1MB` çš„æ˜¾å­˜ã€‚

å¦å¤–ï¼Œå¯¹äºè®¡ç®—é‡å’Œæ˜¾å­˜é‡ï¼Œæˆ‘ä»¬ä¹Ÿå¾ˆå®¹æ˜“è§åˆ°ä¸åŒçš„è®¡ç®—ç»“æœï¼Œè¿™åŸºæœ¬æ˜¯å› ä¸ºè®¡ç®—åŸåˆ™ä¸åŒï¼Œæ¯”å¦‚ï¼šæ¢¯åº¦å¯èƒ½æ˜¯FP16å­˜å‚¨ï¼Œå‚æ•°å¯èƒ½æ˜¯FP32å­˜å‚¨ï¼Œæ˜¯å¦é‡‡ç”¨é‡è®¡ç®—ç­‰ç­‰ã€‚

##### å¹¶è¡Œ

å®é™…å·¥ä½œä¸­ï¼ŒLLMæ€»æ˜¯ä»¥å„ç§å¹¶è¡Œç­–ç•¥è¿›è¡Œè®­ç»ƒæˆ–è€…æ¨ç†ï¼Œæ¿€æ´»åˆå„ä¸ç›¸åŒã€‚ä¸‹å›¾æ˜¯å„ç§å¹¶è¡Œç­–ç•¥ä¸‹ï¼Œæ¯ä¸ªTransfromerå±‚çš„æ¿€æ´»å¤§å°ï¼ˆbytesï¼‰ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181231924-873542608.jpg)

æˆ‘ä»¬å†æ¥çœ‹çœ‹å¹¶è¡Œç­–ç•¥ä¸‹ï¼Œå¯¹äº l å±‚transformeræ¨¡å‹ï¼Œembeddingå±‚ã€æœ€åçš„LayerNormå’Œè¾“å‡ºå±‚æ‰€è¾“å‡ºçš„æ¿€æ´»ã€‚

*   ä½ç½®å’Œå•è¯åµŒå…¥ä¸éœ€è¦ä¸ºåå‘ä¼ æ’­å­˜å‚¨ä»»ä½•å¤§é‡çš„æ¿€æ´»ã€‚ä½†æ˜¯dropoutéœ€è¦å­˜å‚¨ã€‚åµŒå…¥å±‚ä¸­çš„dropoutä¹Ÿä¼šæ²¿ç€åºåˆ—ç»´åº¦è¿›è¡Œå¹¶è¡Œï¼ˆsequence parallelismï¼‰ã€‚å› æ­¤ï¼Œå®ƒçš„å­˜å‚¨å°†å æ®sbhp/tå¤§å°ã€‚è¯·æ³¨æ„ï¼Œç³»æ•°pæ˜¯å› ä¸ºæµæ°´çº¿å¹¶è¡Œä¸­ï¼Œæˆ‘ä»¬éœ€è¦å­˜å‚¨pä¸ªmicrobatchesï¼ˆå¾®æ‰¹æ¬¡ï¼‰ã€‚
    
*   è¾“å‡ºå±‚ä¹‹å‰çš„Layer Normä¹Ÿä½¿ç”¨åºåˆ—å¹¶è¡Œï¼ˆsequence parallelismï¼‰ï¼Œå› æ­¤éœ€è¦2sbh/tå­˜å‚¨ã€‚è¾“å‡ºå±‚ä¼šæŠ•å½±åˆ°è¯æ±‡è¡¨ç»´åº¦ï¼Œè¿™éœ€è¦å­˜å‚¨å¤§å°ä¸º2sbh/tçš„è¾“å…¥ã€‚æœ€åï¼Œäº¤å‰ç†µæŸå¤±ï¼ˆcross entropy lossï¼‰éœ€è¦å­˜å‚¨ä»¥32ä½æµ®ç‚¹è¿›è¡Œè®¡ç®—çš„logitï¼Œå› æ­¤éœ€è¦4sbv/tçš„å­˜å‚¨ç©ºé—´ã€‚è¯·æ³¨æ„ï¼Œç”±äºæˆ‘ä»¬åªè€ƒè™‘æµæ°´çº¿ç¬¬ä¸€é˜¶æ®µçš„æ¿€æ´»ï¼Œå› æ­¤ä¸Šè¿°æ¿€æ´»ï¼Œå³æ€»å…±4sbh/t(1+v/h)ï¼Œä»…åœ¨æ²¡æœ‰æµæ°´çº¿å¹¶è¡Œï¼ˆp=1ï¼‰çš„æƒ…å†µä¸‹æ‰ä¼šè€ƒè™‘åœ¨å†…ã€‚
    
*   è¾“å…¥åµŒå…¥ã€æœ€åä¸€ä¸ªLayerNormå’Œè¾“å‡ºå±‚è€Œäº§ç”Ÿçš„æ€»å…±é¢å¤–å†…å­˜ä¸ºï¼š![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181243053-1734045339.jpg)
    

0x04 Transformerè®¡ç®—é‡
-------------------

å¹¿ä¹‰ä¸Šï¼Œå½“å¤„ç†ä¸€ä¸ª token æ—¶ï¼Œæ¨¡å‹æ‰§è¡Œä¸¤ç§ç±»å‹çš„æ“ä½œï¼šæ³¨æ„åŠ›è®¡ç®—å’ŒçŸ©é˜µ-å‘é‡ä¹˜æ³•ã€‚

*   MHAï¼ˆçº¢æ¡†ï¼‰ï¼š\\(W\_Q\\)ï¼Œ\\(W\_K\\)ï¼Œ\\(W\_V\\) å¯¹åº”çš„è®¡ç®—é‡éƒ½ä¸º 2 x (d x d x l)ï¼Œå…¶ä¸­ 2 è¡¨ç¤ºä¸€ä¸ªä¹˜æ³•å’Œä¸€ä¸ªåŠ æ³•ã€‚
*   MHAï¼ˆè“æ¡†ï¼‰ï¼š\\(W\_{out}\\) å¯¹åº”çš„è®¡ç®—é‡ä¸º 2 x (d x d x l)ã€‚
*   MHA Attentionï¼ˆç»¿è‰²åœ†è§’æ–¹å—ï¼‰ï¼šè®¡ç®—é‡æ˜¯2 x (l x d/h x l + l x d/h x l) x h = 4 x d x l x lã€‚å¦‚æœæ˜¯ Decoderï¼ˆLLMï¼‰ï¼Œç”±äº Causal Mask çš„å­˜åœ¨ï¼Œæ­¤å¤„çš„è®¡ç®—é‡åº”è¯¥å‡åŠï¼Œä¹Ÿå°±æ˜¯ 2 x d x l x lã€‚
*   FFNï¼ˆç»¿æ¡†ï¼‰ï¼šW1 å’Œ W2 å¯¹åº”çš„è®¡ç®—é‡ä¸º $2 \\times (d\_{FFN} \\times d \\times l) $å’Œ \\(2\\times (d \\times \_{FFN} \\times l)\\)ã€‚LLaMA çš„ SwiGLU ç±»ä¼¼ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181336825-1748092788.jpg)

æˆ‘ä»¬åç»­ä¹ŸæŒ‰ç…§megatronè®ºæ–‡çš„æœ¯è¯­è¿›è¡Œåˆ†æï¼Œå¿½ç•¥å¤šå¤´ï¼Œå³å¤´æ•°ä¸º1ã€‚

### 4.1 çŸ©é˜µä¹˜æ³•

åœ¨decodeé˜¶æ®µï¼Œåˆ™ä¸»è¦æ˜¯çŸ©é˜µ-å‘é‡ä¹˜æ³•ã€‚ä¸€ä¸ªå¤§çŸ©é˜µä¹˜ä»¥ä¸€ä¸ªå‘é‡ï¼Œå¾—åˆ°å¦ä¸€ä¸ªå‘é‡ã€‚

å› æ­¤æˆ‘ä»¬é¦–å…ˆçœ‹çœ‹çŸ©é˜µä¹˜æ³•çš„è®¡ç®—ç‰¹ç‚¹ã€‚äººä»¬å®šä¹‰ç®—æœ¯å¼ºåº¦ï¼ˆArithmetic Intensityï¼‰ä¸ºFLOP : I/Oã€‚å½“å°†ä¸€ä¸ª\\(N\\times M\\)çŸ©é˜µä¸ä¸€ä¸ª\\(M\\times P\\)çŸ©é˜µç›¸ä¹˜ä»¥äº§ç”Ÿä¸€ä¸ª\\(N\\times P\\)çŸ©é˜µæ—¶ï¼ŒçŸ©é˜µ-å‘é‡ä¹˜æ³•å¯¹æ¯ä¸ªçŸ©é˜µå…ƒç´ æ‰§è¡Œä¸€æ¬¡ä¹˜åŠ è¿ç®—ã€‚FLOPï¼ˆæµ®ç‚¹æ“ä½œï¼Œå³è®¡ç®—é‡ï¼‰ä¸º\\(2M\\times P \\times N\\)ï¼ŒI/Oï¼ˆä»GPUå†…å­˜ä¼ è¾“åˆ°GPUå¯„å­˜å™¨çš„æ•°æ®ä¼ è¾“ï¼‰è®¡æ•°ä¸º\\(M\\times N + M \\times P + N \\times P\\) ã€‚

### 4.2 å‰å‘ä¼ æ’­è®¡ç®—é‡

#### Embedding

Embeddingæ“ä½œçš„è¾“å…¥æ˜¯\[b,s\]ã€‚åœ¨å®é™…è®¡ç®—çš„çŸ©é˜µ-å‘é‡ä¹˜æ³•ä¸­ï¼Œembeddingæ“ä½œå¹¶ä¸ä¼šä½¿ç”¨è¿™æ•´ä¸ªembeddingå¤§çŸ©é˜µï¼Œæ¯ä¸ª token åªè¯»å–è¿™ä¸ªçŸ©é˜µä¸­çš„ä¸€è¡Œï¼Œå°±æ˜¯æŸ¥è¡¨æ“ä½œã€‚æœ€ç»ˆè¾“å‡ºå¼ é‡å˜æˆ\[b,s,h\]ã€‚å› æ­¤è®¡ç®—é‡ç›¸å¯¹å¾ˆå°ï¼Œåé¢æˆ‘ä»¬å°†å¿½ç•¥è¿™éƒ¨åˆ†ã€‚

#### MHA

åœ¨æ ‡å‡†çš„Transformerè®¡ç®—ä¸­ï¼Œå‡è®¾\\(Q,K,V \\in R^{s\\times h}\\)ï¼Œåˆ™è®¡ç®—å¦‚ä¸‹ï¼ˆçœç•¥äº†\\(\\sqrt h\\)ï¼‰ã€‚Næ˜¯åºåˆ—é•¿åº¦ï¼Œhæ˜¯ç»´åº¦ã€‚

*   è·å–æ³¨æ„åŠ›åˆ†æ•° ï¼š$ S = QK^T \\in R^{s \\times s}$ã€‚å¯¹æ¯ä¸ª query å‘é‡ï¼Œéƒ½è®¡ç®—å®ƒä¸æ‰€æœ‰ä½ç½®çš„ key å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ã€‚
*   è·å–æ³¨æ„åŠ›æƒé‡ï¼š$ P = softmax(S) \\in R^{s \\times s}$ã€‚å³å½’ä¸€åŒ–å¾—åˆ°çš„ä¸€ç»„æ ‡é‡ã€‚
*   è®¡ç®—æœ€ç»ˆè¾“å‡ºï¼š\\(O = PV \\in R^{s \\times h}\\)ã€‚ä½¿ç”¨æ³¨æ„åŠ›æƒé‡ï¼Œå¯¹æ‰€æœ‰ä¹‹å‰çš„ value å‘é‡è¿›è¡ŒåŠ æƒæ±‚å’Œæ¥è®¡ç®—ä¸€ä¸ªå‘é‡oã€‚

å› æ­¤æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼Œè®¡ç®—Så’ŒOæ˜¯ä¸»è¦çš„éƒ¨åˆ†ã€‚

##### è®¡ç®—Qã€Kã€V

å•ä¸ªçŸ©é˜µä¹˜æ³•æ˜¯ï¼š\[b, s, h\] x \[h, h\] å¾—åˆ° \[b, s, h\]ï¼Œå› æ­¤å…¶è®¡ç®—é‡æ˜¯\\(2bsh^2\\)ã€‚ä¸‰ä¸ªçŸ©é˜µçš„è®¡ç®—é‡æ˜¯ \\(3 \\times 2 bsh^2 = 6 bsh^2\\)

##### QK^T

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œé’ˆå¯¹æ¯ä¸ªqueryå…ƒç´ ï¼Œæ³¨æ„åŠ›è®¡ç®—ä¼šå¯¹æ¯ä¸ªé”®å…ƒç´ æ‰§è¡Œä¸€æ¬¡ä¹˜åŠ æ“ä½œä»¥è®¡ç®—ç‚¹ç§¯ã€‚æ€»ä½“æ“ä½œä¸ºï¼š\[b, s, h\] x \[b, h, s\] = \[b, s, s\] ï¼Œå…¶è®¡ç®—é‡æ˜¯ï¼š\\(2bs^2h\\)

softmax å‡½æ•°ä¸ä¼šæ”¹å˜è¾“å…¥çŸ©é˜µçš„ç»´åº¦ï¼Œå³ \[ğ‘ ,ğ‘ \]â†’\[_s_,_s_\]ï¼Œnative softmax çš„ `FLOPs` ä¸º (4/5)_sh_ã€‚å› ä¸ºæ¯”è¾ƒå°ï¼Œæ‰€ä»¥å¯ä»¥å¿½ç•¥ã€‚ç¼©æ”¾ \\(\\sqrt d\\) æ˜¯é€å…ƒç´ æ“ä½œï¼Œä¹Ÿå¯ä»¥å¿½ç•¥ã€‚

##### ä¹˜ä»¥V

ä¹˜ä»¥Vï¼ˆattention over valuesï¼‰é˜¶æ®µä¼šå¯¹æ¯ä¸ªå€¼å…ƒç´ æ‰§è¡Œä¸€æ¬¡ä¹˜åŠ æ“ä½œä»¥è®¡ç®—åŠ æƒå’Œã€‚æ€»ä½“æ“ä½œä¸ºï¼š \[b, s, s\] x \[b, s, h\] = \[b, s, h\]ï¼Œè®¡ç®—é‡æ˜¯ï¼š\\(2bs^2h\\)ã€‚

##### çº¿æ€§æ˜ å°„

çº¿æ€§æ˜ å°„ï¼ˆpost-attention linear projectionï¼‰è¿™ä¸€æ­¥æ˜¯ä¸\\(W^O\\)çš„å¤šå¤´èåˆï¼ŒçŸ©é˜µä¹˜æ³•çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ä¸º \[b,s,â„\]Ã—\[â„,â„\]â†’\[b,s,â„\] ã€‚è®¡ç®—é‡ä¸º \\(2bsâ„^2\\) ã€‚

#### MLP

è¿™ä¸€æ­¥æ¶‰åŠä¸¤ä¸ªæ“ä½œã€‚

*   ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ï¼ŒçŸ©é˜µä¹˜æ³•çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ä¸º \[b,s,â„\]Ã—\[â„,4â„\]â†’\[b,s,4â„\] ã€‚è®¡ç®—é‡ä¸º \\(8bsâ„^2\\) ã€‚
    
*   ç¬¬äºŒä¸ªçº¿æ€§å±‚ï¼ŒçŸ©é˜µä¹˜æ³•çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ä¸º \[b,s,4â„\]Ã—\[4â„,â„\]â†’\[b,s,â„\] ã€‚è®¡ç®—é‡ä¸º \\(8bsâ„^2\\) ã€‚
    

#### LayerNorm

`LayerNorm` æ“ä½œæ˜¯é€å…ƒç´ è¿›è¡Œçš„ï¼Œå› æ­¤ä¸å­˜åœ¨é€šç”¨çš„å…¬å¼æ¥ã€‚`LayerNorm` å±‚çš„ä¸¤ä¸ªæƒé‡éƒ½æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º â„ çš„å‘é‡ï¼Œ`FLOPs` å¯ä»¥é¢„ä¼°ä¸º: 2â„ï¼Œä½†é€šå¸¸å¿½ç•¥ä¸è®¡ã€‚

#### å•å±‚layer

å°†ä¸Šè¿°è®¡ç®—é‡ç›¸åŠ ï¼Œå¾—åˆ°å‰å‘ä¼ æ’­é˜¶æ®µä¸­æ¯ä¸ªtransformerå±‚çš„è®¡ç®—é‡å¤§çº¦ä¸º $24bsâ„2+4bs2â„ $ï¼Œå¯ä»¥å‘ç°ï¼š

*   å‚æ•°é‡å’Œè®¡ç®—é‡è·Ÿheadæ•°é‡æ— å…³ï¼Œheadåˆ’åˆ†æ›´å¤šæ˜¯é€šè¿‡ç‰¹å¾å­ç©ºé—´åˆ’åˆ†æé«˜ç²¾åº¦ï¼Œè€Œä¸æ˜¯ä¸ºäº†èŠ‚çœå‚æ•°é‡æˆ–è€…è®¡ç®—é‡ã€‚
    
*   å›å¿†å‚æ•°é‡æ˜¯\\(12lh^2\\)ï¼Œæ‰€ä»¥åœ¨ç»™å®šå›ºå®šåºåˆ—é•¿åº¦çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—é‡ä¹Ÿéšç€å‚æ•°çš„æ•°é‡å¢åŠ è€Œçº¿æ€§å¢åŠ ã€‚
    
*   è®¡ç®—å¤æ‚åº¦éšç€åºåˆ—é•¿åº¦çš„å¢åŠ å‘ˆäºŒæ¬¡æ–¹å¢åŠ çš„è¶‹åŠ¿ã€‚
    

Attention

è®¡ç®—é‡

FFN

è®¡ç®—é‡

è®¡ç®—Qã€Kã€V

\\(6 bsh^2\\)

ç¬¬ä¸€ä¸ªçº¿æ€§å±‚

\\(8 bsâ„^2\\)

QK^T

\\(2 bs^2h\\)

ç¬¬äºŒä¸ªçº¿æ€§å±‚

\\(8 bsâ„^2\\)

ä¹˜ä»¥V

\\(2 bs^2h\\)

çº¿æ€§æ˜ å°„

\\(2 bsâ„^2\\)

### 4.3 ç»¼åˆæ€è€ƒ

æ¨¡å‹çš„è®­ç»ƒåŒ…å«å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ã€‚ä¸Šè¿°åªæ˜¯ä¸»è¦è€ƒè™‘åˆ°å‰å‘ä¼ æ’­é˜¶æ®µä¸­ï¼ŒTransformerçš„è®¡ç®—é‡ã€‚æˆ‘ä»¬æ¥ä¸‹æ¥ç»“åˆåå‘ä¼ æ’­æ¥ç»¼åˆè€ƒè™‘ã€‚åå‘ä¼ æ’­è¿‡ç¨‹å®é™…ä¸ŠåŒ…å«ä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ˜¯å¯¹è¾“å…¥çš„æ¢¯åº¦çš„è®¡ç®—ï¼Œä¸€éƒ¨åˆ†æ˜¯å¯¹æƒé‡çš„æ¢¯åº¦ã€‚å…¶å®è¿™ä¸¤éƒ¨åˆ†ä¸»è¦çš„è®¡ç®—é‡éƒ½æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œå¹¶ä¸”å¤§å°ä¸ å‰å‘ä¼ æ’­ä¸­çš„è®¡ç®—é‡å¤§å°ä¸€è‡´ï¼Œå› æ­¤å¾€å¾€ä¼šç›´æ¥æŠŠåå‘ä¼ æ’­çš„è®¡ç®—é‡è¿‘ä¼¼ä¸ºå‰å‘ä¼ æ’­çš„ 2 å€ã€‚

#### åå‘ä¼ æ’­

æˆ‘ä»¬æŠŠåå‘ä¼ æ’­åŠ è¿›æ¥ç»§ç»­åˆ†æã€‚

##### å•å±‚

å•ä¸ªTransformerå±‚çš„è®¡ç®—é‡ç°åœ¨å¦‚ä¸‹ï¼š

*   å‰å‘ä¼ æ’­æ‰€éœ€è¦çš„æµ®ç‚¹æ•°è¿ç®—ï¼š\\(24 bsâ„^2 + 4 bs^2â„\\)ã€‚

*   å¯¹äºbackwardï¼Œå¯¹äºç¥ç»ç½‘ç»œä¸­çš„æƒé‡å’Œè¾“å…¥éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œå› æ­¤åå‘ä¼ æ’­éœ€è¦2å€FLOPsã€‚
*   å¦‚æœä½¿ç”¨activation checkpointingï¼šåœ¨backwardçš„æ—¶å€™ï¼Œæ¯ä¸€å±‚éœ€è¦é¢å¤–çš„è®¡ç®—forwardã€‚

æ‰€ä»¥æ¯å±‚éœ€è¦çš„æ€»æµ®ç‚¹æ•°è®¡ç®—ä¸º\\(4Ã—(24 bsâ„^2 + 4 bs^2â„)=96bsâ„^2(1+s/6â„)\\)ã€‚

##### logits

å¦ä¸€ä¸ªè€—è´¹è®¡ç®—é‡çš„éƒ¨åˆ†æ˜¯logitsçš„è®¡ç®—ï¼šå°†éšè—å‘é‡æ˜ å°„ä¸ºè¯è¡¨å¤§å°ï¼Œå¾—åˆ°æ¯ä¸ª token å¯¹åº”çš„ logits å‘é‡ã€‚çŸ©é˜µä¹˜æ³•çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ä¸º \[b,s,â„\]Ã—\[â„,V\]â†’\[b,s,V\] ã€‚çŸ©é˜µä¹˜æ³•çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ä¸º: \[ğ‘ ,â„\]Ã—\[â„,ğ‘‰\]âˆ’>\[_s_,_V_\]ã€‚

å› æ­¤å‰å‘ä¼ æ’­éœ€è¦ 2bsâ„V ï¼Œåå‘ä¼ æ’­éœ€è¦ 4bsâ„V ï¼Œæ€»ä½“éœ€è¦ 6bsâ„V çš„è®¡ç®—é‡ã€‚

#### æ€»ä½“è®¡ç®—é‡

Megatron-LMçš„ç»å…¸è®ºæ–‡ "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"ç»™å‡ºäº†ä¸€ä¸ªè®¡ç®—æ ‡å‡†Transformer-decoderç»“æ„æµ®ç‚¹æ•°è¿ç®—çš„å…¬å¼ã€‚å¯¹äºä¸€ä¸ª l å±‚çš„transformeræ¨¡å‹ï¼Œè¾“å…¥å½¢çŠ¶ä¸º \[b,s\] æ—¶ï¼Œå…¶è®¡ç®—é‡å¦‚ä¸‹ã€‚

*   å•æ¬¡æ¨ç†ï¼Œå‰å‘ä¼ æ’­æ‰€éœ€è¦çš„æµ®ç‚¹æ•°è¿ç®—ï¼š\\(l\\times(24bsâ„^2+4bs^2â„)+2bsâ„V\\)
    
*   å•æ¬¡è®­ç»ƒï¼Œå‰å‘åå‘ä¼ æ’­éœ€è¦æµ®ç‚¹è¿ç®—ä¸ºï¼š
    

\\\[96blsh^2(1+\\frac{s}{6â„}+\\frac{V}{16lâ„}) + 2bshV \\\]

å¦‚æœæ²¡æœ‰å¦‚æœä½¿ç”¨activation checkpointingï¼Œåˆ™æ˜¯

\\\[72blsh^2(1+\\frac{s}{6â„}+\\frac{V}{16lâ„}) + 2bshV \\\]

åœ¨Megatron-Deepspeedçš„ä»£ç é‡Œï¼Œæˆ‘ä»¬ä¹Ÿèƒ½çœ‹åˆ°ç”¨è¿™ä¸ªå…¬å¼æ¥è®¡ç®—TFLOPSï¼ˆæ¯ç§’æ‰€æ‰§è¡Œçš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼Œfloating-point operations per secondï¼‰ï¼š

    # General TFLOPs formula (borrowed from Equation 3 in Section 5.1 of
    # https://arxiv.org/pdf/2104.04473.pdf).
    # The factor of 4 is when used with activation check-pointing,
    # otherwise it will be 3, but for 200B model, activation check-pointing will always be on.
    checkpoint_activations_factor = 4 if args.checkpoint_activations else 3
    # GLU activations double the hidden states in the upscaling feed-forward in each transformer layer
    # This leads to 16bsh^2 instead of 8bsh^2 per first feed-forward layer in MLP, thus we increase the coefficient by 8.
    # Refer to https://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/283#issue-1260805063 for more details.
    coefficient = 32 if args.glu_activation else 24
    flops_per_iteration = (coefficient * checkpoint_activations_factor * batch_size * seq_len * num_layers * (hidden_size**2)) * (1. + (seq_len / (6. * hidden_size)) + (vocab_size / (16. * num_layers * hidden_size)))
    tflops = flops_per_iteration / (elapsed_time_per_iteration * args.world_size * (10**12))
    
    

### 4.4 è®¡ç®—ç‰¹ç‚¹

#### ä¸å‚æ•°é‡çš„å…³ç³»

æˆ‘ä»¬å…ˆç»™å‡ºç»“è®ºï¼šè®¡ç®—é‡ä¸»è¦å’Œæ¨¡å‹å‚æ•°å’Œtokenæ•°ç›¸å…³ã€‚å‡è®¾æ•°æ®é›†ä¸­æ€»å…±åŒ…å« D ä¸ª Tokenï¼Œæ¨¡å‹å‚æ•°é‡ä¸ºNï¼Œåˆ™å¯¹äºåºåˆ—ä¸æ˜¯ç‰¹åˆ«é•¿çš„åœºæ™¯ï¼Œæ‰€æœ‰ Token Forwardçš„è®¡ç®—é‡å¯ä»¥è¿‘ä¼¼ä¸º2NDã€‚

##### å•æ¬¡æ¨ç†

å•æ¬¡æ¨ç†æ—¶å€™ï¼Œè®¡ç®—é‡å’Œå‚æ•°é‡çš„å…³ç³»å¦‚ä¸‹ï¼š

\\\[\\frac{è®¡ç®—é‡}{å‚æ•°é‡} = \\frac{lâˆ—(24bsâ„^2+4bs^2â„)+2bsâ„V}{l(12h^2 + 13h) + 2vh} \\approx \\frac{24lbsh^2}{12lh^2} = 2bs \\\]

å› ä¸ºå•æ¬¡æ¨ç†æ—¶è¾“å…¥çš„tokenæ•°ä¸ºbsï¼Œå› æ­¤å¯ä»¥è¿‘ä¼¼è®¤ä¸ºï¼Œåœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­ï¼Œå¯¹äºæ¯ä¸ªtokenï¼Œæ¯ä¸ªæ¨¡å‹å‚æ•°éœ€è¦è¿›è¡Œ2æ¬¡æµ®ç‚¹è¿ç®—ï¼ˆä¸€æ¬¡ä¹˜æ³•ï¼Œä¸€æ¬¡åŠ æ³•ï¼‰ã€‚å³ä»å•ä¸ª Token å•ä¸ªçŸ©é˜µä¹˜çš„è§†è§’ï¼Œå¯ä»¥è¿‘ä¼¼è®¤ä¸ºï¼Œå•æ¬¡æ¨ç†æ—¶ï¼ˆåªåŒ…å«æ­£å‘ä¼ æ’­ï¼‰çš„è®¡ç®—é‡å°±æ˜¯å‚æ•°é‡çš„ 2 å€ï¼Œå°±æ˜¯æ¯ä¸ª token è¿‡ä¸€éæ‰€æœ‰å‚æ•°çš„è®¡ç®—é‡ã€‚

ä¸€æ¬¡è¿­ä»£è®­ç»ƒåŒ…å«äº†å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œåå‘ä¼ æ’­çš„è®¡ç®—é‡æ˜¯å‰å‘ä¼ æ’­çš„ `2` å€ã€‚å› æ­¤ï¼Œå³ä¸€æ¬¡è¿­ä»£è®­ç»ƒä¸­ï¼Œå¯¹äºæ¯ä¸ª token å’Œ æ¯ä¸ªæ¨¡å‹å‚æ•°ï¼Œéœ€è¦è¿›è¡Œ 6 æ¬¡æµ®ç‚¹æ•°è¿ç®—ã€‚

åœ¨è®ºæ–‡"Scaling Laws for Neural Language Model"ä¸­ä¹Ÿæœ‰ç±»ä¼¼çš„è®¡ç®—å…¬å¼ï¼Œå…·ä½“å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181401046-827998950.jpg)

##### å•æ¬¡è®­ç»ƒ

ä¸€æ¬¡è®­ç»ƒè¿­ä»£åŒ…å«äº†å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œå› ä¸ºåå‘ä¼ æ’­è®¡ç®—é‡æ˜¯å‰å‘ä¼ æ’­çš„2å€ï¼Œæ‰€ä»¥å•æ¬¡è®­ç»ƒï¼Œå¯¹äºæ¯ä¸ªtokenï¼Œæ¯ä¸ªæ¨¡å‹å‚æ•°éœ€è¦è¿›è¡Œ6æ¬¡æµ®ç‚¹è¿ç®—ã€‚è®­ç»ƒæ€»ç®—åŠ›ï¼ˆFlopsï¼‰= 6 \* æ¨¡å‹çš„å‚æ•°é‡ \* è®­ç»ƒæ•°æ®çš„ token æ•°ã€‚è¿™å°±æ˜¯æ‰€æœ‰è®­ç»ƒæ•°æ®è¿‡ä¸€éè®­ç»ƒæ‰€éœ€çš„ç®—åŠ›ã€‚å¦‚æœéœ€è¦è®­ç»ƒéœ€è¦å¤šå°‘æ—¶é•¿ï¼Œåˆ™å¯ä»¥è¿‘ä¼¼ä½¿ç”¨ä¸‹é¢å…¬å¼ï¼š

\\\[\\frac{6 \\times æ¨¡å‹å‚æ•°é‡ \\times æ•°æ®é‡ }{(GPUæ•°é‡ \\times GPU\\ FLOPS \\times GPUåˆ©ç”¨ç‡)} \\\]

#### å¸¦å®½å—é™

ç®—åŠ›å¹¶ä¸èƒ½è¯´æ˜ä¸€åˆ‡ï¼Œæ¨¡å‹è¿˜éœ€è¦è®¿é—® GPU å†…å­˜ï¼Œå†…å­˜å¸¦å®½ä¹Ÿå¯èƒ½æˆä¸ºç“¶é¢ˆã€‚å› ä¸ºéœ€è¦æŠŠå‚æ•°ä»å†…å­˜é‡Œé¢è¯»å‡ºæ¥å§ï¼Ÿå†…å­˜è®¿é—®é‡ = å‚æ•°æ•°é‡ \* 2 bytesã€‚é’ˆå¯¹å†…å­˜å¸¦å®½éƒ¨åˆ†ï¼Œå¤§è¯­è¨€æ¨¡å‹ä¸­çš„è®¡ç®—å…·å¤‡ä¸€äº›é²œæ˜çš„ç‰¹ç‚¹ã€‚æˆ‘ä»¬ä¸€ä¸€è¿›è¡Œåˆ†æã€‚

##### æ³¨æ„åŠ›è®¡ç®—

åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­ï¼Œæ³¨æ„åŠ›è®¡ç®—æ˜¯è®¿å­˜å¯†é›†å‹çš„ï¼Œå…¶è€—æ—¶å—é™äºç¡¬ä»¶çš„è®¿å­˜å¸¦å®½ï¼Œè€Œéè¿ç®—é€Ÿåº¦ã€‚

å¯¹äºçŸ©é˜µä¹˜æ³•ç®—å­ï¼Œå…¶ç‰¹ç‚¹å¦‚ä¸‹ã€‚

*   å‚æ•°é‡å¤ªå¤§ä¼šå¯¼è‡´çŸ©é˜µä¹˜æ³•ç®—å­æˆä¸ºè®¿å­˜å¯†é›†å‹ã€‚å½“è¾“å…¥çš„æ•°æ®ä¸å¤Ÿå¤šï¼Œè¿ç®—é‡ä¸å¤Ÿå¤§çš„æ—¶å€™ï¼Œè¿™äº›ç®—å­ä¼šå› ä¸ºå‚æ•°è®¿å­˜è¿‡å¤šè€Œå—é™äºè®¿å­˜å¸¦å®½ã€‚
*   è®¡ç®—é‡å°†éšç€Batchsizeå¢é•¿è€Œå¿«é€Ÿå¢åŠ ã€‚å½“Batchsizeå°äº16æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºçŸ©é˜µä¹˜æ³•ç®—å­ä¸ºè®¿å­˜å¯†é›†å‹çš„ã€‚åªæœ‰å½“Batchsizeå……åˆ†å¤§æ—¶ï¼ŒçŸ©é˜µä¹˜æ³•ç®—å­æ‰ä¼šå˜æˆè®¡ç®—å¯†é›†å‹çš„ï¼Œå®ƒä»¬çš„æ€§è´¨ä¼šéšç€Batchsizeå˜åŒ–è€Œå˜åŒ–ã€‚

##### FFNè®¡ç®—

å¯¹äºFFNç®—å­ï¼Œåœ¨å¤§å¤šæ•°ç«¯ä¾§åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬éƒ½æ˜¯ä»¥Batchsize=1çš„æ–¹å¼å»è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œæ­¤æ—¶ç½‘ç»œä¸­å¤§éƒ¨åˆ†çš„è®¡ç®—é‡å’Œè®¿å­˜é‡éƒ½é›†ä¸­åœ¨FFNä¸­ã€‚å¤§è¯­è¨€æ¨¡å‹æ•´ä½“çš„è¿ç®—-è®¿å­˜æ¯”æä½ï¼Œæ•´ä¸ªç½‘ç»œéƒ½å°†æ˜¯è®¿å­˜å¯†é›†çš„ï¼Œå…¶è¿è¡Œè€—æ—¶å®Œå…¨å—é™äºè®¿å­˜å¸¦å®½è€Œéç¡¬ä»¶ç®—åŠ›ã€‚

#### KV Cacheçš„å½±å“

KV Cacheæ˜¯å¯¹æ³¨æ„åŠ›ä¼˜åŒ–çš„é‡è¦é€”ç»ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯æ–‡æœ¬ä¸­æ¯ä¸ªä¹‹å‰ä½ç½®çš„ key å‘é‡å’Œ value å‘é‡çš„é›†åˆã€‚è¿™é¡¹æŠ€æœ¯çš„å‡ºç°å¤§å¤§ç¼©å‡äº†Self Attentionçš„è®¡ç®—é‡ã€‚è¿™ä½¿å¾—åœ¨KV CacheæŠ€æœ¯å‡ºç°åï¼Œå¯ä»¥æŠŠæ¨ç†æµç¨‹åˆ†ä¸ºprefillå’Œdecodeé˜¶æ®µï¼ˆæˆ‘ä»¬ä¼šåœ¨åæ–‡è¯¦ç»†åˆ†æï¼‰ã€‚ä¸‹å›¾å°±æ˜¯decodeé˜¶æ®µå¯¹åº”çš„å›¾ä¾‹ã€‚æ¦‚æ‹¬åœ°è¯´ï¼Œå…¶ä¸­åŒ…å«äº†ä¸¤ç§ç®—å­ï¼š

*   è‡ªæ³¨æ„åŠ›ï¼ˆ_Self-Attention_ï¼Œé»„è‰²æ ‡å‡ºï¼‰æ¶‰åŠçŸ©é˜µ-çŸ©é˜µä¹˜æ³•ã€‚
*   å¯†é›†æŠ•å½±ï¼ˆ_Dense Projection_ï¼Œç»¿è‰²æ ‡å‡ºï¼‰æ¶‰åŠå‘é‡-çŸ©é˜µä¹˜æ³•ã€‚

Self Attentionç®—å­çš„è®¡ç®—ç‰¹ç‚¹éå¸¸æ˜¾è‘—ï¼šè¿™æ˜¯ä¸€ä¸ªè¿ç®—è®¿å­˜æ¯”æ¥è¿‘1:1çš„è®¿å­˜å¯†é›†å‹ç®—å­ã€‚å¯¹å…¶è®¿å­˜é‡å’Œè®¡ç®—é‡è¿›è¡Œç†è®ºä¼°è®¡ï¼Œå¯å¾—å‘ç°ï¼Œå…¶å†…å­˜è®¿é—®é‡å’Œè®¡ç®—é‡çš„å¤æ‚åº¦éƒ½æ˜¯\\(O(batch\\ size \\times sequence\\ length \\times hidden \\ dimension)\\)ã€‚ä½œä¸ºå¯¹æ¯”ï¼Œå¯¹MatMulå’ŒFeedForwardï¼ˆéƒ½æ˜¯çŸ©é˜µä¹˜æ³•ç®—å­ï¼‰åšç±»ä¼¼çš„ä¼°è®¡ï¼Œå¯å¾—ç»“è®ºï¼šå…¶å†…å­˜è®¿é—®é‡å’Œè®¡ç®—é‡çš„å¤æ‚åº¦éƒ½æ˜¯\\(O(batch\\ size \\times hidden \\ dimension)\\)ã€‚

![](https://img2024.cnblogs.com/blog/1850883/202503/1850883-20250321181427443-822489092.jpg)

##### prefill

MHA å—çš„ `FLOPs`: \\(8sh^2 + 4s^2h\\)ã€‚FFNçš„æ˜¯\\(16sh^2\\)ã€‚

##### decode

`MHA` å±‚æ¯ä¸€è½®è§£ç çš„ `FLOPs`: \\(8h^2 + 4(s+1)h\\)ã€‚FFNçš„æ˜¯\\(16h^2\\)ã€‚

##### æ€»ä½“

è€Œåœ¨è¾“å…¥æ•°æ®å½¢çŠ¶ä¸º \[ğ‘,ğ‘ \]çš„æƒ…å†µä¸‹ï¼Œä¸€æ¬¡è®­ç»ƒ/æ¨ç†ï¼š

`prefill` é˜¶æ®µæ¯è½®æ€»è®¡ç®—é‡ï¼š\\(ğ‘Ã—(24lâ„^2ğ‘ +4lâ„ğ‘ ^2)+2ğ‘ğ‘ â„ğ‘‰)=24lâ„^2ğ‘ğ‘ +4lâ„ğ‘ğ‘ ^2+2ğ‘ğ‘ â„ğ‘‰\\)

`decode` é˜¶æ®µæ¯è½®æ€»è®¡ç®—é‡ï¼š\\(ğ‘Ã—(8lâ„^2+4lâ„(ğ‘ +1)+16lâ„^2)+2ğ‘â„ğ‘‰=24lâ„^2ğ‘+4lâ„ğ‘(ğ‘ +1)+2ğ‘ğ‘ â„ğ‘‰\\)

##### kv cache èŠ‚çœäº†å¤šå°‘è®¡ç®—é‡

å¯¹äºä¸Šä¸‹æ–‡é•¿åº¦ _s_ï¼Œä¸ä½¿ç”¨ kv cache dçš„ self-attention çš„æ€»è®¡ç®—é‡å¤æ‚åº¦ä¸ºï¼šæ€»è®¡ç®—é‡ï¼š\\(ğ‘‚(ğ‘ ^3â„)\\)ï¼Œä½¿ç”¨åçš„æ€»è®¡ç®—é‡è¿‘ä¼¼ä¸º \\(ğ‘‚(ğ‘ ^2â„)\\)ã€‚è®¡ç®—é‡èŠ‚çœæ¯”ç‡ï¼š

\\\[èŠ‚çœæ¯”ç‡=ğ‘‚(ğ‘ ^3â„)âˆ’ğ‘‚(ğ‘ ^2â„)=1âˆ’\\frac{1}{s} \\\]

è®¡ç®—å¤æ‚åº¦ä» \\(ğ‘‚(ğ‘ ^3â„)\\) é™ä½åˆ° \\(ğ‘‚(ğ‘ ^2â„)\\)ï¼Œå³ä½¿ç”¨ kv cache å¯èŠ‚çœçº¦ ğ‘  å€çš„è®¡ç®—é‡ã€‚å½“ ğ‘ è¾ƒå¤§æ—¶ï¼Œ1/sæ¥è¿‘äº 0ã€‚è¾“å‡º tokens æ•°è¶Šå¤šï¼Œè®¡ç®—é‡èŠ‚çœè¶Šå¯è§‚ã€‚

0x05 ä¼˜åŒ–æ–¹å‘
---------

è‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹åœ¨è¿è¡Œæ•ˆç‡ä¸Šé¢æœ€å¤§çš„ç¼ºé™·æ˜¯è§£ç è¿‡ç¨‹æ˜¯ä¸²è¡Œå’Œå˜é•¿çš„ï¼Œå¹¶è¡Œè®¡ç®—å’Œå†…å­˜å¸¦å®½èµ„æºæ— æ³•å¾—åˆ°é«˜æ•ˆåˆ©ç”¨ï¼Œè¿›è€Œä¹Ÿå¯¼è‡´äº†å†…å­˜çš„ç®¡ç†å’Œå›æ”¶é—®é¢˜ã€‚é’ˆå¯¹æ­¤æƒ…å½¢ï¼Œå·¥ä¸šç•Œå·²ç»å‡ºç°äº†ä¸å°‘çš„ç³»ç»Ÿä¼˜åŒ–æ–¹æ¡ˆï¼Œè¿™äº›ä¸Šé¢æ¯ç§æŠ€æœ¯æ‰‹æ®µéƒ½å¯ä»¥å¤§å¹…åº¦åœ°æå‡æ¨¡å‹æ¨ç†çš„é€Ÿåº¦ã€æ€§èƒ½ã€‚

### 5.1 åŸºäºæ³¨æ„åŠ›æœºåˆ¶æ¥ä¿®æ”¹å¤–æ¨æŠ€æœ¯

åšæ–‡â€œHow Do Language Models put Attention Weights over Long Contextâ€œä¸­æåˆ°ï¼Œä¸åŒå±‚çš„æ³¨æ„åŠ›åˆ†å¸ƒæœ‰æ˜¾è‘—å·®å¼‚ï¼š

*   èµ·å§‹å±‚ä¸»è¦æ˜¯è¯åµŒå…¥å’Œè¯åµŒå…¥çš„ä¸€å±‚å±‚æ··åˆï¼Œæ³¨æ„åŠ›åˆ†å¸ƒå¤§è‡´å‡åŒ€ã€‚
*   ä¸­é—´å±‚çš„æ³¨æ„åŠ›æ¨¡å¼å˜å¾—æ›´åŠ å¤æ‚ï¼Œå¤§éƒ¨åˆ†æ¦‚ç‡è´¨é‡é›†ä¸­åœ¨åˆå§‹æ ‡è®°ï¼ˆæ³¨æ„åŠ›æ±‡èšï¼‰å’Œæœ€è¿‘çš„/æœ€åæ ‡è®°ï¼ˆè¿‘æœŸåè§ï¼‰ä¸Šã€‚
*   æœ€åå±‚åˆ™å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚

ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä¸­é—´å±‚å¤§éƒ¨åˆ†éƒ½æ˜¯â€œVå½¢â€æ³¨æ„åŠ›åˆ†å¸ƒï¼Œæ„å‘³ç€ä¸­é—´å±‚å¾ˆå¤šçš„tokenå…¶å®ä½œç”¨ä¸å¤§ã€‚å› æ­¤å¯ä»¥è€ƒè™‘é’ˆå¯¹ä¸åŒçš„å±‚æ¥é€šè¿‡å‡å°‘tokençš„æ–¹å¼æ¥åŠ é€Ÿæ¨ç†ï¼Œå¢åŠ å¤–æ¨èƒ½åŠ›ã€‚

æˆ‘ä»¬æ¥ä¸‹æ¥å°±çœ‹çœ‹å¦‚ä½•åŸºäºæ³¨æ„åŠ›æœºåˆ¶æ¥å¢åŠ å¤–æ¨èƒ½åŠ›ã€‚

åç§°

ä¸»è¦æ€æƒ³

StreamLLM

åœ¨ç»„è£…KV-Cacheçš„æ—¶ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¤´éƒ¨çš„tokenï¼ˆSinkæ¨¡å¼ï¼‰ï¼ŒåŒæ—¶å¼•å…¥Window Attentionæœºåˆ¶æ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚

LM-Infinite

é‡‡ç”¨V-shapedæ³¨æ„åŠ›æœºåˆ¶ã€‚å› ä¸ºä¸­é—´tokenæ³¨æ„åŠ›åˆ†å¸ƒè¾ƒå°‘ï¼Œå› æ­¤å¼•å…¥Î›å½¢æ³¨æ„åŠ›æ©ç ï¼Œä¹Ÿè®¾ç½®ä¸€ä¸ªè·ç¦»ä¸Šé™æ¥é™åˆ¶â€œæœ‰æ•ˆè·ç¦»â€ã€‚åŒæ—¶å¯ä»¥é€‰æ‹©æ€§åœ°å…³æ³¨ä¸­é—´çš„å…·æœ‰æœ€å¤§çš„æ³¨æ„åŠ›logitsçš„kä¸ªtokensã€‚

SirLLM

é€šè¿‡åº¦é‡Tokençš„ç†µå’Œä¸€ä¸ªè®°å¿†è¡°å‡æœºåˆ¶æ¥ç­›é€‰å…³é”®çŸ­è¯­ã€‚ç†µå€¼é«˜çš„tokenè¢«è®¤ä¸ºåŒ…å«æ›´å¤šçš„ä¿¡æ¯ã€‚è®°å¿†è¡°å‡æœºåˆ¶æ˜¯ï¼šå°†tokenç†µç¼“å­˜ä¸­çš„æ¯ä¸ªç†µå€¼ä¹˜ä»¥ä¸€ä¸ªå°äº1çš„è¡°å‡æ¯”ç‡ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œè¾ƒæ—©çš„ä¿¡æ¯ä¼šé€æ¸è¢«é—å¿˜ï¼Œè€Œæœ€è¿‘çš„å…³é”®ä¿¡æ¯åˆ™è¢«ä¿ç•™ã€‚

Sparase-Q

ä»¤ç‰Œé€šå¸¸åªå…³æ³¨åºåˆ—çš„ä¸€å°éƒ¨åˆ†ã€‚å¦‚æœèƒ½æœ‰æ•ˆåœ°é¢„æµ‹å“ªäº›ä»¤ç‰Œå°†è·å¾—é«˜æ³¨æ„åŠ›åˆ†æ•°ï¼Œå°±å¯ä»¥ä»…å­˜å‚¨é«˜åˆ†ä»¤ç‰Œçš„é”®å€¼ï¼Œä»è€Œæé«˜å†…å­˜å¸¦å®½æ•ˆç‡ã€‚å› æ­¤æå‡ºä¸€ç§å‹ç¼©æ€æƒ³ï¼Œé€šè¿‡ä¼°è®¡æœ€å¤§æ³¨æ„åŠ›åˆ†æ•°æ¥é€‰æ‹©rä¸ªåˆ†é‡ï¼Œç„¶åç¡®å®štop-kçš„keyå‘é‡å’Œvalueå‘é‡ã€‚

Dynamic Memory Compression

DMCåœ¨é¢„è®­ç»ƒçš„LLMsä¸Šè¿›è¡Œå¾®è°ƒæ¥å­¦ä¹ å‹ç¼©ç­–ç•¥ï¼Œç„¶ååœ¨æ¨ç†æ—¶å¯¹å…³é”®å€¼ç¼“å­˜è¿›è¡Œåœ¨çº¿å‹ç¼©ã€‚DMCå¼•å…¥äº†å†³ç­–å˜é‡Î±å’Œé‡è¦æ€§å˜é‡Ï‰ï¼Œè¿™äº›å˜é‡åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤å†³å®šæ˜¯å°†å½“å‰çš„keyå’Œvalueè¡¨ç¤ºè¿½åŠ åˆ°ç¼“å­˜ä¸­ï¼Œè¿˜æ˜¯ä¸ç¼“å­˜ä¸­çš„é¡¶éƒ¨å…ƒç´ è¿›è¡ŒåŠ æƒå¹³å‡ã€‚

Infini-attention

å°†å‹ç¼©è®°å¿†ï¼ˆcompressive memoryï¼‰æ•´åˆåˆ°æ ‡å‡†çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¹¶åœ¨å•ä¸ª Transformer å—ä¸­æ„å»ºäº†æ©è”½å±€éƒ¨æ³¨æ„åŠ›ï¼ˆmasked local attentionï¼‰å’Œé•¿æœŸçº¿æ€§æ³¨æ„åŠ›ï¼ˆlong-term linear attentionï¼‰æœºåˆ¶ã€‚

LongLoRA

å¼•å…¥Shifted Sparse Attentionå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æ­¤å¯¹ä¸Šä¸‹æ–‡é•¿åº¦è¿›è¡Œæ‹“å±•ã€‚ç»è¿‡Shifted Sparse Attentionå¾®è°ƒçš„æ¨¡å‹åœ¨æ¨ç†æ—¶ä¿ç•™äº†åŸå§‹çš„æ ‡å‡†è‡ªæ³¨æ„åŠ›æ¶æ„ã€‚è¿™æ„å‘³ç€åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹å¯ä»¥ä½¿ç”¨æœªä¿®æ”¹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œä½¿å¾—å¤§éƒ¨åˆ†ç°æœ‰çš„ä¼˜åŒ–å’ŒåŸºç¡€è®¾æ–½å¯ä»¥é‡ç”¨ã€‚

self-extend Attention

ä½¿ç”¨ç®€å•çš„floor divisionæ“ä½œå°†æœªè§è¿‡çš„å¤§çš„ç›¸å¯¹ä½ç½®æ˜ å°„åˆ°é¢„è®­ç»ƒæœŸé—´é‡åˆ°çš„ç›¸å¯¹ä½ç½®ã€‚ä¸ºäº†è§£å†³é•¿è·ç¦»ä¾èµ–å’Œé‚»è¿‘ä¾èµ–çš„é—®é¢˜ï¼ŒSelf Extendå¼•å…¥äº†åŒå±‚æ³¨æ„åŠ›æœºåˆ¶ï¼šåˆ†ç»„æ³¨æ„åŠ›ï¼ˆGrouped Attentionï¼‰å’Œé‚»è¿‘æ³¨æ„åŠ›ï¼ˆNeighbor Attentionï¼‰ã€‚

Dual Chunk Attention

é€šè¿‡å°†é•¿åºåˆ—çš„æ³¨æ„åŠ›è®¡ç®—åˆ†è§£ä¸ºåŸºäºå—çš„æ¨¡å—ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·åŒä¸€å—å†…ï¼ˆIntra-Chunkï¼‰å’Œä¸åŒå—é—´ï¼ˆInter-Chunkï¼‰çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚ç„¶åå°†å†…éƒ¨å—ã€è·¨å—å’Œè¿ç»­å—çš„æ³¨æ„åŠ›è¾“å‡ºåˆå¹¶ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºè¡¨ç¤ºã€‚è¿™ä¸€è¡¨ç¤ºè€ƒè™‘äº†åºåˆ—ä¸­çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œä»è€Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†é•¿åºåˆ—ã€‚

### 5.2 åŸºäºMemoryæœºåˆ¶å¤–æ¨æŠ€æœ¯

åŸºäºMemoryæœºåˆ¶çš„å¤–æ¨æŠ€æœ¯å…¶å®æ²¿ç”¨çš„è¿˜æ˜¯å‹ç¼©æ€æƒ³ï¼Œå€ŸåŠ©å¤–éƒ¨å­˜å‚¨å°†å†å²ä¿¡æ¯å­˜å‚¨ï¼Œç„¶åä½¿ç”¨æœ€è¿‘çš„tokenè¿›è¡ŒæŸ¥è¯¢è·å–ä¸€äº›å†å²ä¸Šé‡è¦çš„tokenã€‚

åç§°

ä¸»è¦æ€æƒ³

InfLLM

é€šè¿‡æ„å»ºä¸€ä¸ªé¢å¤–çš„ä¸Šä¸‹æ–‡è®°å¿†æ¨¡å—æ¥è®©å­˜å‚¨è¿œç¦»å½“å‰å¤„ç†ä½ç½®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªé«˜æ•ˆçš„æœºåˆ¶æ¥æŸ¥æ‰¾ä¸å½“å‰å¤„ç†çš„æ ‡è®°ç›¸å…³çš„å•å…ƒï¼Œä»¥ä¾¿åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­ä½¿ç”¨ã€‚

Recurrent Memory Transformer (RMT)

é€šè¿‡ç»“åˆå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„å¾ªç¯æœºåˆ¶å’ŒTransformeræ¨¡å‹çš„è®°å¿†å¢å¼ºèƒ½åŠ›æ¥å®ç°ä¸Šä¸‹æ–‡æ‹“å±•ã€‚RMTåœ¨Transformeræ¨¡å‹çš„åŸºç¡€ä¸Šå¼•å…¥äº†ä¸€ä¸ªè®°å¿†æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ç”±ä¸€ç»„å¯è®­ç»ƒçš„å®å€¼å‘é‡ï¼ˆç§°ä¸ºè®°å¿†æ ‡è®°ï¼‰ç»„æˆã€‚è¿™äº›è®°å¿†å‘é‡å¯ä»¥å­˜å‚¨å’Œå¤„ç†å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å¾ªç¯æœºåˆ¶åœ¨é•¿åºåˆ—çš„ä¸åŒæ®µä¹‹é—´ä¼ é€’ä¿¡æ¯ã€‚

0xFF å‚è€ƒ
-------

[å¤šä¸ªå¤§è¯­è¨€å¾®è°ƒæ¨¡å‹å¹¶è¡Œæ¨æ–­çš„æ½œåŠ›](https://abcdabcd987.com/2023/09/11/multi-lora-potentials/)

[Contiguous Batching/Inflight Batching](https://www.usenix.org/system/files/osdi22-yu.pdf)

[Full Stack Transformer Inference Optimization Season 2: Deploying Long-Context Models](https://yaofu.notion.site/Full-Stack-Transformer-Inference-Optimization-Season-2-Deploying-Long-Context-Models-ee25d3a77ba14f73b8ae19147f77d5e2) Yao Fu [Paper version](https://arxiv.org/abs/2405.08944)

[GPTQ](https://arxiv.org/abs/2210.17323)/[AWQ](https://arxiv.org/abs/2306.00978)

[How Do Language Models put Attention Weights over Long Context](https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context-10250219d5ce42e8b465087c383a034e) Yao Fu

[HunYuan MoEï¼šèŠä¸€èŠ LLM å‚æ•°é‡ã€è®¡ç®—é‡å’Œ MFU ç­‰](https://mp.weixin.qq.com/s?__biz=Mzk0ODU3MjcxNA==&mid=2247488396&idx=1&sn=d58aaec004c2a4c0a0db597579e27eb4&chksm=c29a8e1154dd5de28a904f8af75b683c2c9ec93a32b9d6eabe8febe75527172e42c293e17f09&mpshare=1&scene=1&srcid=111493vB4moJWO5thWubiqww&sharer_shareinfo=232541b218d616dd703ebe350f295cc3&sharer_shareinfo_first=232541b218d616dd703ebe350f295cc3#rd) AIé—²è°ˆ

[llm å‚æ•°é‡-è®¡ç®—é‡-æ˜¾å­˜å ç”¨åˆ†æ](https://www.armcvai.cn/2024-09-20/llm-params-flops.html) Zhang

[LLM å¤§æ¨¡å‹è®­ç»ƒ-æ¨ç†æ˜¾å­˜å ç”¨åˆ†æ](https://bruceyuan.com/post/llm-train-infer-memoery-usage-calculation.html) [chaofaç”¨ä»£ç æ‰“ç‚¹é…±æ²¹](https://bruceyuan.com/)

[LLMï¼ˆå»¿ä¸‰ï¼‰ï¼šLLM ä¸­çš„é•¿æ–‡æœ¬é—®é¢˜](https://zhuanlan.zhihu.com/p/640641794) [ç´«æ°”ä¸œæ¥](https://www.zhihu.com/people/zi-qi-dong-lai-1)

[Notion â€“ The all-in-one workspace for your notes, tasks, wikis, and databases.](https://yaofu.notion.site/Full-Stack-Transformer-Inference-Optimization-Season-2-Deploying-Long-Context-Models-ee25d3a77ba14f73b8ae19147f77d5e2)

[OpenPPL-LLM | OpenPPLä¹‹å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“æ¥å•¦](https://zhuanlan.zhihu.com/p/653808774) [OpenPPL](https://www.zhihu.com/people/openppl)

[PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)

[Towards 100x Speedup: Full Stack Transformer Inference Optimization](https://yaofu.notion.site/Towards-100x-Speedup-Full-Stack-Transformer-Inference-Optimization-43124c3688e14cffaf2f1d6cbdf26c6c) Yao Fu

[Transformer ä¼°ç®— 101](https://mp.weixin.qq.com/s/MFgTUDAOODgMDb59eZC9Cw)

[Transformer æ•°æ®ä¼°è®¡- æ˜¾å­˜å ç”¨](https://zhuanlan.zhihu.com/p/678503627) [Bruce ä»—å‰‘èµ°å¤©æ¶¯](https://www.zhihu.com/people/void-73-73)

[åˆ†ætransformeræ¨¡å‹çš„å‚æ•°é‡ã€è®¡ç®—é‡ã€ä¸­é—´æ¿€æ´»ã€KV cache](https://zhuanlan.zhihu.com/p/624740065) [å›æ—‹æ‰˜é©¬æ–¯x](https://www.zhihu.com/people/springxchen)

[å‰–æGPTæ¨æ–­ä¸­çš„æ‰¹å¤„ç†æ•ˆåº”](https://abcdabcd987.com/2023/05/13/transformer-batching/) [Lequn Chen || abcdabcd987](https://abcdabcd987.com/blog)

[å¤šä¸ªå¤§è¯­è¨€å¾®è°ƒæ¨¡å‹å¹¶è¡Œæ¨æ–­çš„æ½œåŠ›](https://abcdabcd987.com/2023/09/11/multi-lora-potentials/) [Lequn Chen || abcdabcd987](https://abcdabcd987.com/blog)

[å¤§æ¨¡å‹ - éƒ¨ç½² - å®¹é‡ä¼°ç®—](https://zhuanlan.zhihu.com/p/694980607) [æ€æƒ³æŸ³å¶åˆ€](https://www.zhihu.com/people/calvin-97-63)

[å¤§æ¨¡å‹æ¨ç†ç“¶é¢ˆåŠæé™ç†è®ºå€¼åˆ†æ](http://mp.weixin.qq.com/s?__biz=Mzg4MTkwMTQ4NA==&mid=2247485270&idx=1&sn=fbbcae4d787bd67e0d4dd5e18aa98e8c&chksm=cf5fad95f828248302e0cb134d2f75987e71b5b6b2344d3dd6288d9a7ae67250b26157f90090&scene=21#wechat_redirect) å–œæ¬¢å·å·çš„ç“¦åŠ›

[æ¿€æ´»å†…å­˜ï¼šæ¨¡å‹æ¨ç†éœ€è¦å¤šå°‘å†…å­˜](https://mp.weixin.qq.com/s?__biz=MzAwMDc2NjQ4Nw==&mid=2663562897&idx=1&sn=436b6c073f892d4f48c2f9d3ea81b6f6&chksm=8035e6969544f4896747afb93b23f28cecf068ac9970724162b6b00f35796f536fc448ef16fd&mpshare=1&scene=1&srcid=0130YyBBgZp8luoN0yD9jHOE&sharer_shareinfo=8a3a771a4a01dfd8c77aa118f9bc1a6e&sharer_shareinfo_first=8a3a771a4a01dfd8c77aa118f9bc1a6e#rd) é­æ–°å®‡ \[å¤§é­åˆ†äº«\]