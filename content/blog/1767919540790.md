---
layout: post
title: '吴恩达深度学习课程五：自然语言处理  第一周：循环神经网络 （四）RNN 中的梯度现象'
date: "2026-01-09T00:45:40Z"
---
吴恩达深度学习课程五：自然语言处理 第一周：循环神经网络 （四）RNN 中的梯度现象
==========================================

此分类用于记录吴恩达深度学习课程的学习笔记。  
课程相关信息链接如下：

1.  原课程视频链接：[\[双语字幕\]吴恩达深度学习deeplearning.ai](https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&from_spmid=playlist.playlist-detail.0.0&is_story_h5=false&mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1713085655&unique_k=DfBgvFW&up_id=8654113&vd_source=e035e9878d32f414b4354b839a4c31a4)
2.  github课程资料，含课件与笔记:[吴恩达深度学习教学资料](https://github.com/robbertliu/deeplearning.ai-andrewNG)
3.  课程配套练习（中英）与答案：[吴恩达深度学习课后习题与答案](https://blog.csdn.net/u013733326/article/details/79827273)

本篇为第五课的第一周内容，[1.8](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=158)的内容以及一些相关基础的补充。

* * *

本周为第五课的第一周内容，与 CV 相对应的，这一课所有内容的中心只有一个：**自然语言处理（Natural Language Processing，NLP）**。  
应用在深度学习里，它是专门用来进行**文本与序列信息建模**的模型和技术，本质上是在全连接网络与统计语言模型基础上的一次“结构化特化”，也是人工智能中**最贴近人类思维表达方式**的重要研究方向之一。  
**这一整节课同样涉及大量需要反复消化的内容，横跨机器学习、概率统计、线性代数以及语言学直觉。**  
语言不像图像那样“直观可见”，更多是抽象符号与上下文关系的组合，因此**理解门槛反而更高**。  
因此，我同样会尽量补足必要的背景知识，尽可能用比喻和实例降低理解难度。  
本篇的内容关于**RNN 中的梯度现象**，是对 RNN 中存在的问题的阐述，也是对之后的门控机制的引入内容。

1\. RNN 中的梯度现象
==============

在很久之前，我们就介绍过深度学习训练中的[梯度现象](https://www.cnblogs.com/Goblinscholar/p/19190303)，这种情况主要出现在深层神经网络中，在反向传播中随着层层的梯度计算导致梯度过大或过小，从而出现梯度爆炸或者梯度消失，导致网络无法训练。  
而在 RNN 中，即使是我们演示过的单层 RNN ，也可能产生梯度现象，而且这一问题会显得**更加隐蔽，却也更加严重**，其原因就在于 RNN 的**时间反向传播**特性。

1.1 RNN 的深度
-----------

首先，我们知道：**RNN 的“深度”并不体现在空间结构上，而是体现在时间维度上。**  
因此，虽然我们画出来的 RNN 看起来只有一层，但在训练时，RNN 会在时间维度上被“展开”为一个**共享参数的深层网络**，就像这样：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260108213552905-491823195.png)

也就是说，如果序列长度为 \\(T\\)，那么在反向传播时，梯度就需要沿着时间轴，从第 \\(T\\) 个时刻一路反传回第 \\(1\\) 个时刻。  
需要说明的是，这里时间展开的长度 \\(T\\) 指的是 **RNN 在时间维度上的递推步数**，在我们介绍的基础 RNN 场景下，通常等同于输入序列的长度 \\(T\_x\\)，而非生成序列的长度 \\(T\_y\\)。  
最终，从效果上看，这相当于： **同一组权重矩阵被反复相乘了 \\(T\\) 次。**

因此，在 RNN 中，我们同样有必要了解训练中出现梯度现象的处理方法。

1.2 梯度爆炸的处理：梯度裁剪（Gradient Clipping）
-----------------------------------

在 RNN 的梯度问题中，**梯度爆炸通常是最先、也是最容易被观察到的现象**。  
其表现非常直观：损失函数在训练过程中剧烈震荡，甚至直接变为 NaN，参数更新完全失控，模型无法继续训练。  
与梯度消失不同，梯度爆炸并不是“学不到”，而是 **“学得太猛”**。  
因此，相比梯度消失，梯度爆炸问题更容易被控制和缓解。这很好理解：东西多了我们可以扔，但少了我们不能凭空创造出来。  
而其中一种最常见、也最直接的方法，就是**梯度裁剪（Gradient Clipping）**。

梯度裁剪的思想非常简单，可以概括为一句话：

> **当梯度过大时，不让它继续放大更新幅度。**

也就是说，我们并不试图改变梯度的“方向”，而只是**限制梯度的“大小”**，从而避免一次参数更新步长过大，破坏训练稳定性。

再打个比方：在下坡骑车时，我们的方向是对的，但速度太快容易摔，所以我们通过“刹车”来控制风险。  
梯度裁剪，本质上就是反向传播阶段的“数值刹车”。

在实际使用中，最常见的是**基于梯度范数（norm）的裁剪方式**。  
设所有参数的梯度拼接成一个向量 \\(g\\)，其 **\\(L\_2\\) 范数**为：

\\\[\\lVert g \\rVert\_=\\sqrt{g\_1^2 + g\_2^2 + \\cdots + g\_n^2} \\\]

我们用 **\\(L\_2\\) 范数**衡量梯度的**整体大小**，它反映的是**这一次反向传播中，参数更新“总体有多激进”**。

下一步，给定一个**阈值 \\(c\\)**，梯度裁剪的规则是：

\\\[g = \\begin{cases} g, & \\lVert g \\rVert \\le c \\\\ \\dfrac{c}{\\lVert g \\rVert} \\, g, & \\lVert g \\rVert > c \\end{cases} \\\]

也就是说：

*   如果梯度范数在可接受范围内：**不做任何处理。**
*   如果梯度范数超过阈值：**整体缩放，使其范数恰好等于 \\(c\\)。**

这样操作下来，你会发现：**梯度裁剪并不会改变梯度各分量之间的相对比例，只是统一缩放其大小到合适程度。**

这样，在反向传播中，梯度裁剪就会**阻断“指数级放大”的最坏情况**， 保证参数更新始终处在一个稳定区间，从而让训练过程“至少可以继续进行下去”。  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260108213603856-931372748.png)  
要强调的是：在实际训练中时，**梯度裁剪几乎是默认配置**，而不是可选技巧。

但是，梯度裁剪也有其局限性：**梯度裁剪只能缓解梯度爆炸，无法解决梯度消失。**  
原因很简单：

*   梯度爆炸是“数值过大”的问题 → 可以强行压缩
*   梯度消失是“信号本身接近于 0” → 裁剪无能为力

因此，对于梯度消失这一更常见也更难缓解的梯度现象，我们需要别的解决方案，这也是我们下面要讨论的主要内容。

2\. RNN 中的梯度消失：长距离依赖问题
======================

我们知道，[RNN](https://www.cnblogs.com/Goblinscholar/p/19449622) 擅长处理序列数据，并能够逐步积累历史信息。然而，在长序列训练中，梯度消失会让早期时间步的影响被逐渐“抹掉”，这就导致了著名的 **长距离依赖问题**：模型难以捕捉序列中相隔较远的信息。  
我们用之前的反向传播例子来演示一下这个问题：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260108213553401-1851089098.png)  
注意我们标红的字体：**当序列过长时，与结尾距离很远的最初几步信息很难实现有效更新。因此梯度已经在层层连乘中所剩无几了。**  
这样带来的后果是什么？来看看：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260108213555854-2054969424.png)

这就是 RNN 中的梯度消失现象，**它直接导致 RNN 难以捕捉序列中相隔较远的依赖关系**，显然，这对模型性能的影响是巨大的。

那么如何缓解 RNN 中的梯度消失现象？  
你可能想到了我们之前介绍过的[残差网络](https://www.cnblogs.com/Goblinscholar/p/19359236)，即通过在 RNN 中引入**残差连接**，为梯度提供了一条直接传递的通路，可以在一定程度上缓解梯度消失问题，使深层或长序列的训练更加稳定。这的确是一种可行的改进方案。但残差路径虽然提供了梯度直通通道，但**无法进行信息选择性控制**，对于非常长序列仍然存在梯度衰减。  
因此，在 RNN 中，我们有一种更好的技术：**门控机制**，这是在实际实验和部署中我们更常使用的方法，它不仅能保持梯度稳定传递，还能智能控制信息流。  
其原理较为复杂，我们经过本篇的引用，在下一篇来详细展开它。

3\. 总结
======

概念

原理

比喻

**梯度现象（Gradient Phenomena）**

在深层网络或 RNN 的反向传播中，梯度可能过大或过小，导致训练不稳定，即梯度爆炸或梯度消失

就像水流管道，如果水压过大管道爆裂，水压过小又无法输送水

**RNN 的“深度”**

RNN 在时间维度上展开为共享参数的深层网络，梯度需要沿时间轴反向传播，连续乘以权重矩阵 \\(T\\) 次

好比一个接力赛，每一棒都必须传递能量，接力棒越多，总能量损耗越大

**梯度裁剪（Gradient Clipping）**

当梯度范数超过阈值 \\(c\\) 时，对梯度整体缩放，使其范数等于 \\(c\\)；不改变梯度方向，只调整大小

就像给过快下坡的车装刹车，保持安全速度

**RNN 的长距离依赖问题**

梯度在层层连乘或长序列反向传播中逐渐趋近 0，早期时间步的影响被“抹掉”，导致长距离依赖难以学习

像传话游戏，信息经过太多人，最开始的话慢慢被模糊甚至忘掉