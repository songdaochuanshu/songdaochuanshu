---
layout: post
title: '[大模型实战 01] 本地大模型初体验：Ollama 部署与 Python 调用指南'
date: "2026-02-02T00:58:30Z"
---
> **核心摘要 (TL;DR)**
> 
> *   **工具**：Ollama (最流行的本地大模型运行工具)。
> *   **目标**：在本地电脑运行大模型，并提供 API 给 Python 调用。
> *   **痛点解决**：教咱们如何用国内 ModelScope 替代 HuggingFace 实现极速下载。
> *   **干货**：包含修改端口、显存计算公式、以及 Embedding/多模态等概念科普。

01\. Ollama 介绍
--------------

官网地址：[https://ollama.com/](https://ollama.com/)

Ollama 是目前最火的本地大模型部署工具。  
简单来说，它能帮咱们快速拉取模型文件，让模型在本地直接运行并进行对话。同时，它还能把模型打包成一个标准的接口，通过端口开放给咱们写的 Python 脚本调用。

对于咱们来说，它就是在大模型时代装在电脑里的“运行环境”，必不可少。

02\. 安装 Ollama
--------------

1.  **下载**：登录官网 [https://ollama.com/](https://ollama.com/) 。  
    ![ollama_site](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134846872-1401470505.png)
2.  **选择版本**：点击 **Download** 按钮，根据咱们的操作系统（Windows/Mac/Linux）下载。  
    ![download_ollama_via_platform](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134846662-18363808.png)
3.  **安装**：打开下载好的安装包，选一个咱们喜欢的位置安装即可。
4.  **验证**：安装完毕后，开始菜单里会出现一个羊驼图标。  
    ![ollama_icon](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134848248-294303447.png)
5.  **测试运行**：按下 `Win+R` 打开运行窗口，输入 `cmd` 打开命令提示符。输入命令 `ollama --version`。如果看到版本号，就说明 Ollama 已经安装完毕，正在运行了。  
    ![run_cmd_command](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134846587-706121212.png)  
    ![check_ollama_version](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134856017-1959008172.png)  
    第一阶段顺利完成！

03\. Ollama 常用命令速查
------------------

这些命令咱们以后会经常用到，建议收藏：

场景

命令示例

备注

**第一次下模型**

`ollama run qwen3:7b`

会自动先 pull 再运行，一步到位

**只下载不运行**

`ollama pull llama3:8b`

适合提前囤模型

**国内加速**

`ollama pull modelscope.cn/Qwen/Qwen3-7B-GGUF`

**推荐**！下文会细讲

**查看本地库存**

`ollama list` 或 `ollama ls`

大小/ID/修改时间一目了然

**删除省空间**

`ollama rm llama2:latest`

支持通配符，可写 `llama2:*`

**给模型改短名**

`ollama cp qwen3:7b q7`

后面直接 `ollama run q7` 方便调用

**查模型详情**

`ollama show q7`

参数量、量化层、标签全列出

04\. 下载模型（解决网速慢的问题）
-------------------

Ollama 官网收录了很多模型，可以通过详情页复制命令下载，但由于服务器在海外，咱们在国内访问经常断连，速度也很慢。

主流的模型平台是 **HuggingFace**，但它也在海外，国内下载需要魔法工具。  
**咱们的解决方案**：使用阿里的 **魔搭社区 (ModelScope)**。

*   HuggingFace 官网：[https://huggingface.co/](https://huggingface.co/)
*   ModelScope (魔搭) 官网：[https://modelscope.cn/](https://modelscope.cn/)

**操作步骤：**

1.  进入 HuggingFace 点击 Models，或者进入魔搭点击模型库。
2.  在搜索框输入咱们想要的模型，比如 `Qwen3-0.6B-GGUF`。
    
    > **注意**：Ollama 目前主要支持 **GGUF** 格式，搜索时一定要带上这个后缀。  
    > ![hugging_face_search_gguf](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134855518-2110640423.png)
    
3.  进入模型详情页，复制模型 ID，例如 `Qwen/Qwen3-0.6B-GGUF`。  
    ![click_to_copy_model_address](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134846780-1552981459.png)
4.  回到命令提示符，加上前缀进行下载，网速直接拉满：
    *   **魔搭下载 (推荐)**: `ollama pull modelscope.cn/Qwen/Qwen3-0.6B-GGUF`
    *   HuggingFace 下载: `ollama pull hf.co/Qwen/Qwen3-0.6B-GGUF`
5.  下载完毕后，运行 `ollama list` 查看信息：

    NAME                                        ID              SIZE      MODIFIED
    modelscope.cn/Qwen/Qwen3-0.6B-GGUF:latest   xxxxxxx         xxx MB    x ago
    

05\. 运行模型
---------

在命令行工具输入 `ollama run modelscope.cn/Qwen/Qwen3-0.6B-GGUF`。  
看到交互界面后，咱们就可以愉快地跟大模型对话了。  
![ollama_run_result](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134847991-12597337.png)

06\. 更改服务端口（进阶）
---------------

Ollama 默认服务运行在端口 `11434` 上。如果咱们在自己的服务器上部署，为了安全或避免端口冲突，可以修改它。

### Windows 环境

1.  **退出 Ollama**：在任务栏右下角的托盘图标上右键，选择 **Quit Ollama**。  
    ![quit_ollama](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134847552-1384482343.png)
2.  **设置环境变量**：
    *   按下 `Win + S`，搜索“编辑账户环境变量”并打开。
    *   在“用户变量”部分，点击“新建”。
    *   **变量名**：`OLLAMA_HOST`
    *   **变量值**：`0.0.0.0:5656` （假设咱们想改到 5656 端口，`0.0.0.0` 表示允许所有网卡访问）。  
        ![add_OLLAMA_HOST_to_env_vairable](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260201134846494-2096112798.png)
3.  **重新启动**：从开始菜单重新运行 Ollama 软件。
4.  **检验**：在浏览器输入 `http://localhost:5656`，如果显示 `Ollama is running` 说明端口修改成功了。

### Linux 环境

1.  执行命令：`sudo systemctl edit ollama.service`
2.  在打开的编辑器中（通常是空白或带注释），加入以下内容：

    [Service]
    Environment="OLLAMA_HOST=0.0.0.0:5656"
    

3.  保存并退出，然后重载并重启服务：

    sudo systemctl daemon-reload
    sudo systemctl restart ollama
    

07\. 在 Python 脚本中使用模型
---------------------

为了运行连接 Ollama 的 Python 脚本，我们需要准备以下环境：

*   **Python 版本**：Python 3.8 以上
*   **OpenAI 库依赖**：在命令行输入 `pip install openai`

Ollama 完美兼容 OpenAI 的 API 格式，所以咱们直接用 OpenAI 的库就行：

    from openai import OpenAI
    
    # 初始化客户端
    client = OpenAI(
        # 这里的端口号要对应咱们上面修改后的端口号，记得加上 /v1
        base_url='http://localhost:5656/v1',
        # Ollama 不需要真正的 Key，但这里随便填一个，不能留空
        api_key='ollama',
    )
    
    # 发起对话请求
    response = client.chat.completions.create(
        # 填入咱们在 ollama list 中看到的模型名称
        model="modelscope.cn/unsloth/Qwen3-0.6B-GGUF",
        messages=[
            {"role": "system", "content": "你是一个有用的助手。"},
            {"role": "user", "content": "你好，请简单介绍一下你自己。"},
        ]
    )
    
    print(response.choices[0].message.content)
    

* * *

08\. 常见问题 (Q&A)
---------------

这里整理了咱们在入门时最关心的问题：

**Q: 除了 Ollama 还有哪些方式可以部署，它们有什么差别？**  
**A:**

*   **LM Studio / AnythingLLM**：带有图形界面的部署工具。适合完全不懂代码或者完全不想碰代码的初学者，也可以一键建立知识库做 RAG。
*   **vLLM**：高性能推理框架。通常用于服务器级别，速度极快，适合多人并发，工业级部署使用。
*   **差别**：Ollama 更轻量，适合开发；LM Studio 胜在可视化；vLLM 胜在极致性能。

**Q: Ollama 开机自动启动，我要怎么关闭？关闭后如何手动启动？**  
**A:**

*   **Windows**：右键点击任务栏图标 -> `Quit Ollama` 只是临时关闭。要彻底关闭自启，请在 **任务管理器 -> 启动应用** 中找到 `Ollama` 并设为禁用。
*   **Linux**：使用命令 `sudo systemctl disable ollama` 关闭自启。
*   **手动启动**：Windows 直接运行桌面图标；Linux 执行 `ollama serve` 即可。

**Q: HuggingFace 和魔搭 (ModelScope) 有什么区别？**  
**A:**

*   **Hugging Face (HF)**：全球最大的“AI 模型图书馆”，资源最全、社区最活跃，但服务器在海外，国内访问速度较慢。
*   **魔搭 (ModelScope)**：阿里旗下的国内版“模型图书馆”。国内下载速度极快，模型齐全（基本和 HF 同步），主要是为了解决国内下载慢、需要魔法的问题。

**Q: 平台看起来很丰富，还有什么别的好玩儿的功能？**  
**A:**

*   **Spaces / 创空间**：可以直接在 Web 上体验最新的模型应用（如 AI 绘画、变声），不用本地部署，但有时需要排队。
*   **Datasets (数据集)**：训练模型的数据集也可以在上面下载。

**Q: 大模型有什么类型？**  
**A:**

*   **语言模型 (LLM)**：常规的大模型，如 Llama3, DeepSeek, 千问。主要是聊天和文字处理。
*   **多模态模型**：如 LLaVA。能看图片，根据图片进行对话，也就是传统的大模型 + 能看图的眼睛。
*   **嵌入模型 (Embedding)**：用来将文字直接转化为向量数值。主要用在 **RAG** (检索增强生成) 中，对问题进行搜索以找到相近的文档回答。
*   **视觉/视频/语音模型**：用以生成图像、视频和语音。

**Q: 我该如何快速计算我的电脑能支持多大的模型？**  
**A:** 一般来说模型的占用可以通过一个快速公式来计算：  
**模型显存占用 ≈ 参数量 × 0.7**

*   比如下载 0.6B 模型，全量参数 (16bit) 就是：`0.6 × 0.7 ≈ 0.42GB`。
*   如果是 7B 模型（4-bit 量化）：`7 × 0.7 ≈ 4.9GB`，咱们至少需要 6GB 显存。

**Q: 大模型不是需要显卡吗？为什么 Ollama 可以运行在没有显卡的设备上？**  
**A:** Ollama 底层使用了 `llama.cpp` 技术。如果它检测到咱们没有显卡，会将模型权重从**显存(VRAM)**加载到 **系统内存 (RAM)** 中，使用 CPU 指令集进行计算。虽然速度比在显卡上慢，但让手机、普通轻薄本等设备也有了运行大模型的可能性。

* * *

**本文作者：** Algieba  
**本文链接：** [https://blog.algieba12.cn/run-our-own-model-on-pc/](https://blog.algieba12.cn/run-our-own-model-on-pc/)  
**版权声明：** 本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！

发表于 2026-02-01 13:47  [阿尔的代码屋](https://www.cnblogs.com/algieba)  阅读(45)  评论(0)    [收藏](javascript:void\(0\))  [举报](javascript:void\(0\))