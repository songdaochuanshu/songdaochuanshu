---
layout: post
title: '国内首个「混合推理模型」Qwen3深夜开源，盘点它的N种对接方式！'
date: "2025-04-30T00:40:30Z"
---
国内首个「混合推理模型」Qwen3深夜开源，盘点它的N种对接方式！
=================================

今日凌晨，通义千问团队正式开源了 Qwen3 大模型，并且一口气发布了 8 个型号，其中包括 0.6B、1.7B、4B、8B、14B、32B 以及 30B-A3B 和 235B-A22B，使用者可以根据自己的业务情况，选择合适的版本进行使用。

更让人惊喜的是，最新的 Qwen3 系列模型具备**双模推理能力（深入思考/快速响应）、支持 119 种语言及方言，并强化了 Agent 功能与代码执行能力**，全面满足复杂问题处理与全球化应用需求。

> PS：Qwen3 也是国内首个「混合推理模型」，「快思考」与「慢思考」集成进同一个模型，对简单需求可低算力「秒回」答案，对复杂问题可多步骤「深度思考」，大大节省算力消耗。

Qwen3 旗舰模型 Qwen3-235B-A22B 在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro 等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型 Qwen3-30B-A3B 的激活参数数量是 QwQ-32B 的 10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能，以下是测试报告：

![](https://img2024.cnblogs.com/blog/172074/202504/172074-20250429164224467-493146404.png)

对接 Qwen3
--------

常见对接大模型的方案有以下几种：

1.  **官方对接方式**：例如，调用阿里百炼平台对接 Qwen3。
2.  **本地模型对接方式**：安装 Ollama 部署 Qwen3，对接 Ollama 实现调用。
3.  **三方平台对接方式**：使用千帆或火山引擎等三方平台，对接调用 Qwen3。

但目前因为 Qwen3 刚刚发布，所以只能使用前两种对接方式，截止发稿时，三方平台还未上线 Qwen3，但也够用了。

具体实现
----

接下来我们就以官方的调用方式，来实现一下 Qwen3 的具体代码对接吧，这里提供 Spring AI 和 LangChain4j 两种对接实现。

Spring AI 对接 Qwen3
------------------

### 1.添加依赖

Spring AI 并没有内置阿里云百炼平台，但百炼平台支持 OpenAI 协议，因此我们可以使用 OpenAI 对接百炼平台，因此我们只需要添加 OpenAI 依赖即可。

    <dependency>
      <groupId>org.springframework.ai</groupId>
      <artifactId>spring-ai-starter-model-openai</artifactId>
    </dependency>
    

### 2.设置配置信息

    spring:
      ai:
        openai:
          base-url: https://dashscope.aliyuncs.com/compatible-mode/
          api-key: ${ALIYUN-AK}
          chat:
            options:
              model: qwen3-235b-a22b
    

其中：

*   base-url 填写百炼平台地址。
*   api-key 为准备阶段在百炼平台申请的 AK 凭证。
*   model 设置为 qwen3-235b-a22b 模型。

> 支持的模型列表参考官方文档：[https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.0.78d848237YTeH1#cefdf0875dorc](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.0.78d848237YTeH1#cefdf0875dorc)

### 3.编写调用代码

    import org.springframework.ai.openai.OpenAiChatModel;
    import org.springframework.beans.factory.annotation.Autowired;
    import org.springframework.web.bind.annotation.RequestMapping;
    import org.springframework.web.bind.annotation.RequestParam;
    import org.springframework.web.bind.annotation.RestController;
    
    @RestController
    @RequestMapping("/ds")
    public class TestController {
    
        private final OpenAiChatModel chatModel;
    
        @Autowired
        public TestController(OpenAiChatModel chatModel) {
            this.chatModel = chatModel;
        }
    
        @RequestMapping("/chat")
        public String chat(@RequestParam("msg") String msg) {
            String result = chatModel.call(msg);
            System.out.println("返回结果：" + result);
            return result;
        }
    }
    

LangChain4j 对接 Qwen3
--------------------

LangChain4j 内置集成了阿里云百炼平台，所以可以直接对接。

### 1.添加依赖

    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-community-dashscope-spring-boot-starter</artifactId>
    </dependency>
    

可以为“langchain4j-community-xxx”其添加统一版本管理：

    <dependencyManagement>
      <dependencies>
        <dependency>
          <groupId>dev.langchain4j</groupId>
          <artifactId>langchain4j-community-bom</artifactId>
          <version>1.0.0-beta3</version>
          <type>pom</type>
          <scope>import</scope>
        </dependency>
      </dependencies>
    </dependencyManagement>
    

### 2.设置配置信息

**注意这里需要配置“chat-model”节点，官方文档有问题**，如果不配置 chat-model 则不能自动注入百炼模型：

    langchain4j:
      community:
        dashscope:
          base-url: https://dashscope.aliyuncs.com/compatible-mode/
          chat-model:
            api-key: ${ALIYUN-AK}
            model-name: qwen-plus
    

支持的模型列表：[https://help.aliyun.com/zh/model-studio/models](https://help.aliyun.com/zh/model-studio/models)

### 3.编写调用代码

    import dev.langchain4j.model.chat.ChatLanguageModel;
    import org.springframework.beans.factory.annotation.Autowired;
    import org.springframework.web.bind.annotation.RequestMapping;
    import org.springframework.web.bind.annotation.RestController;
    
    @RestController
    @RequestMapping("/qw")
    public class QwenController {
    
        @Autowired
        private ChatLanguageModel qwenChatModel;
    
        @RequestMapping("/chat")
        public String chat(String question) {
            return qwenChatModel.chat(question);
        }
    }
    

小结
--

当然，以上对接方式是全量输出（得到结果之后一次性返回），生产级别我们通常要使用流式输出，并且需要实现连续（上下文）对话，以及历史对话信息持久化等功能，文章篇幅有限，这里就不一一实现了，大家可以下来自己试试。

> 本文已收录到我的技术小站 [www.javacn.site](https://www.javacn.site)，其中包含的内容有：Spring AI、LangChain4j、MCP、Function Call、RAG、向量数据库、Prompt、多模态、向量数据库、嵌入模型等内容。

关注下面二维码，订阅更多精彩内容。  
![](https://images.cnblogs.com/cnblogs_com/vipstone/848916/o_211225130402_gognzhonghao.jpg)

![](http://icdn.apigo.cn/gitchat/rabbitmq.png?imageView2/0/w/500/h/400)

![微信打赏](http://icdn.apigo.cn/myinfo/wchat-pay.png "微信打赏")  

关注公众号（加好友）： ![](http://icdn.apigo.cn/gongzhonghao2.png?imageView2/0/w/120/h/120)

  
作者： [王磊的博客](http://vipstone.cnblogs.com/)  
出处： [http://vipstone.cnblogs.com/](http://vipstone.cnblogs.com/)