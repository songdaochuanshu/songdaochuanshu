---
layout: post
title: 'MCP神器！MCP-USE 一键部署连接任何MCP服务器'
date: "2025-08-15T00:43:58Z"
---
MCP神器！MCP-USE 一键部署连接任何MCP服务器
============================

![MCP神器！MCP-USE 一键部署连接任何MCP服务器](https://img2024.cnblogs.com/blog/1654515/202508/1654515-20250814113912776-1275464309.png) \`mcp-use\` 是连接任何 LLM 到任何 MCP 服务器并构建自定义 MCP 智能体最简单的开源方式，无需依赖闭源或特定应用客户端。 它解决了开发者在构建 AI 智能体时面临的工具集成复杂性问题，让开发者能够轻松地将 LLM 连接到各种工具，如网页浏览、文件操作等。

Hello， 大家好，我是程序员海军, `全栈开发` |`AI爱好者` ｜ `独立开发`。  
最近一直在研究MCP方面的事情，使用的技术栈是Python + FastAPi + FastMCP，开发了多个MCP-Server，本地化访问没啥问题，准备部署试着玩一下，调研发现这样的一个 MCP 神器，可一键部署MCP 服务器托管，并且它简化了很多操作，简直太方便了。  

`mcp-use` 是连接任何 LLM 到任何 MCP 服务器并构建自定义 MCP 智能体最简单的开源方式，无需依赖闭源或特定应用客户端。 它解决了开发者在构建 AI 智能体时面临的工具集成复杂性问题，让开发者能够轻松地将 LLM 连接到各种工具，如网页浏览、文件操作等。

什么是 `mcp-use`
-------------

`mcp-use` 是一个开源 `Python` 库，专门用于连接`LLM` 和 `MCP服务器`。它充当了 `LLM` 和各种工具服务之间的桥梁，让开发者能够创建具有工具访问能力的自定义智能体。

核心价值

*   开放性：完全开源，不依赖任何闭源或特定应用的客户端
*   通用性：支持任何 LangChain 兼容的 LLM 提供商（OpenAI、Anthropic、Groq 等）
*   灵活性：通过简单的 JSON 配置即可连接各种 MCP 服务器
*   易用性：提供简洁的 Python API，几行代码即可创建功能强大的智能体

mcp-use 功能
----------

### LLM 灵活性

*   支持各大模型系列的模型

### 多种连接方式

*   Stdio 连接：标准输入输出连接方式
*   HTTP 连接：支持连接到特定端口的 MCP 服务器
*   SSE 连接：支持服务端事件流连接
*   沙盒执行：通过 E2B 云基础设施运行 MCP 服务器

### 高级功能

*   多服务器支持：同时连接多个 MCP 服务器
*   动态服务器选择：智能选择最合适的服务器执行任务
*   工具访问控制：限制智能体可使用的工具范围
*   流式输出：支持实时输出智能体的执行过程
*   调试模式：提供详细的调试信息帮助开发

### 2.4 配置管理

*   支持 JSON 配置文件
*   支持字典配置
*   环境变量管理
*   灵活的服务器配置选项

mcp-use 如何使用
------------

安装
--

    # 基础安装
    pip install mcp-use
    
    # 安装 LLM 提供商依赖
    pip install langchain-openai  # OpenAI
    pip install langchain-anthropic  # Anthropic
    
    # 安装沙盒支持（可选）
    pip install "mcp-use[e2b]"
    

### 基本使用

    import asyncio
    from dotenv import load_dotenv
    from langchain_openai import ChatOpenAI
    from mcp_use import MCPAgent, MCPClient
    
    async def main():
        load_dotenv()
        
        # 配置 MCP 服务器
        config = {
            "mcpServers": {
                "playwright": {
                    "command": "npx",
                    "args": ["@playwright/mcp@latest"],
                    "env": {"DISPLAY": ":1"}
                }
            }
        }
        
        # 创建客户端和智能体
        client = MCPClient.from_dict(config)
        llm = ChatOpenAI(model="gpt-4o")
        agent = MCPAgent(llm=llm, client=client, max_steps=30)
        
        # 执行任务
        result = await agent.run(
            "上海有哪些美食"
        )
        print(f"结果: {result}")
    
    if __name__ == "__main__":
        asyncio.run(main())
    

### 使用配置文件

创建 mcp-config.json 文件：

    {
      "mcpServers": {
        "playwright": {
          "command": "npx",
          "args": ["@playwright/mcp@latest"],
          "env": {
            "DISPLAY": ":1"
          }
        },
        "airbnb": {
          "command": "npx",
          "args": ["-y", "@openbnb/mcp-server-airbnb"]
        }
      }
    }
    

在代码中使用mcpserver

    client = MCPClient.from_config_file("mcp-config.json")
    

### 流式输出

    async for chunk in agent.astream("山西哪里好玩？"):
        print(chunk["messages"], end="", flush=True)
    

### 工具访问控制

    # 限制智能体可使用的工具
    agent = MCPAgent(
        llm=llm,
        client=client,
        disallowed_tools=["get_Personal",]  # 禁用一些工具调用
    )
    

### 调试模式

    
    #在代码中设置
    import mcp_use
    mcp_use.set_debug(2)  # 启用详细调试信息
    

总结
--

`mcp-use` 为开发者提供了一个强大而灵活的解决方案，解决了 `LLM` 与外部工具集成的复杂性问题。

我们可以通过几行代码快速构建AI Agent，并且还可以轻松的集成MCP 服务器和工具了。

随着 MCP 生态系统的不断发展，我觉得不管是大模型的开发还是Agent 开发等等，门槛都会被降低下来了，现在已经是这个趋势了。AI 的飞速发展，以往的很多知识点可能被推翻，化繁为简，变的更简单。

mcp-use: `https://mcp-use.com/`