---
layout: post
title: 'ä»é›¶å®ç°ä¸€ä¸ªç”Ÿäº§çº§ RAG è¯­ä¹‰æœç´¢ç³»ç»Ÿï¼šC++ + ONNX + FAISS å®æˆ˜'
date: "2026-02-13T01:03:20Z"
---
ä»é›¶å®ç°ä¸€ä¸ªç”Ÿäº§çº§ RAG è¯­ä¹‰æœç´¢ç³»ç»Ÿï¼šC++ + ONNX + FAISS å®æˆ˜
==========================================

æœ¬æ–‡ä»é›¶æ„å»ºäº†ä¸€ä¸ªè½»é‡çº§ã€é«˜æ€§èƒ½çš„ C++ è¯­ä¹‰æœç´¢ç³»ç»Ÿï¼ŒåŸºäº ONNX è¿è¡Œ BGE åµŒå…¥æ¨¡å‹ã€FAISS å‘é‡ç´¢å¼•ä¸ Markdown è¯­ä¹‰åˆ†å—ï¼Œå®Œæ•´å®ç°æ”¯æŒå¢åˆ æ”¹æŸ¥çš„ç”Ÿäº§çº§ RAG æ£€ç´¢åç«¯ã€‚

1\. å¼•è¨€
======

æ—¢ç„¶æ˜¯â€œä»é›¶å®ç°â€ï¼Œæœ¬æ–‡æš‚ä¸æ·±å…¥æ¢è®¨ç¹å¤çš„ç†è®ºèƒŒæ™¯ï¼Œè€Œæ˜¯å…ˆèšç„¦ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šè¯­ä¹‰åŒ–æœç´¢ä¸­çš„â€œè¯­ä¹‰åŒ–â€åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

ä¼ ç»Ÿçš„å…³é”®è¯æœç´¢ä¾èµ–å­—é¢åŒ¹é…â€”â€”æ¯”å¦‚æœç´¢â€œå¦‚ä½•åºåˆ—åŒ– JSONâ€ï¼Œç³»ç»Ÿåªä¼šè¿”å›åŒ…å«è¿™äº›ç²¾ç¡®å…³é”®è¯çš„æ–‡æ¡£ã€‚è¿™ç§æ–¹å¼ç®€å•ç›´æ¥ï¼Œä½†ååˆ†â€œå‘†æ¿â€ï¼šå®ƒæ— æ³•ç†è§£â€œJSON åºåˆ—åŒ–æ–¹æ³•â€æˆ–â€œæŠŠå¯¹è±¡è½¬æˆ JSON å­—ç¬¦ä¸²â€å…¶å®è¡¨è¾¾äº†ç›¸åŒçš„æ„å›¾ã€‚

è€Œè¯­ä¹‰åŒ–æœç´¢çš„çªç ´åœ¨äºï¼šå®ƒä¸å†æ¯”è¾ƒæ–‡å­—ï¼Œè€Œæ˜¯æ¯”è¾ƒå«ä¹‰ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ–‡æœ¬ï¼ˆæ— è®ºæ˜¯æŸ¥è¯¢è¿˜æ˜¯æ–‡æ¡£ï¼‰é€šè¿‡åµŒå…¥æ¨¡å‹ï¼ˆEmbedding Modelï¼‰è½¬æ¢ä¸ºé«˜ç»´å‘é‡â€”â€”è¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºâ€œå‘é‡åŒ–â€æˆ–â€œåµŒå…¥â€ï¼ˆEmbeddingï¼‰ã€‚åœ¨å‘é‡ç©ºé—´ä¸­ï¼Œè¯­ä¹‰ç›¸è¿‘çš„å¥å­ä¼šè¢«æ˜ å°„åˆ°å½¼æ­¤é è¿‘çš„ä½ç½®ã€‚äºæ˜¯ï¼Œæœç´¢å°±å˜æˆäº†ä¸€ä¸ªå‘é‡ç›¸ä¼¼åº¦è®¡ç®—é—®é¢˜ï¼šæ‰¾å‡ºä¸æŸ¥è¯¢å‘é‡æœ€æ¥è¿‘çš„æ–‡æ¡£å‘é‡ã€‚

è¡¨é¢ä¸Šçœ‹ï¼Œè¿™æ˜¯æ•°å­¦ï¼ˆå‘é‡ã€å†…ç§¯ã€è·ç¦»ï¼‰ï¼›æœ¬è´¨ä¸Šï¼Œè¿™æ˜¯æ™ºèƒ½â€”â€”å› ä¸ºæ¨¡å‹é€šè¿‡æµ·é‡æ•°æ®å­¦ä¹ åˆ°äº†äººç±»è¯­è¨€çš„è¯­ä¹‰ç»“æ„ã€‚è€Œæˆ‘ä»¬è¦åšçš„ï¼Œå°±æ˜¯ç”¨ç¼–ç¨‹è¯­è¨€æŠŠè¿™å¥—â€œç”¨æ•°å­¦ç†è§£è¯­è¨€â€çš„èƒ½åŠ›ï¼Œå˜æˆä¸€ä¸ªé«˜æ•ˆã€å¯é ã€å¯éƒ¨ç½²çš„ç”Ÿäº§çº§ç³»ç»Ÿã€‚

2\. ç›®æ ‡
======

æœ¬æ–‡çš„åˆè¡·ï¼Œæ˜¯ä¸ºæˆ‘çš„ä¸ªäººåšå®¢ç½‘ç«™ [**Charlee44 çš„æŠ€æœ¯é©¿ç«™**](https://charlee44.com/) å®ç°ä¸€å¥—çœŸæ­£å¯ç”¨çš„ç«™å†…æœç´¢åŠŸèƒ½ã€‚ç”±äºåšå®¢éƒ¨ç½²åœ¨èµ„æºæå…¶æœ‰é™çš„äº‘æœåŠ¡å™¨ä¸Šï¼ˆæ¯”å¦‚1æ ¸ CPUã€1GB å†…å­˜ï¼‰ï¼Œæˆ‘å¯¹ç³»ç»Ÿæå‡ºäº†ä¸¤ä¸ªâ€œæè‡´â€è¦æ±‚ï¼š**æè‡´çš„æ€§èƒ½** ä¸ **æè‡´çš„èµ„æºæ•ˆç‡**ã€‚

æ¯•ç«Ÿï¼Œæ¯ä¸€åˆ†ç®—åŠ›éƒ½æ¥è‡ªè‡ªå·±çš„é’±åŒ…â€”â€”è¿™ä¿ƒä½¿æˆ‘æ”¾å¼ƒäº†ä¸»æµä½†ç›¸å¯¹â€œé‡â€çš„ Python ç”Ÿæ€ï¼ˆå¦‚ Sentence Transformers + ChromaDB çš„å¸¸è§„ç»„åˆï¼‰ï¼Œè½¬è€Œé€‰æ‹© **C++** ä½œä¸ºå®ç°è¯­è¨€ã€‚C++ ä¸ä»…èƒ½æä¾›æ›´ä½çš„å†…å­˜å¼€é”€å’Œæ›´é«˜çš„æ¨ç†ååï¼Œä¹Ÿè®©æˆ‘æœ‰æœºä¼šæ·±å…¥ç†è§£ embedding æ¨ç†ã€å‘é‡ç´¢å¼•ã€æ–‡æœ¬åˆ†å—ç­‰æ¨¡å—çš„åº•å±‚æœºåˆ¶ã€‚

å½“ç„¶ï¼Œéœ€è¦è¯´æ˜çš„æ˜¯ï¼š**å¦‚æœæ˜¯åœ¨ä¼ä¸šç¯å¢ƒä¸­å¼€å‘ï¼Œæˆ–å¯¹è¿­ä»£é€Ÿåº¦è¦æ±‚æ›´é«˜ï¼ŒPython ç”Ÿæ€ä»æ˜¯æ›´é«˜æ•ˆã€æ›´æˆç†Ÿçš„é€‰æ‹©**ã€‚è€Œæœ¬é¡¹ç›®æ›´å¤šæ˜¯ä¸€æ¬¡â€œç”¨å·¥ç¨‹çº¦æŸé©±åŠ¨æ·±åº¦å­¦ä¹ â€çš„å®è·µâ€”â€”åœ¨æœ‰é™èµ„æºä¸‹ï¼Œäº²æ‰‹æ„å»ºä¸€ä¸ªè½»é‡ã€å¯æ§ã€å¯è½åœ°çš„è¯­ä¹‰æœç´¢ç³»ç»Ÿã€‚

3\. åµŒå…¥æ¨¡å‹
========

æ—¢ç„¶è¯­ä¹‰åŒ–æœç´¢çš„æ ¸å¿ƒåœ¨äºå°†æ–‡æœ¬è½¬åŒ–ä¸ºå¯Œå«è¯­ä¹‰çš„å‘é‡ï¼Œé‚£ä¹ˆå®ç°è¿™ä¸€èƒ½åŠ›çš„å…³é”®ï¼Œå°±åœ¨äºé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„åµŒå…¥æ¨¡å‹ã€‚è¿™ç±»æ¨¡å‹çš„ä½œç”¨ï¼Œæ˜¯å°†ä»»æ„é•¿åº¦çš„æ–‡æœ¬æ˜ å°„ä¸ºå›ºå®šç»´åº¦çš„ç¨ å¯†å‘é‡ï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘ã€‚ç¬”è€…è¿™é‡Œä½¿ç”¨çš„æ˜¯ [bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) ã€‚bge-small-zh-v1.5 æ˜¯ç”±åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ï¼ˆBAAIï¼‰æ¨å‡ºçš„ BGEï¼ˆBAAI General Embeddingï¼‰ç³»åˆ—ä¸­çš„è½»é‡çº§ä¸­æ–‡æ¨¡å‹ã€‚å®ƒåŸºäº Transformer æ¶æ„ï¼Œåœ¨å¤§è§„æ¨¡ä¸­è‹±æ–‡è¯­æ–™ä¸Šè¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼Œä¸“ä¸ºæ£€ç´¢ä»»åŠ¡ä¼˜åŒ–ã€‚å°½ç®¡å®ƒå¹¶éè¯¥ç³»åˆ—ä¸­æœ€æ–°æˆ–æœ€å¤§çš„ç‰ˆæœ¬ï¼ˆå¦‚ bge-largeï¼‰ï¼Œä½†å…¶åœ¨æ¨ç†é€Ÿåº¦ã€å†…å­˜å ç”¨ä¸è¯­ä¹‰è´¨é‡ä¹‹é—´å–å¾—äº†æä½³çš„å¹³è¡¡ï¼Œå°¤å…¶é€‚åˆèµ„æºå—é™æˆ–å¯¹å»¶è¿Ÿæ•æ„Ÿçš„ç”Ÿäº§ç¯å¢ƒã€‚

bge-small-zh-v1.5 å¯ä»¥ä» Hugging Face ä¸Šä¸‹è½½ï¼Œä¸‹è½½åçš„æ•°æ®æ–‡ä»¶ç»„ç»‡ç»“æ„å¦‚ä¸‹ï¼š

    bge-small-zh-v1.5/
    â”œâ”€â”€ config.json                     # æ¨¡å‹æ¶æ„é…ç½®ï¼ˆå±‚æ•°ã€éšè—å±‚ç»´åº¦ç­‰ï¼‰
    â”œâ”€â”€ pytorch_model.bin               # PyTorch æ ¼å¼çš„æ¨¡å‹æƒé‡ï¼ˆæ ¸å¿ƒæ–‡ä»¶ï¼‰
    â”œâ”€â”€ tokenizer.json                  # åˆ†è¯å™¨å®šä¹‰ï¼ˆWordPiece è¯æ±‡è¡¨ + ç®—æ³•å‚æ•°ï¼‰
    â”œâ”€â”€ tokenizer_config.json           # åˆ†è¯å™¨é…ç½®ï¼ˆå¦‚æ˜¯å¦å°å†™åŒ–ã€ç‰¹æ®Š token ç­‰ï¼‰
    â”œâ”€â”€ vocab.txt                       # WordPiece è¯æ±‡è¡¨ï¼ˆçº¯æ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ª tokenï¼‰
    â”œâ”€â”€ special_tokens_map.json         # ç‰¹æ®Š token æ˜ å°„ï¼ˆ[CLS], [SEP], [PAD] ç­‰ï¼‰
    â”œâ”€â”€ modules.json                    # ï¼ˆå¯é€‰ï¼‰æ¨¡å‹æ¨¡å—ä¿¡æ¯
    â”œâ”€â”€ sentence_bert_config.json       # ï¼ˆå¯é€‰ï¼‰Sentence-BERT ç›¸å…³é…ç½®
    â”œâ”€â”€ README.md                       # æ¨¡å‹å¡ç‰‡ï¼ˆå«ä½¿ç”¨è¯´æ˜ã€å¼•ç”¨ä¿¡æ¯ç­‰ï¼‰
    â””â”€â”€ .gitattributes                  # Git LFS é…ç½®ï¼ˆç”¨äºå¤§æ–‡ä»¶ç®¡ç†ï¼‰
    

ä¸è¿‡ï¼Œbge-small-zh-v1.5 åŸç”ŸåŸºäº PyTorchï¼ˆå±äº Python ç”Ÿæ€ï¼‰ï¼Œç›´æ¥åœ¨ C++ ç¯å¢ƒä¸­è°ƒç”¨å¹¶ä¸æ–¹ä¾¿ï¼Œä¹Ÿéš¾ä»¥æ»¡è¶³æˆ‘ä»¬å¯¹æ€§èƒ½å’Œèµ„æºå ç”¨çš„è¦æ±‚ã€‚ä¸ºäº†åœ¨ C++ ä¸­é«˜æ•ˆè¿è¡Œè¯¥æ¨¡å‹ï¼Œæœ€ä½³å®è·µæ˜¯å°†å…¶å¯¼å‡ºä¸º ONNX æ ¼å¼ã€‚

ONNXï¼ˆOpen Neural Network Exchangeï¼‰æ˜¯ä¸€ç§å¼€æ”¾çš„ç¥ç»ç½‘ç»œæ¨¡å‹äº¤æ¢æ ¼å¼ï¼Œç”±å¾®è½¯ã€Facebook ç­‰å…¬å¸å…±åŒæ¨åŠ¨ï¼Œæ—¨åœ¨å®ç°â€œä¸€æ¬¡è®­ç»ƒï¼Œå¤šç«¯éƒ¨ç½²â€ã€‚å®ƒä¸ä¾èµ–ç‰¹å®šæ¡†æ¶ï¼ˆå¦‚ PyTorch æˆ– TensorFlowï¼‰ï¼Œè€Œæ˜¯å°†æ¨¡å‹ç»“æ„ä¸æƒé‡ç»Ÿä¸€åºåˆ—åŒ–ä¸ºæ ‡å‡†æ ¼å¼ï¼Œä»è€Œå¯åœ¨ä¸åŒç¡¬ä»¶å’Œè¯­è¨€ç¯å¢ƒä¸­é«˜æ•ˆæ¨ç†ã€‚

è¦åœ¨ C++ ä¸­åŠ è½½å’Œè¿è¡Œ ONNX æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ ONNX Runtimeâ€”â€”è¿™æ˜¯å¾®è½¯å¼€æºçš„é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œæ”¯æŒ CPU/GPU åŠ é€Ÿã€è·¨å¹³å°ï¼ˆWindows/Linux/macOSï¼‰ä»¥åŠ C/C++/Python ç­‰å¤šç§è¯­è¨€ç»‘å®šã€‚é€šè¿‡ ONNX Runtimeï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ—  Python ä¾èµ–çš„æƒ…å†µä¸‹ï¼Œä»¥æä½çš„å¼€é”€å®Œæˆ embedding æ¨ç†ï¼Œå®Œç¾å¥‘åˆæœ¬é¡¹ç›®çš„è½»é‡çº§ç›®æ ‡ã€‚

å› æ­¤ï¼Œå…³é”®çš„ç¬¬ä¸€æ­¥æ˜¯éœ€è¦å°† bge-small-zh-v1.5 è½¬æ¢æˆ ONNX æ ¼å¼çš„åµŒå…¥æ¨¡å‹ã€‚åµŒå…¥æ¨¡å‹çš„æ ¼å¼è½¬æ¢æ˜¯é¢„å¤„ç†æ­¥éª¤ï¼Œå¯ä»¥é€šè¿‡ Python è„šæœ¬æ¥å®ç°ï¼š

    # export_onnx.py
    from transformers import AutoTokenizer, AutoModel
    from optimum.onnxruntime import ORTModelForFeatureExtraction
    from pathlib import Path
    
    # æ”¹ä¸ºä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæ³¨æ„ï¼šä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²æˆ–æ­£æ–œæ ï¼‰
    model_path = "C:/Github/search/bge-small-zh-v1.5" 
    onnx_path = "./bge-small-zh-onnx"
    
    # å¯¼å‡º ONNXï¼ˆä»æœ¬åœ°æ¨¡å‹åŠ è½½ï¼‰
    ort_model = ORTModelForFeatureExtraction.from_pretrained(
        model_path,           # â† å…³é”®ï¼šä½¿ç”¨æœ¬åœ°è·¯å¾„
        export=True,
        provider="CPUExecutionProvider"
    )
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)  # â† åŒæ ·ç”¨æœ¬åœ°è·¯å¾„
    
    # ä¿å­˜ ONNX æ¨¡å‹å’Œ tokenizer
    ort_model.save_pretrained(onnx_path)
    tokenizer.save_pretrained(onnx_path)
    
    print(f"âœ… ONNX æ¨¡å‹å·²å¯¼å‡ºåˆ° {onnx_path}")
    

è½¬æ¢åçš„ ONNX æ ¼å¼åµŒå…¥æ¨¡å‹ bge-small-zh-onnx çš„æ•°æ®æ–‡ä»¶ç»„ç»‡ç»“æ„å¦‚ä¸‹ï¼š

    bge-small-zh-onnx/
    â”œâ”€â”€ model.onnx                      # âœ… æ ¸å¿ƒï¼šONNX æ ¼å¼çš„æ¨¡å‹è®¡ç®—å›¾ï¼ˆå«æƒé‡ï¼‰
    â”œâ”€â”€ config.json                     # æ¨¡å‹æ¶æ„é…ç½®ï¼ˆä¸åŸç‰ˆä¸€è‡´ï¼‰
    â”œâ”€â”€ tokenizer.json                  # åˆ†è¯å™¨å®šä¹‰ï¼ˆWordPiece + é¢„å¤„ç†è§„åˆ™ï¼‰
    â”œâ”€â”€ tokenizer_config.json           # åˆ†è¯å™¨è¡Œä¸ºé…ç½®ï¼ˆå¦‚ do_lower_caseï¼‰
    â”œâ”€â”€ vocab.txt                       # WordPiece è¯æ±‡è¡¨ï¼ˆçº¯æ–‡æœ¬å¤‡ä»½ï¼‰
    â””â”€â”€ special_tokens_map.json         # ç‰¹æ®Š token æ˜ å°„ï¼ˆ[CLS], [SEP], [PAD] ç­‰ï¼‰
    

4\. åˆ†è¯å™¨
=======

åœ¨ä½¿ç”¨ ONNX Runtime åŠ è½½åµŒå…¥æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦äº†è§£ä¸€ä¸ªå…³é”®ç»„ä»¶ï¼š**åˆ†è¯å™¨**ï¼ˆTokenizerï¼‰ã€‚

æ‰€è°“â€œå­—ã€è¯ã€å¥ã€æ®µã€ç¯‡â€ï¼Œè¯­è¨€çš„ç†è§£æ˜¯åˆ†å±‚çš„ã€‚ä½†å¯¹äºç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯åŸºäº Transformer çš„æ¶æ„ï¼‰è€Œè¨€ï¼Œå®ƒä»¬å¹¶ä¸ç›´æ¥å¤„ç†åŸå§‹å­—ç¬¦ï¼Œè€Œæ˜¯å°†æ–‡æœ¬**åˆ‡åˆ†ä¸ºæ›´å°çš„è¯­ä¹‰å•å…ƒâ€”â€”â€œè¯å…ƒâ€**ï¼ˆtokenï¼‰ã€‚è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯ **Tokenization**ï¼ˆè¯å…ƒåŒ–ï¼‰ï¼Œç”±åˆ†è¯å™¨å®Œæˆã€‚

åˆ†è¯å™¨çš„é‡è¦æ€§ä½“ç°åœ¨ä»¥ä¸‹ä¸¤ç‚¹ï¼š

1.  **æ¨¡å‹è¾“å…¥çš„å‰æ**ï¼šåµŒå…¥æ¨¡å‹åªæ¥å— token ID åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œè€ŒéåŸå§‹å­—ç¬¦ä¸²ã€‚æ²¡æœ‰æ­£ç¡®çš„åˆ†è¯ï¼Œå°±æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„ embeddingã€‚
2.  **å½±å“è¯­ä¹‰ç²¾åº¦**ï¼šä¸åŒçš„åˆ†è¯ç­–ç•¥ä¼šå¯¼è‡´ä¸åŒçš„ token åºåˆ—ï¼Œè¿›è€Œå½±å“å‘é‡è¡¨è¾¾çš„è´¨é‡ã€‚

4.1 è‡ªå®šä¹‰åˆ†è¯å™¨
----------

å…¶å®åˆ†è¯å™¨çš„å®ç°ä¹Ÿå¹¶ä¸ç¥ç§˜ï¼Œæˆ‘ä»¬å®Œå…¨å¯ä»¥å®ç°ä¸€ä¸ªç®€å•ç‰ˆæœ¬çš„ï¼š

    #include <fstream>
    #include <string>
    #include <unordered_map>
    #include <vector>
    
    #ifdef _WIN32
    #include <Windows.h>
    #endif
    
    using namespace std;
    
    std::unordered_map<std::string, int> LoadVocab(const std::string& path) {
      std::unordered_map<std::string, int> vocab;
      std::ifstream file(path);
      std::string token;
      int id = 0;
      while (std::getline(file, token)) {
        vocab[token] = id++;
      }
      return vocab;
    }
    
    // æ³¨æ„ï¼šéœ€å¤„ç† UTF-8 å¤šå­—èŠ‚å­—ç¬¦ï¼
    std::vector<std::string> SplitTextChars(const std::string& text) {
      std::vector<std::string> chars;
      for (size_t i = 0; i < text.size();) {
        if ((text[i] & 0x80) == 0) {  // ASCII
          chars.push_back(std::string(1, text[i++]));
        } else {  // UTF-8 å¤šå­—èŠ‚
          int bytes = 0;
          if ((text[i] & 0xE0) == 0xC0)
            bytes = 2;
          else if ((text[i] & 0xF0) == 0xE0)
            bytes = 3;
          else if ((text[i] & 0xF8) == 0xF0)
            bytes = 4;
          else
            throw std::runtime_error("Invalid UTF-8");
          chars.push_back(text.substr(i, bytes));
          i += bytes;
        }
      }
      return chars;
    }
    
    //è¾“å…¥å­—ç¬¦ä¸²ï¼Œè¾“å‡º input_ids å’Œ attention_mask
    std::pair<std::vector<int64_t>, std::vector<int64_t>> Tokenize(
        const std::string& text, const std::unordered_map<std::string, int>& vocab,
        int maxLength = 512) {
      if (maxLength < 2) {
        throw std::invalid_argument("maxLength must be at least 2");
      }
    
      // é¢„å–ç‰¹æ®Š token ID
      const int clsId = vocab.at("[CLS]");
      const int sepId = vocab.at("[SEP]");
      const int padId = vocab.at("[PAD]");
      const int unkId = vocab.at("[UNK]");
    
      // åˆå§‹åŒ–å‘é‡ï¼ˆè‡ªåŠ¨ paddingï¼‰
      std::vector<int64_t> inputIds(maxLength, padId);
      std::vector<int64_t> attentionMask(maxLength, 0);
    
      //å¼€å¤´åŠ  "[CLS]"
      size_t currentIndex = 0;
      inputIds[currentIndex] = clsId;
      attentionMask[currentIndex] = 1;
      currentIndex++;
    
      //ä¸­é—´æ¯ä¸ªå­—ä½œä¸ºä¸€ä¸ª tokenï¼ˆæŸ¥ vocab å¾— IDï¼‰
      auto chars = SplitTextChars(text);
      for (const auto& ch : chars) {
        if (currentIndex >= maxLength - 1ULL) {
          break;
        }
    
        const auto& it = vocab.find(ch);
        inputIds[currentIndex] = (it == vocab.end() ? unkId : it->second);
        attentionMask[currentIndex] = 1;
        currentIndex++;
      }
    
      //ç»“å°¾åŠ  "[SEP]"
      inputIds[currentIndex] = sepId;
      attentionMask[currentIndex] = 1;
    
      return {inputIds, attentionMask};
    }
    
    void TestTokenize() {
      string vocabPath = "C:/Github/search/bge-small-zh-onnx/vocab.txt";
      string text = "gitæ’¤å›æäº¤";
    
      auto vocab = LoadVocab(vocabPath);
      const auto& [inputIds, attMask] = Tokenize(text, vocab);
    
      // æ‰“å°å‰10ä¸ª token ID éªŒè¯
      cout << "Input IDs (first 10): ";
      for (int i = 0; i < 10; ++i) {
        cout << inputIds[i] << " ";
      }
    
      cout << "\nAttention Mask (first 10): ";
      for (int i = 0; i < 10; ++i) {
        cout << attMask[i] << " ";
      }
    
      cout << endl;
    }
    
    int main() {
    #ifdef _WIN32
      SetConsoleOutputCP(65001);
    #endif
    
      TestTokenize();
    
      return 0;
    }
    

æ— è®ºæ˜¯åŸå§‹çš„ `bge-small-zh-v1.5` æ¨¡å‹ï¼Œè¿˜æ˜¯è½¬æ¢åçš„ ONNX ç‰ˆæœ¬ï¼Œå…¶è¯æ±‡è¡¨ `vocab.txt` çš„å†…å®¹å®Œå…¨ä¸€è‡´ã€‚æˆ‘ä»¬å¯ä»¥å°†è¾“å…¥æ–‡æœ¬åˆ‡åˆ†ä¸ºå•ä¸ªçš„ token å•å…ƒï¼Œå¹¶åœ¨ `vocab.txt` ä¸­æŸ¥æ‰¾åˆ°å¯¹åº”çš„ IDï¼Œå°±èƒ½ç”Ÿæˆä¸æˆ‘ä»¬æƒ³è¦åµŒå…¥çš„è¾“å…¥åºåˆ—ã€‚åœ¨è¿™ä¸ªç®€åŒ–å®ç°ä¸­ï¼Œä½¿ç”¨çš„æ‹†åˆ†ç­–ç•¥æ˜¯**é€å­—åˆ‡åˆ†**ï¼Œä¹Ÿå°±æ˜¯æŒ‰ UTF-8 å­—ç¬¦è¾¹ç•Œæ‹†åˆ†å­—ç¬¦ä¸²ã€‚

æ­¤å¤–ï¼Œæ¨¡å‹ä½¿ç”¨äº†å››ä¸ªå…³é”®çš„ç‰¹æ®Š tokenï¼Œå®ƒä»¬åœ¨ `vocab.txt` å’Œ `special_tokens_map.json` ä¸­å‡æœ‰å®šä¹‰ï¼š

*   `[CLS]`ï¼šåºåˆ—èµ·å§‹æ ‡è®°ï¼Œå…¶å¯¹åº”ä½ç½®çš„è¾“å‡ºå‘é‡å¸¸ç”¨äºå¥å­çº§ embeddingï¼ˆBGE é»˜è®¤ä½¿ç”¨è¯¥å‘é‡ä½œä¸ºæ•´ä¸ªæ–‡æœ¬çš„è¡¨ç¤ºï¼‰ï¼›
*   `[SEP]`ï¼šåºåˆ—ç»“æŸæˆ–åˆ†éš”æ ‡è®°ï¼Œç”¨äºåŒºåˆ†ä¸åŒå¥å­ï¼ˆå•å¥è¾“å…¥æ—¶ç½®äºæœ«å°¾ï¼‰ï¼›
*   `[PAD]`ï¼šå¡«å……æ ‡è®°ï¼Œç”¨äºå¯¹é½ batch ä¸­ä¸åŒé•¿åº¦çš„åºåˆ—ï¼›
*   `[UNK]`ï¼šæœªçŸ¥è¯æ ‡è®°ï¼Œå½“è¾“å…¥ token ä¸åœ¨è¯æ±‡è¡¨ä¸­æ—¶ä½¿ç”¨ã€‚

æœ€ç»ˆè¾“å‡ºçš„ç»“æœæ˜¯ï¼š

    Input IDs (first 10): 101 149 151 162 3059 1726 2990 769 102 0
    Attention Mask (first 10): 1 1 1 1 1 1 1 1 1 0
    

æˆ‘ä»¬å¯ä»¥æ‰“å¼€ `vocab.txt` è¿›è¡Œå¯¹ç…§ï¼Œæ¯”å¦‚æœ€åä¸€ä¸ªæ±‰å­—â€œäº¤â€ï¼Œä½äºæ–‡æœ¬çš„ç¬¬770è¡Œï¼Œè¯´æ˜æ‹†åˆ†çš„ token æ˜¯æ­£ç¡®çš„ï¼ˆå¯¹åº” ID æ˜¯769ï¼‰ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

å¦ä¸€ä¸ªè¾“å‡ºç»“æœ Attention Maskï¼ˆæ³¨æ„åŠ›æ©ç ï¼‰ æ˜¯ Transformer ç±»æ¨¡å‹ï¼ˆåŒ…æ‹¬ BGEï¼‰ä¸­ä¸€ä¸ªå…³é”®çš„è¾“å…¥å¼ é‡ï¼Œå®ƒçš„ä½œç”¨æ˜¯å‘Šè¯‰æ¨¡å‹ï¼šâ€œå“ªäº› token æ˜¯çœŸå®å†…å®¹ï¼Œå“ªäº›æ˜¯å¡«å……ï¼ˆpaddingï¼‰â€ã€‚

4.2 Hugging Face Tokenizer
--------------------------

ä¸Šè¿°çš„ä¾‹å­åªæ˜¯æ¼”ç¤ºåˆ†è¯å™¨çš„åŸºæœ¬æµç¨‹ï¼Œå¦‚æœå¯¹ç…§åˆ†è¯å™¨çš„è¯æ±‡è¡¨ï¼Œæµ‹è¯•æ–‡æœ¬â€œgitæ’¤å›æäº¤â€åœ¨ä¸Šè¿°åˆ†è¯å™¨çš„ç»“æœæ˜¯ï¼š

    ['g', 'i', 't', 'æ’¤', 'å›', 'æ', 'äº¤']
    

ä½†æ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›çš„åˆ†è¯ç»“æœæ˜¯ï¼š

    ['git', 'æ’¤', 'å›', 'æ', 'äº¤']
    

è¿™è¯´æ˜ç®€å•çš„ UTF-8 å­—ç¬¦åˆ‡åˆ†æ— æ³•å¤ç°æ¨¡å‹è®­ç»ƒæ—¶çš„çœŸå®åˆ†è¯è¡Œä¸ºï¼Œä¼šå¯¼è‡´ embedding è´¨é‡ä¸‹é™ï¼Œå°¤å…¶åœ¨æ··åˆä¸­è‹±æ–‡çš„åšå®¢åœºæ™¯å¯èƒ½ä¼šé€ æˆä¸å°çš„å½±å“ã€‚å®é™…ä¸Šä¸€ä¸ªå·¥ä¸šçº§çš„åˆ†è¯å™¨è¿œæ¯”â€œé€å­—åˆ‡åˆ†â€å¤æ‚å¾—å¤šï¼Œä¸€ä¸ªå…³é”®çš„é—®é¢˜å°±æ˜¯éœ€è¦åœ¨å¥å­ä¸­é¢„åˆ‡åˆ†ç‰‡æ®µï¼Œä½¿ç”¨**WordPiece ç®—æ³•**è´ªå¿ƒåŒ¹é…å­è¯ã€‚

é¢å¯¹å¦‚æ­¤å¤æ‚çš„åˆ†è¯é€»è¾‘ï¼Œä»é›¶å®ç°ä¸€ä¸ªå®Œå…¨å…¼å®¹ WordPiece çš„ tokenizer ä¸ä»…è€—æ—¶ï¼Œè¿˜ææ˜“å¼•å…¥ç»†å¾®åå·®â€”â€”è€Œè¿™äº›åå·®ä¼šç›´æ¥å¯¼è‡´ embedding å‘é‡åç¦»é¢„æœŸï¼Œæœ€ç»ˆå½±å“æœç´¢è´¨é‡ã€‚å¥½åœ¨ï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦é‡å¤é€ è½®å­ã€‚

Hugging Face Tokenizer æ˜¯å½“å‰ NLP é¢†åŸŸäº‹å®ä¸Šçš„å·¥ä¸šæ ‡å‡†ã€‚å®ƒç”± Hugging Face å®˜æ–¹ç»´æŠ¤ï¼Œé«˜åº¦ä¼˜åŒ–ã€è·¨è¯­è¨€æ”¯æŒã€ä¸”ä¸ transformers ç”Ÿæ€æ— ç¼é›†æˆã€‚å› æ­¤ï¼Œæœ€å¥½ä½¿ç”¨è¿™ä¸ªåˆ†è¯å™¨çš„å®˜æ–¹å®ç°ï¼Œç¡®ä¿è¾“å…¥åºåˆ—ä¸æ¨¡å‹è®­ç»ƒæ—¶å®Œå…¨å¯¹é½ï¼Œä»è€Œé‡Šæ”¾ BGE æ¨¡å‹çš„å…¨éƒ¨è¯­ä¹‰èƒ½åŠ›ã€‚

ä¸è¿‡ï¼Œéº»çƒ¦çš„æ˜¯ Hugging Face Tokenizer æ˜¯ä½¿ç”¨ Rust å®ç°çš„ï¼Œå¹¶ä¸”å®˜æ–¹åªæä¾›äº† Python å’Œ node çš„ç»‘å®šå®ç°ã€‚è¦é›†æˆåˆ° C++ ç¨‹åºä¸­ï¼Œæœ€å¥½çš„åŠæ³•å°±æ˜¯è‡ªå·±å°è£… Hugging Face tokenizers çš„ C ç»‘å®šã€‚å½“ç„¶ä¹Ÿä¸æ˜¯å°è£…æ‰€æœ‰çš„ FFIï¼ˆForeign Function Interfaceï¼‰æ¥å£ï¼Œè€Œæ˜¯å°è£…è‡ªå·±éœ€è¦çš„æ¥å£å°±å¯ä»¥äº†ã€‚æ¯”å¦‚æ‰§è¡Œåˆ†è¯æ¥å£å’Œè®¡ç®—Tokençš„æ¥å£ï¼š

    use std::ffi::CStr;
    use std::os::raw::c_char;
    use tokenizers::{PaddingParams, Tokenizer, TruncationParams};
    
    // === 1. å®šä¹‰ C å…¼å®¹çš„è¿”å›ç»“æ„ä½“ ===
    #[repr(C)]
    pub struct TokenizerResult {
        pub input_ids: *mut i64,
        pub attention_mask: *mut i64,
        pub token_type_ids: *mut i64,
        pub length: u64,
    }
    
    // === 2. å†…éƒ¨çŠ¶æ€ï¼šåŒ…è£… Tokenizer ===
    struct TokenizerHandle {
        tokenizer: Tokenizer,     // ç”¨äº encodeï¼ˆå¸¦ paddingï¼‰
        raw_tokenizer: Tokenizer, // ç”¨äº countï¼ˆæ—  paddingï¼‰
    }
    
    // === 3. è¾…åŠ©å‡½æ•°ï¼šå°† Rust Vec è½¬ä¸º C å¯æ‹¥æœ‰çš„æŒ‡é’ˆ ===
    fn vec_to_c_ptr(vec: Vec<i64>) -> *mut i64 {
        let mut boxed = vec.into_boxed_slice();
        let ptr = boxed.as_mut_ptr();
        std::mem::forget(boxed); // é˜²æ­¢ Rust è‡ªåŠ¨é‡Šæ”¾
        ptr
    }
    
    // === 4. åˆ›å»º tokenizer ===
    #[unsafe(no_mangle)] // ç¦ç”¨ name manglingï¼Œè®© C èƒ½æ‰¾åˆ°ç¬¦å·
    pub extern "C" fn tokenizer_create(tokenizer_json_path: *const c_char) -> *mut std::ffi::c_void {
        if tokenizer_json_path.is_null() {
            return std::ptr::null_mut();
        }
    
        let path_cstr = unsafe { CStr::from_ptr(tokenizer_json_path) };
        let path_str = match path_cstr.to_str() {
            Ok(s) => s,
            Err(_) => return std::ptr::null_mut(),
        };
    
        let mut tokenizer = match Tokenizer::from_file(path_str) {
            Ok(t) => t,
            Err(_) => return std::ptr::null_mut(),
        };
    
        // è®¾ç½® padding/truncation åˆ° 512ï¼ˆBGE é»˜è®¤ï¼‰
        tokenizer.with_padding(Some(PaddingParams {
            strategy: tokenizers::PaddingStrategy::Fixed(512),
            ..Default::default()
        }));
    
        if tokenizer
            .with_truncation(Some(TruncationParams {
                max_length: 512,
                ..Default::default()
            }))
            .is_err()
        {
            return std::ptr::null_mut();
        }
    
        let mut raw_tokenizer = tokenizer.clone();
        raw_tokenizer.with_padding(None);
        raw_tokenizer.with_truncation(None).ok();
    
        let handle = TokenizerHandle {
            tokenizer,
            raw_tokenizer,
        };
        Box::into_raw(Box::new(handle)) as *mut std::ffi::c_void
    }
    
    //è®¡ç®—å¥å­token
    #[unsafe(no_mangle)] // ç¦ç”¨ name manglingï¼Œè®© C èƒ½æ‰¾åˆ°ç¬¦å·
    pub extern "C" fn tokenizer_count(handle: *mut std::ffi::c_void, text: *const c_char) -> u64 {
        if handle.is_null() || text.is_null() {
            return 0;
        }
    
        let handle_ref = unsafe { &*(handle as *mut TokenizerHandle) };
    
        let text_cstr = unsafe { CStr::from_ptr(text) };
        let text_str = match text_cstr.to_str() {
            Ok(s) => s,
            Err(_) => return 0,
        };
    
        match handle_ref.raw_tokenizer.encode(text_str, true) {
            Ok(encoding) => encoding.len() as u64,
            Err(_) => 0,
        }
    }
    
    // === 5. é”€æ¯ tokenizer ===
    #[unsafe(no_mangle)]
    pub extern "C" fn tokenizer_destroy(handle: *mut std::ffi::c_void) {
        if !handle.is_null() {
            unsafe {
                let _ = Box::from_raw(handle as *mut TokenizerHandle);
                // Drop è‡ªåŠ¨è°ƒç”¨
            }
        }
    }
    
    // === 6. æ‰§è¡Œåˆ†è¯ ===
    #[unsafe(no_mangle)]
    pub extern "C" fn tokenizer_encode(
        handle: *mut std::ffi::c_void,
        text: *const c_char,
    ) -> TokenizerResult {
        let default_result = TokenizerResult {
            input_ids: std::ptr::null_mut(),
            attention_mask: std::ptr::null_mut(),
            token_type_ids: std::ptr::null_mut(),
            length: 0,
        };
    
        if handle.is_null() || text.is_null() {
            return default_result;
        }
    
        let handle_ref = unsafe { &*(handle as *mut TokenizerHandle) };
    
        let text_cstr = unsafe { CStr::from_ptr(text) };
        let text_str = match text_cstr.to_str() {
            Ok(s) => s,
            Err(_) => return default_result,
        };
    
        let encoding = match handle_ref.tokenizer.encode(text_str, true) {
            Ok(e) => e,
            Err(_) => return default_result,
        };
    
        let input_ids: Vec<i64> = encoding.get_ids().iter().map(|&x| x as i64).collect();
        let attention_mask: Vec<i64> = encoding
            .get_attention_mask()
            .iter()
            .map(|&x| x as i64)
            .collect();
        let token_type_ids: Vec<i64> = encoding.get_type_ids().iter().map(|&x| x as i64).collect();
        // BGE ä¸éœ€è¦ï¼Œä½† C++ ä»£ç ä¼ äº†
        // let token_type_ids: Vec<u32> = vec![0u32; input_ids.len()];
    
        let len = input_ids.len(); // åº”è¯¥æ˜¯ 512ï¼Œä½†æ›´é€šç”¨
    
        TokenizerResult {
            input_ids: vec_to_c_ptr(input_ids),
            attention_mask: vec_to_c_ptr(attention_mask),
            token_type_ids: vec_to_c_ptr(token_type_ids),
            length: len as u64,
        }
    }
    
    // === 7. é‡Šæ”¾ç»“æœå†…å­˜ ===
    #[unsafe(no_mangle)]
    pub extern "C" fn tokenizer_result_free(result: TokenizerResult) {
        if !result.input_ids.is_null() {
            unsafe {
                let _ = Vec::from_raw_parts(
                    result.input_ids,
                    result.length as usize,
                    result.length as usize,
                );
            }
        }
    
        if !result.attention_mask.is_null() {
            unsafe {
                let _ = Vec::from_raw_parts(
                    result.attention_mask,
                    result.length as usize,
                    result.length as usize,
                );
            }
        }
    
        if !result.token_type_ids.is_null() {
            unsafe {
                let _ = Vec::from_raw_parts(
                    result.token_type_ids,
                    result.length as usize,
                    result.length as usize,
                );
            }
        }
    }
    

å¯¹åº”çš„ C æ¥å£å¦‚ä¸‹ï¼š

    // tokenizer_result.h
    #pragma once
    
    #include <stdint.h>
    
    // å®šä¹‰ç»“æ„ä½“
    struct TokenizerResult {
        int64_t* input_ids;
        int64_t* attention_mask;
        int64_t* token_type_ids;
        uint64_t length;
    };
    
    typedef struct TokenizerResult TokenizerResult;
    

    // hf_tokenizer_ffi
    #pragma once
    
    #include "tokenizer_result.h"
    
    #ifdef __cplusplus
    extern "C" {
    #endif
    
    void* tokenizer_create(const char* tokenizer_json_path);
    void tokenizer_destroy(void* handle);
    TokenizerResult tokenizer_encode(void* handle, const char* text);
    uint64_t tokenizer_count(void* handle, const char* text);
    void tokenizer_result_free(TokenizerResult result);
    
    #ifdef __cplusplus
    }
    #endif
    

æˆ‘ä»¬åœ¨ C++ ç¨‹åºä¸­è°ƒç”¨è¿™ä¸ª C ç»‘å®šçš„æ¥å£ï¼š

    #include <hf_tokenizer_ffi.h>
    
    #include <string>
    #include <vector>
    
    #ifdef _WIN32
    #include <Windows.h>
    #endif
    
    using namespace std;
    
    void TestTokenize() {
      void* handle =
          tokenizer_create("C:/Github/search/bge-small-zh-onnx/tokenizer.json");
    
      if (!handle) {
        std::cerr << "Failed to create tokenizer\n";
        return;
      }
    
      TokenizerResult result = tokenizer_encode(handle, "gitæ’¤å›æäº¤");
    
      if (!result.input_ids) {
        std::cerr << "Tokenize failed\n";
        tokenizer_destroy(handle);
        return;
      }
    
      std::cout << "Length: " << result.length << "\n";
      for (int i = 0; i < 10; ++i) {
        std::cout << result.input_ids[i] << " ";
      }
      std::cout << "\n";
    
      // å¿…é¡»é‡Šæ”¾ï¼
      tokenizer_result_free(result);
      tokenizer_destroy(handle);
    
      return;
    }
    
    int main() {
    #ifdef _WIN32
      SetConsoleOutputCP(65001);
    #endif
    
      TestTokenize();
    
      return 0;
    }
    

è¿è¡Œç»“æœå¦‚ä¸‹ï¼š

    Length: 512
    101 10807 3059 1726 2990 769 102 0 0 0
    

å¾ˆæ˜¾ç„¶ï¼Œè¿”å›çš„å‰10ä¸ª ID å€¼ä¸­æœ‰æ•ˆ ID å°‘äº†ä¸¤ä¸ªã€‚å†æ£€æŸ¥ä¸€ä¸‹ç¬¬ä¸€ä¸ª token ä¹Ÿå°±æ˜¯ç¬¬10808è¡Œï¼ˆå¯¹åº” ID 10807ï¼‰çš„è¯æ±‡ï¼š

æ­£å¥½æ˜¯â€œgitâ€ï¼Œè¯´æ˜åˆ†è¯å™¨æ­£ç¡®æ‹†åˆ†äº†å­è¯ã€‚

éªŒè¯äº†æ­£ç¡®æ€§ï¼Œå°±è¦ä¿è¯å®‰å…¨æ€§ã€‚è™½ç„¶åœ¨ C++ ç¨‹åºä¸­å¯ä»¥è¿™æ ·ä½¿ç”¨ C æ¥å£ï¼Œä½†æ˜¯æ›´åŠ å®‰å…¨é«˜æ•ˆçš„ä½¿ç”¨æ–¹å¼æ˜¯ä½¿ç”¨ RAII åŸåˆ™è¿›è¡Œå°è£…ã€‚è¿™ä¸€ç‚¹ç¬”è€…çš„æ–‡ç« [ã€ŠC++ å°è£… C FFI æ¥å£æœ€ä½³å®è·µï¼šä»¥ Hugging Face Tokenizer ä¸ºä¾‹ã€‹](https://charlee44.com/post.html?id=62bdd64be665488fa0c33abc399176e7)æœ‰è¿‡ç›¸å…³çš„è®ºè¿°ã€‚è¿™é‡Œç›´æ¥æ”¾å‡ºå°è£…å¥½çš„ç»“æœï¼š

    // HfTokenizer.h
    #pragma once
    
    #include <tokenizer_result.h>
    
    #include <memory>
    #include <string>
    
    namespace embedding {
    
    namespace hf {
    class Tokenizer {
     public:
      explicit Tokenizer(const std::string& path);
    
      // ç¼–è¯‘å™¨è‡ªåŠ¨ç”Ÿæˆï¼š
      // - ææ„å‡½æ•°ï¼ˆè°ƒç”¨ Deleterï¼‰
      // - ç§»åŠ¨æ„é€  / ç§»åŠ¨èµ‹å€¼
      // - ç¦æ­¢æ‹·è´ï¼ˆå› ä¸º unique_ptr ä¸å¯æ‹·è´ï¼‰
    
      // å…¶ä»–æ¥å£æ–¹æ³•
      uint64_t Count(const std::string& text) const;
    
      // å‘é‡åŒ–
      using ResultPtr =
          std::unique_ptr<TokenizerResult, void (*)(TokenizerResult*)>;
      ResultPtr Encode(const std::string& text) const;
    
     private:
      std::unique_ptr<void, void (*)(void*)> handle;
    };
    
    }  // namespace hf
    
    }  // namespace embedding
    

    // HfTokenizer.cpp
    #include "HfTokenizer.h"
    
    #include <hf_tokenizer_ffi.h>
    
    #include <stdexcept>
    
    #ifdef __cplusplus
    static_assert(std::is_standard_layout_v<TokenizerResult> &&
                      std::is_trivially_copyable_v<TokenizerResult>,
                  "TokenizerResult must be C ABI compatible");
    #endif
    
    namespace embedding {
    
    namespace hf {
    
    static void HandleDeleter(void* handle) noexcept {
      if (handle) {
        tokenizer_destroy(handle);
      }
    }
    
    static void ResultDeleter(TokenizerResult* p) noexcept {
      if (p) {
        tokenizer_result_free(*p);
        delete p;
      }
    }
    
    Tokenizer::Tokenizer(const std::string& path)
        : handle(tokenizer_create(path.c_str()), HandleDeleter) {
      if (!handle) {
        throw std::runtime_error("Failed to create tokenizer from " + path);
      }
    }
    
    uint64_t Tokenizer::Count(const std::string& text) const {
      return tokenizer_count(handle.get(), text.c_str());
    }
    
    Tokenizer::ResultPtr Tokenizer::Encode(const std::string& text) const {
      auto result = std::make_unique<TokenizerResult>(
          tokenizer_encode(handle.get(), text.c_str()));
      return {result.release(), ResultDeleter};
      // TokenizerResult raw = tokenizer_encode(handle.get(), text.c_str());
      // return {new TokenizerResult(std::move(raw)), ResultDeleter};
    };
    
    }  // namespace hf
    
    }  // namespace embedding
    

5\. åµŒå…¥å™¨
=======

åœ¨è§£å†³äº†åˆ†è¯å™¨çš„é—®é¢˜ä¹‹åï¼Œå°±å¯ä»¥å¼€å§‹å®ç°å¥å­åµŒå…¥çš„åŠŸèƒ½äº†ã€‚å¦‚å‰æ–‡æåˆ°çš„ï¼Œè¯¥åŠŸèƒ½å¯ä»¥å¼•å…¥ ONNX Runtime ç»„ä»¶æ¥å®ç°ã€‚ä¸ºäº†æ–¹ä¾¿è¿›ä¸€æ­¥ä½¿ç”¨ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªåŠŸèƒ½å°è£…ä¸ºåµŒå…¥å™¨ï¼š

    // BgeOnnxEmbedder.h
    #pragma once
    
    #include <memory>
    #include <string>
    #include <vector>
    
    namespace embedding {
    
    namespace hf {
    class Tokenizer;
    }
    
    class BgeOnnxEmbedder {
     public:
      explicit BgeOnnxEmbedder(const std::string& modelPath,
                               const hf::Tokenizer& tokenizer);
      ~BgeOnnxEmbedder();
    
      const int64_t& EmbeddingDim() const;
    
      std::vector<float> Embed(const std::string& text) const;
    
     private:
      class Impl;  // å‰å‘å£°æ˜
      std::unique_ptr<Impl> impl;
    };
    
    }  // namespace embedding
    

    // BgeOnnxEmbedder.cpp
    #include "BgeOnnxEmbedder.h"
    
    #include <onnxruntime_cxx_api.h>
    
    #include "HfTokenizer.h"
    #include "Util/StringEncode.h"
    
    namespace embedding {
    
    class BgeOnnxEmbedder::Impl {
     public:
      Ort::Env& GetOrtEnv() {
        static Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "BgeOnnxEmbedder");
        return env;
      }
    
      const int64_t& EmbeddingDim() const { return embeddingDim; }
    
      explicit Impl(const std::string& modelPath, const hf::Tokenizer& tokenizer)
          : session{GetOrtEnv(),
    #ifdef _WIN32
                    util::StringEncode::Utf8StringToWideString(modelPath).c_str(),
    #else
                    modelPath.c_str(),
    #endif
                    Ort::SessionOptions()},
            memInfo{Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU)},
            tokenizer(tokenizer),
            embeddingDim(0) {
    
        //
        const auto& outputInfo = session.GetOutputTypeInfo(0);
        const auto& tensorInfo = outputInfo.GetTensorTypeAndShapeInfo();
        const auto& shape = tensorInfo.GetShape();
    
        // å‡è®¾è¾“å‡ºæ˜¯ [batch, seq, dim] æˆ– [batch, dim]
        // æˆ‘ä»¬å–æœ€åä¸€ä¸ªé -1 çš„ç»´åº¦
        for (auto it = shape.rbegin(); it != shape.rend(); ++it) {
          if (*it != -1) {
            embeddingDim = *it;
            break;
          }
        }
    
        if (embeddingDim == 0) {
          throw std::runtime_error(
              "Failed to infer embedding dimension from ONNX model.");
        }
      }
    
      std::vector<float> Embed(const std::string& text) const {
        hf::Tokenizer::ResultPtr result = tokenizer.Encode(text);
        if (!result) {
          throw std::runtime_error("tokenizer_encode failed");
        }
    
        // å®šä¹‰å¼ é‡ç»´åº¦
        int64_t seqLen = static_cast<int64_t>(result->length);
        std::vector<int64_t> inputShape = {1, seqLen};
        size_t dataByteCount = sizeof(int64_t) * seqLen;
    
        Ort::Value inputIdsTensor = Ort::Value::CreateTensor(
            memInfo.GetConst(), result->input_ids, dataByteCount, inputShape.data(),
            inputShape.size(),
            ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);
    
        Ort::Value attentionMaskTensor = Ort::Value::CreateTensor(
            memInfo.GetConst(), result->attention_mask, dataByteCount,
            inputShape.data(), inputShape.size(),
            ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);
    
        Ort::Value tokenTypeIdsTensor = Ort::Value::CreateTensor(
            memInfo.GetConst(), result->token_type_ids, dataByteCount,
            inputShape.data(), inputShape.size(),
            ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);
    
        // è¾“å…¥åå¿…é¡»ä¸æ¨¡å‹å®šä¹‰ä¸€è‡´
        const char* inputNames[] = {"input_ids", "attention_mask",
                                    "token_type_ids"};
        const char* outputNames[] = {"last_hidden_state"};
    
        // æŠŠä¸‰ä¸ªè¾“å…¥å¼ é‡æ”¾è¿›æ•°ç»„
        std::vector<Ort::Value> inputs;
        inputs.push_back(std::move(inputIdsTensor));
        inputs.push_back(std::move(attentionMaskTensor));
        inputs.push_back(std::move(tokenTypeIdsTensor));
    
        // æ‰§è¡Œæ¨ç†
        auto outputs = session.Run(Ort::RunOptions(),  // è¿è¡Œé€‰é¡¹ï¼ˆé€šå¸¸ nullptrï¼‰
                                   inputNames,         // è¾“å…¥åæ•°ç»„
                                   inputs.data(),  // è¾“å…¥å¼ é‡æ•°ç»„
                                   inputs.size(),  // è¾“å…¥æ•°é‡ï¼ˆ3ï¼‰
                                   outputNames,    // è¾“å‡ºåæ•°ç»„
                                   1               // è¾“å‡ºæ•°é‡ï¼ˆ1ï¼‰
        );
    
        // è·å–è¾“å‡ºä¿¡æ¯
        auto& output_tensor = outputs[0];
        auto output_shape = output_tensor.GetTensorTypeAndShapeInfo().GetShape();
        if (output_shape.size() != 3 || output_shape[0] != 1) {
          throw std::runtime_error("Unexpected output shape");
        }
    
        // è·å–è¾“å‡ºå¼ é‡çš„åŸå§‹ float æŒ‡é’ˆ
        const float* outputData = outputs[0].GetTensorData<float>();
    
        // æå– [CLS] token çš„ embeddingï¼ˆç¬¬0ä¸ªtokenï¼‰
        int64_t hiddenSize = output_shape[2];
        std::vector<float> embedding(outputData, outputData + hiddenSize);
    
        // L2 å½’ä¸€åŒ–ï¼ˆBGE è¦æ±‚ï¼‰
        float norm = 0.0f;
        for (float v : embedding) norm += v * v;
        norm = std::sqrt(norm);
        if (norm > 1e-8) {
          for (float& v : embedding) v /= norm;
        }
    
        return embedding;
      }
    
     private:
      mutable Ort::Session session;
      Ort::MemoryInfo memInfo;
      const hf::Tokenizer& tokenizer;
      int64_t embeddingDim;
    };
    
    BgeOnnxEmbedder::BgeOnnxEmbedder(const std::string& modelPath,
                                     const hf::Tokenizer& tokenizer)
        : impl(std::make_unique<Impl>(modelPath, tokenizer)) {}
    
    BgeOnnxEmbedder::~BgeOnnxEmbedder() = default;  // æ­¤æ—¶ Impl å·²å®šä¹‰ï¼Œå¯å®‰å…¨ææ„
    
    const int64_t& BgeOnnxEmbedder::EmbeddingDim() const {
      return impl->EmbeddingDim();
    }
    
    std::vector<float> BgeOnnxEmbedder::Embed(const std::string& text) const {
      return impl->Embed(text);
    }
    
    }  // namespace embedding
    

åœ¨è°ƒç”¨ ONNX Runtime æ‰§è¡Œæ¨ç†ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»å°†åˆ†è¯å™¨è¾“å‡ºçš„åŸå§‹ token åºåˆ—è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„å¼ é‡ï¼ˆTensorï¼‰æ ¼å¼ã€‚è¿™æ­£æ˜¯ `inputIdsTensor`ã€`attentionMaskTensor` å’Œ `tokenTypeIdsTensor` ä¸‰ä¸ªå¼ é‡çš„ç”±æ¥â€”â€”å®ƒä»¬å…±åŒæ„æˆäº† Transformer æ¨¡å‹çš„æ ‡å‡†è¾“å…¥ä¸‰å…ƒç»„ã€‚

*   **`input_ids`**ï¼ˆå¯¹åº” `inputIdsTensor`ï¼‰ï¼š  
    è¿™æ˜¯æœ€æ ¸å¿ƒçš„è¾“å…¥ï¼Œè¡¨ç¤ºæ–‡æœ¬ç»è¿‡åˆ†è¯åå¾—åˆ°çš„ token ID åºåˆ—ã€‚æ¯ä¸ª ID æ˜¯è¯æ±‡è¡¨ï¼ˆ`vocab.txt`ï¼‰ä¸­çš„ç´¢å¼•ï¼Œæ¨¡å‹é€šè¿‡åµŒå…¥å±‚ï¼ˆEmbedding Layerï¼‰å°†å…¶æ˜ å°„ä¸ºç¨ å¯†å‘é‡ã€‚ä¾‹å¦‚ï¼Œå¥å­ â€œgitæ’¤å›æäº¤â€ ç» Hugging Face åˆ†è¯å™¨å¤„ç†åï¼Œä¼šå˜æˆ `[101, 10807, 3059, 1726, 2990, 769, 102, 0, 0, ...]`ï¼Œå…¶ä¸­ `101` æ˜¯ `[CLS]`ï¼Œ`102` æ˜¯ `[SEP]`ï¼Œå…¶ä½™ä¸ºå®é™…å†…å®¹æˆ–å¡«å……ï¼ˆ`[PAD]`ï¼ŒID=0ï¼‰ã€‚è¿™ä¸ªåºåˆ—è¢«ç»„ç»‡æˆå½¢çŠ¶ä¸º `(1, seq_len)` çš„äºŒç»´å¼ é‡ï¼ˆbatch\_size=1ï¼‰ï¼Œä½œä¸ºæ¨¡å‹çš„ä¸»è¾“å…¥ã€‚
*   **`attention_mask`**ï¼ˆå¯¹åº” `attentionMaskTensor`ï¼‰  
    ç”±äº BGE æ¨¡å‹è¦æ±‚æ‰€æœ‰è¾“å…¥åºåˆ—é•¿åº¦å¯¹é½ï¼ˆæ­¤å¤„å›ºå®šä¸º 512ï¼‰ï¼ŒçŸ­æ–‡æœ¬ä¼šè¢« `[PAD]` å¡«å……ã€‚ä½†æ¨¡å‹ä¸åº”â€œå…³æ³¨â€è¿™äº›æ— æ„ä¹‰çš„å¡«å……ä½ç½®ã€‚`attention_mask` å°±æ˜¯ä¸€ä¸ªäºŒå€¼æ©ç ï¼šçœŸå® token å¯¹åº” `1`ï¼Œå¡«å……éƒ¨åˆ†å¯¹åº” `0`ã€‚Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼šåˆ©ç”¨è¯¥æ©ç ï¼Œåœ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡æ—¶è‡ªåŠ¨å¿½ç•¥ `[PAD]`ï¼Œä»è€Œé¿å…å™ªå£°å¹²æ‰°è¯­ä¹‰è¡¨ç¤ºã€‚
*   **`token_type_ids`**ï¼ˆå¯¹åº” `tokenTypeIdsTensor`ï¼‰  
    è¯¥è¾“å…¥ä¸»è¦ç”¨äºåŒºåˆ†å¥å­å¯¹ä»»åŠ¡ï¼ˆå¦‚é—®ç­”ã€å¥å­ç›¸ä¼¼åº¦åˆ¤æ–­ï¼‰ï¼Œä¾‹å¦‚ `[Aå¥][SEP][Bå¥]` ä¸­ A å’Œ B åˆ†å±ä¸åŒ segmentã€‚ä½†åœ¨å•å¥åµŒå…¥åœºæ™¯ï¼ˆå¦‚æˆ‘ä»¬çš„åšå®¢æœç´¢ï¼‰ä¸­ï¼Œæ•´ä¸ªè¾“å…¥åªåŒ…å«ä¸€ä¸ªå¥å­ï¼Œå› æ­¤æ‰€æœ‰æœ‰æ•ˆ token çš„ `token_type_id` å‡ä¸º `0`ã€‚å°½ç®¡ BGE æ¨¡å‹åœ¨è®­ç»ƒæ—¶å¯èƒ½ä½¿ç”¨äº†è¯¥å­—æ®µï¼Œä½†åœ¨æ¨ç†é˜¶æ®µå³ä½¿ä¼ å…¥å…¨é›¶å‘é‡ï¼Œä¹Ÿä¸ä¼šå½±å“ `[CLS]` è¡¨ç¤ºçš„è´¨é‡ã€‚å‡ºäºæ¥å£å…¼å®¹æ€§è€ƒè™‘ï¼Œæˆ‘ä»¬ä»æŒ‰è§„èŒƒæä¾›æ­¤å¼ é‡ã€‚

è¿™ä¸‰ä¸ªå¼ é‡é€šè¿‡ ONNX Runtime çš„ C++ API è¢«å°è£…ä¸º `Ort::Value` å¯¹è±¡ï¼Œå¹¶ä»¥åç§°åŒ¹é…çš„æ–¹å¼ä¼ å…¥æ¨¡å‹ã€‚ONNX æ¨¡å‹å†…éƒ¨ä¼šæ ¹æ® `config.json` ä¸­å®šä¹‰çš„è¾“å…¥èŠ‚ç‚¹åï¼ˆé€šå¸¸ä¸º `"input_ids"`ã€`"attention_mask"`ã€`"token_type_ids"`ï¼‰æ­£ç¡®ç»‘å®šæ•°æ®ã€‚éšåï¼Œæ¨¡å‹æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œè¾“å‡ºæœ€åä¸€å±‚éšè—çŠ¶æ€ï¼ˆ`last_hidden_state`ï¼‰ï¼Œå…¶å½¢çŠ¶ä¸º `(1, seq_len, hidden_size)`ã€‚æˆ‘ä»¬ä»ä¸­æå–ç¬¬ 0 ä¸ªä½ç½®ï¼ˆå³ `[CLS]` tokenï¼‰çš„å‘é‡ä½œä¸ºæ•´å¥çš„è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶æŒ‰ BGE å®˜æ–¹æ¨èè¿›è¡Œ **L2 å½’ä¸€åŒ–**â€”â€”è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼Œå› ä¸ºåç»­çš„å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ä¾èµ–äºå•ä½å‘é‡ç©ºé—´ä¸­çš„ç‚¹ç§¯ç­‰ä»·æ€§ã€‚

> åµŒå…¥æ¨¡å‹ bge-small-zh-v1.5 æœ€å¤§è¾“å…¥é•¿åº¦ = 512 tokensï¼ŒåµŒå…¥å‘é‡ç»´åº¦ = 512ç»´ã€‚å‚æ•°å€¼éƒ½æ˜¯ 512 ï¼Œä½†æ˜¯ä¸¤è€…å¹¶æ²¡æœ‰ç»å¯¹çš„å…³ç³»ï¼Œè¯·æ³¨æ„åŒºåˆ†ã€‚

è‡³æ­¤ï¼Œä¸€æ®µä»»æ„é•¿åº¦çš„ä¸­æ–‡æ–‡æœ¬å°±è¢«æˆåŠŸè½¬åŒ–ä¸ºä¸€ä¸ª 512 ç»´ï¼ˆ`bge-small-zh-v1.5` çš„è¾“å‡ºç»´åº¦ï¼‰çš„æ ‡å‡†åŒ–è¯­ä¹‰å‘é‡ã€‚çœ¼è§æ‰ä¸ºå®ï¼Œå¾ˆå¤šäººï¼ˆåŒ…æ‹¬ç¬”è€…ï¼‰æƒ³çœ‹çœ‹æŸä¸ªæ–‡æœ¬ï¼ˆæ¯”å¦‚å‰é¢çš„â€œgitæ’¤å›æäº¤â€ï¼‰çš„å‘é‡åŒ–è¾“å‡ºï¼Œé‚£ä¹ˆå¯ä»¥å…ˆæµ‹è¯•ä¸€ä¸‹ï¼š

    #include <string>
    #include <vector>
    
    #include "BgeOnnxEmbedder.h"
    #include "HfTokenizer.h"
    
    #ifdef _WIN32
    #include <Windows.h>
    #endif
    
    using namespace std;
    using namespace embedding;
    
    int main() {
    #ifdef _WIN32
      SetConsoleOutputCP(65001);
    #endif
    
      hf::Tokenizer tokenizer("C:/Github/search/bge-small-zh-onnx/tokenizer.json");
      BgeOnnxEmbedder bgeOnnxEmbedder(
          "C:/Github/search/bge-small-zh-onnx/model.onnx", tokenizer);
    
      string text = "gitæ’¤å›æäº¤";
      std::vector<float> embedding = bgeOnnxEmbedder.Embed(text);
    
      for (auto value : embedding) {
        printf("%lf\t", value);
      }
    
      return 0;
    }
    

è¾“å‡ºç»“æœæ˜¯ï¼š

    -0.031685       -0.035368       0.069596        -0.030640       -0.011818       -0.023779       -0.043855       0.0064050.067668        0.004901        0.025522        -0.269526       -0.024420       0.065645    ...      0.003664        -0.002409      -0.061532        0.002052        -0.056993       -0.057258       -0.073969       -0.020673       0.044549        0.005769-0.009345       -0.004969       0.013322        0.041202
    

å…¶å®ä¹Ÿæ²¡ä»€ä¹ˆç‰¹åˆ«çš„ï¼Œå°±æ˜¯ä¸€ä¸ªé«˜ç»´å‘é‡ï¼›è¿™ä¸ªå‘é‡æ˜¯ä¹Ÿæœºå™¨ç†è§£çš„ï¼Œæ‰€ä»¥ä¹Ÿæ²¡ä»€ä¹ˆå¯é˜…è¯»æ€§ã€‚

å¦å¤–è¡¥å……ä¸€ç‚¹ï¼Œè¿™ä¸ªåµŒå…¥å™¨å®ç°æ˜¯åŸºäº PIMPL æŠ€æœ¯å®ç°çš„ï¼Œç¬”è€…çš„æ–‡ç« [ã€Šä¸ºä»€ä¹ˆç°ä»£ C++ åº“éƒ½ç”¨ PIMPLï¼Ÿä¸€åœºå…³äºå°è£…ã€ä¾èµ–ä¸å®‰å…¨çš„æ¼”è¿›ã€‹](https://charlee44.com/post.html?id=b89125a9380d44bc865557f3d1781c9f)å°±ä»¥è¿™ä¸ªåµŒå…¥å™¨ä¸ºä¾‹è®ºè¿°äº† PIMPL æŠ€æœ¯ã€‚

6\. è¯­ä¹‰åˆ†æ®µå™¨
=========

åœ¨æˆåŠŸå®ç°å¥å­åµŒå…¥åŠŸèƒ½åï¼Œä¼¼ä¹è¯­ä¹‰åŒ–æœç´¢çš„æ ¸å¿ƒéš¾é¢˜å·²ç»è§£å†³ã€‚ç„¶è€Œï¼ŒçœŸæ­£çš„æŒ‘æˆ˜æ‰åˆšåˆšå¼€å§‹ï¼š**åµŒå…¥æ¨¡å‹å¯¹è¾“å…¥é•¿åº¦æ˜¯æœ‰é™åˆ¶çš„**ã€‚ä»¥æˆ‘ä»¬ä½¿ç”¨çš„ `bge-small-zh-v1.5` ä¸ºä¾‹ï¼Œå…¶æœ€å¤§æ”¯æŒçš„ token é•¿åº¦ä¸º 512ã€‚è¿™æ„å‘³ç€ï¼Œè‹¥ç›´æ¥å°†ä¸€ç¯‡é•¿åšå®¢æ–‡ç« ï¼ˆå¯èƒ½åŒ…å«æ•°åƒå­—ï¼‰é€å…¥æ¨¡å‹ï¼Œè¶…å‡ºéƒ¨åˆ†ä¼šè¢«æˆªæ–­ï¼Œå¯¼è‡´å¤§é‡è¯­ä¹‰ä¿¡æ¯ä¸¢å¤±ã€‚

ä¹çœ‹ä¹‹ä¸‹ï¼Œè§£å†³æ–¹æ¡ˆä¼¼ä¹å¾ˆç®€å•ï¼šæŒ‰ 512 token çš„é•¿åº¦å¯¹æ–‡æ¡£è¿›è¡Œæœºæ¢°åˆ‡åˆ†å³å¯ã€‚ä½†è¿™ç§â€œæš´åŠ›åˆ†å—â€ä¼šå¸¦æ¥ä¸¥é‡é—®é¢˜ï¼š

1.  **ç ´åè¯­ä¹‰å®Œæ•´æ€§**ï¼šä¸€ä¸ªå®Œæ•´çš„å¥å­æˆ–æ®µè½å¯èƒ½è¢«ä»ä¸­åˆ‡æ–­ï¼Œå¯¼è‡´ç”Ÿæˆçš„å‘é‡æ— æ³•å‡†ç¡®è¡¨è¾¾åŸå§‹å«ä¹‰ï¼›
2.  **å‰²è£‚ä¸Šä¸‹æ–‡å…³è”**ï¼šç›¸é‚»ä½†è¢«å¼ºè¡Œåˆ†å‰²çš„å—ä¹‹é—´ç¼ºä¹è¯­ä¹‰è¡”æ¥ï¼Œå½±å“åç»­æ£€ç´¢çš„ç›¸å…³æ€§ï¼›
3.  **æµªè´¹åµŒå…¥å®¹é‡**ï¼šçŸ­æ®µè½å•ç‹¬æˆå—ä¼šé€ æˆå¤§é‡å¡«å……ï¼ˆpaddingï¼‰ï¼Œé™ä½å‘é‡å¯†åº¦å’Œæ£€ç´¢æ•ˆç‡ã€‚

å› æ­¤ï¼Œ**å…³é”®ä¸åœ¨äºâ€œå¦‚ä½•åˆ‡â€ï¼Œè€Œåœ¨äºâ€œåœ¨å“ªé‡Œåˆ‡â€**â€”â€”æˆ‘ä»¬éœ€è¦çš„æ˜¯**å…·æœ‰è¯­ä¹‰è¾¹ç•Œçš„åˆ†å—**ï¼Œè€Œéä»»æ„ä½ç½®çš„æˆªæ–­ã€‚

å¯¹äº Markdown æ ¼å¼çš„åšå®¢æ–‡ç« è€Œè¨€ï¼Œè¿™ä¸€ç›®æ ‡å¤©ç„¶å…·å¤‡å®ç°åŸºç¡€ï¼š**æ ‡é¢˜ï¼ˆå¦‚ `#`ã€`##` ç­‰ï¼‰æœ¬èº«å°±æ˜¯æ¸…æ™°çš„è¯­ä¹‰è¾¹ç•Œ**ã€‚æ¯ä¸ªæ ‡é¢˜ä¸‹çš„å†…å®¹é€šå¸¸æ„æˆä¸€ä¸ªé€»è¾‘è‡ªæ´½çš„ä¸»é¢˜å•å…ƒï¼ˆä¾‹å¦‚â€œå®‰è£…æ­¥éª¤â€ã€â€œåŸç†åˆ†æâ€ã€â€œå¸¸è§é—®é¢˜â€ï¼‰ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€ä¸ª **Markdown è¯­ä¹‰åˆ†æ®µå™¨ï¼ˆ`MdSemanticSplitter`ï¼‰**ï¼Œå…¶æ ¸å¿ƒä»»åŠ¡ä¸æ˜¯ç”Ÿæˆæœ€ç»ˆç”¨äºåµŒå…¥çš„â€œå—â€ï¼ˆchunkï¼‰ï¼Œè€Œæ˜¯å…ˆå°†æ•´ç¯‡æ–‡æ¡£**æŒ‰è¯­ä¹‰ç»“æ„æ‹†åˆ†ä¸ºè‹¥å¹²â€œè¯­ä¹‰æ®µâ€ï¼ˆsemantic sectionsï¼‰**ã€‚

> ğŸ“Œ æ³¨æ„ï¼š**è¯­ä¹‰åˆ†æ®µ â‰  æœ€ç»ˆåˆ†å—**ã€‚  
> åˆ†æ®µæ˜¯é¢„å¤„ç†é˜¶æ®µï¼Œå…³æ³¨çš„æ˜¯**å†…å®¹ç»“æ„ä¸é€»è¾‘è¾¹ç•Œ**ï¼›è€Œåˆ†å—ï¼ˆchunkingï¼‰æ˜¯åç»­æ­¥éª¤ï¼Œå…³æ³¨çš„æ˜¯**å¦‚ä½•å°†è¯­ä¹‰æ®µé€‚é…åˆ°æ¨¡å‹é•¿åº¦é™åˆ¶å†…**ï¼ˆä¾‹å¦‚åˆå¹¶çŸ­æ®µã€æ»‘åŠ¨çª—å£é‡å ç­‰ï¼‰ã€‚

è¿™ç§ä¸¤é˜¶æ®µç­–ç•¥å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ï¼šæ— è®ºæ˜¯ Markdownã€Word è¿˜æ˜¯ HTML æ–‡æ¡£ï¼Œåªè¦èƒ½æå–å‡ºè¯­ä¹‰å±‚çº§ç»“æ„ï¼ˆå¦‚ç« èŠ‚ã€æ®µè½ã€åˆ—è¡¨ã€ä»£ç å—ç­‰ï¼‰ï¼Œå°±å¯ä»¥å…ˆé€šè¿‡å¯¹åº”çš„ **è¯­ä¹‰åˆ†æ®µå™¨** å¾—åˆ°ç»“æ„åŒ–ç‰‡æ®µï¼Œå†äº¤ç”±é€šç”¨çš„ **æ–‡æœ¬åˆ†å—å™¨ï¼ˆ`TextChunker`ï¼‰** è¿›è¡Œé•¿åº¦é€‚é…ä¸å‘é‡åŒ–å‡†å¤‡ã€‚

å› æ­¤ï¼Œåœ¨æˆ‘ä»¬çš„ç³»ç»Ÿä¸­ï¼Œè¯­ä¹‰åˆ†æ®µå™¨`MdSemanticSplitter` çš„èŒè´£éå¸¸æ˜ç¡®ï¼š**è§£æ Markdownï¼Œè¯†åˆ«æ ‡é¢˜å±‚çº§ï¼ŒæŒ‰è¯­ä¹‰è¾¹ç•Œåˆ‡åˆ†å‡ºå®Œæ•´çš„æ®µè½å•å…ƒ**ï¼Œä¸ºåç»­é«˜æ•ˆã€å‡†ç¡®çš„åµŒå…¥æ‰“ä¸‹åšå®åŸºç¡€ã€‚å…·ä½“çš„å®ç°å¦‚ä¸‹ï¼š

    // SemanticBlock.h
    #pragma once
    
    #include <string>
    #include <vector>
    
    // è¯­ä¹‰å—
    struct SemanticBlock {
      std::string context;                //ä¸Šä¸‹æ–‡ï¼Œé€šå¸¸æ˜¯æ ‡ç­¾
      std::vector<std::string> contents;  //æ–‡æœ¬å†…å®¹
    };
    

    // MdSemanticSplitter.h
    #pragma once
    
    #include "SemanticBlock.h"
    
    namespace embedding {
    
    class MdSemanticSplitter {
     public:
      std::vector<SemanticBlock> Split(const std::string& title,
                                       const std::string& content,
                                       const std::string& summary) const;
    };
    }  // namespace embedding
    

    // MdSemanticSplitter.cpp
    #include "MdSemanticSplitter.h"
    
    #include <md4c.h>
    
    #include <iostream>
    #include <list>
    
    using namespace std;
    
    namespace embedding {
    
    enum class BlockKind { Heading, Paragraph, ListItem, BlockQuote };
    
    struct Block {
      BlockKind kind;
      int level = 0;  // heading level
      std::string text;
    };
    
    struct ParseContext {
      std::vector<Block> result;
      std::vector<Block> blockStack;  // Blockæ ˆ
      // int activeDepth = 0;            //æ·±åº¦è®¡æ•°
    };
    
    static int EnterBlock(MD_BLOCKTYPE type, void* detail, void* userdata) {
      auto* ctx = static_cast<ParseContext*>(userdata);
    
      Block block{};
      switch (type) {
        case MD_BLOCK_H: {
          auto* h = static_cast<MD_BLOCK_H_DETAIL*>(detail);
          block.kind = BlockKind::Heading;
          block.level = h->level;
          break;
        }
        case MD_BLOCK_P: {
          block.kind = BlockKind::Paragraph;
          break;
        }
        case MD_BLOCK_LI: {
          block.kind = BlockKind::ListItem;
          break;
        }
        case MD_BLOCK_QUOTE: {
          block.kind = BlockKind::BlockQuote;
          break;
        }
        default:
          // å…¶ä»– blockï¼ˆå¦‚ UL / OL / CODEï¼‰ä¸å¤„ç†
          break;
      }
    
      ctx->blockStack.emplace_back(std::move(block));
      return 0;
    }
    
    static int TextCallback(MD_TEXTTYPE type, const MD_CHAR* text, MD_SIZE size,
                            void* userdata) {
      auto* ctx = static_cast<ParseContext*>(userdata);
      if (ctx->blockStack.empty()) return 0;
    
      // åªæ¥æ”¶çœŸå®å¯è¯»æ–‡æœ¬
      if (type == MD_TEXT_NORMAL || type == MD_TEXT_CODE) {
        ctx->blockStack.back().text.append(text, size);
      }
    
      return 0;
    }
    
    static int LeaveBlock(MD_BLOCKTYPE type, void*, void* userdata) {
      auto* ctx = static_cast<ParseContext*>(userdata);
      if (ctx->blockStack.empty()) {
        return 0;
      }
    
      Block block = std::move(ctx->blockStack.back());
      ctx->blockStack.pop_back();
    
      bool emit = false;
      switch (type) {
        case MD_BLOCK_H:
        case MD_BLOCK_P:
        case MD_BLOCK_LI:
        case MD_BLOCK_QUOTE:
          emit = true;
          break;
        default:
          break;
      }
    
      if (emit && !block.text.empty()) {
        ctx->result.emplace_back(std::move(block));
      }
    
      return 0;
    }
    
    static int EnterSpan(MD_SPANTYPE /*type*/, void* /*detail*/,
                         void* /*userdata*/) {
      return 0;
    }
    
    static int LeaveSpan(MD_SPANTYPE /*type*/, void* /*detail*/,
                         void* /*userdata*/) {
      return 0;
    }
    
    std::vector<SemanticBlock> MdSemanticSplitter::Split(
        const std::string& title, const std::string& content,
        const std::string& summary) const {
      ParseContext ctx{};
    
      MD_PARSER parser{};
      parser.abi_version = 0;
      parser.enter_block = EnterBlock;
      parser.leave_block = LeaveBlock;
      parser.enter_span = EnterSpan;
      parser.leave_span = LeaveSpan;
      parser.text = TextCallback;
      parser.flags = MD_FLAG_COLLAPSEWHITESPACE;  //åˆå¹¶ç©ºæ ¼
    
      md_parse(content.data(), (MD_SIZE)content.size(), &parser, &ctx);
    
      // std::cout << "[Parsed blocks] " << ctx.result.size() << "\n\n";
      // for (const auto& b : ctx.result) {
      //  switch (b.kind) {
      //    case BlockKind::Heading:
      //      std::cout << "[H" << b.level << "] ";
      //      break;
      //    case BlockKind::Paragraph:
      //      std::cout << "[P] ";
      //      break;
      //    case BlockKind::ListItem:
      //      std::cout << "[LI] ";
      //      break;
      //    case BlockKind::BlockQuote:
      //      std::cout << "[Q] ";
      //      break;
      //  }
      //  std::cout << b.text << "\n";
      //}
    
      vector<SemanticBlock> semanticBlocks;
      {
        SemanticBlock semanticBlock;
        semanticBlock.context = title;
        semanticBlock.contents = {summary};
        semanticBlocks.emplace_back(std::move(semanticBlock));
      }
    
      std::list<std::pair<int, std::string>> currentTitleChain;  // å­˜ (çº§åˆ«, æ–‡æœ¬)
      std::vector<std::string> currentBlockContents;
    
      for (auto& b : ctx.result) {
        if (b.kind == BlockKind::Heading) {
          //é‡åˆ°æ–°æ ‡é¢˜ï¼Œå­˜å…¥æ•°æ®
          if (!currentBlockContents.empty()) {
            string context;
            for (auto it = currentTitleChain.begin(); it != currentTitleChain.end();
                 ++it) {
              context += it->second;  // åªå–æ ‡é¢˜æ–‡æœ¬
              if (std::next(it) != currentTitleChain.end()) context += "ï½œ";
            }
            SemanticBlock semanticBlock;
            semanticBlock.context = std::move(context);
            semanticBlock.contents = std::move(currentBlockContents);
            semanticBlocks.emplace_back(std::move(semanticBlock));
            currentBlockContents = {};
          }
    
          // å¼¹å‡ºæ‰€æœ‰çº§åˆ« >= å½“å‰æ ‡é¢˜çº§åˆ«çš„ç¥–å…ˆï¼ˆç²¾å‡†å›æº¯ï¼‰
          while (!currentTitleChain.empty() &&
                 currentTitleChain.back().first >= b.level) {
            currentTitleChain.pop_back();
          }
          currentTitleChain.emplace_back(b.level,
                                         std::move(b.text));  // å­˜çº§åˆ«+æ–‡æœ¬
        } else {
          currentBlockContents.emplace_back(std::move(b.text));
        }
      }
      if (!currentBlockContents.empty()) {
        string context;
        for (auto it = currentTitleChain.begin(); it != currentTitleChain.end();
             ++it) {
          context += it->second;  // åªå–æ ‡é¢˜æ–‡æœ¬
          if (std::next(it) != currentTitleChain.end()) context += "ï½œ";
        }
    
        SemanticBlock semanticBlock;
        semanticBlock.context = std::move(context);
        semanticBlock.contents = std::move(currentBlockContents);
        semanticBlocks.emplace_back(std::move(semanticBlock));
        currentBlockContents = {};
      }
    
      return semanticBlocks;
    }
    
    }  // namespace embedding
    

è¿™æ®µ `MdSemanticSplitter` çš„å®ç°åŸºäº **MD4C** â€”â€” ä¸€ä¸ªè½»é‡ã€ç¬¦åˆ CommonMark æ ‡å‡†çš„ C è¯­è¨€ Markdown è§£æå™¨ã€‚å®ƒä¸ä¾èµ–æ­£åˆ™è¡¨è¾¾å¼æˆ–å­—ç¬¦ä¸²æš´åŠ›åŒ¹é…ï¼Œè€Œæ˜¯é€šè¿‡äº‹ä»¶é©±åŠ¨çš„æ–¹å¼ï¼Œåœ¨è§£æè¿‡ç¨‹ä¸­é€å—ï¼ˆblockï¼‰è¯†åˆ« Markdown ç»“æ„ã€‚

å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ³¨å†Œäº†ä¸‰ä¸ªæ ¸å¿ƒå›è°ƒå‡½æ•°ï¼š

*   `EnterBlock`ï¼šå½“è¿›å…¥ä¸€ä¸ªæ–°å—ï¼ˆå¦‚æ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨é¡¹ç­‰ï¼‰æ—¶è§¦å‘ï¼Œè®°å½•å…¶ç±»å‹å’Œå±‚çº§ï¼›
*   `TextCallback`ï¼šæ”¶é›†å—å†…çš„åŸå§‹æ–‡æœ¬å†…å®¹ï¼ˆåŒ…æ‹¬æ™®é€šæ–‡æœ¬å’Œè¡Œå†…ä»£ç ï¼‰ï¼›
*   `LeaveBlock`ï¼šå½“ç¦»å¼€ä¸€ä¸ªå—æ—¶ï¼Œå°†å·²æ”¶é›†çš„æ–‡æœ¬å°è£…ä¸ºç»“æ„åŒ– `Block` å¹¶æš‚å­˜ã€‚

é€šè¿‡ç»´æŠ¤ä¸€ä¸ª **æ ‡é¢˜æ ˆï¼ˆ`currentTitleChain`ï¼‰**ï¼Œæˆ‘ä»¬èƒ½åŠ¨æ€è¿½è¸ªå½“å‰å†…å®¹æ‰€å¤„çš„ç« èŠ‚è·¯å¾„ã€‚æ¯å½“é‡åˆ°æ–°çš„æ ‡é¢˜ï¼ˆ`#`, `##` ç­‰ï¼‰ï¼Œå°±å°†æ­¤å‰ç´¯ç§¯çš„æ‰€æœ‰æ®µè½ã€åˆ—è¡¨é¡¹ç­‰å†…å®¹æ‰“åŒ…ä¸ºä¸€ä¸ª `SemanticBlock`ï¼Œå…¶ä¸Šä¸‹æ–‡ï¼ˆ`context`ï¼‰ç”±ä»æ ¹åˆ°å½“å‰æ ‡é¢˜çš„å®Œæ•´è·¯å¾„æ„æˆï¼ˆä¾‹å¦‚ `"éƒ¨ç½²ï½œDocker é…ç½®ï½œç¯å¢ƒå˜é‡"`ï¼‰ã€‚è¿™æ ·ï¼Œæ¯ä¸ªè¯­ä¹‰å—ä¸ä»…åŒ…å«åŸå§‹æ–‡æœ¬ï¼Œè¿˜æºå¸¦äº†æ˜ç¡®çš„ç»“æ„åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæå¤§æå‡äº†åç»­åµŒå…¥çš„è¯­ä¹‰å‡†ç¡®æ€§ã€‚

> è¡¥å……ä¸€ä¸ªç»†èŠ‚ï¼Œå¯¹äºæ¯ä¸€ç¯‡åšæ–‡ï¼Œç¬¬ä¸€ä¸ªè¯­ä¹‰åˆ†æ®µçš„ä¸Šä¸‹æ–‡æ˜¯æ–‡ç« æ ‡é¢˜ï¼Œå†…å®¹æ˜¯æ–‡ç« æ‘˜è¦ã€‚è¿™æ ·åšå¯ä»¥ä¿è¯æ–‡æ¡£çš„è¯­ä¹‰é”šç‚¹ï¼Œç¡®ä¿å³ä½¿åç»­å†…å®¹è¢«æ‹†åˆ†ï¼Œæ£€ç´¢ç³»ç»Ÿä»èƒ½å‡†ç¡®ç†è§£è¯¥æ–‡æ¡£çš„æ ¸å¿ƒä¸»é¢˜ã€‚

æœ€ç»ˆè¾“å‡ºçš„ `std::vector<SemanticBlock>` å³ä¸ºæ–‡æ¡£çš„â€œè¯­ä¹‰éª¨æ¶â€â€”â€”å®ƒä¿ç•™äº† Markdown çš„é€»è¾‘å±‚æ¬¡ï¼ŒåŒæ—¶ä¸ºä¸‹ä¸€æ­¥çš„ **æ–‡æœ¬åˆ†å—ï¼ˆchunkingï¼‰** æä¾›äº†é«˜è´¨é‡ã€ç»“æ„æ„ŸçŸ¥çš„è¾“å…¥å•å…ƒã€‚

7\. æ–‡æœ¬åˆ†å—å™¨
=========

åœ¨è¯­ä¹‰åˆ†æ®µå™¨å°† Markdown æ–‡æ¡£è§£æä¸ºç»“æ„åŒ–çš„ **è¯­ä¹‰æ®µ**ï¼ˆ`SemanticBlock`ï¼‰ä¹‹åï¼Œæˆ‘ä»¬è·ç¦»çœŸæ­£å¯åµŒå…¥çš„è¾“å…¥å•å…ƒä»å·®ä¸€æ­¥ï¼š**å°†è¿™äº›è¯­ä¹‰æ®µé€‚é…åˆ°åµŒå…¥æ¨¡å‹çš„é•¿åº¦é™åˆ¶å†…**ã€‚

åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ `bge-small-zh-v1.5`ï¼‰å¯¹è¾“å…¥é•¿åº¦æœ‰ç¡¬æ€§ä¸Šé™ï¼ˆé€šå¸¸ä¸º 512 tokensï¼‰ã€‚ç„¶è€Œï¼Œç°å®ä¸­çš„åšå®¢å†…å®¹åƒå·®ä¸‡åˆ«â€”â€”æœ‰çš„ç« èŠ‚å¯¥å¯¥æ•°è¯­ï¼Œæœ‰çš„æ®µè½æ´‹æ´‹æ´’æ´’ã€‚è‹¥ç›´æ¥å¯¹æ¯ä¸ªè¯­ä¹‰æ®µç‹¬ç«‹åµŒå…¥ï¼Œä¼šé¢ä¸´ä¸¤ä¸ªæç«¯é—®é¢˜ï¼š

1.  **è¿‡é•¿æ®µè½è¢«æˆªæ–­**ï¼šè¶…å‡º 512 tokens çš„å†…å®¹ä¼šè¢«æ— æƒ…ä¸¢å¼ƒï¼Œå¯¼è‡´å…³é”®ä¿¡æ¯ä¸¢å¤±ï¼›
2.  **è¿‡çŸ­æ®µè½æµªè´¹å®¹é‡**ï¼šä¸€ä¸ªä»…å«ä¸¤å¥è¯çš„æ®µè½å æ®æ•´ä¸ª 512-token è¾“å…¥æ§½ä½ï¼Œå¤§é‡ä½ç½®è¢« `[PAD]` å¡«å……ï¼Œä¸ä»…é™ä½å‘é‡å¯†åº¦ï¼Œè¿˜å¯èƒ½ç¨€é‡Šè¯­ä¹‰ä¿¡å·ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ™ºèƒ½çš„ **æ–‡æœ¬åˆ†å—å™¨**ï¼ˆ`TextChunker`ï¼‰ï¼Œå®ƒä¸ä»…è¦å¤„ç†é•¿åº¦çº¦æŸï¼Œæ›´è¦**åœ¨ä¿ç•™è¯­ä¹‰è¿è´¯æ€§çš„å‰æä¸‹ï¼ŒåŠ¨æ€åœ°æ‹†åˆ†æˆ–åˆå¹¶å†…å®¹**ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒçš„æ ¸å¿ƒèŒè´£åŒ…æ‹¬ï¼š

*   **æŒ‰ token æ•°ç²¾ç¡®åˆ‡åˆ†é•¿æ®µ**ï¼šå½“å•ä¸ªè¯­ä¹‰æ®µè¶…è¿‡æœ€å¤§é•¿åº¦æ—¶ï¼Œä»¥å¥å­ä¸ºå•ä½è¿›è¡Œæ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼Œé¿å…åœ¨è¯æˆ–å­—ä¸­é—´æš´åŠ›æˆªæ–­ï¼›
*   **åˆå¹¶ç›¸é‚»çŸ­æ®µ**ï¼šå°†å¤šä¸ªè¯­ä¹‰ç›¸å…³ä½†å„è‡ªè¿‡çŸ­çš„æ®µè½ç»„åˆæˆä¸€ä¸ªç´§å‡‘å—ï¼Œæå‡åµŒå…¥æ•ˆç‡ï¼›
*   **æ³¨å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼šåœ¨åˆ†å—æ—¶ä¿ç•™å…¶æ‰€å±çš„æ ‡é¢˜è·¯å¾„ï¼ˆå¦‚ `"æ•°æ®åº“ï½œè¿æ¥æ± é…ç½®ï½œHikariCP å‚æ•°è¯¦è§£"`ï¼‰ï¼Œä½œä¸ºå‰ç¼€æ‹¼æ¥åˆ°æ–‡æœ¬å¼€å¤´ï¼Œä½¿åµŒå…¥å‘é‡â€œçŸ¥é“â€è‡ªå·±æ¥è‡ªå“ªä¸€éƒ¨åˆ†å†…å®¹ã€‚

è¿™ç§ç­–ç•¥ç¡®ä¿äº†æ¯ä¸ªæœ€ç»ˆç”¨äºåµŒå…¥çš„ **æ–‡æœ¬å—**ï¼ˆchunkï¼‰æ—¢æ»¡è¶³æ¨¡å‹è¾“å…¥çº¦æŸï¼Œåˆå°½å¯èƒ½å®Œæ•´ã€è‡ªæ´½åœ°è¡¨è¾¾ä¸€ä¸ªå±€éƒ¨è¯­ä¹‰å•å…ƒã€‚

> ğŸ’¡ è¿™æ­£æ˜¯ **RAG**ï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿä¸­â€œæ£€ç´¢â€ç¯èŠ‚çš„å…³é”®å‰ç½®æ­¥éª¤ã€‚  
> åœ¨å…¸å‹çš„ RAG æ¶æ„ä¸­ï¼Œå¤–éƒ¨çŸ¥è¯†åº“é¦–å…ˆè¢«åˆ‡åˆ†ä¸ºé«˜è´¨é‡ chunksï¼Œå†ç»åµŒå…¥æ¨¡å‹è½¬åŒ–ä¸ºå‘é‡å¹¶å­˜å…¥ç´¢å¼•ã€‚å½“ç”¨æˆ·æé—®æ—¶ï¼Œç³»ç»Ÿé€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦å¬å›æœ€ç›¸å…³çš„è‹¥å¹² chunksï¼Œå°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆã€‚  
> å› æ­¤ï¼Œ**åˆ†å—è´¨é‡ç›´æ¥å†³å®šäº†æ£€ç´¢ç²¾åº¦ï¼Œè¿›è€Œå½±å“æœ€ç»ˆå›ç­”çš„å‡†ç¡®æ€§ä¸ç›¸å…³æ€§**ã€‚

åœ¨æˆ‘ä»¬çš„è½»é‡çº§åšå®¢æœç´¢ç³»ç»Ÿä¸­ï¼Œè™½ç„¶æ²¡æœ‰åç»­çš„å¤§æ¨¡å‹ç”Ÿæˆç¯èŠ‚ï¼Œä½†â€œæ£€ç´¢å³æœåŠ¡â€çš„æ€æƒ³ä¾ç„¶é€‚ç”¨â€”â€”ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢éœ€è¦ç²¾å‡†åŒ¹é…åˆ°æœ€ç›¸å…³çš„æ–‡ç« ç‰‡æ®µã€‚è€Œè¿™ä¸€åˆ‡ï¼Œéƒ½å§‹äºä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬åˆ†å—å™¨ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åŸºäº Hugging Face Tokenizer æä¾›çš„ token è®¡æ•°èƒ½åŠ›ï¼Œå®ç°ä¸€ä¸ªå…¼é¡¾æ•ˆç‡ä¸è¯­ä¹‰çš„ `TextChunker`ï¼Œä¸ºæ„å»ºå®Œæ•´çš„å‘é‡ç´¢å¼•é“ºå¹³é“è·¯ï¼š

    // TextChunker.h
    #pragma once
    
    #include <vector>
    
    #include "HfTokenizer.h"
    #include "SemanticBlock.h"
    
    namespace embedding {
    
    class TextChunker {
     public:
      explicit TextChunker(size_t maxTokens, const hf::Tokenizer& tokenizer);
    
      // æ”¯æŒå¤šä¸ªè¯­ä¹‰å—
      std::vector<std::string> SemanticBlocks(
          std::vector<SemanticBlock> semanticBlockList) const;
    
      // æ”¯æŒå¤šä¸ªå—å…ƒç´ 
      std::vector<std::string> Blocks(std::vector<std::string> blockList) const;
    
      // æ”¯æŒæ®µè½æˆ–é•¿æ–‡æœ¬
      std::vector<std::string> Paragraph(std::string text) const;
    
     private:
      // æŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ‡åˆ†ä¸ºå¥å­ï¼ˆä¿ç•™æ ‡ç‚¹ï¼‰
      std::vector<std::string> SplitIntoSentences(
          const std::string& paragraph) const;
    
      // å°†å¥å­åˆ—è¡¨åˆå¹¶ä¸ºæ»¡è¶³ token é™åˆ¶çš„ chunks
      std::vector<std::string> MergeSentencesToChunks(
          std::vector<std::string>&& sentences) const;
    
      size_t maxTokens;
      const hf::Tokenizer& tokenizer;
    };
    
    }  // namespace embedding
    

    // TextChunker.cpp
    #include "TextChunker.h"
    
    #include <iostream>
    #include <unordered_set>
    
    using namespace std;
    
    namespace embedding {
    
    TextChunker::TextChunker(size_t maxTokens, const hf::Tokenizer& tokenizer)
        : maxTokens(maxTokens), tokenizer(tokenizer) {}
    
    std::vector<std::string> TextChunker::SemanticBlocks(
        std::vector<SemanticBlock> semanticBlockList) const {
      vector<std::string> totalChunkList;
    
      for (auto& semanticBlock : semanticBlockList) {
        string contextText = semanticBlock.context + "\n";
        vector<std::string> chunkList = Blocks(std::move(semanticBlock.contents));
    
        // Tokenè®¡æ•°ä¸ç”¨å¤ªä¸¥æ ¼ï¼ŒmaxTokensè‚¯å®šå°äºnlpæ¨¡å‹å¤„ç†æ•°ï¼Œä¸”ä¸Šä¸‹æ–‡éå¸¸å°
        for (auto& chunk : chunkList) {
          chunk = contextText + std::move(chunk);
        }
    
        totalChunkList.insert(totalChunkList.end(),
                              std::make_move_iterator(chunkList.begin()),
                              std::make_move_iterator(chunkList.end()));
      }
    
      return totalChunkList;
    }
    
    std::vector<std::string> TextChunker::Blocks(
        std::vector<std::string> blockList) const {
      // æŒ‰ç…§æ–‡æ¡£ä¸­çš„å—å…ƒç´ è¿›è¡Œåˆå¹¶
      vector<std::string> chunkList;
      vector<size_t> chunkTokenCount;
      string currentChunk;  //å½“å‰å—
      size_t totalTokenCount = 0;
      for (size_t i = 0; i < blockList.size(); ++i) {
        string currentText(std::move(blockList[i]));
        currentText += '\n';
        size_t currentTokenCount = tokenizer.Count(currentText.c_str());
        if (totalTokenCount + currentTokenCount > maxTokens) {
          if (!currentChunk.empty()) {
            chunkList.emplace_back(std::move(currentChunk));
            chunkTokenCount.emplace_back(totalTokenCount);
          }
          currentChunk = std::move(currentText);
          totalTokenCount = currentTokenCount;
        } else {
          currentChunk += std::move(currentText);
          totalTokenCount += currentTokenCount;
        }
      }
      if (!currentChunk.empty()) {
        chunkList.emplace_back(std::move(currentChunk));
        chunkTokenCount.emplace_back(totalTokenCount);
      }
    
      // æœ‰çš„å—å…ƒç´ æœ¬èº«å°±è¶…è¿‡maxTokensï¼Œç»§ç»­æ‹†åˆ†
      vector<std::string> resultChunkList;
      for (size_t ci = 0; ci < chunkList.size(); ++ci) {
        if (chunkTokenCount[ci] > maxTokens) {
          vector<std::string> subChunks = Paragraph(std::move(chunkList[ci]));
          for (size_t si = 0; si < subChunks.size(); ++si) {
            string chunk = std::move(subChunks[si]);
            if (si + 1 != subChunks.size()) {
              chunk += '\n';
            }
            resultChunkList.emplace_back(chunk);
          }
        } else {
          resultChunkList.emplace_back(std::move(chunkList[ci]));
        }
      }
      return resultChunkList;
    }
    
    // Todo:å¯¹äºRAGè¦å¤„ç†
    //ã€Œå¥å­å•å¥ > maxTokensã€çš„ fallback
    //å¢åŠ  token-based overlap
    std::vector<std::string> TextChunker::Paragraph(std::string text) const {
      return MergeSentencesToChunks(SplitIntoSentences(text));
    }
    
    // æŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ‡åˆ†ä¸ºå¥å­ï¼ˆä¿ç•™æ ‡ç‚¹ï¼‰
    std::vector<std::string> TextChunker::SplitIntoSentences(
        const std::string& paragraph) const {
      if (paragraph.empty()) {
        return {};
      }
    
      // å®šä¹‰ä¸­æ–‡å¥æœ«æ ‡ç‚¹é›†åˆï¼ˆUTF-8 å­—ç¬¦ä¸²ï¼‰
      static const std::unordered_set<std::string> delimiters = {"ã€‚", "ï¼", "ï¼Ÿ",
                                                                 "ï¼›", "â€¦â€¦"};
    
      std::vector<std::string> chunks;
      size_t i = 0;
      size_t start = 0;
      const char* data = paragraph.data();
      size_t len = paragraph.size();
    
      while (i < len) {
        // åˆ¤æ–­å½“å‰ä½ç½®æ˜¯å¦æ˜¯æŸä¸ªåˆ†éš”ç¬¦çš„èµ·å§‹
        bool found = false;
        for (const auto& delim : delimiters) {
          if (paragraph.compare(i, delim.size(), delim) == 0) {
            // æ‰¾åˆ°åˆ†éš”ç¬¦ï¼šæå– [start, i + delim.size())
            std::string chunk = paragraph.substr(start, i - start + delim.size());
            if (!chunk.empty()) {
              chunks.push_back(chunk);
            }
            // è·³è¿‡åˆ†éš”ç¬¦
            i += delim.size();
            start = i;
            found = true;
            break;
          }
        }
    
        if (!found) {
          // æ™®é€šå­—ç¬¦ï¼šå‘å‰ç§»åŠ¨ä¸€ä¸ª UTF-8 å­—ç¬¦
          unsigned char c = static_cast<unsigned char>(data[i]);
          if ((c & 0x80) == 0) {
            i += 1;
          } else if ((c & 0xE0) == 0xC0) {
            i += 2;
          } else if ((c & 0xF0) == 0xE0) {
            i += 3;
          } else if ((c & 0xF8) == 0xF0) {
            i += 4;
          } else {
            i += 1;  // invalid UTF-8 fallback
          }
        }
      }
    
      // å¤„ç†æœ«å°¾æ²¡æœ‰æ ‡ç‚¹çš„å‰©ä½™éƒ¨åˆ†
      if (start < len) {
        std::string last = paragraph.substr(start);
        if (!last.empty()) {
          chunks.push_back(last);
        }
      }
    
      return chunks;
    }
    
    std::vector<std::string> TextChunker::MergeSentencesToChunks(
        std::vector<std::string>&& sentences) const {
      vector<string> chunkList;
      string currentChunk;  //å½“å‰å—
      size_t totalTokenCount = 0;
      for (size_t i = 0; i < sentences.size(); ++i) {
        // cout << mdBlockList[i].level << '\t' << mdBlockList[i].text << '\n';
        string currentText(std::move(sentences[i]));
        size_t currentTokenCount = tokenizer.Count(currentText.c_str());
        if (totalTokenCount + currentTokenCount > maxTokens) {
          chunkList.emplace_back(std::move(currentChunk));
          currentChunk = std::move(currentText);
          totalTokenCount = currentTokenCount;
        } else {
          currentChunk += std::move(currentText);
          totalTokenCount += currentTokenCount;
        }
      }
      if (!currentChunk.empty()) {
        chunkList.emplace_back(std::move(currentChunk));
      }
    
      return chunkList;
    }
    
    }  // namespace embedding
    

ä¸Šè¿° `TextChunker` çš„è®¾è®¡éµå¾ªä¸€ä¸ªæœ´ç´ ä½†æœ‰æ•ˆçš„åŸåˆ™ï¼š**ä¼˜å…ˆå°Šé‡æ–‡æ¡£åŸæœ‰çš„ç»“æ„å•å…ƒï¼Œå†åœ¨å¿…è¦æ—¶è¿›è¡Œè¯­ä¹‰æ„ŸçŸ¥çš„åˆ‡åˆ†**ã€‚

å®ƒçš„å…¥å£æ–¹æ³• `SemanticBlocks()` æ¥æ”¶ç”± `MdSemanticSplitter` äº§å‡ºçš„è¯­ä¹‰æ®µåˆ—è¡¨ã€‚æ¯ä¸ªè¯­ä¹‰æ®µéƒ½æºå¸¦äº†å®Œæ•´çš„ä¸Šä¸‹æ–‡è·¯å¾„ï¼ˆå¦‚ `"éƒ¨ç½²ï½œå®¹å™¨åŒ–ï½œDockerfile ç¼–å†™"`ï¼‰ï¼Œæˆ‘ä»¬å°†å…¶ä½œä¸ºå‰ç¼€é™„åŠ åˆ°è¯¥æ®µæ‰€æœ‰å†…å®¹å—çš„å¼€å¤´â€”â€”è¿™æ ·ï¼Œå³ä½¿æŸæ®µå†…å®¹è¢«å•ç‹¬æ£€ç´¢åˆ°ï¼Œå…¶å‘é‡ä¹Ÿèƒ½â€œè®°ä½â€è‡ªå·±æ‰€å±çš„ä¸»é¢˜å±‚çº§ï¼Œæå¤§å¢å¼ºæœç´¢çš„ç›¸å…³æ€§ã€‚

å†…éƒ¨å¤„ç†åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼š

1.  **å—çº§åˆå¹¶ï¼ˆ`Blocks`ï¼‰**  
    é¦–å…ˆå°è¯•å°†åŒä¸€è¯­ä¹‰æ®µå†…çš„å¤šä¸ªå°å—ï¼ˆå¦‚è‹¥å¹²æ®µè½ã€åˆ—è¡¨é¡¹ï¼‰æŒ‰é¡ºåºæ‹¼æ¥ï¼Œç›´åˆ°å³å°†è¶…å‡º `maxTokens` é™åˆ¶ä¸ºæ­¢ã€‚è¿™ç§ç­–ç•¥èƒ½æœ‰æ•ˆé¿å…çŸ­å†…å®¹ç¢ç‰‡åŒ–ï¼ŒåŒæ—¶ä¿æŒåŸå§‹ Markdown å—çš„å®Œæ•´æ€§ã€‚è‹¥æŸä¸ªå—æœ¬èº«å·²è¶…é•¿ï¼ˆä¾‹å¦‚ä¸€æ®µæœªæ¢è¡Œçš„ä»£ç è¯´æ˜æˆ–é•¿å¼•ç”¨ï¼‰ï¼Œåˆ™äº¤ç”±ä¸‹ä¸€å±‚å¤„ç†ã€‚
2.  **å¥å­çº§åˆ‡åˆ†ï¼ˆ`Paragraph` â†’ `SplitIntoSentences` + `MergeSentencesToChunks`ï¼‰**  
    å¯¹äºè¶…é•¿æ®µè½ï¼Œæˆ‘ä»¬ä¸å†æš´åŠ›æˆªæ–­ï¼Œè€Œæ˜¯å…ˆæŒ‰ä¸­æ–‡å¥æœ«æ ‡ç‚¹ï¼ˆ`ã€‚ï¼ï¼Ÿï¼›â€¦â€¦`ï¼‰å°†å…¶æ‹†åˆ†ä¸ºå¥å­åºåˆ—ã€‚éšåï¼Œä»¥è´ªå¿ƒæ–¹å¼å°†è¿™äº›å¥å­é‡æ–°ç»„åˆæˆä¸è¶…è¿‡ `maxTokens` çš„æ–‡æœ¬å—ã€‚è™½ç„¶å½“å‰å®ç°å°šæœªå¼•å…¥æ»‘åŠ¨çª—å£é‡å ï¼ˆoverlapï¼‰ï¼Œä½†åœ¨ç»å¤§å¤šæ•°åšå®¢åœºæ™¯ä¸­ï¼Œè¿™ç§â€œå¥è¾¹ç•Œå¯¹é½â€çš„åˆ‡åˆ†å·²èƒ½å¾ˆå¥½åœ°ä¿ç•™å±€éƒ¨è¯­ä¹‰è¿è´¯æ€§ã€‚

æ•´ä¸ªè¿‡ç¨‹å®Œå…¨åŸºäº **çœŸå® token è®¡æ•°**ï¼ˆé€šè¿‡ Hugging Face Tokenizer çš„ `Count` æ¥å£ï¼‰ï¼Œè€Œéç²—ç•¥çš„å­—ç¬¦æˆ–å­—æ•°ä¼°ç®—ï¼Œç¡®ä¿åˆ†å—ç»“æœä¸¥æ ¼æ»¡è¶³æ¨¡å‹è¾“å…¥çº¦æŸã€‚

è‡³æ­¤ï¼Œä»åŸå§‹ Markdown åˆ°å¯åµŒå…¥æ–‡æœ¬å—çš„é¢„å¤„ç†æµæ°´çº¿å·²å…¨çº¿è´¯é€šã€‚

8\. å‘é‡åº“
=======

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»èƒ½å°†åšå®¢ä¸­çš„æ¯ä¸€æ®µæ–‡æœ¬è½¬åŒ–ä¸ºä¸€ä¸ª 512 ç»´çš„è¯­ä¹‰å‘é‡ã€‚ä½†è‹¥æ²¡æœ‰ä¸€ç§é«˜æ•ˆçš„æ–¹å¼ç»„ç»‡å’Œæ£€ç´¢è¿™äº›å‘é‡ï¼Œè¯­ä¹‰æœç´¢å°±æ— ä»è°ˆèµ·ã€‚

è¿™æ­£æ˜¯å‘é‡åº“ï¼ˆVector Database / Vector Indexï¼‰çš„ç”¨æ­¦ä¹‹åœ°ã€‚å®ƒä¸å…³å¿ƒæ–‡æœ¬å†…å®¹ï¼Œåªä¸“æ³¨äºä¸€ä»¶äº‹ï¼šç»™å®šä¸€ä¸ªæŸ¥è¯¢å‘é‡ï¼Œåœ¨ç™¾ä¸‡çº§å‘é‡æ± ä¸­å¿«é€Ÿæ‰¾å‡ºæœ€ç›¸ä¼¼çš„è‹¥å¹²ä¸ªã€‚

åœ¨èµ„æºæåº¦å—é™çš„ä¸ªäººæœåŠ¡å™¨ä¸Šï¼Œæˆ‘ä»¬æ— æ³•éƒ¨ç½²é‡é‡çº§çš„å‘é‡æ•°æ®åº“ï¼ˆå¦‚ Milvus æˆ– Weaviateï¼‰ã€‚ç»è¿‡æƒè¡¡ï¼Œæœ€ç»ˆé€‰æ‹©äº† FAISS â€”â€” ä¸€ä¸ªç”± Meta å¼€æºçš„è½»é‡ã€é«˜æ€§èƒ½ã€çº¯ C++ å®ç°çš„å‘é‡ç›¸ä¼¼æ€§æœç´¢åº“ã€‚å®ƒæ”¯æŒå¤šç§ç´¢å¼•ç±»å‹ï¼ˆå¦‚ IndexFlatIPã€IndexIVFFlatã€HNSW ç­‰ï¼‰ï¼Œæ—¢èƒ½æ»¡è¶³ç²¾ç¡®æœç´¢éœ€æ±‚ï¼Œä¹Ÿèƒ½é€šè¿‡è¿‘ä¼¼ç®—æ³•å¤§å¹…å‹ç¼©å†…å­˜ä¸è®¡ç®—å¼€é”€ã€‚

é‚£ä¹ˆæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±æ‰¹é‡æŠŠ Markdown æ ¼å¼çš„åšå®¢æ–‡æ¡£å‘é‡åŒ–ï¼Œå­˜å…¥åˆ° FAISS åˆ›å»ºçš„å‘é‡åº“ä¸­ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªåŠŸèƒ½æ˜¯æ•´ä¸ªè¯­ä¹‰æœç´¢ç³»ç»Ÿçš„â€œç¦»çº¿æ„å»ºâ€æ ¸å¿ƒ: åœ¨éƒ¨ç½²å‰ä¸€æ¬¡æ€§å°†æ‰€æœ‰åšå®¢æ–‡ç« å¤„ç†ä¸ºå‘é‡ï¼Œå¹¶æŒä¹…åŒ–ä¸º FAISS ç´¢å¼•æ–‡ä»¶ï¼Œä¾›çº¿ä¸Šæœç´¢æœåŠ¡åŠ è½½ä½¿ç”¨ã€‚å…·ä½“ä»£ç å¦‚ä¸‹ï¼š

    // TaskEmbed.cpp
    #include "TaskEmbed.h"
    
    #include <faiss/IndexFlat.h>
    #include <faiss/IndexIDMap.h>
    #include <faiss/index_io.h>
    #include <gflags/gflags.h>
    #include <onnxruntime_cxx_api.h>
    
    #include <filesystem>
    #include <fstream>
    #include <iostream>
    
    #include "Application/Context.h"
    #include "Embedding/BgeOnnxEmbedder.h"
    #include "Embedding/MdSemanticSplitter.h"
    #include "Embedding/TextChunker.h"
    #include "LaunchConfig.h"
    #include "LaunchInit.h"
    #include "Persistence/Sqlite3Crud.h"
    #include "Persistence/TableBlogChunksField.h"
    #include "Persistence/TableBlogsField.h"
    
    using namespace std;
    using namespace embedding;
    using namespace persistence;
    
    DECLARE_string(input);
    
    //æ‰€æœ‰åšæ–‡å‘é‡åŒ–å¹¶å­˜å…¥å‘é‡åº“
    namespace Bootstrap::Task::Embed {
    
    void Run() {
      if (FLAGS_input.empty() /*|| FLAGS_output.empty()*/) {
        std::cerr << "é”™è¯¯: sitemap ä»»åŠ¡éœ€è¦è¾“å…¥ --input å’Œ --output å‚æ•°\n";
        return;
      }
    
      filesystem::path launchConfigPath{FLAGS_input};
      if (!filesystem::exists(launchConfigPath)) {
        std::cerr << "HTTPæœåŠ¡é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼" << endl;
        return;
      }
    
      if (!LaunchInit(launchConfigPath)) {
        return;
      }
    
      application::Context::GetInstance();
    
      string onnxModelPath = launchConfig.onnxModelPath;
      string faissIndexPath = launchConfig.faissIndexPath;
    
      std::filesystem::path modelPath(onnxModelPath);
      if (!std::filesystem::exists(onnxModelPath)) {
        std::cerr << "Error: ONNX model file does not exist at: "
                  << modelPath.string() << std::endl;
        return;
      }
    
      // è·å–æ¨¡å‹æ‰€åœ¨ç›®å½•ï¼Œå¹¶æ‹¼æ¥ tokenizer.json
      std::filesystem::path tokenizerJsonFile =
          modelPath.parent_path() / "tokenizer.json";
    
      // æ£€æŸ¥ tokenizer.json æ˜¯å¦å­˜åœ¨ï¼ˆæ¨èï¼‰
      if (!std::filesystem::exists(tokenizerJsonFile)) {
        std::cerr << "Warning: tokenizer.json not found at: "
                  << tokenizerJsonFile.generic_string() << std::endl;
        return;
      }
      u8string tokenizerJsonFilePath = tokenizerJsonFile.generic_u8string();
    
      const size_t chunkMaxTokens = 396;
    
      vector<Sqlite3Crud::TableValue> result;
      std::vector<TableBlogsField> fieldNames{
          TableBlogsField::id,
          TableBlogsField::title,
          TableBlogsField::summary,
          TableBlogsField::content,
      };
    
      if (!Sqlite3Crud::Query<TableBlogsField>(TableName::blogs, fieldNames,
                                               result)) {
        return;
      }
    
      // å…¨é‡é‡å»ºå‰ï¼Œæ¸…ç©º chunk è¡¨
      if (!Sqlite3Crud::Delete<TableBlogChunksField>(TableName::blog_chunks)) {
        std::cerr << "æ¸…ç©º blog_chunks è¡¨å¤±è´¥ï¼\n";
        return;
      }
      std::cout << "å·²æ¸…ç©º blog_chunks è¡¨\n";
    
      MdSemanticSplitter mdSplitter;  //æ–‡æ¡£æ‹†åˆ†å™¨
      hf::Tokenizer tokenizer(
          (const char *)tokenizerJsonFilePath.c_str());    //åˆå§‹åŒ–åˆ†è¯å™¨
      TextChunker textChunker(chunkMaxTokens, tokenizer);  // tokenæ„ŸçŸ¥æ–‡æœ¬æ‹†åˆ†
      BgeOnnxEmbedder embedder(onnxModelPath, tokenizer);
    
      int64_t embeddingDim = embedder.EmbeddingDim();
      auto flatIndex =
          std::make_unique<faiss::IndexFlatIP>(embeddingDim);  // åˆ›å»º FlatIP ç´¢å¼•
      faiss::IndexIDMap2 index(
          flatIndex.release());  // è‡ªåŠ¨æ¥ç®¡ flatIndex çš„ç”Ÿå‘½å‘¨æœŸ
    
      size_t colNum = fieldNames.size();
      size_t rowNum = result.size() / colNum;
    
      for (size_t ri = 0; ri < rowNum; ++ri) {
        size_t m = ri * colNum;
        string blogId = std::move(std::get<std::string>(result[m]));
        string title = std::move(std::get<std::string>(result[m + 1]));
        string summary = std::move(std::get<std::string>(result[m + 2]));
        string content = std::move(std::get<std::string>(result[m + 3]));
    
        // if (title != "C++ä¸­JSONåºåˆ—åŒ–å’Œååºåˆ—åŒ–çš„å®ç°") {
        //  continue;
        //}
    
        cout << ri << '\t' << title << endl;
    
        std::vector<SemanticBlock> semanticBlocks =
            mdSplitter.Split(title, content, summary);
    
        // for (const auto& block : semanticBlocks) {
        //  cout << block.context << endl;
        //  for (const auto& content : block.contents) {
        //    cout << content << endl;
        //  }
        //  cout << "----------------------\n";
        //}
    
        std::vector<std::string> chunks =
            textChunker.SemanticBlocks(std::move(semanticBlocks));
    
        // cout << chunks.size() << endl;
        // for (const auto& chunk : chunks) {
        //  cout << chunk;
        //  cout << "-----------------------------\n";
        //}
    
        // åµŒå…¥æ¯ä¸ª chunkï¼Œå¹¶åˆ†é…å”¯ä¸€ ID
        for (const auto &chunk : chunks) {
          int64_t chunkId = Sqlite3Crud::InsertAndReturnId<TableBlogChunksField>(
              TableName::blog_chunks,
              {TableBlogChunksField::blog_id, TableBlogChunksField::title,
               TableBlogChunksField::chunk_text},
              {blogId, title, chunk});
    
          if (chunkId <= 0) {
            std::cerr << "æ’å…¥ blog_chunks å¤±è´¥\n";
            continue;
          }
    
          std::vector<float> embedding = embedder.Embed(chunk);
          index.add_with_ids(1, embedding.data(), &chunkId);
        }
      }
    
      // ä¿å­˜ç´¢å¼•
      faiss::write_index(&index, faissIndexPath.c_str());
    }
    }  // namespace Bootstrap::Task::Embed
    

å¯ä»¥æŠŠä»¥ä¸Šæµç¨‹æ¦‚æ‹¬ä¸ºä¸‰æ­¥ï¼š

1.  **æ•°æ®è¯»å–**  
    ä» SQLite æ•°æ®åº“ä¸­å…¨é‡æ‹‰å–åšå®¢çš„ `id`ã€`title`ã€`summary` å’Œ `content`ï¼Œä½œä¸ºåŸå§‹è¾“å…¥ã€‚
    
2.  **æ–‡æœ¬ â†’ å‘é‡**  
    å¯¹æ¯ç¯‡æ–‡ç« ä¾æ¬¡æ‰§è¡Œï¼š
    
    *   ç”¨ `MdSemanticSplitter` æŒ‰æ ‡é¢˜å±‚çº§æ‹†åˆ†ä¸ºè¯­ä¹‰æ®µï¼›
    *   ç”¨ `TextChunker` å°†è¯­ä¹‰æ®µåˆ‡åˆ†ä¸ºä¸è¶…è¿‡ 396 tokens çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–‡æœ¬å—ï¼ˆé¢„ç•™ç©ºé—´ç»™ `[CLS]`/`[SEP]` ç­‰ç‰¹æ®Š tokenï¼‰ï¼›
    *   è°ƒç”¨ `BgeOnnxEmbedder` ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ 512 ç»´ L2 å½’ä¸€åŒ–çš„ embeddingã€‚
3.  **å‘é‡ â†’ ç´¢å¼•**  
    ä½¿ç”¨ FAISS çš„ `IndexFlatIP`ï¼ˆç²¾ç¡®å†…ç§¯ç´¢å¼•ï¼Œç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰æ„å»ºå‘é‡åº“ï¼Œå¹¶é€šè¿‡ `IndexIDMap2` å°†æ¯ä¸ªå‘é‡å…³è”åˆ°å…¶åœ¨æ•°æ®åº“ä¸­çš„å”¯ä¸€ `chunk_id`ã€‚æœ€ç»ˆï¼Œç´¢å¼•è¿åŒ ID æ˜ å°„ä¸€èµ·å†™å…¥ç£ç›˜ï¼ˆå¦‚ `faiss.index`ï¼‰ã€‚
    

æ­¤å¤–ï¼Œæ¯ä¸ª chunk çš„å…ƒä¿¡æ¯ï¼ˆæ‰€å±æ–‡ç«  IDã€æ ‡é¢˜ã€åŸå§‹æ–‡æœ¬ï¼‰è¢«å­˜å…¥ `blog_chunks` è¡¨ï¼Œå®ç°**å‘é‡ ID åˆ°åŸæ–‡å†…å®¹çš„åæŸ¥**â€”â€”è¿™æ˜¯æœç´¢ç»“æœå±•ç¤ºçš„å…³é”®æ¡¥æ¢ã€‚

è‡³æ­¤ï¼Œä¸€ä¸ªè½»é‡ã€è‡ªåŒ…å«ã€æ— å¤–éƒ¨ä¾èµ–çš„è¯­ä¹‰å‘é‡åº“å°±æ„å»ºå®Œæˆäº†ã€‚

9\. ä¸šåŠ¡åº”ç”¨
========

æœ€åï¼Œå°±æ˜¯è¿ç”¨è¿™ä¸ªå‘é‡åº“å»å®ç°å…·ä½“çš„ä¸šåŠ¡åº”ç”¨äº†ã€‚

9.1 åˆå§‹åŒ–
-------

å½“å¯åŠ¨åç«¯æ—¶ï¼Œéœ€è¦åˆå§‹åŒ–ç›¸å…³çš„å†…å®¹ï¼š

    // EmbedComponent.h
    #pragma once
    #include "Embedding/BgeOnnxEmbedder.h"
    #include "Embedding/MdSemanticSplitter.h"
    #include "Embedding/NotDeletedIdSelector.h"
    #include "Embedding/TextChunker.h"
    
    namespace faiss {
    struct Index;
    }
    
    namespace application {
    struct EmbedComponent {
      explicit EmbedComponent(const std::string& tokenizerJsonFilePath,
                              size_t chunkMaxTokens,
                              const std::string& onnxModelPath,
                              const std::string& faissIndexPath);
      ~EmbedComponent();
    
      embedding::MdSemanticSplitter mdSplitter;    // æ–‡æ¡£æ‹†åˆ†å™¨
      embedding::hf::Tokenizer tokenizer;          // Hfåˆ†è¯å™¨
      embedding::TextChunker textChunker;          // tokenæ„ŸçŸ¥æ–‡æœ¬æ‹†åˆ†
      embedding::BgeOnnxEmbedder embedder;         // æ–‡æœ¬åµŒå…¥
      std::unique_ptr<faiss::Index> index;         // å‘é‡åº“
      embedding::NotDeletedIdSelector idSelector;  // è®°å½•åˆ é™¤çš„ID
    };
    }  // namespace application
    

    // EmbedComponent.cpp
    #include "EmbedComponent.h"
    
    #include <faiss/Index.h>
    #include <faiss/index_io.h>
    
    namespace application {
    EmbedComponent::EmbedComponent(const std::string& tokenizerJsonFilePath,
                                   size_t chunkMaxTokens,
                                   const std::string& onnxModelPath,
                                   const std::string& faissIndexPath)
        : tokenizer((const char*)tokenizerJsonFilePath.c_str()),
          textChunker(chunkMaxTokens, tokenizer),
          embedder(onnxModelPath, tokenizer),
          index(faiss::read_index(faissIndexPath.c_str())) {}
    
    EmbedComponent::~EmbedComponent() = default;
    
    }  // namespace application
    

9.2 æœç´¢
------

å½“ç”¨æˆ·è¿›è¡Œæœç´¢æ—¶ï¼š

    #include "Search.h"
    
    #include <faiss/Index.h>
    #include <faiss/index_io.h>
    
    #include <iostream>
    #include <set>
    
    #include "Application/Context.h"
    #include "Application/EmbedComponent.h"
    #include "Domain/SearchHit.h"
    #include "Embedding/BgeOnnxEmbedder.h"
    #include "Persistence/Sqlite3Crud.h"
    #include "Persistence/TableBlogChunksField.h"
    
    using namespace std;
    using namespace embedding;
    using namespace persistence;
    using namespace domain;
    using namespace application;
    
    namespace Service {
    
    namespace search {
    
    bool Posts(const std::string& query,
               std::vector<domain::SearchHit>& searchHits) {
      EmbedComponent* embedComponent = Context::GetInstance().GetEmbedComponent();
    
      const BgeOnnxEmbedder& embedder = embedComponent->embedder;
      const faiss::Index* index = embedComponent->index.get();
    
      std::vector<float> queryVec = embedder.Embed(query);
    
      constexpr int k = 200;  // è¿”å› top200
      std::vector<faiss::idx_t> indices(k);
      std::vector<float> distances(k);
    
      faiss::SearchParameters params;
      params.sel = &(embedComponent->idSelector);
      index->search(1, queryVec.data(), k, distances.data(), indices.data(),
                    &params);
    
      // è¾“å‡ºç»“æœ
      set<string> blogIdSet;
      for (int i = 0; i < k; ++i) {
        if (indices[i] < 0) {
          continue;
        }
    
        if (distances[i] < 0.5) {
          break;
        }
        // std::cout << "Chunk ID: " << indices[i] << "\tScore: " << distances[i]
        //          << "\n";
    
        int64_t chunkId = indices[i];
        std::vector<Sqlite3Crud::TableValue> result;
        Sqlite3Crud::Query<TableBlogChunksField>(
            TableName::blog_chunks,
            {TableBlogChunksField::blog_id, TableBlogChunksField::title,
             TableBlogChunksField::chunk_text},
            result, {TableBlogChunksField::chunk_id}, {chunkId});
    
        string blogId = std::move(std::get<string>(result.at(0)));
        if (blogIdSet.find(blogId) == blogIdSet.end()) {
          blogIdSet.insert(blogId);
          SearchHit searchHit;
          searchHit.id = std::move(blogId);
          searchHit.title = std::move(std::get<string>(result.at(1)));
          searchHit.snippetText = std::move(std::get<string>(result.at(2)));
          searchHits.emplace_back(std::move(searchHit));
        }
      }
    
      return true;
    }
    
    }  // namespace search
    }  // namespace Service
    

æœ‰äº†é¢„æ„å»ºçš„å‘é‡åº“å’Œé…å¥—çš„å…ƒæ•°æ®è¡¨ï¼ŒçœŸæ­£çš„è¯­ä¹‰æœç´¢å°±å˜å¾—å¼‚å¸¸ç®€æ´ï¼š**å°†ç”¨æˆ·æŸ¥è¯¢ä¹ŸåµŒå…¥ä¸ºå‘é‡ï¼Œç„¶ååœ¨ FAISS ç´¢å¼•ä¸­æ‰§è¡Œè¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰æŸ¥æ‰¾å³å¯**ã€‚

æ•´ä¸ªåœ¨çº¿æœç´¢æµç¨‹åˆ†ä¸ºä¸‰æ­¥ï¼š

1.  **æŸ¥è¯¢åµŒå…¥**  
    ç”¨æˆ·è¾“å…¥çš„æœç´¢è¯ï¼ˆå¦‚â€œgitæ’¤å›æäº¤â€ï¼‰é€šè¿‡ä¸æ„å»ºé˜¶æ®µå®Œå…¨ç›¸åŒçš„ `BgeOnnxEmbedder` æµç¨‹â€”â€”åŒ…æ‹¬ Hugging Face åˆ†è¯ã€ONNX æ¨ç†ã€L2 å½’ä¸€åŒ–â€”â€”è½¬åŒ–ä¸ºä¸€ä¸ª 512 ç»´çš„å•ä½å‘é‡ã€‚è¿™ç¡®ä¿äº†æŸ¥è¯¢å‘é‡ä¸æ–‡æ¡£å‘é‡å¤„äºåŒä¸€è¯­ä¹‰ç©ºé—´ã€‚
2.  **å‘é‡æ£€ç´¢**  
    å°†æŸ¥è¯¢å‘é‡é€å…¥ FAISS ç´¢å¼•ï¼Œæ‰§è¡Œ `index.search()`ã€‚æˆ‘ä»¬ä½¿ç”¨å†…ç§¯ï¼ˆIPï¼‰ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡ï¼ˆå› å‘é‡å·² L2 å½’ä¸€åŒ–ï¼ŒIP ç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼Œå¹¶è¯·æ±‚ Top-Kï¼ˆä¾‹å¦‚ K=200ï¼‰æœ€ç›¸ä¼¼çš„ chunk ID åŠå…¶å¾—åˆ†ã€‚
3.  **ç»“æœç»„è£…ä¸å»é‡**  
    æ ¹æ®è¿”å›çš„ chunk IDï¼Œä» `blog_chunks` è¡¨ä¸­æŸ¥å‡ºå¯¹åº”çš„åŸå§‹æ–‡æœ¬ã€æ‰€å±æ–‡ç« æ ‡é¢˜å’Œåšå®¢ IDã€‚ä¸ºé¿å…åŒä¸€ç¯‡æ–‡ç« å› å¤šä¸ª chunk è¢«é‡å¤å¬å›ï¼Œæˆ‘ä»¬æŒ‰ `blog_id` å»é‡ï¼Œä¼˜å…ˆä¿ç•™å¾—åˆ†æœ€é«˜çš„ç‰‡æ®µï¼Œå¹¶å°†å…¶ä½œä¸ºæœç´¢ç»“æœçš„æ‘˜è¦ï¼ˆsnippetï¼‰å‘ˆç°ç»™ç”¨æˆ·ã€‚

9.3 æ–°å¢
------

æ‰¹é‡å‘é‡åŒ–æ‰€æœ‰çš„åšæ–‡æˆæœ¬å¤ªé«˜ï¼Œæœ€å¥½æ˜¯èƒ½å¤Ÿå®ç°å¢é‡æ›´æ–°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

    // ä¸ºåšå®¢ç”Ÿæˆå¹¶æ·»åŠ å‘é‡åµŒå…¥ï¼ˆç”¨äºè¯­ä¹‰æœç´¢ï¼‰
    static void AddBlogEmbedding(const string& blogId, const string& title,
                                 const string& content, const string& summary) {
      //æ–‡æ¡£æ‹†åˆ†
      const EmbedComponent* embedComponent =
          Context::GetInstance().GetEmbedComponent();
      std::vector<std::string> chunks = embedComponent->textChunker.SemanticBlocks(
          std::move(embedComponent->mdSplitter.Split(title, content, summary)));
    
      // åµŒå…¥æ¯ä¸ª chunkï¼Œå¹¶åˆ†é…å”¯ä¸€ ID
      for (const auto& chunk : chunks) {
        int64_t chunkId = Sqlite3Crud::InsertAndReturnId<TableBlogChunksField>(
            TableName::blog_chunks,
            {TableBlogChunksField::blog_id, TableBlogChunksField::title,
             TableBlogChunksField::chunk_text},
            {blogId, title, chunk});
    
        if (chunkId <= 0) {
          std::cerr << "æ’å…¥ blog_chunks å¤±è´¥\n";
          continue;
        }
    
        std::vector<float> embedding = embedComponent->embedder.Embed(chunk);
        embedComponent->index->add_with_ids(1, embedding.data(), &chunkId);
      }
    
      // åˆ›å»ºæ–°çº¿ç¨‹æ¥å†™å…¥ï¼Œæå‡å“åº”æ—¶é—´
      // Todoï¼šåæœŸè€ƒè™‘ç§»æ¤åˆ°å®šæ—¶ä»»åŠ¡ä¸­ï¼Œå¹¶ä¸”éœ€è¦ä¿è¯æ¨å‡ºåæ­£å¸¸å†™å‡º
      std::thread writeIndexThread([embedComponent]() {
        faiss::write_index(embedComponent->index.get(),
                           launchConfig.faissIndexPath.c_str());
      });
    
      // ç¡®ä¿ä¸»çº¿ç¨‹ä¸ç­‰å¾…æ­¤çº¿ç¨‹å®Œæˆï¼Œé¿å…é˜»å¡
      writeIndexThread.detach();
    }
    

è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆä½¿ç”¨ `IndexIDMap2` çš„åŸå› ï¼šå¯ä»¥å°†æ–°å¢çš„å‘é‡å…³è”åˆ°å…¶åœ¨æ•°æ®åº“ä¸­å…ƒæ•°æ®çš„å”¯ä¸€ `chunk_id` ä¸Šã€‚ç”±äºåœ¨åˆå§‹åŒ–æ—¶å·²ç»å°†å‘é‡ç´¢å¼•è¯»å–åˆ°å†…å­˜ä¸­ï¼Œåœ¨æ–°å¢å‘é‡ååªéœ€è¦å†å°†å‘é‡ç´¢å¼•å¯¹è±¡ä¿å­˜å³å¯ã€‚

9.4 åˆ é™¤
------

åˆ é™¤çš„å®ç°æœ‰ç‚¹éº»çƒ¦ï¼Œå› ä¸º FAISS çš„ `IndexFlatIP` æ²¡æœ‰æä¾›åˆ é™¤çš„æ¥å£ã€‚å½“ç„¶è¿™ä¹Ÿä¸æ˜¯è®¾è®¡ç¼ºé™·ï¼Œè€Œæ˜¯æ•°æ®ç»“æ„å°±æ˜¯è¿™ä¹ˆè®¾è®¡çš„ï¼Œä¸ºäº†ä¿è¯æ£€ç´¢çš„æ€§èƒ½å¿…ç„¶æœ‰æ‰€èˆå¼ƒã€‚å› æ­¤ï¼Œè¦å®ç°åˆ é™¤åªèƒ½è½¯åˆ é™¤ï¼Œä¹Ÿå°±æ˜¯åœ¨å…ƒæ•°æ®ä¸­æ ‡è®°ä¸€ä¸‹ç›¸åº”çš„ chunk å·²ç»è¢«åˆ é™¤äº†ã€‚åŒæ—¶ï¼Œè¦æ³¨æ„åˆ°å‰é¢åœ¨æœç´¢çš„å®ç°ä¸­ï¼Œ`index.search()` ä¼ å…¥ä¸€ä¸ªè‡ªå®šä¹‰çš„ `IDSelector`å‚æ•°å¯¹è±¡ï¼š

    // NotDeletedIdSelector.h
    #pragma once
    
    #include <faiss/impl/IDSelector.h>
    
    #include <unordered_set>
    
    namespace embedding {
    
    /// @brief å®šä¹‰ä¸€ä¸ª selectorï¼šåªä¿ç•™â€œæœªè¢«åˆ é™¤â€çš„ ID
    class NotDeletedIdSelector : public faiss::IDSelector {
     public:
      // æ„é€ æ—¶ä¼ å…¥â€œå·²åˆ é™¤çš„ ID é›†åˆâ€
      // explicit NotDeletedIDSelector();
    
      // é‡å†™ is_memberï¼šå¦‚æœ id åœ¨ deletedSet ä¸­ï¼Œè¿”å› falseï¼ˆå±è”½ï¼‰
      bool is_member(faiss::idx_t id) const override;
    
      void Add(faiss::idx_t id);
    
     private:
      std::unordered_set<faiss::idx_t> deletedSet;
    };
    
    }  // namespace embedding
    

    // NotDeletedIdSelector.cpp
    #include "NotDeletedIdSelector.h"
    
    namespace embedding {
    
    // é‡å†™ is_memberï¼šå¦‚æœ id åœ¨ deletedSet ä¸­ï¼Œè¿”å› falseï¼ˆå±è”½ï¼‰
    bool NotDeletedIdSelector::is_member(faiss::idx_t id) const {
      return deletedSet.find(id) == deletedSet.end();
    }
    
    void NotDeletedIdSelector::Add(faiss::idx_t id) { deletedSet.insert(id); }
    
    }  // namespace embedding
    

è¿™ä¸ª `NotDeletedIdSelector` å¯¹è±¡ä¼ å…¥ `index.search()` æ—¶ä¼šåŠ¨æ€è¿‡æ»¤æ‰å·²è¢«æ ‡è®°åˆ é™¤çš„ chunk IDï¼Œå®ç°â€œé€»è¾‘åˆ é™¤â€è€Œæ— éœ€ç‰©ç†é‡å»ºç´¢å¼•ã€‚æ­£å› ä¸ºå¦‚æ­¤ï¼Œåœ¨åˆ é™¤æ“ä½œæ—¶ï¼Œéœ€è¦è®°å½•æ ‡è®°åˆ é™¤çš„ chunk IDï¼š

    // ä»å‘é‡ç´¢å¼•ä¸­ç§»é™¤åšå®¢çš„åµŒå…¥ï¼Œå¹¶æ ‡è®° chunk ä¸ºå·²åˆ é™¤
    static void RemoveBlogEmbedding(const std::string& blogId) {
      std::vector<Sqlite3Crud::TableValue> result;
      Sqlite3Crud::Query<TableBlogChunksField>(
          TableName::blog_chunks, {TableBlogChunksField::chunk_id}, result,
          {TableBlogChunksField::blog_id}, {blogId});
    
      EmbedComponent* embedComponent = Context::GetInstance().GetEmbedComponent();
      for (const auto& value : result) {
        embedComponent->idSelector.Add(std::get<int64_t>(value));
      }
    
      Sqlite3Crud::Update<TableBlogChunksField>(
          TableName::blog_chunks,
          {TableBlogChunksField::is_deleted, TableBlogChunksField::blog_id},
          {1LL, blogId});
    }
    

å½“ç„¶ï¼Œåœ¨åˆå§‹åŒ–æ—¶ä¹Ÿè¦è®°å½•è¿™äº›è¢«æ ‡è®°åˆ é™¤çš„ chunk IDï¼š

    std::vector<Sqlite3Crud::TableValue> result;
    Sqlite3Crud::Query<TableBlogChunksField>(
        TableName::blog_chunks, {TableBlogChunksField::chunk_id}, result,
        {TableBlogChunksField::is_deleted}, {1});
    
    for (const auto &value : result) {
      embedComponent->idSelector.Add(std::get<int64_t>(value));
    }
    

9.5 æ›´æ–°
------

å¦‚æœè¦æ›´æ–°åšæ–‡åµŒå…¥çš„ç›¸å…³å†…å®¹ï¼Œç”±äºåµŒå…¥æ“ä½œçš„å¤æ‚æ€§ï¼Œç›´æ¥åˆ é™¤åæ–°å¢å³å¯ï¼Œç›´æ¥å¤ç”¨å‰é¢çš„è½¯åˆ é™¤å’Œå¢é‡æ›´æ–°ï¼š

    RemoveBlogEmbedding(blogId);
    AddBlogEmbedding(blogId, blogMeta.title, blogData.content, blogMeta.summary);
    

10\. ä¼˜åŒ–æå‡
=========

è‡³æ­¤ï¼Œä¸€ä¸ªä»é›¶æ„å»ºã€ç«¯åˆ°ç«¯çš„è½»é‡çº§è¯­ä¹‰æœç´¢ç³»ç»Ÿä¾¿å®Œæ•´è½åœ°ã€‚æ•´ä¸ªæœç´¢è¿‡ç¨‹ä¸ä¾èµ–ä»»ä½•å¤–éƒ¨æœåŠ¡ï¼Œä»…éœ€åŠ è½½ä¸€æ¬¡ FAISS ç´¢å¼•å’Œ ONNX æ¨¡å‹ï¼Œå³å¯åœ¨æ¯«ç§’çº§å†…å“åº”æŸ¥è¯¢ã€‚åœ¨ 1GB å†…å­˜çš„äº‘æœåŠ¡å™¨ä¸Šï¼Œå†…å­˜å ç”¨ç¨³å®šåœ¨ 200MB ä»¥å†…ï¼ˆåç«¯ç¨‹åº+åµŒå…¥æ¨¡å‹+å‘é‡åº“ï¼‰ï¼Œå®Œç¾å¥‘åˆâ€œæè‡´æ€§èƒ½ + æè‡´èµ„æºæ•ˆç‡â€çš„åˆå§‹ç›®æ ‡ã€‚åœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢ç¬”è€…å·²ç»æ¯”è¾ƒæ»¡æ„äº†ï¼Œæœ‰æ—¶é—´å°±æµ‹è¯•ä¸€ä¸‹å…·ä½“çš„æ€§èƒ½å‚æ•°ä»¥åŠä¸ Python ç”Ÿæ€çš„å¯¹æ¯”ã€‚

ä¸è¿‡ï¼Œç”±äºå®ç°çš„æ¯”è¾ƒä»“ä¿ƒï¼Œå…¶å®è¿˜æœ‰å¾ˆå¤šå¯ä»¥ä¼˜åŒ–å’Œæå‡çš„åœ°æ–¹ï¼š

10.1 æ··åˆæ£€ç´¢
---------

å°½ç®¡åŸºäºå‘é‡çš„è¯­ä¹‰æœç´¢åœ¨ç†è§£ç”¨æˆ·æ„å›¾ã€å¤„ç†åŒä¹‰è¯å’Œæ¨¡ç³Šè¡¨è¾¾æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†å®ƒå¹¶éä¸‡èƒ½ï¼š

*   **ç»†èŠ‚ä¸¢å¤±é—®é¢˜**ï¼šè¯­ä¹‰åµŒå…¥å€¾å‘äºæ•æ‰æ•´ä½“è¯­ä¹‰ï¼Œè€Œå¯¹ç²¾ç¡®å…³é”®è¯æ•æ„Ÿåº¦è¾ƒä½ã€‚å½“ç”¨æˆ·æ˜ç¡®çŸ¥é“è¦æ‰¾æŸä¸ªå…·ä½“æœ¯è¯­æ—¶ï¼Œè¯­ä¹‰æ¨¡å‹å¯èƒ½å› â€œè¿‡åº¦æ³›åŒ–â€è€Œå°†ç›¸å…³ä½†ä¸ç²¾ç¡®çš„ç»“æœæ’åœ¨å‰é¢ï¼Œåè€Œæ¼æ‰çœŸæ­£åŒ¹é…çš„æ–‡æ¡£ã€‚
*   **é•¿å°¾æŸ¥è¯¢è¡¨ç°ä¸ç¨³å®š**ï¼šå¯¹äºç½•è§æœ¯è¯­ã€ä¸“ä¸šç¼©å†™æˆ–æ‹¼å†™å˜ä½“ï¼ŒåµŒå…¥æ¨¡å‹è‹¥æœªåœ¨è®­ç»ƒæ•°æ®ä¸­å……åˆ†è¦†ç›–ï¼Œå…¶å‘é‡è¡¨ç¤ºå¯èƒ½åç¦»çœŸå®è¯­ä¹‰ï¼Œå¯¼è‡´æ£€ç´¢å¤±æ•ˆã€‚
*   **å¯æ§æ€§å¼±**ï¼šä¼ ç»Ÿå…³é”®è¯æœç´¢å¯é€šè¿‡å¸ƒå°”é€»è¾‘ï¼ˆAND/OR/NOTï¼‰ã€å­—æ®µé™å®šï¼ˆtitle:xxxï¼‰ã€é€šé…ç¬¦ç­‰æä¾›ç²¾ç»†æ§åˆ¶ï¼Œè€Œçº¯å‘é‡æœç´¢ç¼ºä¹æ­¤ç±»æœºåˆ¶ï¼Œéš¾ä»¥æ»¡è¶³é«˜çº§ç”¨æˆ·çš„ç²¾ç¡®ç­›é€‰éœ€æ±‚ã€‚

å› æ­¤ï¼Œ**å•ä¸€ä¾èµ–è¯­ä¹‰æœç´¢å¹¶ä¸è¶³ä»¥è¦†ç›–å…¨éƒ¨æœç´¢åœºæ™¯**ã€‚æ›´åˆç†çš„æ–¹æ¡ˆæ˜¯æ„å»º**æ··åˆæ£€ç´¢ç³»ç»Ÿï¼ˆHybrid Searchï¼‰**ï¼šå°†**å…³é”®è¯åŒ¹é…ï¼ˆå¦‚ BM25ï¼‰** ä¸ **å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢** çš„ç»“æœè¿›è¡Œèåˆâ€”â€”ä¾‹å¦‚é€šè¿‡åŠ æƒæ‰“åˆ†ï¼ˆReciprocal Rank Fusionï¼‰ã€é‡æ’åºï¼ˆRerankï¼‰æˆ–å‰ç«¯åˆ† Tab å±•ç¤ºã€‚è¿™æ ·æ—¢èƒ½åˆ©ç”¨è¯­ä¹‰æœç´¢ç†è§£â€œæ’¤å› Git æäº¤â€ç­‰è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œåˆèƒ½ç¡®ä¿â€œ`git revert HEAD`â€è¿™ç±»ç²¾ç¡®å‘½ä»¤è¢«å‡†ç¡®å‘½ä¸­ã€‚

10.2 åˆ†å—å™¨å¼ºåŒ–
----------

åœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œè¿™é‡Œåˆ†å—å™¨ **ç»“æ„æ„ŸçŸ¥ + å¥å­å¯¹é½ + ä¸Šä¸‹æ–‡æ³¨å…¥** çš„ä¸‰é‡ç­–ç•¥ï¼Œå·²åœ¨æ•ˆæœä¸æ•ˆç‡ä¹‹é—´å–å¾—äº†ä»¤äººæ»¡æ„çš„å¹³è¡¡ã€‚ä¸è¿‡ï¼Œåœ¨æ›´å¤æ‚çš„ RAG ç³»ç»Ÿä¸­ï¼Œå¯èƒ½è¿˜éœ€è¦ä»¥ä¸‹ä¼˜åŒ–ï¼š

*   å†…å®¹ç±»å‹æ„ŸçŸ¥åˆ†å—ï¼šå¯¹ä»£ç å—ã€è¡¨æ ¼ã€LaTeX å…¬å¼ç­‰éè¿ç»­æ–‡æœ¬è¿›è¡Œç‰¹æ®Šå¤„ç†ã€‚ä¾‹å¦‚ï¼Œå°†å®Œæ•´ä»£ç å—è§†ä¸ºä¸å¯åˆ†å‰²å•å…ƒï¼›å¯¹è¡¨æ ¼æŒ‰è¡Œæˆ–è¯­ä¹‰åˆ—æ‹†åˆ†ï¼›é¿å…åœ¨å…¬å¼ä¸­é—´å¼ºè¡Œåˆ‡åˆ†ã€‚
*   åŠ¨æ€è¯­ä¹‰è¾¹ç•Œæ£€æµ‹ï¼šå¼•å…¥è½»é‡çº§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ Sentence-BERT æˆ–ä¸“ç”¨è¾¹ç•Œåˆ†ç±»å™¨ï¼‰è¯†åˆ«æ®µè½å†…éƒ¨çš„è¯­ä¹‰è½¬æŠ˜ç‚¹ï¼Œå®ç°æ›´è‡ªç„¶çš„åˆ†æ®µï¼Œè€Œéä»…ä¾èµ–æ ‡ç‚¹æˆ–å›ºå®šé•¿åº¦ã€‚
*   æ»‘åŠ¨çª—å£é‡å æœºåˆ¶ï¼šåœ¨å›ºå®šæœ€å¤§ token é•¿åº¦ï¼ˆå¦‚ 512ï¼‰åŸºç¡€ä¸Šï¼Œé‡‡ç”¨æ»‘åŠ¨çª—å£ï¼ˆå¦‚æ­¥é•¿ 384ï¼Œé‡å  128 tokensï¼‰ç”Ÿæˆè¿ç»­é‡å å—ã€‚è¿™èƒ½ç¼“è§£å› ç¡¬æ€§åˆ‡åˆ†å¯¼è‡´çš„å…³é”®ä¿¡æ¯è¢«å‰²è£‚çš„é—®é¢˜ï¼Œå°¤å…¶é€‚ç”¨äºé•¿æŠ€æœ¯è¯´æ˜æˆ–è¿è´¯è®ºè¯æ®µè½ã€‚
*   åˆ—è¡¨ä¸æšä¸¾é¡¹ç²¾ç»†åŒ–å¤„ç†ï¼šå½“å‰åˆ†å—å™¨å¯èƒ½å°†å¤šè¡Œæ— åº/æœ‰åºåˆ—è¡¨æ•´ä½“ä¿ç•™ï¼Œä½†è‹¥åˆ—è¡¨é¡¹æœ¬èº«è¾ƒé•¿ä¸”ç‹¬ç«‹ï¼Œå¯è¿›ä¸€æ­¥æŒ‰é¡¹æ‹†åˆ†ï¼Œå¹¶ä¸ºæ¯é¡¹æ³¨å…¥æ‰€å±æ ‡é¢˜ä¸Šä¸‹æ–‡ï¼Œæå‡ç»†ç²’åº¦æ£€ç´¢èƒ½åŠ›ã€‚
*   è·¨å—ä¸Šä¸‹æ–‡é“¾æ¥ï¼šåœ¨å­˜å‚¨ chunk æ—¶ï¼Œé¢å¤–è®°å½•å…¶å‰é©±/åç»§ chunk IDï¼Œä¾¿äºåœ¨åç»­ RAG é˜¶æ®µæ‹¼æ¥ä¸Šä¸‹æ–‡ï¼Œè¿˜åŸæ›´å®Œæ•´çš„åŸå§‹è¯­å¢ƒã€‚

è¿™äº›æ”¹è¿›å¹¶ééƒ½è¦åŒæ—¶å¼•å…¥ï¼Œè€Œåº”æ ¹æ®å®é™…å†…å®¹åˆ†å¸ƒå’Œç”¨æˆ·æŸ¥è¯¢æ¨¡å¼é€æ­¥è¿­ä»£ã€‚æ¯•ç«Ÿï¼Œåˆ†å—çš„æœ¬è´¨æ˜¯åœ¨â€œä¿¡æ¯å¯†åº¦â€ã€â€œè¯­ä¹‰å®Œæ•´æ€§â€ä¸â€œæ£€ç´¢æ•ˆç‡â€ä¹‹é—´å¯»æ‰¾æœ€ä¼˜å¹³è¡¡ç‚¹â€”â€”è€Œè¿™ä¸ªå¹³è¡¡ç‚¹ï¼Œä¼šéšç€ä¸šåŠ¡æ¼”è¿›è€Œä¸æ–­ç§»åŠ¨ã€‚

10.3 é«˜äº®æº¯æº
---------

å½“å‰æœç´¢ç»“æœè™½å·²è¿”å› `snippetText`ï¼ˆå³å‘½ä¸­çš„æ–‡æœ¬ç‰‡æ®µï¼‰ï¼Œä½†ç”¨æˆ·ä»éœ€è‡ªè¡Œåœ¨åŸæ–‡ä¸­å®šä½å…³é”®è¯ä½ç½®ï¼Œä½“éªŒä¸å¤Ÿç›´è§‚ã€‚ä¸ºæå‡å¯ç”¨æ€§ï¼Œåº”å®ç°**å…³é”®è¯é«˜äº®ä¸ç‰‡æ®µæº¯æº**åŠŸèƒ½ã€‚

å…¶å®ç°å¯åˆ†ä¸ºä¸¤æ­¥ï¼š

1.  **å‘½ä¸­è¯æå–ä¸é«˜äº®**  
    åœ¨è¿”å› `snippetText` å‰ï¼Œå¯¹åŸå§‹æŸ¥è¯¢è¿›è¡Œåˆ†è¯ï¼ˆå¤ç”¨ `Tokenizer`ï¼‰ï¼Œæå–æ ¸å¿ƒå…³é”®è¯ï¼ˆæˆ–ä½¿ç”¨ BM25 é€†å‘åŒ¹é…é«˜é¢‘ termï¼‰ï¼Œç„¶ååœ¨ `snippetText` ä¸­è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„å­ä¸²åŒ¹é…ï¼Œå¹¶ç”¨ `<mark>` æˆ– `**` ç­‰æ ‡è®°åŒ…è£¹å‘½ä¸­è¯ã€‚ä¾‹å¦‚ï¼š
    
    > â€œå¦‚ä½•**æ’¤å›** Git **æäº¤**â€ â†’ â€œ...å¯ä»¥ä½¿ç”¨ `git revert` æ¥**æ’¤å›**æŸæ¬¡**æäº¤**...â€
    
2.  **ä¸Šä¸‹æ–‡æ‰©å±•ä¸é”šç‚¹å®šä½**  
    å½“å‰ `snippetText` ä»…ä¸º chunk å†…å®¹ï¼Œå¯èƒ½ç¼ºä¹è¶³å¤Ÿä¸Šä¸‹æ–‡ã€‚å¯è¿›ä¸€æ­¥æ ¹æ® chunk æ‰€å±çš„æ®µè½ç»“æ„ï¼ˆå¦‚ Markdown æ ‡é¢˜å±‚çº§ï¼‰ï¼Œå‘å‰/åæ‰©å±• 1â€“2 å¥ä½œä¸ºè¡¥å……ä¸Šä¸‹æ–‡ï¼Œå¹¶åœ¨å‰ç«¯é€šè¿‡æ»šåŠ¨é”šç‚¹ï¼ˆå¦‚ `#chunk-12345`ï¼‰ç›´æ¥è·³è½¬åˆ°åŸæ–‡å¯¹åº”ä½ç½®ï¼Œå®ç°â€œæ‰€è§å³æ‰€è¾¾â€çš„æº¯æºä½“éªŒã€‚
    

é«˜äº®æº¯æºè™½æ˜¯ç»†èŠ‚ï¼Œå´æ˜¯è¿æ¥â€œæ£€ç´¢ç»“æœâ€ä¸â€œç”¨æˆ·ä¿¡ä»»â€çš„å…³é”®ä¸€ç¯â€”â€”è®©ç”¨æˆ·ä¸€çœ¼çœ‹æ¸…â€œä¸ºä»€ä¹ˆè¿™æ¡ç»“æœè¢«å¬å›â€ï¼Œä»è€Œæå‡æœç´¢ç³»ç»Ÿçš„é€æ˜åº¦ä¸å¯ä¿¡åº¦ã€‚

10.4 å‘é‡ç´¢å¼•
---------

åœ¨ç›®å‰å°è§„æ¨¡åœºæ™¯ï¼ˆé€šå¸¸å‡ ç™¾åˆ°å‡ åƒä¸ª chunkï¼‰ï¼Œä½¿ç”¨ `IndexFlatIP` ç²¾ç¡®æœç´¢çš„å¼€é”€å®Œå…¨å¯æ¥å—ï¼Œä¸”èƒ½ä¿è¯æœ€é«˜å¬å›è´¨é‡ã€‚è‹¥æœªæ¥æ•°æ®é‡æ¿€å¢ï¼Œå¯æ— ç¼æ›¿æ¢ä¸º `IndexIVFFlat` æˆ– `HNSW` ç­‰è¿‘ä¼¼ç´¢å¼•ï¼Œè€Œä¸Šå±‚æ¥å£ä¸å˜ã€‚

ä¸ºäº†ä¾¿äºåç»­æ¼”è¿›ï¼Œè¿™é‡Œç»™å‡ºä¸€ä¸ªåŸºäºæ•°æ®è§„æ¨¡çš„ç»éªŒæ€§å‡çº§è·¯å¾„å»ºè®®ï¼š

**Chunk æ•°é‡èŒƒå›´**

**æ¨è FAISS ç´¢å¼•ç±»å‹**

**ç‰¹ç‚¹ä¸é€‚ç”¨åœºæ™¯**

**å…¸å‹å‚æ•°ï¼ˆç¤ºä¾‹ï¼‰**

**æ˜¯å¦æ”¯æŒåŠ¨æ€æ’å…¥**

â‰¤ 10,000

`IndexFlatIP`

ç²¾ç¡®æœç´¢ï¼Œå®ç°ç®€å•ï¼Œå»¶è¿Ÿæä½ï¼ˆ<10msï¼‰ï¼Œé€‚åˆå°è§„æ¨¡é™æ€çŸ¥è¯†åº“

æ— 

âœ… æ˜¯

10,000 ~ 100,000

`IndexIVFFlat`

è¿‘ä¼¼æœç´¢ï¼Œé€šè¿‡èšç±»åŠ é€Ÿï¼Œå¬å›ç‡é«˜ï¼ˆ>95%ï¼‰ï¼Œé€‚åˆä¸­ç­‰è§„æ¨¡ã€éœ€é¢‘ç¹å¢é‡æ›´æ–°çš„åœºæ™¯

`nlist=100`, `nprobe=10`

âœ… æ˜¯

\> 100,000

`IndexHNSW`

å›¾ç»“æ„è¿‘ä¼¼æœç´¢ï¼ŒæŸ¥è¯¢é€Ÿåº¦æå¿«ä¸”å‡ ä¹ä¸éšæ•°æ®å¢é•¿å˜æ…¢ï¼Œé€‚åˆå¤§è§„æ¨¡ã€é«˜å¹¶å‘åªè¯»åœºæ™¯

`M=32`, `efConstruction=200`, `efSearch=64`

âŒ å¦ï¼ˆFAISS é»˜è®¤ä¸æ”¯æŒï¼‰

\> 1,000,000ï¼ˆè¶…å¤§è§„æ¨¡ï¼‰

`IndexIVFPQ` æˆ– `HNSW+PQ`

å¼•å…¥å‘é‡é‡åŒ–ï¼ˆProduct Quantizationï¼‰å‹ç¼©å†…å­˜ï¼Œç‰ºç‰²å°‘é‡ç²¾åº¦æ¢å–æ›´ä½å†…å­˜å ç”¨å’Œæ›´å¿«æ£€ç´¢

`nlist=1000`, `pq.M=64`, `pq.nbits=8`

âœ…ï¼ˆIVF æ”¯æŒï¼‰

10.5 ä¸šåŠ¡æ€§èƒ½
---------

åœ¨æ¶‰åŠåšæ–‡çš„å¢åˆ æ”¹ï¼ˆCRUDï¼‰æ“ä½œæ—¶ï¼Œ**åµŒå…¥ç”Ÿæˆä¸ç´¢å¼•æ›´æ–°å±äºè¾…åŠ©æ€§åå°ä»»åŠ¡**ï¼Œå¹¶ä¸å½±å“ä¸»ä¸šåŠ¡æµç¨‹ï¼ˆå¦‚æ–‡ç« å‘å¸ƒã€ç¼–è¾‘ä¿å­˜ç­‰ï¼‰ã€‚ç”¨æˆ·çš„æ ¸å¿ƒè¯‰æ±‚æ˜¯â€œæ“ä½œå³æ—¶å“åº”â€ï¼Œè€Œå¯¹æœç´¢ç»“æœçš„æ”¶å½•å»¶è¿Ÿé€šå¸¸å…·æœ‰è¾ƒé«˜å®¹å¿åº¦â€”â€”å‡ ç§’ç”šè‡³å‡ åˆ†é’Ÿçš„æ»ååœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹æ˜¯å¯ä»¥æ¥å—çš„ã€‚

å› æ­¤ï¼Œ**ä¸åº”åœ¨ä¸»çº¿ç¨‹ä¸­åŒæ­¥æ‰§è¡ŒåµŒå…¥è®¡ç®—ä¸ FAISS å†™å…¥**ï¼Œå¦åˆ™ä¼šæ˜¾è‘—æ‹–æ…¢ HTTP å“åº”æ—¶é—´ï¼Œå°¤å…¶å½“ ONNX æ¨ç†æˆ–ç£ç›˜ I/O æˆä¸ºç“¶é¢ˆæ—¶ã€‚

æ¨èé‡‡ç”¨ä»¥ä¸‹ç­–ç•¥ä¹‹ä¸€ï¼š

*   **å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæ¨èï¼‰**ï¼šå°†åµŒå…¥ä»»åŠ¡ï¼ˆå¦‚ `AddBlogEmbedding` / `RemoveBlogEmbedding`ï¼‰æ¨å…¥å†…å­˜é˜Ÿåˆ—æˆ–è½»é‡çº§ä»»åŠ¡è°ƒåº¦å™¨ï¼ˆå¦‚åŸºäº `std::queue` + å•ç‹¬å·¥ä½œçº¿ç¨‹ï¼‰ï¼Œç”±åå°çº¿ç¨‹æŒ‰åºå¤„ç†ã€‚è¿™æ ·æ—¢èƒ½ä¿è¯ä¸»çº¿ç¨‹å¿«é€Ÿè¿”å›ï¼Œåˆèƒ½é¿å…å¤šçº¿ç¨‹å¹¶å‘å†™ FAISS ç´¢å¼•å¼•å‘çš„æ•°æ®ç«äº‰ï¼ˆFAISS çš„ `Index` é»˜è®¤éçº¿ç¨‹å®‰å…¨ï¼‰ã€‚
*   **åç¨‹æˆ–å¼‚æ­¥ I/Oï¼ˆè¿›é˜¶ï¼‰**ï¼šè‹¥ç³»ç»Ÿå·²åŸºäºæ”¯æŒåç¨‹çš„æ¡†æ¶ï¼ˆå¦‚ Seastarã€Boost.Asio + coroutine TSï¼‰ï¼Œå¯å°†åµŒå…¥ä¸ç´¢å¼•å†™å…¥å°è£…ä¸º suspendable ä»»åŠ¡ï¼Œåœ¨ä¸é˜»å¡äº‹ä»¶å¾ªç¯çš„å‰æä¸‹å®Œæˆè€—æ—¶æ“ä½œã€‚ä½†è€ƒè™‘åˆ°å½“å‰ç³»ç»Ÿè½»é‡çº§å®šä½ï¼Œæ­¤æ–¹æ¡ˆæ”¶ç›Šæœ‰é™ï¼Œå¤æ‚åº¦è¾ƒé«˜ã€‚
*   **å®šæœŸæ‰¹é‡æ›´æ–°ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰**ï¼šå¯¹äºä½é¢‘æ›´æ–°åœºæ™¯ï¼Œä¹Ÿå¯æš‚å­˜å¾…å¤„ç†çš„ blog ID åˆ°æ•°æ®åº“æ ‡è®°è¡¨ï¼Œç”±å®šæ—¶ä»»åŠ¡ï¼ˆå¦‚æ¯ 5 åˆ†é’Ÿï¼‰ç»Ÿä¸€æ‰«æå¹¶æ‰¹é‡æ‰§è¡ŒåµŒå…¥ä¸ç´¢å¼•æ›´æ–°ã€‚è¿™ç§æ–¹å¼å®ç°ç®€å•ï¼Œä¸”å¤©ç„¶è§„é¿å¹¶å‘é—®é¢˜ï¼Œé€‚åˆ MVP æˆ–èµ„æºå—é™ç¯å¢ƒã€‚

æ­¤å¤–ï¼ŒFAISS ç´¢å¼•çš„æŒä¹…åŒ–ï¼ˆ`faiss::write_index`ï¼‰å±äº I/O å¯†é›†å‹æ“ä½œï¼Œå»ºè®®åœ¨åå°ä»»åŠ¡å®Œæˆæ‰€æœ‰å‘é‡æ·»åŠ å**ä¸€æ¬¡æ€§å†™å…¥**ï¼Œè€Œéæ¯æ¬¡æ–°å¢ chunk éƒ½è§¦å‘å†™ç›˜ï¼Œä»¥å‡å°‘ç£ç›˜å‹åŠ›å¹¶æå‡ååã€‚

é€šè¿‡ä¸Šè¿°å¼‚æ­¥åŒ–è®¾è®¡ï¼Œç³»ç»Ÿæ—¢èƒ½ç»´æŒæ¯«ç§’çº§çš„å‰ç«¯å“åº”ï¼Œåˆèƒ½ç¨³å¥åœ°ä¿éšœæœç´¢æ•°æ®çš„é€æ­¥æ”¶æ•›ï¼ŒçœŸæ­£å®ç°â€œç”¨æˆ·ä½“éªŒâ€ä¸â€œç³»ç»Ÿæ•ˆç‡â€çš„åŒèµ¢ã€‚