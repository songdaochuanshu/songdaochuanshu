---
layout: post
title: 'AIReview 实战：用 AI 把代码评审提质提速'
date: "2025-10-21T00:41:09Z"
---
AIReview 实战：用 AI 把代码评审提质提速
==========================

AIReview 实战：用 AI 把代码评审提质提速
==========================

GitHub 仓库：[https://github.com/wosledon/AIReview](https://github.com/wosledon/AIReview)

如果你也在为“评审慢、质量不稳定、沟通碎片化、重复劳动多”而头疼，这篇文章会把我们在 AIReview 项目中的实践完整分享给你：我们如何把多模型 LLM 能力、Prompt 可定制、异步分析、实时协作、Git 集成等组合起来，让代码评审真正落地、可量化、可持续改进。

我们要解决什么问题？
----------

*   评审效率低：PR 大、改动多，人工通读耗时长且容易遗漏风险。
*   质量难对齐：不同评审人标准不一，建议分散在聊天和评论里，缺少沉淀与复用。
*   反馈不成体系：只见“问题”，不见“维度”，难以形成团队共识与可追踪的改进路线。
*   重复性劳动：套路化的检查和描述（如 PR 摘要、风险提示、测试建议）一遍遍重复。

AIReview 的目标，是用 AI 把“可自动化的部分”自动化，把“需要人判断的部分”信息充分化、结构化，从而让评审既快又准。

AIReview 怎么做的？一图胜千言
-------------------

下面几张项目内页截图，带你快速感受从“创建项目”到“进入评审”的整体体验：

关键能力一览
------

1.  智能 AI 代码评审（多维度）

*   质量、安全、性能、可维护性等多维度分析与风险评分
*   上下文相关的改进建议，聚焦“可落地”
*   多 LLM 支持（OpenAI、Azure OpenAI、自建/私有模型等），可按项目或用户配置
*   异步后台分析（Hangfire），面对大型代码库依然响应迅速

2.  Prompt 可定制（三级模板）

*   内置 → 用户级 → 项目级逐级覆盖，灵活适配团队风格与场景
*   支持 {{CONTEXT}}、{{DIFF}}、{{FILE\_NAME}} 等占位符
*   前端可视化管理模板，CRUD 一站式完成

3.  高级 PR 分析与摘要

*   自动生成变更摘要、影响评估、部署注意事项与回滚提示
*   变更类型分类（特性、修复、重构、文档等）
*   测试建议与关注点提示，帮助评审人“少走弯路”

4.  改进建议引擎

*   按类别（质量、性能、安全、架构等）组织建议
*   基于影响 × 成本的优先级评分，便于排期落地
*   接受/忽略反馈回路，长期跟踪采纳率与趋势

5.  Git 深度集成

*   导入现有仓库、解析 Diff、绑定提交历史
*   多分支工作流与评审记录关联

6.  实时协作与工作流

*   SignalR 推送通知，评论/状态实时更新
*   评审请求 → 指派 → 审批/驳回/请求修改的完整生命周期

7.  可观测与成本意识

*   Token 用量与调用统计（API 侧已提供 TokenUsage 控制器），便于成本评估与优化

一套顺手的评审流程（从 0 到 1）
------------------

1.  创建项目并配置 LLM 与 Prompt 模板
    
    *   根据团队规范定制模板，确保建议“说人话、可执行”。
2.  关联/导入 Git 仓库并触发分析
    
    *   对 PR 或特定分支发起评审，系统自动拉取 Diff 并进行异步分析。
3.  在“评审主页”先看大局
    
    *   先读自动摘要与风险评分，快速锁定重点文件与变更块。
4.  进入文件级/行级视图
    
    *   查看 AI 建议与证据（上下文/代码片段），必要时补充人类判断与团队惯例。
5.  输出明确的结论与动作
    
    *   通过评论/任务清单明确修复项；必要时请求修改或批准合并。
6.  回收经验，沉淀到模板
    
    *   把“讨论中达成的新共识”沉淀进 Prompt 模板，下一次自动做到位。

架构与技术选型（简版）
-----------

*   分层与领域清晰：API（ASP.NET Core）/ Core（领域与业务）/ Infrastructure（EF Core、外部服务、Hangfire、Redis）
*   实时通信：SignalR 推送评审状态与消息
*   数据库：SQLite（默认）或 PostgreSQL（生产推荐）
*   前端：React + TypeScript + Vite + TailwindCSS + React Query
*   异步处理：Hangfire 负责长耗时 AI 分析任务
*   模块化：Repository + Unit of Work，接口驱动，方便替换 LLM 与外部集成

扩展阅读：

*   架构设计（中文）见 `docs/design.md`
*   Architecture Design (English) 见 `docs/design.en-us.md`

上手非常简单（开发环境）
------------

后端（.NET 8）：

*   配置 `AIReview.API/appsettings.Development.json`（连接串、JWT、可选 Redis）
*   运行数据库迁移并启动 API（Swagger 可用）

前端（React + Vite）：

*   安装依赖，配置 `VITE_API_BASE_URL`
*   本地启动开发服务器，浏览器访问即可

提示：本仓库提供 `AIReview.Tests` 便于后端用例验证，前端可按需接入 Vitest/Jest。

为什么值得一试？
--------

*   更准确：多模型 + 模板可定制，建议贴合你们的代码与语境
*   更迅速：异步分析 + 缓存机制，面对大 PR 也能快速定位风险
*   更可控：Token 用量可追踪，分析粒度可调，成本与收益可量化
*   更协作：实时评论、统一工作流，评审沟通不再碎片化
*   更可扩：接口驱动的架构，易于新增 LLM 或接入企业内网能力

Roadmap（节选）
-----------

*   更强的“代码修复建议”：生成可预览的补丁与 Diff
*   多模型集成：融合多 LLM 提升鲁棒性
*   IDE 集成：VS Code 插件，边写边评审
*   分析报表：质量趋势、团队效率、技术债务看板
*   安全/合规模块：更深度的安全扫描与许可证合规

完整路线图与特性详情，见仓库根目录 `README.md` 与 `docs/` 文档。

结语
--

AI 不是“取代评审人”，而是“放大评审人”的判断力与影响力。把重复机械的工作交给机器，把有限的时间留给需要经验与共识的部分。欢迎试用 AIReview，并把你们团队的实践分享回来，一起把“AI 时代的代码评审”做得更好。

*   源码与问题反馈：仓库 [https://github.com/wosledon/AIReview（Issues](https://github.com/wosledon/AIReview%EF%BC%88Issues) 与 Discussions）
*   许可证：MIT（见根目录 `LICENSE`）

如果这篇文章或项目对你有帮助，别忘了给仓库点个 Star～