---
layout: post
title: 'DeepSeek-R1 技术全景解析：从原理到实践的“炼金术配方” ——附多阶段训练流程图与核心误区澄清'
date: "2025-02-08T00:34:40Z"
---
DeepSeek-R1 技术全景解析：从原理到实践的“炼金术配方” ——附多阶段训练流程图与核心误区澄清
====================================================

> **字数：约3200字｜预计阅读时间：8分钟**

（调试着R1的API接口，看着控制台瀑布般流淌的思维链日志）此刻我仿佛看到AlphaGo的棋谱在代码世界重生——**这是属于推理模型的AlphaZero时刻**。

DeepSeek 发布的 V3、R1-Zero、R1 三大模型，代表了一条从通用基座到专用推理的完整技术路径。许多读者对三者的关系存在困惑，本文将通过**流程图解、差异对比、训练逻辑拆解**三大模块，彻底厘清它们的定位与联系。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=NDZkYWFmMzhjNDgzNGRkM2U5MzhlZjg1Nzg4NzI4ODNfNGRFZjFDR25DaWQwZFNhYW14MWM2YWpEa3FudlQxODlfVG9rZW46QWVkWmJCb05Ub3RPRU14ZWFiUWN6bUtHbml4XzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

一、模型定位与技术差异
-----------

**维度**

**DeepSeek-V3**

**R1-Zero**

**DeepSeek-R1**

定位

通用基座模型

纯 RL 训练的推理实验模型

多阶段优化的商用推理模型

训练方法

预训练 + SFT

纯强化学习（GRPO 算法）

SFT → RL → SFT → RL与SFT混合训练

数据依赖

通用语料 + 标注数据

数学/代码数据（无需标注）

RL 生成数据 + 人类偏好数据

推理能力

基础问答

强推理但语言混杂

强推理 + 语言规范

可用性

通用场景

实验性（不可直接商用）

全场景适配（客服、编程等）

开源状态

开源

未开源

开源

二、训练关系全流程图解
-----------

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=YmQ4NmMwODZkN2EzZDNlZjlkOWViZDIxYmYyNTU3ODVfa2RxYll0emE5bmxkbGRFNXlsVUZJMW80a0JnclIzTVFfVG9rZW46RU81R2JQUHIzb05PWE94aWRKdWNMbUxObjRlXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

**流程图解读**：

1.  **V3 是起点**：作为通用基座模型，提供基础语言能力。
2.  **R1-Zero 是过渡实验体**：通过纯 RL 训练验证推理能力，但语言混乱不可用。
3.  **R1 是终极形态**：融合冷启动、RL 锻造、数据反哺、人类偏好四阶段，兼顾能力与实用性。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=ZTg3M2YxYzVjNTdhOThkY2QyZjM5ZWFkZDVlNmExZDhfSjdmcG1ZOGFlaWNiTHU5WHRubmlEZ2R6UTZta2pMZnFfVG9rZW46VVQyamJiYUFkb0gwNm14THN1TGN5SDFqbmxlXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

三、技术演进逻辑拆解
----------

### 1\. 基座奠基：DeepSeek-V3 的“原始积累”

*   **核心能力**：通用文本生成、基础问答、多任务处理。
*   **短板暴露**：
    *   无法生成连贯的思维链（CoT）。
    *   依赖监督微调（SFT），推理能力天花板低。
*   **破局方向**：引入**强化学习**（**RL**），让模型通过奖励机制自主探索推理路径。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MjMyNzdmZWQwMmZkMWY0NWY2NmU2ZDA2OTE4M2Y1OWVfRlVxUElwekdQR25sek96RjdxMU1WZ2ZFNlNnS3pvYlhfVG9rZW46S1M5MmJvVEdCb3RsWmV4S1VKaGN6ZlZmbjRFXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

### 2\. 纯 RL 试炼：R1-Zero 的“无监督觉醒”

*   **设计理念**：**“不依赖人类标注，仅靠** **RL** **自我进化”**
*   **训练逻辑**：
    *   **数据选择**：数学推导与代码执行（每一步可客观验证）。
    *   **算法核心**：GRPO（组内奖励对比优化），避免训练额外评判模型。
    *   **成果与代价**：推理指标超越 V3，但语言混杂、格式混乱。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=M2U0YzQzNjhkZWQ3N2Y2ZTQ5NmI4M2RkMzQ1ZmI1NjlfRzEzVHRBU2lzR3doYWVOUGU1TGJuVjBBWVBuRE95QmRfVG9rZW46V2VCV2JmSjl2b0tsUUd4UGRpTmN2VndUblNnXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MDY2MjlmNzI5MWM5MjM1NzE0ZjU5YjY1M2M4NWFmMjhfZ0dpaWZtaXZoNkllVjVPT0xNRFBJdDVBQkZqNnh2aWRfVG9rZW46SWdRNGI4NTBZbzFjdzV4SlVIWWN3ZW1HbjRaXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

### 3\. 最终形态满血R1

DeepSeek-R1 通过四大阶段实现了能力与实用性的完美平衡：

*   **冷启动**：利用少量高质量的 CoT 数据，使模型学会标准答案格式。
*   **RL** **锻造**：引入 GRPO 算法，让模型在多种推理路径中自主选择最优策略。
*   **数据反哺**：模型自生成高质量数据，减少对人工标注的依赖。
*   **人机融合**：引入人类偏好奖励，确保输出结果不仅推理精准，更符合实际使用需求。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MDMzODBkMWQyYTliM2MyNmM2YjA0MGYwNjU3YWUwY2RfakFZNDdrUGNCY3dmbGFJME5oMHlzVFRHMHZJbWExZ0hfVG9rZW46QTZYbWJFQmw2b2xRSmp4Y3JsUGNncHJlbnVjXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

当看到DeepSeek论文中训练曲线图时我突然意识到：**强化学习** **正在重写游戏规则**。就像AlphaZero抛弃人类棋谱

四、DeepSeek-R1 的“四步炼金术”
----------------------

DeepSeek-R1 的训练过程可以分为四大步骤，每一步都像是炼金术中的独门秘技：

### Step 1：冷启动——“抄作业”阶段

这个阶段，DeepSeek-R1 还只是个“新手”。它要做的就是“抄作业”——学习少量高质量的 CoT（思维链）数据。这些数据就像“武功秘籍”中的“图解”，告诉 DeepSeek-R1 什么是正确的推理过程。

（对照自己调试Agent的经历）初始阶段如同给新生儿植入基础反射：

    收集1000+高质量CoT数据（相当于婴儿的看图识字卡）
    精细调整prompt格式（建立神经元的「输入输出规范」）
    引入语言一致性奖励（避免中英文混杂的「精神分裂」）
    

这个阶段的核心矛盾是：**如何在最小化人工干预的前提下，建立可扩展的推理** **范式**。DeepSeek的方案像给模型安装「脚手架」，既约束探索方向，又不限制创新空间。

*   **目标**：防止 RL 初期盲目探索，奠定基础推理格式。
*   **核心操作**：
    *   **数据精选**：少量高质量长思维链（Long-CoT）数据，包含清晰推理步骤。
    *   **SFT** **预热**：对 V3 微调，使其初步学会“抄写”标准答案。

> 如同教孩童写字，先临摹字帖，再迈向自主创作。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=ODk4MTlkNmE2NjNiOWZkOGQ0ZmVlN2U1NzZhYTY3NTNfb3pVV0RzTlpiY3NwMWNpUlJVTGZBcGl4djZkdkRXbHNfVG9rZW46R1dEcmJrWUJPb01rdDh4Y2lsMWNiVmZZblVlXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

### Step 2：RORL——“实战演练”阶段

有了“冷启动”的基础，DeepSeek-R1 进入了“实战演练”阶段——RORL（推理导向的强化学习）。

这个阶段，DeepSeek-R1 不再只是“抄作业”，而是要自己“解题”。它会尝试各种推理路径，并通过 GRPO（Group Relative Policy Optimization）算法来评估自己的表现。

GRPO 就像一位“裁判”，根据 DeepSeek-R1 的“答题”情况打分，并指导它如何改进。

这个阶段，DeepSeek-R1 主要依靠规则奖励（Rule-based Reward）来“修炼”。规则奖励就像“武功秘籍”中的“口诀”，告诉 DeepSeek-R1 哪些是“正确”的推理步骤。

*   **算法核心**：GRPO 的三大创新设计
    *   **组内基线估计**：同一批输出的奖励对比，降低训练开销。
    *   **规则奖励**：答案正确性、推理格式规范性（如步骤编号、符号统一）。
    *   **语言一致性奖励**：强制中英文分离，解决 R1-Zero 的“语言混搭”问题。
*   **效果验证**：模型逐步涌现长推理链能力，甚至能自我修正错误步骤（“Aha Moment”）。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MzE1NDkzOWIzOWY4ZGM0YzE5YmZhNWQzOGVkZDI3ZTVfQkJZd2NOeEhDWGZFazYxY0FQUURrRTBOZzFuWXlRaUtfVG9rZW46U1BzM2JiUHNWb242dWl4eGFkWWN5UHh3blZDXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

### Step 3：重构——“自创武功”阶段

经过“实战演练”，DeepSeek-R1 已经具备了一定的推理能力。接下来，它开始“自创武功”——**生成高质量的** **训练数据**。

这个阶段，DeepSeek-R1 会利用拒绝采样（Rejection Sampling）和 CoT 提示（CoT Prompting）来生成数据。拒绝采样就像“筛选器”，确保生成的数据符合要求；CoT 提示则像“模板”，帮助 DeepSeek-R1 生成各种类型的 SFT 数据。

*   **拒绝采样（Rejection Sampling）**：
    *   **生成**：模型输出推理过程。
    *   **筛选**：规则校验（格式）→ V3 模型二次过滤 → 保留优质数据。
*   **CoT** **提示工程**：生成非推理任务数据（如写作、对话），增强泛化性。

> 类似厨师研发新菜后，将成功配方整理成食谱，供团队学习。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=OWZiNDNhYTg1NGU1MDUzZGVhMmMxZGQzZjIyMzBkZmNfVzdqYjhuVDhrbm5Udks0MkZNcFViVzlLUDQ3bXRiUXZfVG9rZW46Ujc3V2JSdUJDb2dtR1p4YnlmdGNkUTNUblRoXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

### Step 4：最终进化——“融会贯通”阶段

这个阶段，DeepSeek-R1 将之前学到的所有“招式”融会贯通。它会再次进行 SFT 微调，并引入人类偏好奖励（Human Preference Reward），让自己的推理能力更上一层楼。

人类偏好奖励就像“武林大会”的“观众投票”，让 DeepSeek-R1 知道什么样的推理结果更受欢迎。

*   **混合奖励信号**：
    *   **规则奖励**：针对推理任务（如数学解题）。
    *   **人类偏好奖励**：引入 Helpfulness（有用性）与 Harmlessness（无害性）评估。
*   **数据多样性**：覆盖多场景提示（客服、编程、创意写作），避免“过拟合推理”。

> 正如武林高手在大赛中通过观众投票验证实力，DeepSeek-R1 经过这一阶段实现了真正的能力整合。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=YWNhYzBlYTJmYjkwNDdjNmJhMjgyNzU3ZjhlZWEzNDRfWDllT21zSDlueGZWQ3ViOFJjWmhBR1lHMEdjem9kdXRfVG9rZW46RlRkc2IyVGZOb1BVMVl4RGQzcmMwU1VBbnZnXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

五、核心误区澄清：打破 RL 训练的“神话滤镜”
------------------------

在推理模型的演进过程中，常见以下几个误区：

1.  **“无需** **SFT** **”意味着完全抛弃** **监督学习**？
    1.  事实：SFT 在冷启动和数据反哺阶段都是不可或缺的润滑剂，为 RL 提供了正确的格式指引。
2.  **RL** **训练成本必然高于** **SFT\*\*\*\*？**
    1.  事实：得益于 GRPO 算法的组内奖励对比机制，R1 的训练成本仅为传统 RLHF 的约 1/3。
3.  **推理模型必须依赖过程监督（****PRM****）？**
    1.  事实：DeepSeek-R1 证明，仅凭规则奖励和 GRPO 也能实现性能突破；不过 PRM 可在加速收敛上起到一定作用。

此外，对于如下常见问题：

*   **Q1：为何不直接发布 R1-Zero？** 纯 RL 训练固然让推理能力飞速提升，但语言表达存在混乱，因此 R1-Zero 更多用于技术验证而非产品化。
*   **Q2：DeepSeek-V3 是否已被淘汰？** V3 作为通用基座在非推理场景依然具有优势，而 R1 则是 V3 的“推理特化版”，二者各有侧重。
*   **Q3：普通开发者应如何选择模型？**
    *   通用任务：DeepSeek-V3
    *   复杂推理：DeepSeek-R1
    *   学术研究：R1-Zero（需具备相应权限）

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=MDY2MzBkN2VlMmExNjQ3Y2NlZmZiMzRmMDViMDAwZTBfSXN6WTB0Tll3Qk1rdm1JMVJ6bjJNUzJmeUFwNnkwMzVfVG9rZW46V0V1emJlVnVNbzlzVUd4UzliaWNNYlNDbmplXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

六、技术启示：开源社区的“破壁宣言”
------------------

1.  范式革新：RL 主导的后训练时代

传统模型依赖海量标注数据，而 RL 训练则让模型从“被动模仿”转变为“主动探索”，极大突破了数据瓶颈，让推理能力迈向新高。

2.  工程哲学：简单即美

拒绝复杂设计，DeepSeek-R1 未采用诸如 MCTS 复杂搜索，而是依托清晰的规则奖励与 Scaling Law，在追求性能的同时兼顾工程效率。

3.  未来预言

*   **推理即服务**：未来两年，50% 的 AI 应用将内置本地推理引擎，R1 类模型将成为关键的“推理中间件”。
*   **RL** **主导训练**：预计到 2025 年，80% 的顶尖模型将优先采用 RL 策略，监督学习将在辅助工具的地位上出现。
*   **开源定义标准**：正如 Linux 定义了服务器操作系统，相信 R1 将成为 AI 时代推理接口的标准之一。

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=OWE0OWJhZDJhOWU1NjRhMDM4N2U0MzczM2EzOTUxMTJfdVZ2NnZyTkc4VU8yaTFEV1pJVVRnMWVuUWRUU1JSWHlfVG9rZW46QWV2TmIyUTZEb1pOZTN4THc0WGNqN1JhbjZjXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

结语：一场重新定义“智能”的旅程
----------------

（保存完最后一个DS案例的日志）看到R1设身处地的为我的粗糙的提示词思考时，我突然想起邱锡鹏教授的话：「强推理模型的终点是Agent」。而今天，我们正站在这个转折点上——这不是终结，而是新炼金时代的开始。

> 技术永远在追问：我们能否做得更优雅？  
> 而R1的回答是：让强化学习重新定义可能性的边界。

从 V3 到 R1，DeepSeek 完成了一条**基座赋能 → 纯** **RL** **验证 → 多阶段优化**的完整技术路径。这场演进不仅是算法的胜利，更是开源精神的体现——**它证明：最强的推理能力，可以诞生于开放协作的土壤。**

正如 DeepSeek 写道：

> **“** **AI** **的终极目标不是取代人类，而是让机器学会思考，人类学会协作。”**

![](https://nxgwqnqda8m.feishu.cn/space/api/box/stream/download/asynccode/?code=YmQzYTA1ZjlkMTY2NDMzMjdkNDJkYTA5NDhjZjc1YjFfS0FXTjVQQkozY3F6ZTlBSzZ2aU9kWGhsREV1TGRDcmVfVG9rZW46VDFlOWJRem1Lb25mUTV4blhUU2NwT0ZobnhXXzE3Mzg5MzY3NTc6MTczODk0MDM1N19WNA)

* * *

**参考资料**

*   Guo D, Yang D, Zhang H, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning\[J\]. arXiv preprint arXiv:2501.12948, 2025.
*   Liu A, Feng B, Xue B, et al. Deepseek-v3 technical report\[J\]. arXiv preprint arXiv:2412.19437, 2024.
*   Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022). React: Synergizing reasoning and acting in language models. _arXiv_ _preprint_ _arXiv:2210.03629_.
*   Muennighoff N, Yang Z, Shi W, et al. s1: Simple test-time scaling\[J\]. arXiv preprint arXiv:2501.19393, 2025.
*   \[DeepSeek-R1\] ([https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file](https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file))
*   \[大白话聊聊Deepseek R1背后的来龙去脉，必读\] ([https://mp.weixin.qq.com/s/je7BId1DZYaZZJHUShFpqA](https://mp.weixin.qq.com/s/je7BId1DZYaZZJHUShFpqA))
*   \[DeepSeek最强专业拆解来了，清交复教授超硬核解读\] ([https://mp.weixin.qq.com/s/LsMOIgQinPZBnsga0imcvA](https://mp.weixin.qq.com/s/LsMOIgQinPZBnsga0imcvA))

* * *

**（本文同步发布于\[遇健李的幸运\] ([https://www.cnblogs.com/li-jian-Lee](https://www.cnblogs.com/li-jian-Lee))，点击“阅读原文”直达技术博客）**