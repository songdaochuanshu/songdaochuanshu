---
layout: post
title: 'Concept Bottleneck Models-概念瓶颈模型用于可解释决策：进展、分类体系 与未来方向综述'
date: "2026-02-09T01:00:33Z"
---
Concept Bottleneck Models-概念瓶颈模型用于可解释决策：进展、分类体系 与未来方向综述
=======================================================

深度神经网络虽然表现出优异的性能，但其不透明性限制了其在需要透明度和人工监管的高风险领域中的应用。概念瓶颈模型(Concept Bottleneck Models, CBMs)通过引入一个人类可理解的概念层来连接输入与决策，从而解决了这一差距，实现了语义解释和测试时干预。本综述从四个维度提供了一个统一的CBMs概览：概念获取、基于概念的决策制定、概念干预和概念评估。我们总结了概念构建的演变过程，从人工标注到基于词典的挖掘、大语言模型(LLM)/视觉语言模型(VLM）引导的生成，以及通过原型和扩散模型实现的视觉关联发现；回顾了超越严格瓶颈的新兴CBM架构；并整合了强调忠实度、稀疏性和可干预性的评估与干预协议，这些对医疗保健等高风险领域尤为重要。我们综合了零散的文献，并勾勒了基于概念的可解释决策面临的关键挑战和未来方向。

深度神经网络虽然表现出优异的性能，但其不透明性限制了其在需要透明度和人工监管的高风险领域中的应用。概念瓶颈模型(Concept Bottleneck Models, CBMs)通过引入一个人类可理解的概念层来连接输入与决策，从而解决了这一差距，实现了语义解释和测试时干预。本综述从四个维度提供了一个统一的CBMs概览：概念获取、基于概念的决策制定、概念干预和概念评估。我们总结了概念构建的演变过程，从人工标注到基于词典的挖掘、大语言模型(LLM)/视觉语言模型(VLM）引导的生成，以及通过原型和扩散模型实现的视觉关联发现；回顾了超越严格瓶颈的新兴CBM架构；并整合了强调忠实度、稀疏性和可干预性的评估与干预协议，这些对医疗保健等高风险领域尤为重要。我们综合了零散的文献，并勾勒了基于概念的可解释决策面临的关键挑战和未来方向。

@article{Wang2026CBMSurvey,
  title   = {Concept Bottleneck Models for Explainable Decision Making: A Survey of Progress, Taxonomy, and Future Directions},
  author  = {Wang, Chunjiang and Li, Fan and Hu, Wenbo and Yan, Rui and Zhang, Kun and Zhou, Shaohua Kevin},
  journal = {ResearchGate Preprint},
  year    = {2026},
  doi     = {10.13140/RG.2.2.30356.16002},
  url     = {https://www.researchgate.net/publication/399898851\_Concept\_Bottleneck\_Models\_for\_Explainable\_Decision\_Making\_A\_Survey\_of\_Progress\_Taxonomy\_and\_Future\_Directions}
}

 This blog is from kkzhang at [https://www.cnblogs.com/lemonzhang/p/19592426](https://www.cnblogs.com/lemonzhang/p/19592426).

1 引言
====

深度神经网络在视觉、语言和多模态学习方面取得了强大的性能，使其在医疗保健\[1\]、医学\[2\]和金融\[3\]等现实世界中得到广泛采用。然而，它们的决策过程往往是不透明的，这在需要信任、问责制和人类监督的高风险环境中造成了风险\[4\]。这种性能与可解释性之间的差距推动了可解释AI (XAI)的发展，旨在使模型的推理过程对人类而言是可理解、可验证和可修正的。

概念瓶颈模型(CBMs)已成为解决这一挑战的一个原则性且有影响力的范式\[5\]。CBMs不是直接将输入映射到输出，而是通过在输入和决策之间引入一个人类可理解的中间概念层，显式地对预测过程进行因式分解。这种结构化的分解实现了语义解释，便于专家检查中间推理过程，并通过概念修正支持测试时干预。这些特性将CBMs与事后解释技术\[6\]区分开来，并将其定位为可解释和可控决策的统一框架。

早期的概念瓶颈模型依赖于手动定义和标注的概念（例如视觉属性或临床发现）来提供透明的中间接口，但受到标注成本、覆盖范围不全和标签噪声的限制\[5\]。最近的进展可以组织为基于概念推理的四个阶段：概念获取正从人工策展转向可扩展的词典挖掘、LLM/VLM引导的生成，以及通过原型或扩散模型实现的视觉接地发现，这些都得益于大规模基础模型\[7, 8, 9\]；基于概念的决策正从严格的瓶颈向软性、混合、概率和基于能量的设计演变，这些设计在保留概念接口的同时提高了预测能力\[10, 11, 12, 13\]；概念干预越来越多地支持结构化和感知依赖关系的修正，以便通过概念层更好地传播人类反馈\[14, 15\]；概念评估正从准确性扩展到以可解释性为中心的指标（例如忠实度、干预下的一致性、对噪声或缺失概念的鲁棒性），但在视觉接地、语义稳定性和与人类推理对齐方面仍面临挑战\[8\]。

尽管进展迅速，但CBM文献仍然是碎片化的。现有的综述\[16\]和评论通常侧重于属性学习、基于原型的解释或一般的可解释性方法，但它们并未捕捉到CBMs作为涵盖概念获取、决策架构、干预机制和评估协议的综合框架的更广泛演变。特别是，CBMs与基础模型、可编辑学习和多模态交互的近期融合尚未得到系统性的综合。

本综述的主要贡献总结如下： (1) 提出了涵盖概念获取、决策、干预和评估的CBMs统一分类法。 (2) 系统回顾了概念构建方法，从人工标注到LLM引导和基于原型的发现。 (3) 综合了具有不同瓶颈设计和可编辑程度的新兴CBM架构。 (4) 整合了评估协议和基准，重点关注忠实度、稀疏性、可干预性和医学应用。我们在[Awesome Concept Bottleneck Models](https://github.com/kkzhang95/Awesome_Concept_Bottleneck_Models)维护了一个包含代表性CBM论文、代码和基准的更新仓库，以支持复现和社区使用。

2 预备知识
======

2.1 概念瓶颈模型
----------

概念瓶颈模型(CBMs)\[5\]是一类可解释的神经架构，通过称为概念的人类可理解的中间变量，显式地将预测分解为两个阶段。形式上，CBM将从输入$x \\in \\mathcal{X}$到目标$y \\in \\mathcal{Y}$的映射因式分解为：

\\begin{equation}  
x \\;\\rightarrow\\; c \\;\\rightarrow\\; y,  
\\end{equation} 其中$c = (c\_1, \\ldots, c\_K) \\in \\mathcal{C}^K$表示一组可解释的概念，如视觉属性、解剖结构、病理发现或语义描述。

一个典型的CBM由两个组件组成：概念预测器$f\_{\\theta}: \\mathcal{X} \\rightarrow \\mathcal{C}^K$，它将原始输入映射到概念激活；以及任务预测器$g\_{\\phi}: \\mathcal{C}^K \\rightarrow \\mathcal{Y}$，它仅基于预测的概念产生最终预测。整体模型可以写为：

\\begin{equation}  
\\hat{y} = g\_{\\phi}(f\_{\\theta}(x)).  
\\end{equation}

这种结构实现了CBMs的两个决定性能力：(i) 语义可解释性，因为每个概念维度都与人类可识别的属性对齐；(ii) 可干预性，因为用户可以在测试时检查或修改以纠正推理错误。这些特性激发了本综述稍后讨论的一系列架构变体，包括放宽严格瓶颈约束、结合概念间结构化依赖关系或利用多模态基础模型等外部知识的模型。

 

图1 CBM流程概览。该图展示了概念是如何被获取、整合至决策制定、进行干预以及评估，从而生成可解释预测的。

2.2 分类法
-------

图1展示了概念瓶颈模型的四部分分类法，而表1总结了这些维度上的代表性方法。我们区分了四个主要维度：(1) 概念获取，关注如何构建人类可解释的概念，范围从专家标注到自动发现和语言诱导的词汇表；(2) 基于概念的决策，指定概念层如何中介预测，包括严格瓶颈以及松弛或混合设计；(3) 概念干预，研究人类或外部系统如何在推理时与概念交互以纠正或指导推理；(4) 概念评估，评估以可解释性为导向的属性，如忠实度、稀疏性和干预下的鲁棒性。

重要的是，越来越多的工作跨越了这些维度，例如在一个架构中联合集成概念获取、决策和干预。因此，我们依次回顾每个维度并强调代表性方法。

表1 涵盖概念获取、决策、干预和评估的CBM方法的统一分类体系。概念来源：人工标注(manual)、词典(dict.)、视觉原型(proto.)、大型语言模型(LLM)；概念空间粒度(Gran.)：概念与视觉证据对齐的空间层级：全局(global)、局部(local)或两者；概念监督(Sup.)：强(strong)、弱(weak)、无监督(unsup.)；概念瓶颈层(Bottleneck)：硬(hard)、软(soft)、混合(hybrid)；概念相关性建模(corr.)：独立(indep.)、联合(joint)、图(graph)、层级(hier.)、因果(causal)；干预机制：数值调整(scalar)、参数化(param.)、掩码/注意力编辑(spatial)、自然语言对话(lang.)；干预策略：随机/人工选择(vanilla)、模型驱动建议(active)、修正传播(prop.)；代码：Link（链接）、– (未发布)。

3 概念获取
======

概念构建是概念瓶颈模型的基础，定义了输入和预测之间的人类可解释接口。概念构建已经从完全的人工标注演变为基于词典的挖掘、LLM/VLM引导的生成和视觉原型发现。总体趋势是在提高概念质量、可扩展性和视觉接地的同时减少人工监督。本节回顾四种代表性范式及其核心机制。

3.1 基于人工标注的概念
-------------

人工标注通过依赖专家定义和显式标记的概念（如视觉属性或临床发现）提供了最强的语义接地。这种范式确保了高可解释性以及与领域知识的清晰对齐，但从根本上受到标注成本、不完全的概念覆盖和标签噪声的限制。

该范式最近的工作侧重于在不完美监督下提高鲁棒性和灵活性。一个研究方向是通过修改训练目标来解决噪声或缺失标注的问题，例如CPO\[17\]，它引入基于偏好的优化来联合稳定概念预测和下游性能。一个互补的方向是对标注概念间的依赖关系进行建模，以减轻泄漏和表达能力的限制。Havasi等人\[18\]用潜在侧信道变量增强了人工标注的概念，并采用自回归预测器来捕捉概念间的结构。为了支持实际部署，Editable CBMs\[19\]支持使用基于影响函数的更新对概念定义和数据集标签进行事后修正，而无需完全重新训练。尽管有这些进展，人工标注仍然难以扩展到复杂领域，且其对预定义概念集的依赖限制了对不断变化任务的适应性。

3.2 基于词典的概念构建
-------------

基于词典的方法用从预定义词汇表（如形容词列表或医学描述符）中的可扩展发现取代了特定任务的人工定义。通过将概念空间限制为语言上有意义的单元，与全自动生成相比，这些方法提供了更好的可控性和透明度。

代表性方法侧重于从大型词库中过滤出视觉接地的概念。V2C-CBM\[20\]构建了一个n-gram候选池，并使用基于CLIP的视觉\-语言相似度来移除与视觉无关或接地较弱的条目，产生紧凑且可解释的概念集。OpenCBM\[9\]进一步将可训练的视觉特征与CLIP嵌入对齐，并引入了发现缺失概念的机制，从而提高了超出初始词典的覆盖范围。

虽然基于词典的构建降低了标注成本并限制了幻觉风险，但其表达能力仍然受到预定义词汇表的限制，可能无法捕捉细粒度或特定任务的语义。

3.3 LLM/VLM引导的概念生成
------------------

LLM和VLM引导的概念生成通过利用大型语言模型产生多样化、高层次且通常是分层的语义，大幅扩展了概念空间。这种范式提供了强大的可扩展性和灵活性，但也引入了与幻觉、弱视觉接地和语义不稳定性相关的新挑战。

最近的工作通过结构、对齐和统计正则化来解决这些问题。Med-MICN\[21\]将概念组织成多层级结构，并采用门控机制来控制抽象程度。视觉\-语言一致性约束\[22, 23, 24\]和提示级对齐策略\[25\]进一步提高了接地的可靠性。从监督角度来看，Label-free CBMs和FCBM\[26, 27\]证明，只要施加了强大的视觉正则化，通过将LLM与CLIP结合，可以在没有显式标注的情况下诱导出可用概念。

总体而言，LLM/VLM引导的概念获取显著降低了人类监督要求，但其可解释性最终取决于语言先验的可靠性和接地约束的有效性。

3.4 基于视觉原型的概念发现
---------------

基于视觉原型的方法通过提取代表性区域、部件或解缠组件，直接从图像空间学习概念，产生强大的视觉接地和空间定位。这种范式对于医学成像和其他形态学证据对决策至关重要的领域特别有吸引力。

基于原型的发现面临固有的挑战，包括不稳定的语义命名、显著性驱动的虚假相关性以及原型粒度与覆盖范围之间的权衡。最近的工作探索了几种设计模式来减轻这些限制。面向解缠的方法，如HU-MCD、LCBM\[28, 29\]，学习多维概念因子以减少语义未指明性。面向部署的方法\[30, 31\]专注于从预训练模型中提取原型词典以实现事后可解释性。面向交互的设计\[32, 33\]显式地将原型与空间区域关联，支持区域级的检查和干预。

尽管接地保真度有所提高，但在数据集之间实现视觉原型的稳定语义对齐和可扩展重用仍然是一个开放的挑战。

**总结。** CBMs中的概念获取涵盖了从专家定义的语义到自动发现的范围，反映了语义精度、监督成本和鲁棒性之间的权衡，并激发了整合语言和视觉线索的混合设计。

4 基于概念的决策
=========

概念瓶颈模型(CBMs)通过可解释概念层中介预测来提高可解释性，使验证和干预成为可能。除了概念构建，CBM架构也在不断演变以增强灵活性和可部署性。本节回顾三个关键方向：瓶颈设计、概念监督和模型适应，强调了从硬性流程向自适应、与人类对齐的决策转变。

4.1 瓶颈设计
--------

CBMs的一项核心创新在于概念瓶颈如何通过中介输入和输出之间的信息流来管理决策过程。早期的设计强制执行严格的流程，即所有预测信号必须通过离散的、可解释的概念层，从而最大化透明度和可干预性。然而，这种刚性结构往往限制了表示能力，并在不完美的概念监督下损害性能。

为了解决这些限制，引入了软瓶颈\[33, 34\]，实现了连续的概念激活以及概念和任务预测器的联合优化。随后的变体通过残差通路\[49\]和结构化依赖关系放宽了严格瓶颈，包括自回归序列\[15\]、图先验\[61, 52\]和随机潜变量\[57\]。Energy-based CBMs\[60\]进一步在联合概率框架内统一了预测和干预。更近期的工作将更高层次的结构注入瓶颈，例如用于鲁棒干预的因果图\[6\]，用于减轻概念\-标签失真的概念解耦\[64\]，以及用于在复杂设置中超越线性聚合的可微逻辑组合\[58, 13\]。

这些进展共同反映了严格瓶颈向更具表达力和结构化架构的逐渐放松，使CBMs能够在复杂的现实环境中更好地平衡可解释性和预测能力，同时保留概念层作为解释和干预的对齐语义接口。

4.2 概念监督
--------

概念监督定义了CBMs的中间语义层是如何构建和接地的。虽然早期模型依赖于具有强领域语义的人工标注概念\[5\]，但这种监督通常成本高昂、稀疏或特定于领域。因此，最近的研究寻求可扩展到更广泛任务且同时保留语义可解释性的替代方案。

Post-hoc CBMs\[44\]通过在冻结的编码器之上学习概念预测器来绕过架构约束，无需重新训练即可恢复概念可解释性。基于探测的方法\[45\]和原型发现方法\[32, 30\]使用稀疏正则化或基于部件的分解从预训练特征中提取可解释单元。利用视觉\-语言模型，基于CLIP的流程\[46, 26, 47\]将概念标签与文本提示对齐，实现开放词汇监督。LLM引导的策略\[21, 22, 48, 49\]更进一步，通过生成分层或特定任务的概念，尽管视觉接地和幻觉方面的挑战仍然存在。

这些方法将概念监督从直接标注转变为基于对齐和提示的归纳，实现了可扩展和自适应的概念构建。这种演变保留了CBMs与人类对齐的语义，同时提高了跨任务和领域的灵活性。

4.3 模型适应
--------

为了使CBMs在现实世界和不断变化的环境中具有实用价值，它们必须支持动态适应。具有固定概念的静态流程在任务需求转变、新概念出现或需要用户修正时难以更新。因此，模型适应已成为可操作CBMs的一个关键前沿。

Editable CBMs\[19\]引入了基于影响函数的参数更新，允许对概念、实例或标签预测进行局部更改，而无需完全重新训练。这允许在保留模型整体结构的同时进行细粒度的修正和数据集更新。概率能量模型\[13\]通过将推理公式化为输入、概念和输出上的能量最小化来支持测试时修正。更具交互性的是，最近的方法如Chat-CBM\[50\]和SALF\[51\]扩展了模型接口以支持自然语言和基于空间区域的干预，使非技术用户也能进行适应。无需训练的测试时适应\[52, 53\]进一步使CBMs能够仅使用少量图像级标签处理概念级分布偏移，而无需重新训练或概念监督。

这些进展标志着从静态可解释性向动态、人在回路决策的转变。适应性CBMs保留了其核心语义瓶颈，同时允许部署后编辑、修正和协作，扩展了其在现实世界高风险系统中的生存能力。

**总结。** CBMs中的基于概念的决策已从严格的瓶颈管道演变为更灵活的架构，更好地平衡了可解释性和性能。瓶颈设计、监督和模型适应方面的进步将概念层定位为用于预测和干预的动态接口。

5 概念干预
======

概念干预通过修正中间概念实现人类与AI的协作，将CBMs从被动的可解释模型转变为交互式系统。在Koh等人\[5\]的开创性工作中，干预被公式化为允许用户修改概念激活值的测试时编辑。虽然具有基础意义，但这种范式依赖于强独立性假设，产生高昂的人力成本，并支持有限的交互模式。

5.1 概念相关性
---------

早期的干预机制通常假设概念是独立的，这意味着修改一个概念不会影响其他概念。这种假设在现实世界设置中通常是不现实的，因为概念在复杂决策任务中实际上表现出很强的语义和统计依赖性。为了解决这一限制，相关性感知的干预方法显式地对概念间关系进行建模并据此传播修正。

Havasi等人\[18\]提出了自回归概念预测器，允许对前概念的干预通过序列依赖建模影响后续预测。Stochastic CBMs\[39\]通过将概念Logits建模为多元正态分布，进一步提高了效率，实现了通过学习到的协方差结构即时传播单个干预。Energy-Based CBMs\[13\]采用输入、概念和输出上的联合能量公式，允许概念修正通过能量最小化进行传播。Singhi等人\[38\]引入了事后概念重对齐模块(CIRM)，根据学习到的统计相关性更新未干预的概念。

虽然这些方法显著提高了干预的一致性和效率，但它们关键地依赖于学习到的相关性的质量。如果捕捉到虚假依赖关系，干预可能会传播错误而不是修正，这突显了一个重要的开放挑战。

5.2 概念定位
--------

确定干预哪些概念仍然是实际部署中的主要瓶颈，因为人工选择是劳动密集的且对认知具有高要求的。为了减轻这一负担，最近的工作已转向模型引导的定位策略，优先考虑高影响力的干预目标。

一些方法将定位内化到训练或模型设计中。Espinosa Zarlenga等人\[10\]引入了感知干预的训练，在学习过程中模拟测试时干预，使模型能够推荐需要修正的概念。Evidential CEM(evi-CEM)\[54\]将概念预测建模为Beta分布，利用认知不确定性指导对模糊概念的干预。其他方法依赖于事后分析或辅助结构。Shin等人\[14\]提出了基于不确定性的概念潜力(UCP)和概念预测损失(LCP)等指标来对干预优先级进行排序。Shen等人\[55\]进一步引入了FIGS-BD，将预测器蒸馏为贪婪求和树，以识别具有高贡献方差的概念组。

5.3 概念交互
--------

除了选择和修正概念值之外，最近的工作越来越多地探索更丰富的交互模式，这些模式对人类用户来说更直观\[56\]。Chat-CBM\[50\]用大语言模型替换基于概念的预测器，允许用户通过自然语言对话进行干预。虽然这在实践中提高了可用性，但可能会削弱传统CBMs的严格可解释性保证。

在视觉领域，Concept-based Similarity Reasoning (CSR)\[33\]通过允许用户绘制边界框来引导模型注意力，实现了空间交互。然而，这种空间引导并没有显式地解耦不同概念间的干预。Spatially-Aware and Label-Free (SALF) CBM\[51\]通过利用空间概念图解决了这一限制，使用户能够指定感兴趣的区域并选择性地调节该区域内单个概念的激活。除了判别模型，概念瓶颈生成模型将概念级交互扩展到GANs、VAEs和扩散模型，实现了生成的可解释引导和调试\[57\]。

**总结。**概念干预将CBMs从静态解释模型转变为交互式的人在回路系统。通过建模概念相关性、实现干预目标的有效定位以及支持更丰富的交互模式，最近的进展提高了概念级修正的有效性和可用性。

6 概念瓶颈模型的评估
===========

概念瓶颈模型引入了可解释和可干预的中间概念，这要求评估协议超越下游任务性能。因此，现有的评估框架评估三个核心维度：概念忠实度、概念稀疏性和可干预性，从而捕捉语义有效性、认知易处理性和人在回路的可控性。

6.1 概念忠实度
---------

概念忠实度衡量学习到的瓶颈表示是否与人类可理解的语义对齐，这是实践中可靠解释和有效干预的前提。评估策略已从直接标签匹配演变为对内在结构和外部接地的更严格评估。

**标签对齐。**一种常见的方法是使用标准指标（如准确率和F1分数）评估预测概念与真实标注之间的一致性。然而，在高维和稀疏概念空间中，这些指标可能由真负样本主导。为了减轻这种偏差，最近的工作强调使用Jaccard相似度\[58\]和概念存在度量(CEM)\[59\]等指标进行主动概念评估，这些指标侧重于被正向激活的概念。

**内在表示质量。**除了标签匹配外，有效的概念表示应具有连贯的拓扑结构和明显的可分离性。在基于嵌入的模型中，概念对齐分数(CAS)\[10\]解决了的前者的评估，验证高维空间中的局部邻域是否保持语义同质性。作为补充，为了在基于标量的瓶颈中解决的后者的评估，Gap\[60, 61\]通过测量正负激活分布之间的散度来评估潜在空间的可分性，从而量化表示的内在判别质量。

**语义和视觉接地。**为了检测虚假相关性，进一步的评估将学习到的概念与外部语义或视觉参考对齐。语义接地指标，包括CGIM\[59\]、Similarity \[36\]和概念纯度\[11\]，评估与人类定义的重要性或与规范嵌入的一致性。视觉接地指标，如概念可信度分数\[32\]和CLM\[59\]，量化概念激活与标注区域之间的空间对齐。虽然在实践中有效，但这些方法通常需要昂贵的标注；基于VLM的替代方案如概念\-图像相关性\[11\]减少了监督，但引入了潜在的幻觉风险。

6.2 概念稀疏性
---------

概念稀疏性评估CBMs是否依赖于一组最小且认知上可管理的概念。过度密集的瓶颈破坏了可解释性并增加了信息泄漏的风险。稀疏度\[58\]和有效概念数(NEC)\[62\]等指标量化了激活密度和推理复杂性。然而，稀疏性必须与任务性能联合考虑。为了捕捉这种权衡，概念利用效率(CUE)\[36\]和概念高效准确率(CEA)\[63\]等复合指标在预测准确率与概念使用之间进行平衡，其中CEA提供了一个有界的、文本不变的公式以进行稳健比较。

6.3 可干预性
--------

可干预性反映了CBMs在推理时支持有效人类修正的能力。此类别的评估协议评估模型预测对概念级修正的响应能力以及干预策略的效率。测试时干预(TTI)曲线\[5\]及其曲线下面积变体\[64\]被广泛用于总结在增加干预预算下的性能增益。扩展这一分析，最近的工作检查了相关概念间的修正传播\[39\]，并通过经验用户时间\[60\]或理论复杂性\[14, 65\]显式地建模干预成本。

**总结。**评估CBMs需要超越下游准确率的整体协议。概念忠实度确保语义有效性，概念稀疏性保持认知易处理性，可干预性量化人在回路修正的有效性和成本。这些维度共同构成了评估CBMs可解释性、可靠性和实用性的原则基础。

7 基准
====

表2总结了用于评估一般和医疗领域概念瓶颈模型(CBMs)的代表性基准。这些数据集在概念标注来源、监督粒度和对概念级干预的支持方面存在显著差异，这反过来决定了CBMs的哪些方面（如忠实度、可扩展性或可干预性）可以得到可靠评估。与深度模型不同，CBMs依赖于基准设计来验证概念忠实度和干预有效性。

**一般领域数据集。**一般领域基准在视觉和文本识别任务上评估CBMs，包括细粒度分类、零样本学习和NLP应用。一个关键区别在于概念监督。具有人工标注概念的数据集，如CUB-200-2011和AWA2，支持概念评估和测试时干预，使概念修正的因果分析成为可能。相比之下，依赖LLM或VLM生成概念的大规模基准评估可扩展性和开放词汇能力，但缺乏经过验证的真实值，因此无法支持基于干预的评估。

**医疗领域数据集。**医疗基准强调临床上有意义的概念和安全关键的可解释性，使其特别适合评估概念忠实度和干预有效性。具有专家标注、有序或连续概念的数据集显式支持干预实验，并能够验证现实临床设置中人在回路的修正。依赖自动生成概念的大型医疗数据集提高了覆盖范围和可扩展性，但缺乏经过验证的概念标注通常限制了它们进行严格干预和因果分析的适用性。

 表2 通用和医疗领域中评估概念瓶颈模型(CBMs)的代表性基准

8 总结与未来方向
=========

概念瓶颈模型通过将预测建立在人类可理解的概念之上，为可解释和可控的决策提供了一个原则性框架。本综述统一了涵盖概念获取、决策、干预和评估全流程的CBM研究，特别强调了高风险医疗应用。尽管取得了实质性进展，但仍存在若干基本挑战。

CBMs的未来研究方向包括：**(1)** **自适应概念获取。**未来的CBMs应超越固定的瓶颈设计，转向自适应和结构化的概念表示，如分层、组合或稀疏激活的瓶颈。这种设计更好地反映了人类语义的多层次和任务依赖性质，同时提高了效率。 **(2)** **忠实可靠的概念。**确保学习到的概念忠实地对应其预期语义仍然是一个核心挑战，特别是在弱监督下或概念源自LLMs或VLMs时。有前景的方向包括不确定性感知建模、减轻虚假相关性的因果表示学习，以及检测或纠正不稳定概念的机制。 **(3)** **结构化概念干预。**除了直接的概念替换，未来的工作应探索结构化干预策略，考虑概念间的依赖关系并识别决策关键子集。支持部分或软干预对于减少工作量并在安全关键领域实现可扩展的专家在环部署至关重要。 **(4)** **鲁棒的概念评估。**鲁棒评估对于可靠部署仍然是必要的。未来的研究应审查概念表示和干预效果如何在跨领域和偏移中泛化，以及基于概念的推理如何支持不确定性估计、分布外检测和感知干预的评估协议。

参考文献
====

\[1\] Andre Esteva, Alexandre Robicquet, et al. A guide to deep learning in healthcare. In Nat. Med., 2019.

\[2\] Berthine Nyunga Mpinda, Mehran Hosseinzadeh, et al. Towards multi-label concept bottleneck models in medical imaging: An exploratory survey. In Medical Imaging with Deep Learning-Validation Papers, 2026.

\[3\] Ahmet Murat Ozbayoglu, Mehmet Ugur Gudelek, et al. Deep learning for financial applications: A survey. In Appl. Soft Comput., 2020.

\[4\] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. In Artif. Intell, 2019.

\[5\] Pang Wei Koh, Thao Nguyen, et al. Concept bottleneck models. In ICML, 2020.

\[6\] Ramprasaath R Selvaraju, Michael Cogswell, et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017.

\[7\] Alec Radford, Jong Wook Kim, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.

\[8\] Yue Yang, Artemis Panagopoulou, et al. Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. In CVPR, 2023.

\[9\] Andong Tan, Fengtao Zhou, et al. Explain via any concept: Concept bottleneck model with open vocabulary concepts. In ECCV, 2024.

\[10\] Mateo Espinosa Zarlenga, Pietro Barbiero, et al. Concept embedding models: Beyond the accuracy-explainability trade-off. In NeurIPS, 2022.

\[11\] Yang Liu, Tianwei Zhang, et al. Hybrid concept bottleneck models. In CVPR, 2025.

\[12\] Eunji Kim, Dahuin Jung, et al. Probabilistic concept bottleneck models. In ICML, 2023.

\[13\] Xinyue Xu, Yi Qin, et al. Energy-based concept bottleneck models: Unifying prediction, concept intervention, and probabilistic interpretations. In ICLR. 2024.

\[14\] Sungbin Shin, Yohan Jo, et al. A closer look at the intervention procedure of concept bottleneck models. In ICML, 2023.

\[15\] Sujin Jeon, Inwoo Hwang, et al. Locality-aware concept bottleneck model. In UniReps. 2024.

\[16\] Junlin Hou, Sicen Liu, et al. Self-explainable ai for medical image analysis: A survey and new outlooks. In arXiv preprint arXiv:2410.02331, 2024.

\[17\] Emiliano Penaloza, Tianyue H Zhang, et al. Addressing concept mislabeling in concept bottleneck models through preference optimization. In ICML, 2025.

\[18\] Marton Havasi, Sonali Parbhoo, et al. Addressing leakage in concept bottleneck models. In NeurIPS, 2022.

\[19\] Lijie Hu, Chenyang Ren, et al. Editable concept bottleneck models. In ICML, 2025.

\[20\] Hangzhou He, Lei Zhu, et al. V2c-cbm: Building concept bottlenecks with vision-to-concept tokenizer. In AAAI, 2025.

\[21\] Lijie Hu, Songning Lai, et al. Towards multi-dimensional explanation alignment for medical classification. In NeurIPS, 2024.

\[22\] Songning Lai, Lijie Hu, et al. Faithful vision-language interpretation via concept bottleneck models. In ICLR, 2023.

\[23\] Sukrut Rao, Sweta Mahajan, et al. Discover-then-name: Task-agnostic concept bottlenecks via automated concept discovery. In ECCV, 2024.

\[24\] Simon Schrodi, Julian Schur, et al. Selective concept bottlenecks without predefined concepts. In TMLR, 2025.

\[25\] Yequan Bie, Luyang Luo, et al. Xcoop: Explainable prompt learning for computer-aided diagnosis via concept-guided context optimization. In MICCAI, 2024.

\[26\] Tuomas Oikarinen, Subhro Das, et al. Label-free concept bottleneck models. In ICLR. 2023.

\[27\] Xingbo Du, Qiantong Dou, et al. Flexible concept bottleneck model. In AAAI, 2025.

\[28\] Arne Grobrügge, Niklas Kühl, et al. Towards human-understandable multi-dimensional concept discovery. In CVPR, 2025.

\[29\] Sujin Jeon, Inwoo Hwang, et al. Locality-aware concept bottleneck model. In UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models, 2024.

\[30\] Andong Tan, ZHOU Fengtao, et al. Post-hoc part-prototype networks. In ICML, 2024.

\[31\] Mohammad Amin Choukali, Mehdi Chehel Amirani, et al. Pseudo-class part prototype networks for interpretable breast cancer classification. In Sci. Rep, 2024.

\[32\] Qihan Huang, Jie Song, et al. On the concept trustworthiness in concept bottleneck models. In AAAI, 2024.

\[33\] Ta Duc Huy, Sen Kim Tran, et al. Interactive medical image analysis with concept-based similarity reasoning. In CVPR, 2025.

\[34\] Anita Mahinpei, Justin Clark, et al. Promises and pitfalls of black-box concept learning models. In arXiv preprint arXiv:2106.13314, 2021.

\[35\] Andrei Margeloiu, Matthew Ashman, et al. Do concept bottleneck models learn as intended? In ICLR Workshop on Responsible AI, 2021.

\[36\] Chenming Shang, Shiji Zhou, et al. Incremental residual concept bottleneck models. In CVPR. 2024.

\[37\] Haotian Xu, Tsui-Wei Weng, et al. Graph concept bottleneck models. In arXiv preprint arXiv:2508.14255, 2025.

\[38\] Nishad Singhi, Jae Myung Kim, et al. Improving intervention efficacy via concept realignment in concept bottleneck models. In ECCV, 2024.

\[39\] Moritz Vandenhirtz, Sonia Laguna, et al. Stochastic concept bottleneck models. In NeurIPS, 2024.

\[40\] Giovanni De Felice, Arianna Casanova, et al. Causally reliable concept bottleneck models. In ICLR 2025 Workshop: XAI4Science: From Understanding Model Behavior to Discovering New Scientific Knowledge, 2025.

\[41\] Rui Zhang, Xingbo Du, et al. The decoupling concept bottleneck model. In TPAMI, 2024.

\[42\] Deepika SN Vemuri, Gautham Bellamkonda, et al. Logiccbms: Logic-enhanced concept-based learning. In WACV, 2026.

\[43\] Yibo Gao, Hangqi Zhou, et al. Learning concept-driven logical rules for interpretable and generalizable medical image classification. In MICCAI, 2025.

\[44\] Mert Yuksekgonul, Maggie Wang, et al. Post-hoc concept bottleneck models. In ICLR, 2023.

\[45\] Andrei Semenov, Vladimir Ivanov, et al. Sparse concept bottleneck models: Gumbel tricks in contrastive learning. In arXiv preprint arXiv:2404.03323, 2024.

\[46\] Rémi Kazmierczak, Eloïse Berthier, et al. Clip-qda: An explainable concept bottleneck model. In TMLR, 2024.

\[47\] Jiakai Lin, Jinchang Zhang, et al. Graph integrated multimodal concept bottleneck model. In arXiv preprint arXiv:2510.00701, 2025.

\[48\] Yunhe Gao, Difei Gu, et al. Aligning human knowledge with visual concepts towards explainable medical image classification. In MICCAI, 2024.

\[49\] Chunjiang Wang, Kun Zhang, et al. Mvp-cbm: Multi-layer visual preference-enhanced concept bottleneck model for explainable medical image classification. In IJCAI, 2025.

\[50\] Hangzhou He, Lei Zhu, et al. Chat-cbm: Towards interactive concept bottleneck models with frozen large language models. In arXiv preprint arXiv:2509.17522, 2025.

\[51\] Itay Benou and Tammy Riklin Raviv. Show and tell: Visually explainable deep neural nets via spatially-aware concept bottleneck models. In CVPR, 2025.

\[52\] Townim F Chowdhury, Vu Minh Hieu Phan, et al. Adacbm: An adaptive concept bottleneck model for explainable and accurate diagnosis. In MICCAI, 2024.

\[53\] Hangzhou He, Jiachen Tang, et al. Training-free test-time improvement for explainable medical image classification. In MICCAI, 2025.

\[54\] Yibo Gao, Zheyao Gao, et al. Evidential concept embedding models: Towards reliable concept explanations for skin disease diagnosis. In MICCAI, 2024.

\[55\] Matthew Shen, Aliyah R Hsu, et al. Adaptive test-time intervention for concept bottleneck models. In ICLR 2025 Workshop Building Trust, 2025.

\[56\] David Steinmann, Wolfgang Stammer, et al. Learning to intervene on concept bottlenecks. In ICML, 2024.

\[57\] Aya Abdelsalam Ismail, Julius Adebayo, et al. Concept bottleneck generative models. In ICLR, 2024.

\[58\] Konstantinos P Panousis, Dino Ienco, et al. Coarse-to-fine concept bottleneck models. In NeurIPS, 2024.

\[59\] Halil Ibrahim AYSEL, Xiaohao Cai, et al. Concept-based explainable artificial intelligence: Metrics and benchmarks. In Available at SSRN 5210908. 2025.

\[60\] Nesta Midavaine, Gregory Hok Tjoan Go, et al. \[re\] on the reproducibility of post-hoc concept bottleneck models. In TMLR, 2024.

\[61\] Seonghwan Park, Jueun Mun, et al. An analysis of concept bottleneck models: Measuring, understanding, and mitigating the impact of noisy annotations. In NeurIPS, 2025.

\[62\] Divyansh Srivastava, Ge Yan, et al. Vlg-cbm: Training concept bottleneck models with vision-language guidance. In NeurIPS, 2024.

\[63\] Delong Zhao, Qiang Huang, et al. Partially shared concept bottleneck models. In AAAI, 2026.

\[64\] Mateo Espinosa Zarlenga, Katie Collins, et al. Learning to receive help: Intervention-aware concept embedding models. In NeurIPS, 2023.

\[65\] Andrea Pugnana, Riccardo Massidda, et al. Deferring concept bottleneck models: Learning to defer interventions to inaccurate experts. In NeurIPS. 2025.