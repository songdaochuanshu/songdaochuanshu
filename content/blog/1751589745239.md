---
layout: post
title: '支持向量机（SVM）分类'
date: "2025-07-04T00:42:25Z"
---
支持向量机（SVM）分类
============

  支持向量机（Support Vector Machine，SVM）是一种经典的监督学习算法，主要用于分类任务，也可扩展到回归问题（称为支持向量回归，SVR）。其核心思想是通过寻找一个最优超平面，最大化不同类别数据之间的间隔（Margin），从而实现高效分类。

一、核心思想
------

  SVM的目标是找到一个决策边界（超平面），将不同类别的数据分开，并确保该边界到最近数据点（支持向量）的距离最大。这种“最大化间隔”的策略使得模型具有更好的泛化能力。

### 超平面（Hyperplane）：

  在n维空间中，一个超平面是n-1维的子空间。对于二维数据，超平面是一条直线；三维数据中是一个平面。

### 支持向量（Support Vectors）：

  距离最优超平面最近的样本点称为支持向量，它们是决定超平面位置的关键样本。其他样本的位置对超平面无影响，这也是“SVM”名称的由来。

### 间隔（Margin）：

  超平面到两类最近支持向量的距离之和。SVM的目标是最大化间隔。  
  设超平面方程为\\(w\\cdot x+b=0\\)（其中\\(w\\)是权重向量，\\(b\\)是偏置），则单个样本点\\(x\_i\\)到超平面的距离为：

\\\[距离=\\frac{\\left| w\\cdot x\_i+b \\right|}{\\left| \\left| w \\right| \\right|} \\\]

  最优超平面需满足：对于正类样本，有\\(w\\cdot x\_i+b\\geq1\\)；对于负类样本，有\\(w\\cdot x\_i+b\\leq-1\\) 。此时，间隔为 \\(\\frac{2}{\\left| \\left| w \\right| \\right|}\\)，最大化间隔等价于最小化\\(\\left| \\left| w \\right| \\right|^{2}\\)。

二、线性可分情况（硬间隔SVM）
----------------

  假设数据线性可分，SVM的优化问题可表示为

    \\(\\min\_{w,b}{\\frac{1}{2}\\left| \\left| w \\right| \\right|^{2}}\\)   s.t. \\(y\_i(w\\cdot x\_i+b)\\geq1 \\quad (\\forall i)\\)

  目标：最小化\\(\\left| \\left| w \\right| \\right|\\)（等价于最大化间隔\\(\\frac{2}{\\left| \\left| w \\right| \\right|}\\)）。  
  约束：确保所有样本被正确分类且位于间隔边界之外。

三、非线性可分情况（软间隔SVM）
-----------------

  当样本无法被线性超平面分隔时，SVM 通过以下方法处理：

### 1\. 引入松弛变量（Slack Variables）

  允许部分样本跨越超平面，但需在优化目标中加入惩罚项（即正则化参数\\(C\\)），平衡间隔最大化和分类错误最小化

    \\(\\min\_{w,b}{\\frac{1}{2}\\left| \\left| x \\right| \\right|^{2}}+C\\sum\_{i}{\\xi\_i}\\)   s.t. \\(y\_i(w\\cdot x\_i+b)\\geq 1-\\xi\_i,\\quad \\xi\_i\\geq0\\)

  \\(C\\)的作用：控制分类错误的惩罚力度。\\(C\\)越大，模型越严格（可能过拟合）；\\(C\\)越小，允许更多错误（可能欠拟合）。

### 2\. 核技巧（Kernel Trick）

  对于非线性可分数据，SVM通过核函数将原始空间映射到高维特征空间，使数据在新空间中线性可分。常见核函数有  
  线性核：\\(K(x\_i,x\_j)=x\_i\\cdot x\_j\\)  
  多项式核：\\(K(x\_i,x\_j)=(x\_i\\cdot x\_j+c)^{d}\\)  
  高斯径向基核（RBF）：\\(K(x\_i,x\_j)=exp(-\\gamma \\left| \\left| x\_i-x\_j \\right| \\right|^{2})\\)  
  Sigmoid核： \\(K(x\_i,x\_j)=tanh(\\alpha x\_i\\cdot x\_j+c)\\)

四、优化与求解
-------

  SVM通常转化为对偶问题，利用拉格朗日乘子法求解：

    \\(max\_{\\alpha}{\\sum\_{i}{\\alpha\_i}}-\\frac{1}{2}\\sum\_{i,j}{\\alpha\_i\\alpha\_jy\_iy\_jK(x\_i,x\_j)}\\)  s.t. \\(0\\leq\\alpha\_i\\leq C,\\sum\_{i}{\\alpha\_iy\_i=0}\\)

  通过拉格朗日对偶性转化为对偶问题，优势在于：

    a) 将高维空间中的内积运算转化为核函数计算（避免直接处理高维数据）；  
    b) 解的形式仅依赖于支持向量，计算效率更高。

五、Python实现示例
------------

    from sklearn import datasets
    from sklearn.model_selection import train_test_split
    from sklearn.svm import SVC
    from sklearn.metrics import accuracy_score
    
    # 加载鸢尾花数据集
    iris = datasets.load_iris()
    X = iris.data  # 特征
    y = iris.target  # 标签
    
    # 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # 创建SVM分类器
    clf = SVC(kernel='linear')  # 使用线性核函数
    
    # 训练模型
    clf.fit(X_train, y_train)
    
    # 预测
    y_pred = clf.predict(X_test)
    
    # 评估模型
    accuracy = accuracy_score(y_test, y_pred)
    print(f"模型准确率: {accuracy:.2f}")
    
    # 预测新样本
    new_samples = [[5.1, 3.5, 1.4, 0.2], [6.3, 3.3, 4.7, 1.6]]
    predictions = clf.predict(new_samples)
    print(f"新样本预测结果: {[iris.target_names[p] for p in predictions]}")
    
    
    

  
  

_**End.**_