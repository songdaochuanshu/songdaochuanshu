---
layout: post
title: '直线思维的进化：线性到广义线性'
date: "2025-04-02T00:39:30Z"
---
直线思维的进化：线性到广义线性
===============

在数据科学领域，**线性模型**和**广义线性模型**是两种基础且重要的统计工具，

它们被广泛应用于各种预测和分析任务中，从简单的回归问题到复杂的分类场景。

今天，让我们深入探讨这两种模型，了解它们的原理、区别以及实际应用。

1\. 线性模型：统计分析的基石
================

**线性模型**是统计学中最早被提出和广泛应用的一类模型。

其基本思想是假设**因变量**（响应变量）与**自变量**（解释变量）之间存在线性关系。

数学上，**线性回归模型**可以表示为：$ y = \\beta\_0 + \\beta\_1 x\_1 + \\cdots + \\beta\_p x\_p + \\epsilon $

其中：

其中，$ y \\(<font style="color:rgb(6, 6, 7);">是因变量，</font>\\) x\_1,x\_2,...,x\_p \\(<font style="color:rgb(6, 6, 7);">是自变量，</font>\\) \\beta\_1,\\beta\_2,...,\\beta\_p \\(<font style="color:rgb(6, 6, 7);">是模型参数，</font>\\) \\epsilon $是误差项，通常假设误差项服从均值为0的**正态分布**。

这种模型通过**最小二乘法**估计参数，广泛应用于房价预测、销量分析等连续值预测场景。其优势在于：

1.  **可解释性强**：参数直接反映变量影响程度
2.  **计算高效**：存在解析解（当矩阵可逆时）
3.  **易于实现**：几乎所有统计软件都支持

2\. 线性模型的"软肋"
=============

然而，现实世界的数据往往比直线复杂得多。**线性模型**的局限性开始显现：

1.  **关系局限性**：只能捕捉线性关系，对非线性模式（如指数增长、周期性波动）无能为力
2.  **分布局限性**：要求误差项服从正态分布，当数据存在异方差或重尾分布时效果骤降
3.  **因变量局限性**：只能处理连续型因变量，无法直接处理分类变量或计数数据
4.  **边界局限性**：预测值可能超出合理范围（如概率预测时出现>1或<0的值）

这些局限促使统计学家们思考：能否扩展线性模型的核心思想，同时突破这些限制？

答案正是**广义线性模型**（`GLM`）。

3\. 线性模型的"进化"
=============

**广义线性模型**在传统线性模型的基础上进行了扩展，放宽了对响应变量分布和线性关系的限制，使其能够适应更广泛的数据类型和复杂关系。

与**线性模型**相比，**广义线性模型**主要改进的地方有3个：

3.1. 因变量分布
----------

**线性模型**假设**因变量**($ y $)是连续型变量，且服从**正态分布**（误差项服从独立同分布的正态分布）。

例如：简单线性回归、多元线性回归。

而广义线性模型允许响应变量来自指数族分布，包括正态分布、二项分布、泊松分布、伽玛分布等。

这意味着广义线性模型可以适用于更多类型的数据，如二分类数据（使用二项分布）、计数数据（使用泊松分布）等。

例如：逻辑回归（二分类问题，二项分布）、泊松回归（计数数据，泊松分布）。

3.2. 模型结构
---------

**线性模型**直接假设**因变量**的均值与线性预测器（$ \\eta = \\beta\_0 + \\beta\_1 x\_1 + \\cdots + \\beta\_p x\_p $）相等：

$ y = \\eta + \\epsilon $误差项独立同分布于正态分布

而**广义线性模型**（`GLM`）通过**连接函数**$ g(\\mu) $将均值 $ \\mu $ 与线性预测器 ($ \\eta = \\beta\_0 + \\beta\_1 x\_1 + \\cdots + \\beta\_p x\_p $) 关联：

$ g(\\mu) = \\eta = \\beta\_0 + \\beta\_1 x\_1 + \\cdots + \\beta\_p x\_p $

具体来说，比如**逻辑回归**模型，使用**对数几率函数**作为**连接函数**：

$ \\mu = p \\(，\\) g(\\mu) = \\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\eta $

其中，**连接函数**必须是单调可微的，例如：

*   逻辑函数（逻辑回归，连接二项分布）
*   对数函数（泊松回归，连接泊松分布）。

3.3. 参数估计方法
-----------

最后，在参数估计方面，**线性模型**使用**最小二乘法**（`OLS`），通过最小化残差平方和求解参数。

而**广义线性模型**（`GLM`）使用**极大似然估计**（`MLE`），通过最大化似然函数求解参数，通常需迭代优化（如牛顿-拉夫森算法）。

3.4. 两者区别
---------

总得来说，两者的主要区别如下表：

**特性**

**线性模型**

**广义线性模型**

目标变量分布

正态分布

指数分布族（泊松、二项、伽马等）

连接函数

恒等函数

Logit、Log、逆函数等

适用场景

连续值预测

分类、计数、偏态数据等

参数估计方法

最小二乘法

最大似然估计

4\. 示例比较
========

理论再多，不如一个示例来的直接，下面我们先通过`scikit-learn`中的`make_moons`函数生成一个包含 `1000` 个样本的月牙形数据集。

    import matplotlib.pyplot as plt
    from sklearn.datasets import make_moons
    
    # 生成月牙形数据集
    X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)
    
    plt.scatter(X[:, 0], X[:, 1], marker="o", c=y, s=25)
    plt.show()
    

![](https://img2024.cnblogs.com/blog/83005/202504/83005-20250401104643016-238415106.png)

从图中可以明显看出，这个数据集呈现出**非线性**的分布。

然后比较使用**线性模型**和**广义线性模型**训练之后的准确率。

    from sklearn.datasets import make_moons
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.metrics import accuracy_score
    from sklearn.preprocessing import PolynomialFeatures
    
    
    # 生成月牙形数据集
    X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)
    
    # 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # 线性模型（线性回归）
    linear_model = LinearRegression()
    linear_model.fit(X_train, y_train)
    # 线性回归预测的是连续值，我们将其转换为分类结果
    linear_pred = (linear_model.predict(X_test) > 0.5).astype(int)
    linear_accuracy = accuracy_score(y_test, linear_pred)
    
    # 广义线性模型（逻辑回归）
    # 先进行多项式特征转换
    poly = PolynomialFeatures(degree=3)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)
    
    logistic_model = LogisticRegression()
    logistic_model.fit(X_train_poly, y_train)
    logistic_pred = logistic_model.predict(X_test_poly)
    logistic_accuracy = accuracy_score(y_test, logistic_pred)
    
    print(f"线性回归的准确率: {linear_accuracy:.2f}")
    print(f"逻辑回归的准确率: {logistic_accuracy:.2f}")
    
    

训练结果：

    线性回归的准确率: 0.87
    逻辑回归的准确率: 0.99
    

在非线性数据集上，明显看出**广义线性模型**（**逻辑回归**）的准确率要高出一截。

5\. 总结
======

总之，**线性模型**和**广义线性模型**都是数据科学中重要的建模工具。

**线性模型**以其简单性和可解释性在连续型数据的回归分析中表现出色，但在面对非正态分布的响应变量和非线性关系时存在局限。

**广义线性模型**通过放宽对响应变量分布的假设并引入链接函数，能够适应更广泛的数据类型和复杂关系，在分类、计数等场景中具有明显优势。

在实际应用中，我们需要根据数据的特点和分析目标选择合适的模型，并结合具体的算法和工具进行实现和优化。