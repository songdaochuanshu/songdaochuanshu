---
layout: post
title: '目标检测高频评价指标的计算过程'
date: "2025-01-22T18:02:34Z"
---
目标检测高频评价指标的计算过程
===============

目标检测高频评价指标的计算过程。计算过程从前到后，涉及到多个指标的计算，分别是： 1. 模型输出值 2. IOU计算 3. 预测结果判断 4. 混淆矩阵计算 5. PR以及PR曲线计算 6. AP计算 7. mAP计算 8. ROC计算 下面详细介绍各个步骤的涉及的概念，计算方法。

在yolo目标检测的评价指标中有如下字段：

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151404528-930053957.png)

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151405008-1674307077.png)

指标的计算过程如图：

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151405523-1992454242.png)

计算过程从前到后，涉及到多个指标的计算，分别是：

1.  模型输出值
2.  IOU计算
3.  预测结果判断
4.  混淆矩阵计算
5.  PR以及PR曲线计算
6.  AP计算
7.  mAP计算
8.  ROC计算

下面详细介绍各个步骤的涉及的概念，计算方法。

模型输出
====

在目标检测中，一张图像推理可能会输出多个预测框，每一个预测框有三个值：

1.  预测框坐标
2.  置信度
3.  类别

如下输出分别代表：x y w h confidence cls

    array([[       1166,         712,        1273,         991,     0.94275,           1],
           [       1267,         711,        1359,         968,     0.93491,           1],
           [        302,         534,         337,         655,     0.86046,           1],
           [        339,         520,         378,         638,     0.85005,           1],
           [        382,         541,         431,         682,     0.75023,           1],
           [        225,         641,         301,         755,     0.65519,           1],
           [        307,         708,         388,         784,     0.39785,           1]])
    

如下是workflow系统的输出演示：

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151410315-1932922481.png)

混淆矩阵计算
======

模型的输出有三个值，要判断一个输出是否正确就需要三个条件都满足：

1.  预测边界框的IoU大于设定阈值，如0.5
2.  类别一致，如都是行人
3.  置信度大于设定阈值，如0.8

IOU
---

IoU （Intersection over Union）也叫交并比

作用：是一种计算不同目标框相互重叠比例的方法，0 表示两个框不相交，1 表示两个框正好重合。

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151410966-881184855.png)

使用方法：

在计算机检测任务中，如果loU≥0.5，就说检测正确。一般是这么约定，但如果希望更严格一点，你可以将loU定得更高，比如说大于0.6或者更大的数字，但loU越高，边界框越精确。

TP FP FN TN的计算
--------------

混淆矩阵的规则如下：

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151411420-469289047.png)

以公安抓捕逃犯为例子，火车站有1000个人，其中有20个逃犯。公安得到消息通过人脸识别技术逐个检测，最终抓了50个嫌疑犯。50人中有18个逃犯，剩下32人都是路人。那么在这个示例中可以说：

真实类别：1000个人，正类：20个逃犯 负类：剩下的1980个路人

预测类别：50个嫌疑犯，正类：18个逃犯 负类：剩下的32个路人

TP：实际为正类，预测也为正类。18个抓住的逃犯，实际为逃犯，预测也为逃犯，属于正检。

FP：实际为负类，预测为正类。32个路人，实际为路人，预测为逃犯，属于误检了。

FN：实际为正类，预测为负类。2个未抓到的逃犯，实际为逃犯，预测为路人，属于漏检了。

FN：实际为负类，预测为负类。1948个路人，实际为路人，预测也为路人

在目标检测中的混淆矩阵和上面还有一点不一样，因为目标检测除了置信度还有预测框，以行人检测为例，标注类别只有一类：行人

真实类别：

1.  正类：和预测框IOU大于阈值的标注框
2.  负类：和预测框IOU低于阈值的标注框或没有标注框区域

预测类别：

1.  正类：置信度大于阈值的结果
2.  负类：置信度小于阈值的结果

如下是一个行人检测的匹配结果，标注框为黑色，预测框为红色。

类别：行人

要求：置信度阈值大于0.8，IOU阈值大于0.5

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151411859-55115267.png)

混淆矩阵计算：

TP：置信度大于0.8，且和标注框IOU大于0.5

FP：置信度大于0.8，且和标注框IOU小于0.5

FN：没有和任何预测框匹配的标注

TN：所有没有出现标注框和预测框的网格（yolo计算方法）

![image](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122155555846-409115672.png)

在这个结果中

TP： 1个

FN：2个

FP：2个

TN：很多，具体计算方法不一致

TN计算
----

在 YOLO 的目标检测中，**True Negative (TN)** 和 **False Positive Rate (FPR)** 的计算有一些独特之处，因为 YOLO 的检测基于预测框的输出，而不是直接对整个图像的每个像素进行分类。

以下是针对 YOLO 的 TN 和 FPR 的计算方法：

1.  **TN 的计算**

在 YOLO 中，**TN** 指的是**模型没有预测框且背景区域确实是背景的情况**。

具体步骤如下：

1.  将图像分成 ( S \\times S ) 的网格，每个网格负责检测目标。
    
2.  对每个网格单元：
    
    *   如果实际为背景区域（即没有任何目标的真实框落在该网格单元内），且模型也没有在该网格中预测任何目标框，则记为 **TN**。
3.  遍历所有网格单元，统计满足上述条件的网格单元数量。
    
4.  **FPR 的计算**
    

**FPR** 是基于 TN 和 FP 计算的。公式为：  
![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122152128875-1346885466.png)

**False Positive (FP)** 的计算：

FP 是模型预测为目标框，但实际上没有目标（背景）的情况：

*   预测框与任何真实框的 IoU 小于设定阈值（通常为 0.5）。
*   并且这些预测框不是属于任何真实目标的。

**具体计算步骤**：

1.  遍历所有预测框：
    *   如果某预测框的 IoU 小于设定阈值，并且它被分类为某个目标类别，则记为 **FP**。
2.  遍历所有网格单元：
    *   统计背景区域中被错误预测为目标的网格单元作为 FP。
    *   统计背景区域中没有任何预测框的网格单元作为 TN。
3.  根据公式计算 **FPR**。

注意事项

*   **FPR 依赖背景区域大小**：在目标检测任务中，背景区域通常很大，因此 TN 数量通常会远远大于 FP。
*   **重点关注 FP，而非 TN**：在 YOLO 的评估中，更重要的是减少 FP 和 FN，而不是精确计算 TN。
*   **AP (Average Precision)** 通常替代 FPR：在实际评估中，COCO 或 PASCAL VOC 的指标（如 mAP）更受关注，而不是直接使用混淆矩阵中的 FPR。

实例化解释

假设一张图像分为 ( 7 \\times 7 ) 的网格（YOLOv1），如下：

1.  图像中有 2 个真实目标，每个占用 3 个网格单元，其余网格为背景。
2.  模型预测出 3 个目标框，其中 1 个为 FP。

TN 和 FPR 计算：

*   背景网格：( 49 - 6 = 43 )（无目标）。
*   如果有 40 个背景网格被正确预测为背景，则 TN = 40。
*   若 3 个背景网格被错误预测为目标，则 FP = 3。

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122152150585-1471888248.png)

PR曲线
====

percision 精确率
-------------

percision 是 **精确率**。

定义：所有**预测**为正样本中实际为正样本的比例

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122152209868-802400185.png)

含义：精确率衡量了模型对正样本预测的准确性，即模型预测的准不准，所以也可以叫查准率

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151413629-1682844878.png)

recall 召回率
----------

recall 是 **召回率**。

定义：所有**标注**为正样本中预测为正样本的比例

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122152225585-603301016.png)

含义：召回率衡量了模型对正样本预测的覆盖程度，即模型预测的全不全，所以也可以叫查全率

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151414457-478997784.png)

PR曲线
----

Precision-Recall曲线很好的展示了在不同阈值取值下精确率和召回率的平衡情况。展示如下：

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151415530-274398265.png)

**横坐标**：recall 召回率

**纵坐标**：precision 精确率

**曲线绘制**：每一个预测结果都有三个值：坐标框、置信度、类别，预测结果可以计算出TP、FP、FN、TN等混淆矩阵，进一步计算出P和R。也就是说每一个坐标框、置信度、类别都有一个对应的P和R的值。即一个置信度会得到一个P和一个R，通常IOU是一个固定值，类别也固定，也就是置信度->(P, R)。

将一个置信度下计算的R作为横坐标，P作为纵坐标就能得到PR曲线中的一个点，将所有置信度的PR绘制出来就能得到一个曲线，那就是PR曲线。

**曲线的意义**：对于一个模型，我们希望它预测的又准确又全面，但实际上这两个指标是一对矛盾体，精确率高的时候预测的结果少，召回率高的时候又不是很准确。

为了找到这样一个又准确有全面的点让模型输出既准确又尽可能的多，遍历所有数据集输出的所有置信度，得到对应的P和R,连起来就是这个PR曲线。使用这个曲线，就能够方便的找到在某一个置信度下模型输出能符合准确率和召回率的要求。

比如下面要求模型输出准确率要超过80%，召回率越高越好，通过曲线就找到这个置信度。

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151416534-876556909.png)

**PR曲线分析：**

如下：

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151417132-551824482.png)

PR 曲线的形状分析

1.  如果一个PR曲线被另一个PR曲线完全包住，则可断言后者的性能优于前者，例如上面的A和B优于C。但是A和B的性能无法直接判断
2.  如果 PR 曲线靠近对角线（Precision 接近正例比例）：说明模型性能接近随机预测。
3.  PR 曲线在 Recall 高时快速下降：可能说明模型的误报增加。
4.  最理想的情况便是随着召回率的提升，精确率也逐步保持提升或保持不变，Precision 和 Recall 均接近 1，曲线趋近于左上角。

AP
==

AP (Average Precision) 平均精度，也就是所有Precision的平均值，简单来说就是对PR曲线上的Precision值求均值。模型越好，AP值越高。

AP的计算有两种方法，分别是：

*   插值法 ：Pascal Voc 2008 的AP计算方式
*   面积积分法：Pascal voc 2010之后计算方式

11点插值法
------

Pascal Voc 2008 的AP计算方式

在平滑处理的PR曲线上，取横轴0-1的10等分点（包括断点共11个点）的Precision的值，计算其平均值为最终AP的值。

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151417592-38512271.png)

插值法计算缺点：

第一、使用11个采样点在精度方面会有损失。

第二、在比较两个AP值较小的模型时，很难体现出两者的差别。

所以这种方法在2009年的Pascal voc之后便不再采用了。

面积积分法
-----

在Pascal voc 2010之后，便开始采用这种精度更高的方式。绘制出平滑后的PR曲线后，用积分的方式计算平滑曲线下方的面积作为最终的AP值。

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151418083-1616722215.png)

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151418574-432078370.png)

每个recall的区间内我们只取这个区间内precision的最大值然后和这个区间的长度做乘积，所以最后体现出来就是一系列的矩形的面积

**注意**：  
AP 是一个模型通用能力的展示，在一些极限情况下的性能不能通过AP来展示，需要使用ROC曲线来展示。

mAP
===

mAP (mean average precision) 意思是平均精度均值。一个模型通常会检测很多种物体，那么每一类都能绘制一个PR曲线，进而计算出一个AP值。那么多个类别的AP值的平均就是mAP。

AP 衡量的是训练好的模型在每个类别上的好坏，mAP 衡量的是模型在所有类别上的好坏。

mAP@0.5
-------

mAP@0.5：mean Average Precision（IoU=0.5）

即将IoU设为0.5时，计算每一个类别的所有图片的AP，然后所有类别求平均，即mAP。

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151419046-1318056959.png)

将Iou设置成0.5,计算得到混淆矩阵，然后计算出PR曲线，最后计算出AP、mAP。mAP@0.5就是IoU=0.5时的mAP值。

设置不同的IoU会影响预测结果为正确还是失败，进而影响混淆矩阵的计算，影响PR矩阵的计算，最后会影响AP、mAP的计算。

IoU 越大预测为正确越少，TP越少，PR曲线越接近中线，AP面积越小，mAP的值越小，模型看上去性能越差。

经验：

所以对于模型性能要求严格的IoU设置大，对于性能要求不严格IoU设置小

mAP@0.5:.95
-----------

mAP@.5:.95（mAP@\[.5:.95\]）

设置一系列IoU的值，计算多个IoU下mAP的平均值：

IOU 从0.5到0.95，步长0.05 创建IoU数组。0.5、0.55、0.6、0.65、0.7、0.75、0.8、0.85、0.9、0.95。

计算在多个的IoU下的mAP的平均值

ROC
===

Receiver Operating Characteristic，中文名字叫接受者操作特征曲线，是评价模型分类性能的重要量度指标。

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151421739-1472659749.png)

TPR 和 FPR
---------

它通过将**真阳性率（True Positive Rate，TPR）**和**假阳性率（False Positive Rate，FPR）**作为横纵坐标来描绘分类器在不同阈值下的性能。

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151419974-1628653045.png)

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151420703-376766769.png)

TPR：代表的含义是**正样本中被正确检测**出来的数量，统计模型正检的能力

FPR：代表的含义是**负样本中被检测成正确**的数量，统计模型误检的能力

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151421219-34635908.png)

曲线绘制
----

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151421739-1472659749.png)

**横坐标**：FPR 误检率

**纵坐标**：TPR 精确率

**曲线绘制**：每一个预测结果都有三个值：坐标框、置信度、类别，预测结果可以计算出TP、FP、FN、TN等混淆矩阵，进一步计算出TPR和FPR。也就是说每一个坐标框、置信度、类别都有一个对应的TPR和FPR的值。即一个置信度会得到一个TPR和一个FPR，通常IOU是一个固定值，类别也固定，也就是置信度->(TPR, FPR)。

将一个置信度下计算的FPR作为横坐标，TPR作为纵坐标就能得到ROC曲线中的一个点，将所有置信度的FPR和TPR绘制出来就能得到一个曲线，那就是ROC曲线。

**曲线的意义：**

ROC曲线使用曲线下面积（AUC）的大小对模型进行评价，AUC的取值范围为0.5到1之间，曲线下面积越大，越接近于1，模型的诊断或预测效果越好。

AUC在 0.5～0.7时，准确性较低；在0.7～0.9时，有一定准确性；AUC在0.9以上时，准确性较高。AUC＝0.5时，说明诊断方法完全不起作用，无诊断价值。

**曲线在生产任务中的作用：**

1.  误检控制，寻找推荐阈值
2.  模型性能对比

误检控制：

FPR代表的是模型的误检水平，假设有1w张测试集，希望误检率控制在0.1以内，那么1w张测试就只能有10000 \* 0.1 = 1000张图像被误检，也就是将负样本检测成正样本了。

为了将误检控制在1000张以内，需要将误检率FPR控制在0.1以下，阈值如何设置才能将FPR控制在0.1以下呢？通过曲线就可以找到FPR=0.1时对应的阈值，那么输出结果置信度在该阈值以上误检率少于0.1。

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151422226-937364841.png)

模型对比：

对比两个模型的能力，可以使用mAP，更推荐使用ROC。在同一个FPR下对应的TPR越高就说明模型极限场景下能力越好。比如一个模型的前后两次迭代，使用误检率为0.01来衡量，A的正检率为0.82，B的正检率为0.90，那么说明B在该误检率下性能高于A。

ROC table
---------

进一步为了方便查看指标，检测工程代码中将误检率为0.1、0.01、0.001等值单独组成一张ROC表格

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151422701-1184347266.png)

mAP和ROC对比
=========

主要区别
----

PR 和 ROC 的区别主要在于不平衡数据的表现:

*   PR对数据不平衡是敏感的, 正负样本比例变化会引起PR发生很大的变化;
*   而ROC曲线是不敏感的, 正负样本比例变化时ROC曲线变化很小.

正负样本均衡：

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151424677-598613595.png)

正负样本不均衡：

![](https://img2024.cnblogs.com/blog/1060878/202501/1060878-20250122151425324-1077144856.png)

使用场景对比
------

特性

mAP

ROC

**适用任务**

多类别任务、多标签任务

二分类任务

**关注点**

精度与召回的平衡

误检率，正检率

**衡量指标**

平均精度 (AP)

曲线下面积 (AUC)

**适用场景示例**

目标检测、多标签分类

疾病诊断、信用评分、分类器评估

**总结：**

*   如果是目标检测、多类别任务，且关注分类的准确性，选择 **mAP**。
*   如果是二分类任务，尤其在不平衡数据集上需要评估不同阈值下的表现，选择 **ROC/AUC**。