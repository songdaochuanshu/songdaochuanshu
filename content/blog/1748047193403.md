---
layout: post
title: '聚类是如何度量数据间的“远近”的？'
date: "2025-05-24T00:39:53Z"
---
聚类是如何度量数据间的“远近”的？
=================

在聚类分析中，**距离度量**是核心概念之一，它决定了数据点之间的相似性或差异性，从而影响聚类结果的质量。

选择合适的距离度量方法，就像为数据选择合适的**“观察视角”**，能够帮助我们发现隐藏的模式结构。

本文将详细介绍几种常用的聚类距离度量方法，包括它们的原理、代码实现，以及这些方法满足的基本性质。

1\. 常用距离度量
==========

1.1. 闵可夫斯基距离（Minkowski Distance）
--------------------------------

**闵可夫斯基距离**是一种通用的距离度量方法，它涵盖了多种常见的距离计算方式。

其公式为：\\(D(x,y)=\\left(\\sum\_{i=1}^{n}|x\_i-y\_i|^p\\right)^{\\frac{1}{p}}\\)

*   当$ p=1 $时，它是**曼哈顿距离**（`Manhattan Distance`），适用于网格状空间，例如城市街区。
*   当$ p=2 $时，它是**欧几里得距离**（`Euclidean Distance`），是最常用的距离度量方式，适用于连续变量。
*   当$ p\\to\\infty $时，它趋近于**切比雪夫距离**（`Chebyshev Distance`），即各维度差的最大值。

基于`scikit-learn`库计算这些距离非常简单：

    from sklearn.metrics.pairwise import pairwise_distances
    import numpy as np
    
    # 示例数据
    x = np.array([[1, 2, 3]])
    y = np.array([[4, 5, 6]])
    
    # 计算不同 p 值的闵可夫斯基距离
    manhattan_distance = pairwise_distances(x, y, metric='manhattan')[0][0]
    euclidean_distance = pairwise_distances(x, y, metric='euclidean')[0][0]
    chebyshev_distance = pairwise_distances(x, y, metric='chebyshev')[0][0]
    
    print("曼哈顿距离:", manhattan_distance)
    print("欧几里得距离:", euclidean_distance)
    print("切比雪夫距离:", chebyshev_distance)
    
    ## 输出结果：
    '''
    曼哈顿距离: 9.0
    欧几里得距离: 5.196152422706632
    切比雪夫距离: 3.0
    '''
    

1.2. 汉明距离（Hamming Distance）
---------------------------

**汉明距离**用于衡量两个等长字符串之间的差异，即对应位置上不同字符的个数。

它常用于离散属性的比较。

代码示例如下：

    from sklearn.metrics import hamming_loss
    
    # 示例数据
    x = np.array([0, 1, 1, 0])
    y = np.array([1, 1, 0, 0])
    
    # 计算汉明距离
    hamming_distance = hamming_loss(x, y)
    print("汉明距离:", hamming_distance)
    
    ## 输出结果：
    '''
    汉明距离: 0.5
    '''
    

1.3. 杰卡德距离（Jaccard Distance）
----------------------------

**杰卡德距离**用于衡量两个集合之间的相似性，定义为两个集合交集的大小与并集大小的比值的补数。

它适用于稀疏数据。

代码示例如下：

    from sklearn.metrics import jaccard_score
    
    # 示例数据
    x = np.array([0, 1, 1, 0])
    y = np.array([1, 1, 0, 0])
    
    # 计算杰卡德距离
    jaccard_similarity = jaccard_score(x, y)
    jaccard_distance = 1 - jaccard_similarity
    print("杰卡德距离:", jaccard_distance)
    
    ## 输出结果：
    '''
    杰卡德距离: 0.6666666666666667
    '''
    

1.4. 余弦距离
---------

**余弦距离**通过向量夹角衡量方向相似性，常用于文本分析。

它的公式是：$ cos(\\theta)=\\frac{X\\cdot Y}{||X||\\cdot ||Y||} $

实际使用常转换为余弦距离：**距离 = 1 - 余弦相似度**。

代码示例如下：

    from sklearn.metrics.pairwise import cosine_distances
    import numpy as np
    
    # 示例数据
    x = np.array([[1, 2, 3]])
    y = np.array([[4, 5, 6]])
    
    # 计算余弦距离
    cosine_dist = cosine_distances(x, y)[0][0]
    
    print("余弦距离:", cosine_dist)
    
    ## 输出结果：
    '''
    余弦距离: 0.025368153802923787
    '''
    

2\. 距离度量的基本性质
=============

**距离度量方法**通常需要满足以下基本性质，以确保其合理性和有效性：

1.  **非负性**（`Non-negativity`）：距离必须是非负的，即$ D(x,y)\\geq 0 $。

这意味着任意两个点之间的距离不能为负值。

2.  **同一性**（`Identity`）：当且仅当两个点相同时，距离为零，即\\(D(x,y)=0\\) 当且仅当\\(x=y\\) 。

这确保了距离能够区分不同的点。

3.  **对称性**（`Symmetry`）：距离是无方向的，即\\(D(x,y)=D(y,x)\\) 。

这意味着从点\\(x\\) 到点\\(y\\) 的距离与从点\\(y\\) 到点\\(x\\) 的距离相同。

4.  **三角不等式**（`Triangle Inequality`）：对于任意三个点\\(x\\) 、\\(y\\) 和\\(z\\) ，满足\\(D(x,z)\\leq D(x,y)+D(y,z)\\) 。

这确保了距离的合理性，即直接从\\(x\\) 到\\(z\\) 的距离不会超过经过\\(y\\) 的距离。

这些性质的意义在于，它们为距离度量提供了数学上的合理性，使得距离能够正确地反映数据点之间的相似性或差异性。

3\. 连续属性与离散属性的距离
================

对于**连续属性**，常用的距离度量方法是**欧几里得距离**和**曼哈顿距离**。

这些方法基于数值的差值来计算距离，适用于数值型数据。

*   **欧几里得距离**：适用于多维空间中的连续数据，计算两点之间的直线距离。
*   **曼哈顿距离**：适用于网格状空间，计算两点之间的“步数”距离。

对于**离散属性**，常用的距离度量方法是**汉明距离**和**杰卡德距离**。

*   **汉明距离**：适用于二进制数据或分类数据，计算两个序列中不同位置的数量。
*   **杰卡德距离**：适用于集合数据，计算两个集合之间的相似性。

4\. 总结
======

**距离度量**是聚类分析中的关键环节。通过选择合适的距离度量方法，可以更好地反映数据点之间的相似性或差异性。

本文介绍了几种常用的距离度量方法，包括闵可夫斯基距离、汉明距离和杰卡德距离等等，并通过代码示例展示了它们的使用方式。

同时，我们还探讨了距离度量的基本性质及其意义，以及如何针对连续属性和离散属性进行距离计算。

在实际应用中，选择哪种距离度量方法取决于数据的类型和聚类的目标。