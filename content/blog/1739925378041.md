---
layout: post
title: 'RLHF各种训练算法科普'
date: "2025-02-19T00:36:18Z"
---
RLHF各种训练算法科普
============

强化学习在LLM中的应用越来越多了，本文针对常见的几种训练算法，用生活中的例子做类比，帮助理解相关概念。

包括：PPO、DRO、DPO、β-DPO、sDPO、RSO、IPO、GPO、KTO、ORPO、SimPO、R-DPO、RLOO，以及GRPO。

PPO（Proximal Policy Optimization，近端策略优化）
----------------------------------------

### 背景：

在强化学习中，策略更新时可能会导致策略发生过大的变化，从而导致训练不稳定。PPO通过限制策略更新的幅度来解决这个问题，确保训练过程的稳定性。

### 例子：

你去水果摊挑苹果，摊主有很多苹果，你希望挑出最甜的苹果，但你不想每次都尝遍所有苹果，也不想因为尝太多而失去味觉的敏感性。

奖励信号：你有一个甜度计，可以大致测量苹果的甜度。这个甜度计就是“奖励信号”，用来帮助你判断哪个苹果更甜。

保持稳定：你不会一下子尝试所有苹果，而是先尝几个，然后根据甜度计的读数，逐步调整你的选择策略。比如，你先尝了5个苹果，发现其中3个比较甜，你就会在这3个里面继续挑选，而不是一下子换一整批苹果。

单独训练奖励信号：你还需要单独校准你的甜度计，确保它能准确测量甜度。比如，你可能会先尝一些已知甜度的苹果，来调整甜度计的准确性。

论文：

Proximal Policy Optimization Algorithms

DRO（Direct Reward Optimization，直接奖励优化）  

-----------------------------------------

### 背景：

在某些强化学习任务中，直接优化奖励信号可能比优化策略更直接有效，尤其是在奖励信号非常明确的情况下。DRO直接优化奖励信号，避免了复杂的策略更新过程。

### 例子：

你去水果摊挑苹果，摊主给你一个甜度计，让你根据甜度计的读数来挑选苹果。

直接奖励：你直接用甜度计测量每个苹果的甜度，然后根据甜度计的读数来挑选最甜的苹果。比如，你测量了10个苹果，发现其中3个甜度最高，你就会在这3个里面继续挑选，直接以甜度计的读数作为奖励信号，挑选出最甜的苹果。

### 论文：

Offline regularised reinforcement learning for large language models alignment

DPO（Direct Preference Optimization，直接偏好优化）  

---------------------------------------------

### 背景：

在某些情况下，直接比较样本的偏好可能比依赖奖励信号更有效，尤其是在奖励信号不准确或难以获取的情况下。DPO通过直接比较样本的偏好来优化选择策略。

### 例子：

这次，你还是在水果摊挑苹果，但你不想用甜度计这种奖励信号了，而是直接比较苹果的口感。

正负样本：你从摊主那里拿了两个苹果，一个是你随便拿的（负例），另一个是摊主推荐的（正例）。比如，摊主说这个苹果特别甜。

对比优化：你直接比较这两个苹果，尝尝哪个更甜。比如，你发现摊主推荐的苹果确实比你随便拿的甜。

直接优化：你通过这种直接的比较，直接学习如何挑选甜苹果，而不需要单独校准甜度计。

### 论文：

Direct preference optimization: Your language model is secretly a reward model

β-DPO (Direct Preference Optimization with Dynamic)  

------------------------------------------------------

### 背景：

在直接偏好优化（DPO）中，性能对超参数β的选择非常敏感，且对偏好数据的质量也很敏感。静态的β值在不同数据质量下表现不稳定，导致优化过程不够鲁棒。β-DPO通过动态调整β值来解决这个问题，使其能够根据数据质量自动调整，从而提高模型的鲁棒性和适应性。

### 例子：

你去水果摊挑苹果，摊主给你两个苹果，一个是你随便拿的（负例），另一个是摊主推荐的（正例）。你希望通过比较这两个苹果来学习如何挑选更甜的苹果。

动态调整β值：你发现不同的苹果对甜度的感知差异很大。有些苹果对甜度的感知差异很明显，而有些则不明显。你决定根据每个苹果对甜度感知的差异来动态调整你的“甜度偏好参数”β。如果两个苹果的甜度差异很大，你会降低β值，这意味着你会更积极地更新你的偏好；如果差异不大，你会增加β值，以保持原有的偏好，避免过度调整。

数据过滤：在比较苹果时，你注意到有些苹果的甜度差异非常小，几乎可以忽略不计。这些苹果可能会误导你的学习过程，因为它们的差异不足以提供有效的学习信号。因此，你决定过滤掉这些差异过小的苹果对，只保留那些有明显甜度差异的苹果对进行学习。

批量调整：你意识到，如果每次只根据一对苹果来调整β值，可能会导致学习过程不稳定。因此，你决定采用批量调整的方法。每次从摊主那里拿一组苹果（比如5对），然后根据这一组苹果的甜度差异来计算一个平均的β值，用于这一组苹果的学习。这种方法可以减少单个苹果对的影响，使学习过程更加稳定。

### 论文：

β-DPO: Direct preference optimization with dynamic β

sDPO（stepwise Direct Preference Optimization，逐步DPO）  

------------------------------------------------------

### 背景：

在复杂的环境中，一次性比较所有样本可能会导致选择困难或策略不稳定。sDPO通过逐步比较样本，逐步优化选择策略，确保每一步的决策都是基于当前最优的信息。

### 例子：

这次，你去水果摊挑苹果，你想通过逐步比较来优化你的选择。

逐步比较：你先从摊主那里拿了两个苹果，比较它们的甜度。然后，你再拿两个苹果，继续比较。

逐步优化：每次比较后，你都会记住哪个苹果更甜，并逐步调整你的选择策略。比如，你先比较了A和B，发现A更甜，然后你再比较C和D，发现C更甜，最后你比较A和C，确定哪个更甜。

逐步学习：这种方法通过逐步比较，逐步优化你的选择策略，避免了一次性比较太多苹果带来的困惑。

### 论文：

sDPO: Don't use your data all at once

RSO（Rejection Sampling Optimization，拒绝采样优化）  

----------------------------------------------

### 背景：

在样本质量参差不齐的情况下，直接拒绝低质量样本可以提高训练效率和模型性能。RSO通过拒绝低质量样本，确保训练过程只基于高质量样本。

### 例子：

这次，你去水果摊挑苹果，摊主给你一个大筐苹果，里面苹果的品质参差不齐。

初步筛选：你先随意挑选一些苹果，尝尝看。比如，你从筐里随便抓了10个苹果，尝了尝，发现其中3个特别不甜，你直接拒绝这3个苹果，把它们放回筐里，不再考虑。

优化策略：然后你从剩下的苹果中继续挑选，尝尝看哪些更甜。比如，你从剩下的7个苹果中又尝了3个，发现其中2个比较甜，你就会在这2个里面继续挑选，同时记住这次拒绝的经验，下次再挑选时尽量避开那些不甜的苹果类型。

### 论文：

Statistical rejection sampling improves preference optimization

IPO（Identity Preference Optimization，身份偏好优化）  

-----------------------------------------------

### 背景：

在某些情况下，样本的身份（如产地、品牌等）可能对选择结果有重要影响。IPO通过考虑样本的身份信息来优化选择策略，确保选择结果符合特定偏好。

### 例子：

你去水果摊挑苹果，摊主告诉你，这些苹果有不同产地，比如山东的、陕西的、辽宁的。

身份标记：你先按照产地把苹果分好类，比如把山东的苹果放一堆，陕西的苹果放一堆，辽宁的苹果放一堆。你认为不同产地的苹果口感可能不同，所以给它们分别标记了身份。

偏好学习：你先尝尝每个产地的苹果，比如你尝了山东的苹果后觉得比较甜，尝了陕西的苹果后觉得酸甜适中，尝了辽宁的苹果后觉得口感稍差。然后你就会根据这种偏好，优先挑选你认为口感好的产地的苹果，比如多选山东的苹果，少选辽宁的苹果。

### 论文：

A general theoretical paradigm to understand learning from human preferences

GPO（Generalized Preference Optimization，广义偏好优化）  

--------------------------------------------------

### 背景：

在某些情况下，单一的偏好指标（如甜度）可能不足以描述样本的优劣。GPO通过综合考虑多个因素（如甜度、大小、颜色等）来优化选择策略，确保选择结果更符合实际需求。

### 例子：

你去水果摊挑苹果，这次你想综合考虑多种因素来挑选苹果，不仅仅是甜度，还有口感、大小、颜色等。

综合评价：你先尝尝几个苹果，同时观察它们的大小、颜色等。比如，你尝了5个苹果，发现其中2个很甜，但其中一个个头小，颜色也不太好看；另一个个头大，颜色鲜艳。你综合考虑后，觉得个头大、颜色鲜艳的苹果更好。

偏好调整：然后你根据这种综合偏好来调整挑选策略，下次挑选时会优先考虑个头大、颜色鲜艳且口感好的苹果，而不是单纯追求甜度。

### 论文：

Generalized preference optimization: A unified approach to offline alignment

KTO（Kahneman-Tversky Optimization，Kahneman-Tversky优化）  

--------------------------------------------------------

### 背景：

在某些情况下，人类的决策过程受到损失厌恶等心理因素的影响。KTO通过模拟这种心理因素来优化选择策略，确保选择结果更符合人类的实际决策过程。

### 例子：

你去水果摊挑苹果，摊主给你两种选择方案。

方案一：你先尝一个苹果，如果甜，就再给你一个；如果不好吃，就什么都没有。

方案二：你先尝一个苹果，如果不好吃，就再给你一个；如果甜，就什么都没有。

损失厌恶：你更倾向于选择方案一，因为你不希望尝到不好吃的苹果后什么都没有，你更害怕损失。这种心理反映了Kahneman-Tversky优化中对损失厌恶的考虑，在挑选苹果时，你会尽量避免选择那些可能导致不好结果的苹果，比如那些看起来颜色不好、表皮有瑕疵的苹果。

### 论文：  

KTO: Model alignment as prospect theoretic optimization

ORPO（Odds Ratio Preference Optimization，比值偏好优化）  

--------------------------------------------------

### 背景：

在某些情况下，直接比较样本的偏好可能不够精确。ORPO通过计算样本之间的比值比来优化选择策略，确保选择结果更精确。

### 例子：

你去水果摊挑苹果，摊主给你两个苹果，让你比较它们的口感。

比值计算：你尝了尝这两个苹果，发现一个很甜，另一个不太甜。你计算一下它们的口感比值，比如甜的苹果口感得分为8，不太甜的苹果口感得分为4，比值为2。你根据这个比值来判断哪个苹果更好。

偏好更新：然后你根据这种比值比来更新你的挑选偏好，下次遇到类似的苹果时，就会优先挑选口感得分高的苹果。

### 论文：

ORPO: Monolithic preference optimization without reference model

SimPO (Simple Preference Optimization，简单偏好优化)
---------------------------------------------

### 背景：

在某些简单任务中，直接比较样本的偏好可能已经足够有效。SimPO通过简单的偏好比较来优化选择策略，适用于简单的决策任务。

### 例子：

你去水果摊挑苹果，摊主给你两个苹果，让你比较它们。

简单比较：你直接尝尝这两个苹果，比较一下哪个更甜。比如，你发现左边的苹果比右边的甜，你就会记住左边的苹果更好。

偏好学习：然后你根据这种简单的比较结果来学习挑选苹果的偏好，下次遇到类似的苹果时，就会优先挑选左边这种类型的苹果。

### 论文：

SimPO: Simple preference optimization with a reference-free reward

R-DPO（Regular DPO，正则化DPO）  

----------------------------

### 背景：

在某些情况下，直接优化偏好可能导致模型过于复杂或过拟合。R-DPO通过引入正则化约束来优化选择策略，确保模型的简洁性和泛化能力。

### 例子：

你去水果摊挑苹果，摊主给你两个苹果比较，但这次你不仅比较它们的口感，还要考虑其他因素，比如苹果的大小、颜色等。

正则化约束：你在比较两个苹果的口感时，还会考虑它们的大小、颜色等因素。比如，你发现一个苹果口感很好，但个头太小；另一个苹果口感稍差，但个头大、颜色鲜艳。你综合考虑这些因素后，会更倾向于选择个头大、颜色鲜艳的苹果。

偏好优化：通过这种正则化约束，你优化了你的挑选偏好，不仅考虑口感，还会考虑其他因素，使挑选出的苹果更符合你的综合要求。

### 论文：

Disentangling length from quality in direct preference optimization

RLOO（REINFORCE Leave-One-Out）  

--------------------------------

### 背景：

在某些情况下，样本之间的相互影响可能导致选择策略不稳定。RLOO通过留一法验证来优化选择策略，确保每一步的决策都是基于独立的样本。

### 例子：

你去水果摊挑苹果，摊主给你一筐苹果，让你从中挑选。

留一法验证：你先从筐里拿出一个苹果，尝尝看。比如，你拿出了一个苹果，发现它很甜。然后你把剩下的苹果分成若干组，每组再拿出一个苹果来尝尝，看看哪组的苹果比你之前拿的那个更甜。

策略更新：通过这种留一法验证，你不断更新你的挑选策略，下次挑选时会优先考虑那些比之前拿的那个更甜的苹果组里的苹果。

### 论文：

Back to basics: Revisiting reinforce style optimization for learning from human feedback in LLMs

GRPO（Group Relative Policy Optimization，群体相对策略优化）  

----------------------------------------------------

### 背景：

在某些情况下，单独评价每个样本可能不够高效。GRPO通过分组比较来优化选择策略，确保每一步的决策都是基于群体的相对优势。

### 例子：

这次，你还是在水果摊挑苹果，但你不想单独评价每个苹果，而是通过分组比较来优化。

分组比较：你从摊主那里拿了5个苹果，分成两组，每组2个，剩下1个单独放。比如，你把苹果分成A组和B组。

相对评价：你尝尝每组里的苹果，比较哪个更甜。你不需要给每个苹果一个具体的甜度分数，只需要知道哪个苹果比另一个甜。比如，你发现A组的苹果比B组的苹果甜。

消除负例：你把那些不甜的苹果（负例）排除掉。比如，你发现B组的苹果不甜，就不再考虑它们，只关注A组的苹果。

更新策略：你告诉自己，下次挑苹果的时候要优先考虑A组的苹果，因为它们更甜。比如，你发现A组的苹果颜色更鲜艳，表皮更光滑，你就记住这些特征，下次挑苹果的时候就优先选这样的苹果。

### 论文：

DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

『注:本文来自博客园“小溪的博客”，若非声明均为原创内容，请勿用于商业用途，转载请注明出处http://www.cnblogs.com/xiaoxi666/』