---
layout: post
title: '开源flux适配昇腾NPU分享，体验120亿参数AI文生图模型'
date: "2025-01-08T00:36:16Z"
---
开源flux适配昇腾NPU分享，体验120亿参数AI文生图模型
===============================

本次适配需要在拥有昇腾NPU的主机上将flux模型运行起来，验证其功能是否可以正常使用。

这一期我们分享一位开源开发者参与flux适配昇腾NPU的实践经验，欢迎广大开发者对**华为技术栈适配**进行讨论。

开源适配实践
======

flux是一个AI图像生成模型，有120亿参数量，具有大量的用户基础，可以根据命令行输入的文字去**生成对应的图片**。本次适配使用的flux模型权重文件是schnell版本。

下面我简单分享一下flux模型适配昇腾NPU的实践心得，如有改进之处，欢迎指正，也希望对华为生态感兴趣的小伙伴可以加入进来一起探讨。

如何验证开源项目
========

本次适配我需要在拥有昇腾NPU的主机上将flux模型运行起来，验证其功能是否可以正常使用。昇腾NPU主机需要先安装CANN软件，CANN软件的安装可参考[昇腾社区](https://www.hiascend.com/)的开发资源。后续的所有操作都在此主机上进行。

在GitHub平台上下载项目源码（[点击链接下载](https://github.com/black-forest-labs/flux)），通过分析README文件中的内容可以知道flux模型开发环境所需要的python版本是python3.10，其相关依赖都封装在pyproject.toml文件中。同时flux模型主要采用python语言进行开发，所以我使用anaconda管理模型环境。anaconda作为环境管理工具，可以为不同的模型创建独立的环境，这样可以避免依赖版本冲突，保持环境的整洁性。该主机需要的anaconda为Linux系统ARM64版，查找相关文档进行安装配置后，使用anaconda去创建属于flux的虚拟环境。

第一步：进入创建的conda环境，安装相关依赖
-----------------------

在代码仓库中，是由pyproject.toml文件去管理的依赖，使用【pip install -e ".\[all\]"】命令下载依赖。我们下载好flux模型所需要的相关依赖之后，需要安装对应版本的torch\_npu，使pytorch框架可以运行在昇腾NPU上，这样flux模型所需要的环境就搭建完成了。torch\_npu是华为为昇腾NPU设计的pytorch后端库，使得pytorch框架能够在昇腾NPU上运行，是连接pytorch框架与昇腾NPU的桥梁。

第二步：准备模型权重文件
------------

我先运行了一次模型，发现权重文件在运行时会自动从Hugging Face上下载，由于模型权重文件较大，**且每次运行都会重新下载**，比较占用内存。所以我们提前下载模型权重文件到主机上。

分析代码文件，涉及到的相关模型权重有以下3个：

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107161932205-42557301.jpg)

t5：文本转换为机器理解的语言

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107161947102-563021689.jpg)

clip：文本转换为图像

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107161959341-112113836.jpg)

flux：图像生成

第三步：编写运行代码
----------

flux模型权重文件下载完成后，就可以修改相关代码，实现模型权重文件从本地加载。

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107162007122-365920015.png)

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107162015440-682399530.png)

在之前的代码逻辑里面，判断加载哪一个模型是基于模型的名称去进行判断的，但是我们现在传入的参数是路径，这个方法不适用，需要修改代码，直接说明加载的模型是T5或者CLIP。

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107162023075-339175964.png)

然后运行模型，报错提示设备为CUDA，但我的设备是NPU。根据这个报错信息，我进行了对应的修改，将device="CUDA"，改为device="NPU"

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107162030441-2067625454.png)

再次运行之后报错提示bfloat16不支持在这个设备上，我根据这个报错信息，找到bfloat16的位置，修改为float32

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107162036884-1740300142.png)

进行这些修改之后，运行模型，查看NPU已经进入了运行状态。

**测试结果：**

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107162043936-1731005006.png)

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107162052736-1375806956.png)

![](https://img2024.cnblogs.com/blog/2030258/202501/2030258-20250107162059863-723408446.png)

经过测试，模型可以通过输入的内容，去生成对应的图片，所以模型的功能在此主机上可以正常使用了。

期待各位小伙伴的加入，一起体验适配带来的乐趣。在体验过程中，如有问题可[点击链接](https://gitcode.com/HuaweiCloudDeveloper/OpenSourceForHuaweiWiki/blob/main/zh_CN/docs/open-source-work-flow.md)进入开源开发者专属问答区，加入开源开发者专项计划。加入我们，您可以在项目中提 Issues与其他开发者进行互动，也可以添加项目相关的微信群进行技术讨论与交流。

沃土云创开源开发者专项计划是华为给开源开发者提供专属激励资源，鼓励开发者积极参与开源 for Huawei适配，践行“让优秀开发者支持更优秀开发者”的理念。

[**点击关注，第一时间了解华为云新鲜技术~**](https://bbs.huaweicloud.com/blogs?utm_source=cnblog&utm_medium=bbs-ex&utm_campaign=other&utm_content=content)