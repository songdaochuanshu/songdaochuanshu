---
layout: post
title: '【架构师系列】风控场景下超高并发频次计算服务的设计与实践'
date: "2026-01-01T00:51:53Z"
---
【架构师系列】风控场景下超高并发频次计算服务的设计与实践
============================

在风控体系里，频次计算几乎是绕不开的基础设施。尤其在流量风控/反爬场景，系统往往需要承接百万到千万级 QPS 的请求，并在固定窗口内基于“计数、比值、组合规则”做策略判定。该篇文章主要介绍在超高并发场景下的思考与实践

原创文章，转载请标注。[https://www.cnblogs.com/boycelee/p/19423815](https://www.cnblogs.com/boycelee/p/19423815)

目录

*   [声明](#声明)
*   [1、背景](#1背景)
*   [2、频次定义](#2频次定义)
*   [3、技术选型](#3技术选型)
    *   [3.1、选择 Flink + Redis 还是 Java + Redis？](#31选择-flink--redis-还是-java--redis)
        *   [选型对比](#选型对比)
    *   [3.2、为什么最终选择 Java + Redis，而不是 Flink + Redis？](#32为什么最终选择-java--redis而不是-flink--redis)
        *   [3.2.1、核心瓶颈是“热点 + ops/request”，不是“聚合能力不足”](#321核心瓶颈是热点--opsrequest不是聚合能力不足)
        *   [3.2.2、在热 Key 工程化做扎实的前提下，简单频次更偏向 Java + Redis](#322在热-key-工程化做扎实的前提下简单频次更偏向-java--redis)
    *   [3.3、选择“实时“频次计算还是”准实时“频次计算？](#33选择实时频次计算还是准实时频次计算)
        *   [3.3.1、实时频次](#331实时频次)
        *   [3.3.2、准实时频次](#332准实时频次)
    *   [准实时频次](#准实时频次)
*   [4、系统设计](#4系统设计)
    *   [4.1、难点](#41难点)
    *   [4.2、思考](#42思考)
        *   [4.2.1、思考清楚是否需要实时频次？](#421思考清楚是否需要实时频次)
        *   [4.2.2、为什么不需要实时频次？](#422为什么不需要实时频次)
            *   [（1）技术角度：5ms 的确定性收益很低](#1技术角度5ms-的确定性收益很低)
            *   [（2）业务角度：窗口统计下，5ms vs 50ms对识别率影响有限](#2业务角度窗口统计下5ms-vs-50ms对识别率影响有限)
    *   [4.3、系统设计](#43系统设计)
        *   [4.3.1、选择 Java + Redis 方案的难点如何解决？](#431选择-java--redis-方案的难点如何解决)
        *   [4.3.2 、从“同步读写”到“同步读、异步写”](#432-从同步读写到同步读异步写)
        *   [4.3.3、热 Key：探测 + 本地累加 + 批量写入（把“读写都热”变成“读热为主”）](#433热-key探测--本地累加--批量写入把读写都热变成读热为主)
            *   [4.3.3.1、为什么常规手段不灵？](#4331为什么常规手段不灵)
            *   [4.3.3.2、为什么选“热 Key 动态探测”？](#4332为什么选热-key-动态探测)
            *   [4.3.3.3、核心机制：热 Key 本地累加 + 固定周期批量刷写](#4333核心机制热-key-本地累加--固定周期批量刷写)
            *   [**4.3.3.4、**本地缓存读结果](#4334本地缓存读结果)
        *   [4.4、存储设计](#44存储设计)
        *   [4.4.1、统计窗口类型：为什么选择“滚动窗口”而不是“滑动窗口”？](#441统计窗口类型为什么选择滚动窗口而不是滑动窗口)
            *   [4.4.1.1、窗口策略的局限性](#4411窗口策略的局限性)
            *   [4.4.1.2、频次策略的应用（解决）](#4412频次策略的应用解决)
            *   [4.4.1.3、多维度的综合拦截策略](#4413多维度的综合拦截策略)
        *   [4.4.2、存储结构设计](#442存储结构设计)
*   [5、整体架构](#5整体架构)
*   [7、总结](#7总结)
*   [8、最后](#8最后)

声明
==

原创文章，转载请标注。[https://www.cnblogs.com/boycelee/p/19423815](https://www.cnblogs.com/boycelee/p/19423815)

1、背景
====

在风控体系里，频次计算几乎是绕不开的基础设施。尤其在流量风控/反爬场景，系统往往需要承接**百万到千万级 QPS** 的请求，并在**固定窗口**内基于“计数、比值、组合规则”做策略判定。

当业务进入 **超高并发 + 指标爆炸 + 维度组合复杂** 的阶段，频次系统通常会被三股力量同时拉扯：

1.  **链路时延预算被极度压缩**：上游网关给流量安全底座分配总时延预算，底座为了保证自身稳定性，会继续压缩下游频次服务的时间（常见只剩 **5ms 级别**）。
2.  **指标与维度呈指数膨胀**：从 10+ 指标增长到 100+；从单维（账号/IP）演进到多维组合（账号+设备+IP+接口+场景…），Key 空间随之快速膨胀。
3.  **热 Key 叠加“读写都热”**：热点接口与热点资源会天然产生热 key，导致单分片 OPS 飙升、负载极不均匀，最终表现为**尾延迟抖动**；在这种模式下，简单“加机器/加分片”很难带来线性收益。

因此，频次系统的核心矛盾往往不是“算不算得动”，而是被迫在同一时间解决：**尾延迟、扩展性、稳定性**。  
当你看到 P50 还不错但 P99/P999 明显飙升、并开始出现超时与失败时，本质上就是系统已经被“共享状态存储 + 热点 + 放大效应”拖进了高并发频次系统的典型瓶颈区间。

**那么问题就变成：在这种极限约束下，频次系统应该怎么设计，才能既能跑得快、又能扩得开、还能稳得住？**

2、频次定义
======

频次统计本质上是在某个窗口内，对某个场景下、某个资源的请求进行计数或比值计算。

**举例说明**

在特定时间段内（如 10分钟、1 小时、1天），统计特定场景（如某个接口）下，特定资源（如账号、设备、IP）的请求数量，例如：

*   在 `search` 接口中，统计账号为 `wangwu123` 的一小时访问量（**累加指标**）。
*   在 `search` 接口中，统计 `ip=192.0.0.1 且账号为空` 的一小时请求量 与 统计 `ip=192.0.0.1` 的一小时请求量的比值（**占比指标**）。

这类需求的计算形式通常是 “ 简单Counter + 少量组合”，在在线链路中最关注“低延迟可用”，而不是复杂流式语义。

3、技术选型
======

3.1、选择 Flink + Redis 还是 Java + Redis？
-------------------------------------

### 选型对比

技术组合

典型适用场景

关键优势

主要代价/风险

**Java + Redis**

计数类、短窗口、少维度、在线强低延迟决策

架构简单、P99 可控、资源性价比高、发布运维轻

扩展性受热 key 与 操作数（ops）/请求数（request） 限制；复杂窗口/多维组合会导致 Redis 成本线性爆炸

**Flink + Redis**

需要实时**聚合**/状态管理，或希望把聚合从在线链路**解耦**

窗口/状态/事件时间语义成熟；可把在线写压力转移走

平台固定成本高（state/checkpoint/作业治理）；运维与故障模型更复杂

3.2、为什么最终选择 Java + Redis，而不是 Flink + Redis？
-------------------------------------------

可能会有疑问：既然 Flink 天生支持窗口聚合与批量输出，为什么不直接 Flink + Redis，把写压力天然压下去？  
我的结论是：**在“简单频次计算 + 强在线决策”的流量风控/反爬场景里，默认应优先 Java + Redis**。Flink 不是不行，而是它解决的是“流式聚合平台化”的问题；而我们当时的主矛盾是“**同步链路的尾延迟与热点写放大**”，且**频次语义本身并不复杂**。

### 3.2.1、核心瓶颈是“热点 + ops/request”，不是“聚合能力不足”

简单频次里计算极轻，真正把系统打爆的是：

*   热 key 导致分片负载不均、尾延迟飙升
*   请求数据（Request）/操作数（OPS）上升导致 Redis CPU/网络往返线性爆炸
*   同步读写叠加重试放大，形成雪崩（读写超时还频繁重试，导致出现踩踏问题，造成雪崩）

这类问题本质是**共享状态存储层的扩展性问题**。关键解法不是“换 Flink”，而是把 Redis 成本结构改掉：

*   热 key：本地累计 + 批量 flush（每请求写 -> 批量写）
*   IO 合并：通过 pipeline/Lua 或 通过单次IO同时读写，压缩操作数（ops）和请求数（request）
*   语义重构：同步读结果、过程写异步化（“读写都热”收敛为“读热为主”）

当这些工程化做扎实之后，Java + Redis 更具性价比。

### 3.2.2、在热 Key 工程化做扎实的前提下，简单频次更偏向 Java + Redis

在 **L1 本地聚合 + 热 Key 探测 + 批量 flush + 在线判定合并** 这一套方案成立且工程化扎实的前提下，可以给出相对明确的结论：

*   对“**简单频次计算 + 强在线决策**”场景，**Java + Redis 往往优于 Flink**（资源更省、延迟更低、运维更轻）。
*   但这个“更优”有严格边界：一旦进入 **事件时间语义、迟到乱序、复杂窗口/特征平台化** 等需求，Flink 的综合优势会反超。

3.3、选择“实时“频次计算还是”准实时“频次计算？
--------------------------

本质区别是两个问题：选择返回“最终判定结果”还是返回“过程值”？对于频次统计值“精确度”是否有明确要求？

### 3.3.1、实时频次

**实时频次**通常指：在请求链路内，频次系统必须在极短时间内返回“当前精确统计值/比值”，策略也在同步链路内完成判定。

这类场景常见于支付/交易等强约束链路：

*   延迟预算严格
*   判定必须实时
*   容错空间极小
*   同步链路内就要拿到精确数据

代价也很明确：同步 IO + 尾延迟敏感，一旦遇到网络抖动、中间件波动、机器性能参差、超卖等情况，服务极容易超过 SLA。

### 3.3.2、准实时频次

准实时频次
-----

**准实时频次**指：允许一定的计算延迟（几十毫秒到秒级），同步链路不一定要拿到“过程统计值”，而是拿到**最终判定结果**（例如命中结果、名单结果、风险决策结果），统计过程与写入过程可以异步化。

这在流量风控/反爬场景中非常常见：绝大多数策略关注的是“是否异常、是否命中、是否拦截”，而不是“当前计数到底是 N 还是 N+1”。

准实时频次带来的价值是结构性的：

*   将“同步读写”变为“同步读、异步写”
*   将底座高频访问从“读过程值”变为“读最终结果”
*   进而把“读写都热”的压力显著收敛为“读热为主”，给工程优化留出空间

4、系统设计
======

4.1、难点
------

1.  **是否真的需要实时频次？**
2.  如果不需要，**为什么不需要？**这是否会降低识别率？
3.  在“读多写多 + 热 Key + 指标爆炸”的前提下，**Java + Redis 的难点如何解决？**

4.2、思考
------

### 4.2.1、思考清楚是否需要实时频次？

在不少网关/底座场景里，下游频次服务会被分配一个类似 **5ms** 的响应预算。

例如：上游网关为流量底座预留 20ms，总链路预算中底座为了保证自身稳定性，会尽可能压缩下游子系统的时间，于是分配给实时计算/频次服务的时间可能只有 5ms。考虑到请求链路往往存在 2 次 IO（网络 + 存储），留给计算服务自身执行的时间甚至可能只剩 1~2ms。

这里的关键点是：**5ms 往往是技术分配指标，而不是业务需求指标。**  
如果我们把“技术性 SLA”当作“业务必须”，就会导致策略与系统被迫走向“同步拿过程值”，从而把系统推入“读写都热 + 尾延迟地狱”的死局。

### 4.2.2、为什么不需要实时频次？

#### （1）技术角度：5ms 的确定性收益很低

由于用户设备、网络与执行路径的差异，请求到达顺序本身就存在不确定性：先点击的不一定先到，后点击的也不一定后到。网络延迟、排队延迟、线程调度带来的误差量级，远大于 5ms。

例如，以下的抢茅台链路：

因此从工程角度看，“必须 5ms 内算完并返回过程统计值”的收益并不高。

#### （2）业务角度：窗口统计下，5ms vs 50ms对识别率影响有限

以“1 小时内统计账号访问次数”为例，5ms 与 50ms 的主要区别是统计频率与数据精细度。除非攻击者能在 50ms 内更换唯一标识，否则两者差别更多体现在“第 N 次识别还是第 N+1 次识别”，通常不会显著影响识别率。

而如果攻击者真的能在 50ms 内更换 PIN/EID/IP 等标识，那么策略重点应当转向**对抗成本与可伪造面**，而不是死磕频次链路的 5ms SLA，例如：

*   为什么账号成本仍然低？
*   指纹伪造难度是否被高估？
*   秒拨 IP/代理池是否未被有效识别？

结论：对流量风控/反爬而言，“同步 5ms 拿过程值”往往是**伪需求**；真正的需求是“同步快速拿到最终判定结果”。

4.3、系统设计
--------

### 4.3.1、选择 Java + Redis 方案的难点如何解决？

下面进入工程落地：在接受“准实时频次”这一核心前提后，Java + Redis 落地的关键，是把架构从“同步读写拿过程值”调整为“同步读拿结果、异步写沉淀过程”，并用热 Key 探测与批量写入把存储压力降下来。

### 4.3.2 、从“同步读写”到“同步读、异步写”

**方案**

1）将“同步读写”模式优化为“同步读、异步写”模式。  
2）将读取“具体统计值”优化为读取“最终识别结果”。

**解释**

*   同步链路依然要求快（例如 5ms 内响应），但我们让它读的是“最终识别结果/名单命中”，而不是“过程统计值”。
*   统计累加、明细沉淀、窗口聚合等写操作允许异步化，从而显著降低同步链路时延压力与存储写放大。

**举例**

*   同步读写模式：底座调用频次服务，查询指标具体值并对本次请求累加，拿到具体值后在流量底座执行策略。
*   同步读、异步写模式：底座调用计算中心，计算中心查询需要的过程值并累加，但在计算中心完成策略判定，并将异常资源（账号、设备ID、IP等）输出到名单服务；底座同步链路只需要快速读取最终判定/名单结果。

### 4.3.3、热 Key：探测 + 本地累加 + 批量写入（把“读写都热”变成“读热为主”）

#### 4.3.3.1、为什么常规手段不灵？

在 “读多写多 + 缓存结果有延续性” 的频次场景里，很多常规手段并不奏效：

*   **热点拆分**：能缓解写热，但读热仍在；查询仍需汇总所有分片结果，Redis OPS不降反升。
*   **全量本地缓存**：指标规模大、存储需求可能达到 TB 级，落在应用本地不现实；即便有 LRU/LFU/TTL，命中率也不足以解决核心矛盾。
*   **预设缓存**：热 Key 不可预见，无法提前配置。

进一步地，当 KV 存储（如 Redis）存在热 Key 时，会出现分片 OPS 极不均匀，某些分片可能达到极高操作数，导致响应变慢，最终表现为**无法靠“加机器/加分片”线性扩展**。

* * *

#### 4.3.3.2、为什么选“热 Key 动态探测”？

既然热 Key 不可预设，就需要动态探测。

*   Redis 的热 Key 探测：存在热 Key 数量限制、客户端处理逻辑固定、且在多客户端写热 Key 时会引发缓存失效通知，难以解决“读写都热”的场景。在热Key数量不多的情况下，能够解决读热问题，但无法解决读写都热的问题。
*   JDHotkey（通用热 Key 探测框架）：不依赖 Redis，客户端引 jar 即可；支持待测 key 上报、热 key 推送、本地缓存、淘汰等；且热 Key 数量与客户端处理逻辑更灵活，更贴合这类场景。

使用JDHotkey作为动态热Key探测方案时，其能够允许自定义热Key出发条件，在X时间窗口内，达到Y次则认为是热Key。

#### 4.3.3.3、核心机制：热 Key 本地累加 + 固定周期批量刷写

*   探测部分：通过JDHotkey进行热Key探测，根据业务场景定义热Key窗口内频率，能够高效利用本地内存，提升缓存命中率
    
*   写本地缓存部分：使用 **双Map + 无锁** 使其能够能够在批量刷写时尽可能保证性能不同步阻塞；
    
*   固定周期批量刷写部分：固定周期可以是能够忍受数据不一致的最长时间；
    

通过 **JDHotkey探测 + 本地累加 + 固定周期批量写** 的方案，能够解决Java + Redis 的写热问题。

#### **4.3.3.4、**本地缓存读结果

*   探测部分：通过JDHotkey进行热Key探测，根据业务场景定义热Key窗口内频率；
*   读本地缓存部分：是用Caffeine针对探测出的热Key进行本地缓存，过期后自动丢弃。

通过 **JDHotkey探测 + 本地读结果 + 定时过期** 的方案，能够解读热问题。

### 4.4、存储设计

### 4.4.1、统计窗口类型：为什么选择“滚动窗口”而不是“滑动窗口”？

#### 4.4.1.1、窗口策略的局限性

无论使用“滚动窗口”还是“滑动窗口”，都可能存在被绕过的风险。“滚动窗口”的缺点在于每次开启新的统计窗口时会重新开始统计，只要使用窗口机制，就可能出现漏过。

**例如，如果爬虫知道访问频次的阈值是每小时100次，可以通过以下方式绕过检测：**

*   对于“滚动窗口”：每小时访问频次保持在100次以下。
    
*   对于“滑动窗口”：每小时访问频次保持在100次以下，并在达到99次后暂停1小时再继续。
    

**结论：仅仅依赖“长窗口、无间断”的计算方式在反爬虫场景中意义有限，攻击者可以通过降低频次来绕过检测。**

#### 4.4.1.2、频次策略的应用（解决）

*   频次策略应作为风险发现工具，而不是唯一的拦截手段。固定的频次阈值容易被攻击者探测，因此会动态调整。
    
*   后续应该考虑“黑白转换”的问题，即如何将多次达到频次阈值并被判断为爬虫的资源加入黑名单，并进行长期封禁。
    

#### 4.4.1.3、多维度的综合拦截策略

反爬策略会从请求、资源、设备、行为等多个维度进行综合拦截，使得攻击者难以从“黑盒视角”预测具体的拦截机制或命中策略，同时也会大大提升阈值探测的难度。

资源消耗与复杂性：

*   相较于“滚动窗口”，“滑动窗口”需要更多的资源和更复杂的机制来支撑，从以上分析结论看，这部分多出的资源和复杂度意义有限。

### 4.4.2、存储结构设计

**设计目标：存储最小；IO最少；占比指标保证原子性。**

*   简单频次：使用string或hash结构
*   去重指标：使用HyperLogLog
*   占比指标：使用hash结构

5、整体架构
======

把以上内容收敛成一张“可运行的架构图”，核心就是两条链路：

1.  **同步链路（必须快）**：读“最终判定结果/名单结果”
2.  **异步链路（可延迟）**：过程统计、本地聚合、批量刷写、存储沉淀

组件视角可概括为：

*   网关：承接业务流量
*   同步底座：被网关直接同步调用，在延迟预算极度紧张的情况下，尽可能只处理简单规则和撞名单
*   异步底座：通过消息中间件与同步底座进行解耦，异步底座允许一定处理延迟（秒级），能够调用外部依赖服务、处理复杂规则等
*   频次计算：可以是独立服务也可以是异步底座的模块，负责针对请求和相关资源进行频次统计
*   策略引擎：在同步底座和异步底座中都应该有策略引擎模块，处于不同引擎其负责的规则也不同，一般情况下同步引擎因为有时延要求处理简单规则，异步引擎时延要求相对宽松负责处理负责规则
*   名单服务：负责将实时、准实时和离线拉黑或拉白的资源进行存储，并提供同步底座和异步底座快速查询的能力
*   策略管理平台：作为策略运维的展示界面，负责承担规则增删查改、灰度、回滚等能力
*   配置中心：作为策略管理平台和策略引擎的中间层，通常情况下策略管理平台会频繁进行需求迭代，其系统优先级通常要低于引擎，为了避免高优先级系统强依赖于低优先级系统，加入一个中间层负责策略规则的推送，能够有效降低系统稳定性风险。

7、总结
====

软件工程没有银弹。频次系统之所以难，并不只是“性能优化难”，而是它很容易被一个看似合理的指标（例如 5ms）带偏方向。

我的核心经验是三步：

1.  **先识别伪需求**：5ms 往往是链路分配指标，不等于业务必须拿到“过程统计值”。
2.  **再重构目标**：同步链路读“最终结果/名单”，过程统计与写入异步化、准实时化。
3.  **最后用工程手段啃难点**：热 Key 探测 + 本地累加 + 批量写入 + 双缓存 + 稳定性优先的取舍，才能把系统真正带到“大促可用”的状态。

8、最后
====

懂的不多，做的太少。欢迎批评、指正。