---
layout: post
title: '论文解读：Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters'
date: "2025-06-07T00:41:46Z"
---
论文解读：Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters
==============================================================================

  论文发表于人工智能顶会NeurIPS（[原文链接](https://proceedings.neurips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html)）。当前的模型编辑器会因多次编辑损害模型性能，提出用于连续编辑的通用检索适配器(General Retrieval Adapters for Continual Editing, GRACE)：使用一个类似字典的结构（适配器）为需要修改的潜在表示构建新的映射，通过更新适配器来实现持续的模型行为编辑。

方法
==

  GRACE是一种不修改模型权重编辑预训练模型行为的方法，适用于任何模型。为模型$f$在需要修改的层$l$添加一个适配器，其中包含一个编码本$\\mathcal{C}=\\{(k\_i,v\_i,\\epsilon\_i)\\}\_{i=1}^C$。其中的$k\_i,v\_i$表示修改后的输入和它对应的输出（通过反向传播微调得到），$\\epsilon\_i$表示当该层输入$h^{l-1}$与$k\_i$的距离$d(h^{l-1},k\_i)$小于$\\epsilon\_i$时，就使用$v\_i$作为该层输出，否则正常输出该层的推理结果。这里的距离计算方式$d(\\cdot)$使用欧氏距离。

  GRACE编辑模型某层$l$的过程就是往编码本$\\mathcal{C}$中添加词条的过程。如果$f(x\_t)\\neq y\_t$，就期望通过修改所选的某层$l$关于$x\_t$的输出$h^l$来使得$f(x\_t)= y\_t$：

  1、对于第一个模型不满足的样本$(x\_0,y\_0)$来说，就是直接往$\\mathcal{C}$中添加$(h^{l-1}\_0,h^l\_0,\\epsilon\_{ini})$。其中$h^{l-1}\_0$是$x\_0$在第$l-1$层的输出，$h^l\_0$则是通过优化第$l$层的输出使得模型输出为$y\_t$得到，$\\epsilon\_{ini}$是需要人为定义的初始化距离。

  2、对于后续$f(x\_t)\\neq y\_t$的情况，使用算法1来对$\\mathcal{C}$进行更新。

  对于PLM来说，作者仅在相应的层上对输入句子的最后一个token进行编辑，通过实验验证。

实验
==

  图2：在toy二分类MLP上的编辑结果。a可视化模型正确分类两类样本。b在红色样本区域额外添加了要被判别为蓝色的样本。c修改前，模型将新增的样本判别为红。d修改后，模型将新增样本判别为蓝，并不影响其他判别。

  表1：各方法在三个数据集和对应模型上的编辑效果。TRR：模型编辑后对原始测试集的保留度。ERR：按顺序编辑模型对之前编辑的记忆的保留度。#E：模型使用各方法编辑的次数。Hallucination是对大模型幻觉的编辑，测试的是根据prompt生成句子的PPL。ROME没有在T5上实验是因为它只适用于GPT。

  图4：初始距离$\\epsilon\_{ini}$和模型编辑块的选择对编辑效果的影响。其中，Holdout是在没见过的数据集上的泛化效果。可以看出：

*   随着编辑数量的增加，在Holdout上的准确率会变高，说明了编辑的泛化效果。
    
*   中间层的编辑效果相较于两端编辑效果更好。
    
*   $\\epsilon\_{ini}$越小，对原始记忆的保留程度越不容易随着编辑次数的增加而降低。
    
*   $\\epsilon\_{ini}$越大，随着编辑次数的增加，在$\\mathcal{C}$中添加的词条增长越缓慢。
    

问题
==

  1、球之间的重叠不能避免：当添加第二个不同标签的球后，第一个球和第二个球相切，如果再来一个样本需要扩大前两个球之一，也就是算法中的Expand情况，两个球就会重叠，导致重叠部分输出不一致。

  2、无法确定潜在变量之间的距离能表征输出之间的语义相似性，如果属于某个非线性空间，计算欧氏距离可能有问题