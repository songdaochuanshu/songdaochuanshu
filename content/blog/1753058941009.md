---
layout: post
title: '一文读懂什么是逻辑回归'
date: "2025-07-21T00:49:01Z"
---
一文读懂什么是逻辑回归
===========

逻辑回归介绍
------

逻辑回归（Logistic Regression）是一种经典的分类算法，尽管名字中带有 “回归”，但它本质上用于解决二分类问题（也可扩展到多分类）。

逻辑回归的本质是 “在线性回归的基础上，通过一个映射函数将输出转化为概率（从而实现对类别概率的预测）”，这个映射函数就是Sigmoid函数。

逻辑回归是机器学习中最基础的分类算法之一，核心是通过 Sigmoid 函数将线性输出转化为概率，结合交叉熵损失和梯度下降求解参数。

它虽简单，但在实际业务中（尤其是需要可解释性的场景）仍被广泛使用，也是理解更复杂分类模型（如神经网络）的基础。

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719150043458-2135026279.png)

sigmoid函数
---------

def sigmoid(z):
    """
    Compute the sigmoid of z

    Args:
        z (ndarray): A scalar, numpy array of any size.

    Returns:
        g (ndarray): sigmoid(z), with the same shape as z
         
    """

    g = 1 / (1 + np.exp(-z))
   
    return g

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719150719892-1818432947.png)

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719150733699-567121821.png)

逻辑回归模型
------

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719152848254-2101356603.png)

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719152916145-1191438561.png)

逻辑回归的决策边界
---------

### 线性逻辑回归

根据sigmoid函数图象：`z=0`是中间位置，视为决策边界；那么为了得到决策边界的特征情况，我们假设：

*   线性模型 `z = w1 * x1 + w2 * x2 + b`
*   参数 `w1=w2=1, b=03`，那么`x2 = -x1 + 3`这条直线就是**决策边界**

如果特征x在这条线的右边，那么此逻辑回归则预测为1，反之则预测为0；（分为两类）

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719153223071-1789906082.png)

### 多项式逻辑回归

多项式回归决策边界，我们假设：

*   多项式模型：`z = w1 * x1**2 + w2 * x2**2 + b`
*   参数：`w1=w2=1, b=-1`

如果特征x在圆的外面，那么此逻辑回归则预测为1，反之则预测为0；（分为两类）

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719153309384-1205378821.png)

扩展：随着多项式的复杂度增加，还可以拟合更更多非线性的复杂情况![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719153426888-199403306.png)

逻辑回归的损失函数
---------

### 平方损失和交叉熵损失

回顾下线性回归的损失函数（平方损失）：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719153719671-1673522679.png)

平方误差损失函数不适用于逻辑回归模型：平方损失在逻辑回归中是 “非凸函数”（存在多个局部最优解），难以优化；

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719153847721-287340641.png)

所以我们需要一个新的损失函数，即交叉熵损失；交叉熵损失是 “凸函数”，可通过梯度下降高效找到全局最优。

交叉熵源于信息论，我们暂时不做深入介绍，直接给出交叉熵损失函数公式：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719155249252-2090346779.png)

### 对数回顾

复习下对数函数的性质，以便理解为什么 交叉熵损失是 “凸函数”？

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719155005206-742988705.png)

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719155015526-1110375783.png)

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719155024370-571284410.png)

### 简化交叉熵损失函数

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719155736926-1495639515.png)

为什么要用这个函数来表示？来源自 最大释然估计（Maximum Likelihood），这里不做过多介绍。

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719155842431-2028539724.png)

简化结果：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719155922737-1965757707.png)

逻辑回归的梯度计算
---------

自然对数求导公式：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719161728120-838523196.png)

 链式求导法则：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719161001371-1990333956.png)

⚠️注意：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719161243287-718641201.png)

过拟合问题
-----

### 线性回归过拟合

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719162438753-234044465.png)

### 逻辑回归过拟合

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719162352906-1216591766.png)

*   欠拟合（underfit），存在高偏差（bias）
*   泛化（generalization）：希望我们的学习算法在训练集之外的数据上也能表现良好（预测准确）
*   过拟合（overfit），存在高方差（variance） 

### 解决过拟合的办法

*   特征选择：只选择部分最相关的特征（基于直觉intuition）进行训练；缺点是丢掉了部分可能有用的信息
*   正则化：正则化是一种更温和的减少某些特征的影响，而无需做像测地消除它那样苛刻的事：
    *   鼓励学习算法缩小参数，而不是直接将参数设置为0（保留所有特征的同时避免让部分特征产生过大的影响）
    *   鼓励把 w1 ~ wn 变小，b不用变小 

正则化模型
-----

_It turns out that regularization is a way_

_to more gently reduce ths impacts of some of the features without doing something as harsh as eliminating it outright._

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719163012154-1372342465.png)

关于正则化项的说明：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719163119228-1230537192.png)

带正则化项的损失函数
----------

### 正则化线性回归

损失函数：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719164152058-118750640.png)

梯度计算：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719164259814-258902269.png)

分析梯度计算公式，由于alpha和lambda通常是很小的值，所以相当于在每次迭代之前把参数w缩小了一点点，这也就是正则化的工作原理，如下所示：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719164633814-599642976.png)

### 正则化逻辑回归

损失函数：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719164213049-1894293751.png)

梯度计算：

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719164900407-1358074746.png)

### 线性回归和逻辑回归正则化总结

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719165347997-602139013.png)

逻辑回归实战
------

### 模型选择

可视化训练数据，基于此数据选择线性逻辑回归模型

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719165914364-1641646286.png)

### 关键代码实现

def sigmoid(z):
	g = 1 / (1 + np.exp(-z))
	return g

def compute\_cost(X, y, w, b, lambda\_= 1):
	"""
    Computes the cost over all examples
    Args:
      X : (ndarray Shape (m,n)) data, m examples by n features
      y : (array\_like Shape (m,)) target value 
      w : (array\_like Shape (n,)) Values of parameters of the model      
      b : scalar Values of bias parameter of the model
      lambda\_: unused placeholder
    Returns:
      total\_cost: (scalar)         cost 
    """

	m, n = X.shape
	total\_cost = 0
	for i in range(m):
		f\_wb\_i = sigmoid(np.dot(X\[i\], w) + b)
		loss = -y\[i\] \* np.log(f\_wb\_i) - (1 - y\[i\]) \* np.log(1 - f\_wb\_i)
		total\_cost += loss

	total\_cost = total\_cost / m
	return total\_cost

def compute\_gradient(X, y, w, b, lambda\_=None): 
    """
    Computes the gradient for logistic regression 
 
    Args:
      X : (ndarray Shape (m,n)) variable such as house size 
      y : (array\_like Shape (m,1)) actual value 
      w : (array\_like Shape (n,1)) values of parameters of the model      
      b : (scalar)                 value of parameter of the model 
      lambda\_: unused placeholder.
    Returns
      dj\_dw: (array\_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. 
      dj\_db: (scalar)                The gradient of the cost w.r.t. the parameter b. 
    """
    m, n = X.shape
    dj\_dw = np.zeros(n)
    dj\_db = 0.

    for i in range(m):
        f\_wb\_i = sigmoid(np.dot(X\[i\], w) + b)
        diff = f\_wb\_i - y\[i\]
        dj\_db += diff
        for j in range(n):
            dj\_dw\[j\] = dj\_dw\[j\] + diff \* X\[i\]\[j\]
    
    dj\_db = dj\_db / m
    dj\_dw = dj\_dw / m
        
    return dj\_db, dj\_dw

def gradient\_descent(X, y, w\_in, b\_in, cost\_function, gradient\_function, alpha, num\_iters, lambda\_): 
    """
    Performs batch gradient descent to learn theta. Updates theta by taking 
    num\_iters gradient steps with learning rate alpha
    
    Args:
      X :    (array\_like Shape (m, n)
      y :    (array\_like Shape (m,))
      w\_in : (array\_like Shape (n,))  Initial values of parameters of the model
      b\_in : (scalar)                 Initial value of parameter of the model
      cost\_function:                  function to compute cost
      alpha : (float)                 Learning rate
      num\_iters : (int)               number of iterations to run gradient descent
      lambda\_ (scalar, float)         regularization constant
      
    Returns:
      w : (array\_like Shape (n,)) Updated values of parameters of the model after
          running gradient descent
      b : (scalar)                Updated value of parameter of the model after
          running gradient descent
    """
    
    # number of training examples
    m = len(X)
    
    # An array to store cost J and w's at each iteration primarily for graphing later
    J\_history = \[\]
    w\_history = \[\]

    w = copy.deepcopy(w\_in)
    b = b\_in
    
    for i in range(num\_iters):
        dj\_db, dj\_dw = gradient\_function(X, y, w, b, lambda\_)
        w = w - alpha \* dj\_dw
        b = b - alpha \* dj\_db
        cost = cost\_function(X, y, w, b, lambda\_)
        J\_history.append(cost)
        w\_history.append(w)
        if i % math.ceil(num\_iters / 10) == 0:
            print(f"{i:4d} cost: {cost:6f}, w: {w}, b: {b}")
        
    return w, b, J\_history, w\_history #return w and J,w history for graphing


def predict(X, w, b): 
    m, n = X.shape   
    p = np.zeros(m)
    for i in range(m):
        f\_wb = sigmoid(np.dot(X\[i\], w) + b)
        p\[i\] = f\_wb >= 0.5 
    return p

### 结果展示

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font\_manager as fm
# 支持显示中文
font\_path = '/System/Library/Fonts/STHeiti Light.ttc'
custom\_font = fm.FontProperties(fname=font\_path)
plt.rcParams\["font.family"\] = custom\_font.get\_name()

# 载入训练集
X\_train, y\_train = load\_data("data/ex2data1.txt")
# 训练模型
np.random.seed(1)
intial\_w = 0.01 \* (np.random.rand(2).reshape(-1,1) - 0.5)
initial\_b = -8
iterations = 10000
alpha = 0.001
w\_out, b\_out, J\_history,\_ = gradient\_descent(X\_train ,y\_train, initial\_w, initial\_b, compute\_cost, compute\_gradient, alpha, iterations, 0)

# 根据训练结果（w\_out和b\_out）计算决策边界
#f = w0\*x0 + w1\*x1 + b
# x1 = -1 \* (w0\*x0 + b) / w1
plot\_x = np.array(\[min(X\_train\[:, 0\]), max(X\_train\[:, 0\])\])
plot\_y = (-1. / w\_out\[1\]) \* (w\_out\[0\] \* plot\_x + b\_out)
# 将训练数据分类
x0s\_pos = \[\]
x1s\_pos = \[\]
x0s\_neg = \[\]
x1s\_neg = \[\]
for i in range(len(X\_train)):
    x = X\_train\[i\]
    # print(x)
    y\_i = y\_train\[i\]
    if y\_i == 1:
        x0s\_pos.append(x\[0\])
        x1s\_pos.append(x\[1\])
    else:
        x0s\_neg.append(x\[0\])
        x1s\_neg.append(x\[1\])

# 绘图
plt.figure(figsize=(8, 6))
plt.scatter(x0s\_pos, x1s\_pos, marker='o', c='green', label="Admitted")
plt.scatter(x0s\_neg, x1s\_neg, marker='x', c='red', label="Not admitted")
plt.plot(plot\_x, plot\_y, lw=1, label="决策边界")
plt.xlabel('Exam 1 score', fontsize=12)
plt.ylabel('Exam 2 score', fontsize=12)
plt.title('在二维平面上可视化分类模型的决策边界', fontsize=14)
plt.legend(fontsize=12, loc='upper center')
plt.grid(True)
plt.show()


# 使用训练集计算预测准确率
p = predict(X\_train, w\_out, b\_out)
print('Train Accuracy: %f'%(np.mean(p == y\_train) \* 100)) 
# Train Accuracy: 92.000000

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719170226106-454878936.png)

正则化逻辑回归实战
---------

### 模型选择

可视化训练数据，基于此数据选择多项式逻辑回归模型

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719170336772-1828071939.png)

### 关键代码实现

由于要拟合非线性决策边界，所以要增加特征的复杂度（训练数据里只有2个特征）。

特征映射函数

\# 将输入特征 X1 和 X2 转换为六次多项式特征
# 这个函数常用于逻辑回归或支持向量机等模型中，通过增加特征的复杂度来拟合非线性决策边界。
def map\_feature(X1, X2):
    """
    Feature mapping function to polynomial features    
    """
    X1 = np.atleast\_1d(X1)
    X2 = np.atleast\_1d(X2)
    degree = 6
    out = \[\]
    for i in range(1, degree+1):
        for j in range(i + 1):
            out.append((X1\*\*(i-j) \* (X2\*\*j)))
    return np.stack(out, axis=1)

正则化后的损失函数和梯度计算函数

def compute\_cost\_reg(X, y, w, b, lambda\_ = 1):
    """
    Computes the cost over all examples
    Args:
      X : (array\_like Shape (m,n)) data, m examples by n features
      y : (array\_like Shape (m,)) target value 
      w : (array\_like Shape (n,)) Values of parameters of the model      
      b : (array\_like Shape (n,)) Values of bias parameter of the model
      lambda\_ : (scalar, float)    Controls amount of regularization
    Returns:
      total\_cost: (scalar)         cost 
    """
    m, n = X.shape
    # Calls the compute\_cost function that you implemented above
    cost\_without\_reg = compute\_cost(X, y, w, b) 
    
    reg\_cost = 0.
    for j in range(n):
        reg\_cost += w\[j\]\*\*2
    
    # Add the regularization cost to get the total cost
    total\_cost = cost\_without\_reg + (lambda\_/(2 \* m)) \* reg\_cost

    return total\_cost

def compute\_gradient\_reg(X, y, w, b, lambda\_ = 1): 
    """
    Computes the gradient for linear regression 
 
    Args:
      X : (ndarray Shape (m,n))   variable such as house size 
      y : (ndarray Shape (m,))    actual value 
      w : (ndarray Shape (n,))    values of parameters of the model      
      b : (scalar)                value of parameter of the model  
      lambda\_ : (scalar,float)    regularization constant
    Returns
      dj\_db: (scalar)             The gradient of the cost w.r.t. the parameter b. 
      dj\_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. 

    """
    m, n = X.shape
    
    dj\_db, dj\_dw = compute\_gradient(X, y, w, b)

    # Add the regularization 
    for j in range(n):
        dj\_dw\[j\] += (lambda\_ / m) \* w\[j\]
        
    return dj\_db, dj\_dw

### 结果展示

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font\_manager as fm
# 支持显示中文
font\_path = '/System/Library/Fonts/STHeiti Light.ttc'
custom\_font = fm.FontProperties(fname=font\_path)
plt.rcParams\["font.family"\] = custom\_font.get\_name()

# 载入训练集
X\_train, y\_train = load\_data("data/ex2data2.txt")
# 通过增加特征的复杂度来拟合非线性决策边界
X\_mapped = map\_feature(X\_train\[:, 0\], X\_train\[:, 1\])
print("Original shape of data:", X\_train.shape)
print("Shape after feature mapping:", X\_mapped.shape)

# 训练模型
np.random.seed(1)
initial\_w = np.random.rand(X\_mapped.shape\[1\])-0.5
initial\_b = 1.
# Set regularization parameter lambda\_ to 1 (you can try varying this)
lambda\_ = 0.5
iterations = 10000
alpha = 0.01
w\_out, b\_out, J\_history, \_ = gradient\_descent(X\_mapped, y\_train, initial\_w, initial\_b, compute\_cost\_reg, compute\_gradient\_reg, alpha, iterations, lambda\_)

# 根据训练结果（w\_out和b\_out）计算决策边界
# - 创建网格点 u 和 v 覆盖特征空间
u = np.linspace(-1, 1.5, 50)
v = np.linspace(-1, 1.5, 50)
# - 计算每个网格点处的预测概率 z
z = np.zeros((len(u), len(v)))
# Evaluate z = theta\*x over the grid
for i in range(len(u)):
    for j in range(len(v)):
        z\[i,j\] = sig(np.dot(map\_feature(u\[i\], v\[j\]), w\_out) + b\_out)
# - 转置 z 是必要的，因为contour函数期望的输入格式与我们的计算顺序不一致      
z = z.T

# 分类
x0s\_pos = \[\]
x1s\_pos = \[\]
x0s\_neg = \[\]
x1s\_neg = \[\]
for i in range(len(X\_train)):
    x = X\_train\[i\]
    # print(x)
    y\_i = y\_train\[i\]
    if y\_i == 1:
        x0s\_pos.append(x\[0\])
        x1s\_pos.append(x\[1\])
    else:
        x0s\_neg.append(x\[0\])
        x1s\_neg.append(x\[1\])

# 绘图
plt.figure(figsize=(8, 6))
plt.scatter(x0s\_pos, x1s\_pos, marker='o', c='black', label="y=1")
plt.scatter(x0s\_neg, x1s\_neg, marker='x', c='orange', label="y=0")
# 绘制决策边界（等高线）
plt.contour(u,v,z, levels = \[0.5\], colors="green")
# 创建虚拟线条用于图例（颜色和线型需与等高线一致）
plt.plot(\[\], \[\], color='green', label="决策边界")

plt.xlabel('Test 1', fontsize=12)
plt.ylabel('Test 2', fontsize=12)
plt.title('正则化逻辑回归模型分类效果可视化（lambda=0.5）', fontsize=14)
# plt.legend(fontsize=12, loc='upper center')
plt.legend(fontsize=12)
plt.grid(True)
plt.show()


#Compute accuracy on the training set
p = predict(X\_mapped, w\_out, b\_out)
print('Train Accuracy: %f'%(np.mean(p == y\_train) \* 100))
# Train Accuracy: 83.050847

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719170656771-525522188.png)

正则化效果对比
-------

### 正则化对损失和决策边界的影响

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719171040392-1971647889.png)

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719171054099-268643739.png)

### 正则化项lambda参数大小对决策边界的影响

![](https://img2024.cnblogs.com/blog/699056/202507/699056-20250719171256465-519796779.png)

参考
--

吴恩达团队在Coursera开设的机器学习课程：https://www.coursera.org/specializations/machine-learning-introduction

在B站学习：https://www.bilibili.com/video/BV1Pa411X76s 

#MySignature { background-color: #f8f8ee; border: solid 1px #e8e7d0; padding: 10px; margin-bottom: 10px; color: gray }

作者：[Standby](http://www.cnblogs.com/standby/) — **一生热爱名山大川、草原沙漠，还有我们小郭宝贝！**  
出处：[http://www.cnblogs.com/standby/](http://www.cnblogs.com/standby/)  
  
本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留追究法律责任的权利。