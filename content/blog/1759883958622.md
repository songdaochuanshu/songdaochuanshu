---
layout: post
title: '吴恩达深度学习课程一：神经网络和深度学习 第二周：神经网络基础（二）'
date: "2025-10-08T00:39:18Z"
---
吴恩达深度学习课程一：神经网络和深度学习 第二周：神经网络基础（二）
==================================

此分类用于记录吴恩达深度学习课程的学习笔记。  
课程相关信息链接如下：

1.  原课程视频链接：[\[双语字幕\]吴恩达深度学习deeplearning.ai](https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&from_spmid=playlist.playlist-detail.0.0&is_story_h5=false&mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1713085655&unique_k=DfBgvFW&up_id=8654113&vd_source=e035e9878d32f414b4354b839a4c31a4)
2.  github课程资料，含课件与笔记:[吴恩达深度学习教学资料](https://github.com/robbertliu/deeplearning.ai-andrewNG)
3.  课程配套练习（中英）与答案：[吴恩达深度学习课后习题与答案](https://blog.csdn.net/u013733326/article/details/79827273)

本篇为第一课第二周，是[2.1](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=7)和[2.2](https://www.bilibili.com/video/BV1FT4y1E74V/?p=8&spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=e035e9878d32f414b4354b839a4c31a4)部分的笔记内容。

* * *

本周的课程以逻辑回归为例详细介绍了神经网络的运行，传播等过程，其中涉及大量机器学习的基础知识和部分数学原理，如没有一定的相关基础，理解会较为困难。  
因为，笔记并不直接复述视频原理，而是从基础开始，尽可能地创造一个较为丝滑的理解过程。

首先，经过之前的[基础补充](https://www.cnblogs.com/Goblinscholar/p/19126763)，我们了解了关于回归的一些概念，本篇我们便跟随课程的顺序进行学习，同样，会补充相当一部分的相关基础知识帮助理解。

我们以一个问题来引入，现在我们知道了回归是什么，课程中又是用逻辑回归为例讲解，但实际上，**逻辑回归是一个分类算法**。  
那么问题来了，为什么一个名字叫回归的算法实际上是用来分类的？这里的回归算法和分类算法又到底差在哪呢？  
我们以此开始本篇笔记的内容。

1.分类
====

1.1 什么是分类？
----------

还是先上概念：

> **分类**（Classification）是机器学习中的一种监督学习方法，其主要任务是将输入的数据样本分配到预定的类别标签中。分类的目标是通过训练模型，使其能够根据输入的特征预测一个或多个类别标签。

结合之前我们了解的回归的概念，不难发现，二者的差别实际上在**目标输出**上：  
通过数据预测房价，身高，体重等，**输出连续型数值即为回归问题**。  
而通过输入数据研判其属于某种标签，如输入一张猫的图片，输出这张图“是猫”或“不是猫”。  
又比如输入一段新闻文字，输出其属于“军事新闻”或“娱乐新闻”等，**输出离散型结果即为分类问题**。

1.2 分类是怎么实现的？
=============

还是先回想之前提到的线性回归，假设存在\\(n\\)个输入特征的情况下，线性回归的拟合通式如下：

\\\[y=w\_{1}x\_{1}+w\_{2}x\_{2}+w\_{3}x\_{3}+w\_{4}x\_{4}+\\cdots+w\_{n}x\_{n}+b \\\]

再专业一些，用求和符号来表示：

\\\[y = \\sum\_{i=1}^{n} w\_i x\_i + b \\\]

不管哪种形式，又或者其他回归算法，让我们很轻松地理解到所谓“回归”的都是一点：**把输入代入公式得到的 \\(y\\) 即为最终输出**。  
不提前后的一些处理和特殊算法，把特征代入已经拟合好的函数，得到 \\(y\\) ，这就是预测结果，回归核心已经结束。

那分类的离散型输出又是如何实现的？  
其中答案之前就已经出现过了，我们再回忆[第一周](https://www.cnblogs.com/Goblinscholar/p/19124999)的房价预测案例：  
我们在隐藏层添加ReLU激活函数后，即可处理房屋面积和房屋价格都不可能为负数的情况。  

如图所示，我们的隐藏层计算结果经过**ReLU激活函数，把所有小于等于0的结果，这些所有连续型的数值映射成离散值0**，来解决实际输出不可能小于0的问题。

再回顾一下之前对激活函数的总结：**激活函数就是对输出引入非线性的变换，提供了处理复杂能力的关系。**  
而现在，我们希望输出结果可用于分类，是不是也能找到相应的激活函数呢？  
答案是当然的。

我们把分类问题分为最基础的二分类和在此之上的多分类问题，而这两类分类问题又有其各自适用的激活函数：

1.  二分类：Sigmoid 激活函数
2.  多分类：Softmax 激活函数

我们先行介绍马上就会用到的Sigmoid 激活函数，Softmax再之后遇到时再展开。

1.3 Sigmoid 激活函数怎么帮助实现二分类？
--------------------------

我们先来看看Sigmoid 激活函数的公式：

\\\[f(x) = \\frac{1}{1 + e^{-x}} \\\]

其图像如下：  

整合其特点如下：

*   **输出范围**：Sigmoid 的输出范围是 (0,1)，因此可以将其结果视为概率值。
*   **平滑性**：Sigmoid 函数是一个平滑的 S 型曲线，随着输入值的增加，输出会逐渐趋近于 1，而输入值减小时，输出会趋近于 0。
*   **单调性**：Sigmoid 函数是单调递增的，意味着输入越大，输出越接近 1。

总结其作用：**Sigmoid会把输入根据大小映射到0到1之间，随着越接近两端，梯度也逐渐减小。**

当然，也不难发现问题，我们不是要做分类吗？为什么经过Sigmoid最后得到的还是一个数？  
这便涉及到分类问题的另一概念：**决策阈值**，这个概念实现了从概率到分类的转换。

1.4 决策阈值：从概率到二分类
----------------

先回看刚刚的总结的sigmoid的一条特点：

> **输出范围**：Sigmoid 的输出范围是 (0,1)，因此可以将其结果视为概率值。

在概率论中，概率值通常是一个在0到1之间的数字，用来描述某个事件发生的可能性。例如，对于一个二分类问题，我们关心的是样本属于某个类的概率。Sigmoid的输出恰好满足这个需求：

*   **值接近1**：输入很大，表示事件（例如样本属于正类）的发生概率非常高。
*   **值接近0**：输入很小，表示事件的发生概率非常低。

**在二分类问题中，通常我们使用0.5作为阈值来决定类别。**  
即如果Sigmoid函数的输出大于0.5，我们就预测为正类（类别1）；如果小于0.5，则预测为负类（类别0）。这个0.5阈值本质上表示模型认为正负类的概率相等，我们也可以根据任务情境手动设置阈值。

由此，我们便用sigmoid帮助实现了二分类，理解了这个过程之后，我们终于可以开始提及已久的逻辑回归了。

2\. 二分类实例：是不是猫
==============

这一部分实际上是对[2.1](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=7)内容的系统阐述，记录一些常见的参数表示，便于之后使用和理解。  
在了解二分类的一些原理后，我们看这样一个问题，输入一幅图像，我们要判断图像中是不是猫。  

如上图所示，首先，我们要知道图像在计算机中的模样。  
**一幅彩色图像在计算机中被理解为红，绿，蓝三个通道，即用三个相同大小的矩阵表示一幅图像，其中，每个元素在0~255之间，表示该像素在该通道上的亮度。**

这里，我们设定每幅图像大小为：64\*64  
为此，如果我们要用特征向量表示一幅图像，这个特征向量的维度大小就应该为：64\*64\*3=12288  
**我们用 \\(n\_{x}\\) 或者直接用 \\(n\\) 来表示特征向量的维度大小。**

现在，一个数据集（数据的集合，比如这里是猫的图像和相应的标签）中用于训练的样本有\\(m\\)  
个，有时候为了强调，会用\\(m\_{train}\\)表示训练集数量，用\\(m\_{test}\\)表示测试集数量作区分。  
而对于一个包含数据和标签的样本，我们写作\\((x,y)\\)  
其中：  
\\(x \\in \\mathbb{R}^n\\)，\\(\\mathbb{R}^n\\) 表示 **n维实数空间**。它是一个包含所有 **n维实数向量** 的空间，用来表示具有 **n个实数特征** 的数据点。  
在 \\(\\mathbb{R}^n\\) 中，向量 \\(\\mathbf{x}\\) 是一个 \\(n\\) 维的列向量，表示为：

\\\[\\mathbf{x} = \\begin{bmatrix} x\_1 \\\\ x\_2 \\\\ \\vdots \\\\ x\_n \\end{bmatrix} \\\]

同时，\\(y \\in \\{0, 1\\}\\) ，代表是猫或者不是猫。  
而对于某个具体的样本则这样表示：第一个样本 \\((x^{(1)},y^{(1)})\\) 第二个 \\((x^{(2)},y^{(2)})\\) 依此类推。

最后，我们定义一个矩阵 \\(X\\) 来表示训练集数据，其内容为

\\\[\\mathbf{X} = \\begin{bmatrix} \\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\ x^{(1)} & x^{(2)} & x^{(3)} & \\cdots & x^{(m)} \\\\ \\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\end{bmatrix} \\\]

根据之前的符号表示，可知，这个矩阵有 \\(m\\) 列， \\(n\\) 行。  
同理，用 \\(Y\\) 表示训练集标签,这个矩阵有 \\(m\\) 列， \\(1\\) 行。

\\\[Y = \\begin{bmatrix} y^{(1)} & y^{(2)} & \\cdots & y^{(m)} \\end{bmatrix} \\\]

3.逻辑回归
======

经过较长的理解过程，逻辑回归算法其实已经呼之欲出了，我们先摆两个之前出现的公式。

\\\[线性回归：y = \\sum\_{i=1}^{n} w\_i x\_i + b \\\]

\\\[sigmoid: f(x) = \\frac{1}{1 + e^{-x}} \\\]

现在我们看下面的逻辑回归公式：

\\\[\\hat{y}=P(y = 1 \\mid \\mathbf{x}) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}} \\\]

没错，逻辑回归就是将线性回归的结果输入sigmoid激活函数得到最终输出后通过决策阈值确定分类。  
简洁一些： **逻辑回归=线性回归+sigmoid激活函数**  
现在来解释一下公式里的一些参数：

1.  \\(\\hat y\\) 代表 \\(y\\) 的预测值。
2.  \\(P(y = 1 \\mid \\mathbf{x})\\)是条件概率的表示，意思是给定特征向量 \\(x\\) 的情况下，目标变量 \\(y\\) 等于 1 的概率。
3.  \\((\\mathbf{w}^T \\mathbf{x} + b)=w\_1 x\_1 + w\_2 x\_2 + \\dots + w\_n x\_n + b\\) ，这里的 \\(w\\) 是权重向量， 上标 \\(T\\) 代表转置，\\(\\mathbf{w}^T \\mathbf{x}\\) 即为权重向量和特征向量的点积，是将所有特征加权求和。\\(b\\) 为偏置。

这样，我们便了解了逻辑回归本身。而拟合，实际上便是不断调整 \\(w\\) 和 \\(b\\) 的过程。  
至于如何调整，这便涉及到了神经网络的一个传播过程，这便是下面的篇章内容了。

最后，既然是分类算法，为什么逻辑回归不叫逻辑分类呢，根据本篇内容答案也有了大致方向：  
逻辑回归本质上是一个 分类模型，而它名字中的“回归”源于其与线性回归的相似性，尤其是它依赖于线性加权和的框架。它通过将线性回归的输出经过 sigmoid 函数的变换，来输出一个介于0和1之间的概率值，用于判断样本属于某一类的概率。因此，尽管逻辑回归最终用于分类，但它保留了“回归”的名称。