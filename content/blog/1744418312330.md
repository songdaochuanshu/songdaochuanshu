---
layout: post
title: 'RL · Exploration | 使用时序距离构造 intrinsic reward，鼓励 agent 探索'
date: "2025-04-12T00:38:32Z"
---
RL · Exploration | 使用时序距离构造 intrinsic reward，鼓励 agent 探索
========================================================

鼓励 agent 探索与当前 episode 历史在到达时间（temporal distance）上较远的状态。

  

*   论文标题：Episodic Novelty Through Temporal Distance.
*   ICLR 2025，8 8 6 5 poster。
*   arxiv：[https://arxiv.org/abs/2501.15418](https://arxiv.org/abs/2501.15418)
*   pdf：[https://arxiv.org/pdf/2501.15418](https://arxiv.org/pdf/2501.15418)
*   html：[https://arxiv.org/html/2501.15418](https://arxiv.org/html/2501.15418)
*   open review：[https://openreview.net/forum?id=I7DeajDEx7](https://openreview.net/forum?id=I7DeajDEx7)

* * *

目录

*   [01 论文主要内容](#01-论文主要内容)
    *   [1.1 这篇论文关注什么，想解决什么任务](#11-这篇论文关注什么想解决什么任务)
    *   [1.2 先前方法一般怎么做，有什么问题](#12-先前方法一般怎么做有什么问题)
    *   [1.3 这篇论文的 motivation，它希望解决什么 gap](#13-这篇论文的-motivation它希望解决什么-gap)
    *   [1.4 这篇论文的主要 method 是什么，算法流程是什么](#14-这篇论文的主要-method-是什么算法流程是什么)
        *   [📌 如何学习 temporal distance](#-如何学习-temporal-distance)
        *   [📌 使用 temporal distance 构造 intrinsic reward](#-使用-temporal-distance-构造-intrinsic-reward)
        *   [📌 具体算法](#-具体算法)
    *   [1.5 实验结果怎么样](#15-实验结果怎么样)
*   [02 行文逻辑](#02-行文逻辑)

* * *

01 论文主要内容
---------

论文概括：

*   这篇论文研究**稀疏奖励环境下的探索问题**，特别是在每次任务环境会变化的"情境 MDP"（Contextual Markov Decision Processes，CMDP，如随机生成的地图）中，如何让 agent 高效探索。
*   这篇论文设计了 ETD（**E**pisodic Novelty Through **T**emporal **D**istance）方法，核心创新是提出 **temporal distance（时序距离）**作为状态新颖性的衡量标准，通过对比学习估计时序距离，并生成 intrinsic reward 驱动探索。

### 1.1 这篇论文关注什么，想解决什么任务

*   **关注点**：解决 CMDP（Contextual Markov Decision Processes，情境 MDP，如随机地图导航、机器人在随机场景中完成任务）中 **稀疏奖励下探索效率低** 的问题。
*   **挑战**：传统方法依赖全局的经验，但 CMDP 每次环境不同（如新地图），历史经验无法直接复用（详见下文的 1.2）。

### 1.2 先前方法一般怎么做，有什么问题

*   **计数法**（如记录访问次数）：在连续/大状态空间失效（每个状态都"独特"，无法判断新颖性）。
*   **相似度法**（如欧氏距离）：无法捕捉状态的**动态关系**（如迷宫中的两个点看似近，但实际需要绕远路才能到达）。

### 1.3 这篇论文的 motivation，它希望解决什么 gap

*   现有方法缺少对**状态间动态关系**的建模（例如："绕远路"和"直达"在欧氏距离上可能相同）。【待 check】
*   提出用**时序距离**（从状态 A 到 B 所需的平均步数）作为更本质的相似性度量，可跨环境泛化。

### 1.4 这篇论文的主要 method 是什么，算法流程是什么

#### 📌 如何学习 temporal distance

首先，定义几个概念：

*   从 x 开始时在时间步 k 达到状态 y 的概率：\\(p^\\pi(s\_k=y|s\_0=x)\\)。
*   状态 x 到 y 的转换概率：\\(p^\\pi\_\\gamma(s\_f=y|s\_0=x)=(1-\\gamma)\\sum\_{k=0}^\\infty \\gamma^kp^\\pi(s\_k=y|s\_0=x)\\)。这个值 ≤ 1。个人理解，\\(s\_f\\) 的意思是在 \\(s\_0\\) 之后的一个状态。
*   准度量（Quasimetric）的定义：满足非负性、同一性（d(x,x)=0）和三角不等式，但无需对称性（d(x,y)≠d(y,x)）。

然后，根据 (Myers et al., 2024)，我们定义 x 到 y 的 temporal distance 为：

\\\[d^\\pi\_\\text{SD}(x,y)=\\log\\left(\\frac{p^\\pi\_\\gamma(s\_f=y|s\_0=y)}{p^\\pi\_\\gamma(s\_f=y|s\_0=x)}\\right) \\\]

x 越难到达 y，分母越小，\\(d^\\pi\_\\text{SD}(x,y)\\) 就越大。  
根据 (Myers et al., 2024)，这个定义是一个 Quasimetric，即使 MDP 是随机的。

然后，根据 (Ma & Collins, 2018; Poole et al., 2019)，我们用对比学习来学 \\(d^\\pi\_\\text{SD}(x,y)\\)。  
定义一个能量函数 \\(f(x,y)\\)，希望它对于两个互相容易到达的状态 x y 分配较大的值，而对于难以到达的状态分配较小的值。  
我们用 InfoNCE loss 训练它：

\\\[\\mathcal{L}\_{\\theta} = \\sum\_{i=1}^{B} \\left\[ \\log \\left( \\frac{\\exp f(x\_i, y\_i)}{\\sum\_{j=1}^{B} \\exp f(x\_j, y\_j)} \\right) + \\log \\left( \\frac{\\exp f(x\_i, y\_i)}{\\sum\_{j=1}^{B} \\exp f(x\_j, y\_j)} \\right) \\right\] \\\]

其中，(x,y) 是从 \\((x,y)\\sim p^\\pi\_\\gamma(s\_f=y|s\_0=x)p\_s(x)\\) 采样得到的，\\(p\_s(x)\\) 是 x 的边缘分布。  
根据 (Ma & Collins, 2018; Poole et al., 2019)，可以使用能量函数 \\(f(x,y)\\) 的唯一解来恢复 \\(d^\\pi\_\\text{SD}(x,y)\\)。  
根据 (Myers et al., 2024)，如果将 \\(f(x,y)\\) 分解为一个 potential \\(c(y)\\) 和一个 \\(d(x,y)\\) 的差值，即 \\(f(x,y)=c(y)-d(x,y)\\)，那么可以直接取用 \\(d(x,y)=d^\\pi\_\\text{SD}(x,y)\\)，这样就训练得到了时序距离。  
在 ETD 里，\\(c(y)\\) 是 MLP，而 \\(d(x,y)\\) 是用非对称的 MRN 实现的。

#### 📌 使用 temporal distance 构造 intrinsic reward

定义 ETD 的 intrinsic reward：\\(b\_\\text{ETD}(s\_t)=\\min\_{k\\in\[0,t\]}d(s\_k,s\_t)\\)，即，希望最大化这一个 episode 里先前状态 \\(s\_k\\) 到这个状态 \\(s\_t\\) 的 temporal distance，希望去一些尽可能难到达的地方。特别的，ETD 最大化的是最小的 temporal distance，即离当前状态 \\(s\_t\\) 最近的 \\(s\_k\\)，它们俩的 temporal distance。

#### 📌 具体算法

*   采样 context c，得到一个 CMDP；
*   while episode 没结束：
    *   根据 s 采样 action a，得到 s'；
    *   计算 ETD intrinsic reward：\\(b\_{t+1}=\\min\_{k\\in\[0,t\]}d(s\_k,s\_t)\\)；
    *   得到总 reward：\\(r\_{t+1} = r^e\_{t+1} + \\beta b\_{t+1}\\)；
*   把这个 episode 存到 replay buffer 里；
*   从 replay buffer 里采一批 。实际中，\\(x = s\_t, y = s\_{t+j}, j \\sim\\text{Geom}(1−\\gamma)\\) 即几何分布；
*   更新 temporal distance 的 loss function。
*   用刚刚采的新轨迹更新 PPO policy。

（个人思考，state 里显然应该包含当前 context 的信息，比如一个迷宫中的迷宫布局。不然，假设在上一个 episode 里 (0,0) (0,2) 很近，但这个 episode 里，两个点之间隔了一堵墙；如果 state 里只包含自己的坐标 (0,0)，而不包含这堵墙，那么完全没法学到“这一局的 (0,0) (0,2) 很远”这种信息）

### 1.5 实验结果怎么样

*   **环境**：MiniGrid 迷宫、像素游戏 Crafter、3D 导航 MiniWorld 等。
*   **优势**：
    *   在 MiniGrid 里，ETD 比 NovelD 更好。
    *   在带噪声的复杂迷宫中，ETD 比现有方法（NovelD、E3B 等）收敛速度**快 2 倍**。
    *   在像素输入的高维环境（如 Crafter）中，ETD 的探索成功率提升 **15-20%**。
    *   对状态噪声（如随机扰动）鲁棒，传统方法（如计数法）完全失效时 ETD 仍有效。

02 行文逻辑
-------

这篇文章讲的主要故事：

*   在稀疏 reward 环境中，探索（exploration）面临挑战。而在 Contextual MDP 中，现有的 count-based 和 similarity-based 都存在问题；ETD 可以解决它们的问题。
*   ETD 通过使用 temporal distance 计算相似性和 intrinsic reward，从而解决它们的问题。temporal distance 使用对比学习进行学习。

1 intro 分析：

*   第一段：稀疏奖励的 RL 探索困难；虽然现有方法在 MDP 中有效，但现实世界通常是 CMDP。
*   第二段：为解决 CMDP 的探索问题，现有方法引入了 episodic bonuses，这些方法可以分为两类：count-based 和 similarity-based。count-based 方法在大状态空间中表现不佳，而 similarity-based 方法在相似性计算上存在不足，无法捕捉状态的新颖性。
*   第三段：这篇文章介绍了 ETD，鼓励 agent 探索与当前 episode 历史在时间上相距较远的状态。
    *   关键创新在于使用 temporal distance，它测量在两个 state 之间转换的预期步数，可以作为相似性计算的稳健指标。
    *   与现有方法不同，temporal distance 不受状态 representation 的影响，因此避免了 noisy-TV 问题，并且适用于 pixel-based 环境。

接下来的 2 background 简单介绍了 CMDP 和 intrinsic reward 的机制。

3 Limitations of Current Episodic Bonuses：

*   专门用一段分析了现有方法的 gap。
*   第一段：基于 episodic count 计算 novelty，会在大状态空间 / noisy state 下失效，因为每个状态都是新颖的。
*   第二段：基于 state similarity 计算 novelty 的方法，如 NGU 和 E3B，可能能解决这一问题。具体来说，可以通过估计状态间转换的难易程度来实现，例如 EC 和 DEIR。然而，这种方法计算的状态相似性往往不够准确，如图 2 所示。

4 method：

*   4.1 如何学习 temporal distance；
*   4.2 基于 temporal distance 计算 intrinsic reward；
*   4.3 介绍具体算法。

5 experiments：

*   5.1 介绍了 minigrid 的 setting 和实验结果，实验涵盖了 8 个 task。
*   5.2 介绍了带 noise 的 minigrid，把各种 baseline 都卡下去了。
*   5.3 是 ablation，比较了 ETD 和欧几里得距离、用 ETD 生成 intrinsic reward 的具体方法、ETD 时序距离的对称性 / 非对称性设计。
*   5.4 介绍了 Pixel-Based Crafter 和 MiniWorld Maze 的环境，并提供了（与少量 baseline 比较）的实验结果。