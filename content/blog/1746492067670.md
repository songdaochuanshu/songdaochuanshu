---
layout: post
title: 'Wan2.1 t2v模型Lora Fine-Tune'
date: "2025-05-06T00:41:07Z"
---
Wan2.1 t2v模型Lora Fine-Tune
==========================

Wan2.1 t2v模型Lora Fine-Tune
==========================

1\. Wan2.1模型
------------

Wan2.1是由阿里巴巴开源的AI视频生成大模型，具备强大的视觉生成能力，支持文本到视频（T2V）和图像到视频（I2V）任务。该模型基于前沿的Diffusion Transformer架构，结合了因果3D变分自编码器（VAE）和优化的训练策略，能够高效处理时空信息，生成高质量、连贯性强的视频。

该模型基于主流的视频Diffusion（扩散模型）和Transformer架构。扩散模型通过逐步去除噪声来生成数据，而Transformer架构则基于自注意力机制（Attention）捕捉长时程依赖关系，从而生成时空一致的高质量视频。在权威评测集VBench中，Wan2.1的14B参数专业版本以总分86.22%的成绩大幅超越了国内外其他模型（如Sora、Luma、Pika等），稳居榜首位置\[1\]。该模型能够生成复杂运动、还原真实物理规律、提升影视质感，并优化指令遵循。

本文不会介绍Wan 2.1 模型的原理，而是主要介绍如何对Wan 2.1 模型进行lora fine-tune，生成需要的视频风格，使用的工具为Diffusion-Pipe\[2\]。

2\. Diffusion Pipe
------------------

Diffusion-Pipe 是一个用于扩散模型（diffusion models）的管道并行训练脚本，旨在通过分布式计算和高效的内存管理，训练那些超出单个 GPU 内存限制的大型模型。

使用这个项目进行训练可以简化很多流程，目前它可以提供的功能包括：

1.  管道并行训练：通过将模型分割到多个 GPU 上进行训练，Diffusion-Pipe 能够处理比单个 GPU 内存更大的模型。
2.  支持多种模型：目前支持 SDXL、Flux、LTX-Video、HunyuanVideo（t2v）、Cosmos、Lumina Image 2.0、Wan2.1（t2v 和 i2v）以及 Chroma 等多种模型。
3.  高效的多进程预缓存：通过多进程和多 GPU 预缓存潜在变量和文本嵌入，减少训练时的内存需求，加速训练过程。
4.  Tensorboard 日志记录：记录训练过程中的关键指标，便于实时监控训练进度。
5.  评估集指标计算：在保留的评估集上计算指标，帮助衡量模型的泛化能力。
6.  训练状态检查点：支持训练状态的自动保存与恢复，确保训练过程的连续性。
7.  易于扩展：通过实现一个简单的子类即可添加对新模型的支持，简化了集成流程。

3\. 环境准备
--------

在这次lora fine-tune中，我们使用的环境为：

1.  计算资源：AWS g5.4xlarge实例（1块Nvidia A10G）
2.  操作系统：ubuntu 22.04
3.  模型：wan2.1-t2v-1.3b

### 3.1. 配置训练环境

设置diffusion-pipe训练环境：

\# 创建conda环境

conda create -n diffusion-pipe python=3.12

conda activate diffusion-pipe

\# 安装pytorch和cuda-nvcc

pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu121

conda install nvidia::cuda-nvcc

\# 下载diffusion-pipe

git clone [https://github.com/tdrussell/diffusion-pipe](https://github.com/tdrussell/diffusion-pipe)

cd diffusion-pipe

pip install -r requirements.txt

\# 最后下载wan 2.1 模型到models/目录下：

huggingface-cli download Wan-AI/Wan2.1-T2V-1.3B-Diffusers --local-dir ./wan2.1-t2v-1.3b

### 3.2. 准备训练数据

在准备数据集时，需要准备符合下面要求的数据：

1.  图片：至少10-15张图片（7-8张一般也可以，但效果相对会差一些），常规图片格式均可以（例如jpg, jpeg, png, webp等）。也可以使用mp4格式的短视频（2-3秒），但是会消耗较多的VRAM
2.  Text Prompt：每张图片必须有一个对应的 .txt 文件，里面包含对应图片的描述说明
3.  触发词：一个唯一的触发关键词，确保模型能映射到新学习到的风格或人物

在准备Text Prompt时，注意以下格式：

1.  要有描述性，但要简洁（避免提示语过长或过短）
2.  包含有关背景、服装、动作等方面的细节
3.  在所有的“.txt”文件中始终使用相同的触发词

示例数据：

A portrait of a GF Carty Chan with short brown hair, wearing a white T-shirt, set against a clear blue sky.

![](https://img2024.cnblogs.com/blog/1287132/202504/1287132-20250430153007954-453055100.jpg)

在数据集准备完成后，将其上传到 /diffusion-pipe/data/input 文件夹中。

4\. 训练
------

首先编辑examples/dataset.toml文件，指定path为数据路径，例如：

path = '/home/ubuntu/diffusion-pipe/data/input'

以及设置：

num\_repeats = 10

然后编辑examples/wan\_14b\_min\_vram.toml文件，指定输出路径以及部分训练参数，例如epoch：

\# change this

output\_dir = '/home/ubuntu/diffusion-pipe/output'

\# and this

dataset = 'examples/dataset.toml'

epochs = 400

save\_every\_n\_epochs = 10

\# 以及修改模型路径

ckpt\_path = '/home/ubuntu/diffusion-pipe/models/wan2.1-t2v-1.3b'

设置好这些配置后，开启一个新的tmux session进行训练：

tmux new -s training

conda activate diffusion-pipe

cd diffusion-pipe

NCCL\_P2P\_DISABLE="1" NCCL\_IB\_DISABLE="1" deepspeed --num\_gpus=1 train.py --deepspeed --config examples/wan\_14b\_min\_vram.toml

5\. 训练结果
--------

在训练到390个epoch后，手动停止训练：

\[Rank 0\] step=31199, skipped=0, lr=\[2e-05\], mom=\[\[0.9, 0.99\]\]

steps: 31199 loss: 0.0023 iter time (s): 1.691 samples/sec: 0.591

\[Rank 0\] step=31200, skipped=0, lr=\[2e-05\], mom=\[\[0.9, 0.99\]\]

steps: 31200 loss: 0.0052 iter time (s): 1.694 samples/sec: 0.590

Saving model to directory epoch390

然后使用ComfyUI里构建一个Wan2.1的text2video的工作流查看效果，使用提示词：

GF Carty Chan, short dark hair, wearing a white T-shirt, Hair blowing in the wind。

未使用lora时生成的视频效果：

![](https://img2024.cnblogs.com/blog/1287132/202504/1287132-20250430153454795-802629385.gif)

使用lora后生成的视频效果：

![](https://img2024.cnblogs.com/blog/1287132/202504/1287132-20250430153517508-1910487011.gif)

![](https://img2024.cnblogs.com/blog/1287132/202504/1287132-20250430153528066-731882550.gif)

可以看到生成的视频与训练数据里的人物基本保持了一致。

5\. 总结
------

通过 Diffusion-Pipe 对 Wan2.1 模型进行 LoRA 微调，可以在有限资源下有效调整视频生成风格，尤其适合特定人物或艺术风格的定制化视频生成任务。整个流程具备良好的可复现性和扩展性，适合进一步探索个性化AI视频创作场景。

下一篇文章我们会继续介绍如何使用图片和视频对 Wan2.1 模型进行 image-to-video 的 LoRA 微调。

References
----------

\[1\] 边缘云玩转通义万相Wan2.1-T2V推理业务最佳实践: [https://help.aliyun.com/zh/ens/use-cases/wan2-1-t2v-1-3b-best-practice-of-reasoning-business](https://help.aliyun.com/zh/ens/use-cases/wan2-1-t2v-1-3b-best-practice-of-reasoning-business)

\[2\] Diffusion-Pine Github: [https://github.com/tdrussell/diffusion-pipe](https://github.com/tdrussell/diffusion-pipe)