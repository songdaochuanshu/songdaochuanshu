---
layout: post
title: '3年，从0到全球领跑：万字长文拆解DeepSeek大模型技术演进'
date: "2026-02-13T01:03:20Z"
---
3年，从0到全球领跑：万字长文拆解DeepSeek大模型技术演进
================================

原文: [https://mp.weixin.qq.com/s/MG9nB7VYK-N4Q3RQFiwcuw](https://mp.weixin.qq.com/s/MG9nB7VYK-N4Q3RQFiwcuw)

关注gzh: AI-Frontiers

自2022年chatgpt发布以来，全球人工智能领域进入了以大语言模型（LLM）为核心的激烈军备竞赛。OpenAI、Google、Anthropic等硅谷巨头，通过数百亿美元的资本投入和数万张H100GPU的算力堆叠，不断刷新着模型智能的上限。在这种大力出奇迹（Scaling Laws）的主流叙事下，算力成为了制约模型发展的核心硬通货，也构建了极高的行业准入门槛。

不同于国外通过堆砌硬件来解决问题的传统路径，中国杭州的AI初创公司Deepseek，走出了一条截然不同的技术演进路线，即对极致效率的追求和对算法边界的探索，其技术哲学可以概括为「算法-硬件协同优化的极致主义」。

DeepSeek作为一家源自量化对冲基金High-Flyer的AI研究机构，成立于2023年，在短短三年内从跟随者迅速蜕变为全球大模型架构创新的引领者。其技术路线展现出鲜明的长期主义与极致效率特征，通过在模型架构、推理算法、多模态及训练基础设施四个维度的持续突破，成功重塑了开源大模型的性能天花板。

DeepSeek的技术演进可清晰地划分为四个阶段：

*   **基石奠定模型（2023年）：** 以DeepSeek-Coder和DeepSeek-LLM为代表，验证了在有限算力下训练高质量稠密模型的能力，确立了「代码+数学」为核心竞争力的差异化路线。
    
*   **架构革新与****MOE****化（2024年）：** 通过DeepSeek-V2和V3，在大模型架构底层进行了革命性创新。提出了多头潜在注意力和细粒度专家混合架构，彻底解决了长上下文推理的显存瓶颈与训练成本问题，以极低的成本实现了对标顶尖闭源模型GPT-4 Turbo的效果。
    
*   **推理与系统2思维（2025年）：** 以DeepSeek-R1为里程碑，探索出纯强化学习驱动的推理能力涌现路径，证明了无需大规模监督微调，即可激发模型的自我反思与修正能力。随后通过V3.1、V3.2系列将这种「思考」能力泛化至工具调用与Agent场景。
    
*   **记忆与因果视觉（2025末-2026初）：** 在DeepSeek-OCR-2中引入视觉因果流，在Engram架构中提出基于查表的可扩展条件记忆机制，试图从根本上突破Transformer的上下文长度限制与视觉理解的逻辑缺陷，为下一代模型DeepSeek-V4奠定基础。
    

官网发布信息，见：[https://api-docs.deepseek.com/news/news251201](https://api-docs.deepseek.com/news/news251201)

TL; DR（总结版）
===========

DeepSeek模型演进时间线汇总（2023-2026.02.10）
----------------------------------

发布日期

模型名称

核心参数/架构

关键技术创新

对标/性能亮点

2023/11/2

DeepSeek Coder

1.3B/6.7B/33B

FIM预训练, 项目级上下文

代码能力超越CodeLlama-34B

2023/11/29

DeepSeek LLM

7B/67B

稠密架构, 中英双语对齐

67B打破LLaMA 2 70B垄断

2023/12/18

DreamCraft3D

N/A

3D一致性生成

高质量文生3D资产

2024/2/5

DeepSeek-Math

7B

GRPO强化学习, 拒绝采样

数学能力逼近GPT-4，RL技术验证

2024/3/11

DeepSeek-VL

1.3B/7B

混合视觉编码器

真实世界视觉理解，高分辨率处理

2024/5/7

DeepSeek-V2

236B (21B激活)

MLA, DeepSeekMoE

重新定义MoE架构，显存占用降93%

2024/6/17

DeepSeek-Coder-V2

16B/236B

MoE for Code, 338种语言

开源模型首次在代码领域对齐GPT-4 Turbo

2024/9/6

DeepSeek-V2.5

236B

Chat与Coder合并

通用与垂类能力统一，硬盘缓存API

2024/10/18

Janus

1.3B

视觉理解/生成解耦

解决多模态理解与生成的冲突

2024/12/13

DeepSeek-VL2

1B/2.8B/4.5B (激活)

MoE多模态, 动态分辨率

提升OCR与图表理解能力

2024/12/26

DeepSeek-V3

671B (37B激活)

FP8训练, MTP, 无损负载均衡

557万美元练出GPT-4o级模型，开源新基座

2025/1/20

DeepSeek-R1

671B

纯RL训练, 思维链涌现, 蒸馏

Aha Moment，开启推理大模型时代

2025/1/27

Janus-Pro

1B/7B

多模态Scaling

更强的文生图与指令遵循

2025/5/1

DeepSeek-Prover-V2

671B

形式化证明 (Lean 4)

解决数学证明步骤分解问题

2025/8/21

DeepSeek-V3.1

671B

混合思维模式, Agent优化

将R1的思考能力泛化至Agent任务

2025/12/1

DeepSeek-V3.2

671B

DeepSeek Sparse Attention (DSA)

长上下文效率突破，Thinking in Tool-Use

2026/1/3

mHC

N/A

引入几何约束

超大规模模型的信号发散与训练不稳定性

2026/1/14

Engram (论文)

N/A

可扩展查表记忆

解决超长上下文记忆遗忘问题

2026/1/27

DeepSeek-OCR-2

3B

视觉因果流

模拟人类阅读顺序，极致文档理解

DeepSeek模型功能分类汇总
----------------

  

2023年：基石奠定与稠密模型时代
=================

2023年是大模型爆发的元年。在LLaMA等开源模型引发「百模大战」的背景下，DeepSeek并未急于发布对话模型，而是敏锐地选择了代码作为切入点，基于两个核心判断：① 代码数据具有极强的逻辑性，是提升模型推理能力的最佳语料；② 代码生成是开发者最高频的刚需，且当时开源界尚缺乏能真正对标OpenAI Codex的模型。

DeepSeek Coder：代码智能的崛起
----------------------

*   发布时间: 2023年11月2日
    
*   核心定位： 垂类代码大模型
    
*   技术创新： Fill-In-the-Middle (FIM) / Project-Level Context
    
*   论文：[https://arxiv.org/abs/2401.14196](https://arxiv.org/abs/2401.14196)
    
*   gitbub：[https://github.com/deepseek-ai/DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder)
    

DeepSeek Coder是DeepSeek推出的首个具有广泛影响力的模型系列，包含1.3B、6.7B、33B等多种尺寸。该系列模型并非简单的微调，而是从零开始预训练。

*   **数据构成与训练策略：** 模型使用了2万亿（2T）Token的高质量数据进行训练，其中87%为代码，13%为中英文自然语言。这种配比确保了模型既具备深厚的编程知识，又拥有流畅的指令遵循能力 。
    
*   **架构亮点：**
    
    *   **Fill-In-the-Middle (FIM)：** 为了适应IDE中的代码补全场景，DeepSeek Coder在预训练阶段引入了FIM任务，即让模型根据上下文填补中间缺失的代码片段。这一能力使其在实际编程辅助中表现远超仅支持「从左到右」生成的模型。
        
    *   **项目级上下文（Project-Level Context）：** 模型支持16K的上下文窗口，能够理解跨文件的代码依赖关系，这在当时是开源代码模型中的领先水平。
        
*   **性能表现：** 在HumanEval和MBPP等基准测试中，DeepSeek Coder 33B不仅超越了同量级的CodeLlama-34B，甚至在部分指标上逼近了GPT-3.5 Turbo，确立了DeepSeek在代码领域的领先地位。
    

DeepSeek LLM (67B)：通用能力的全面对标
----------------------------

*   发布时间： 2023年11月29日
    
*   核心定位： 通用稠密大模型
    
*   技术创新： 中英双语对齐/规模化训练稳定性
    
*   论文：[https://arxiv.org/abs/2401.02954](https://arxiv.org/abs/2401.02954)
    
*   github：[https://github.com/deepseek-ai/DeepSeek-LLM](https://github.com/deepseek-ai/DeepSeek-LLM)
    

在代码模型取得成功后，DeepSeek迅速发布了通用大模型DeepSeek LLM 67B。这是中国首个完全开源参数量达到670亿级别的模型，直接对标Meta的LLaMA-2 70B。

*   **双语优势：** 相比LLaMA-2主要以英语为主，DeepSeek LLM在预训练数据中大幅增加了中文语料的权重，使其在中文理解、文化常识及逻辑推理上表现出显著优势。
    
*   **推理与数学能力：** 继承了DeepSeek Coder的基因，DeepSeek LLM在数学推理（GSM8K）和逻辑推断任务上表现出色，证明了「代码训练提升通用推理」的假设。
    
*   **开源策略：** DeepSeek不仅开源了模型权重，还开放了中间检查点（Checkpoints），极大地促进了学术界对大模型训练动态的研究，展示了DeepSeek独特的开源精神，提供结果，更提供过程。
    

DreamCraft3D：多模态生成的早期探索
-----------------------

*   发布时间： 2023年12月18日
    
*   核心定位： 文生3D内容生成
    
*   技术创新： 3D一致性优化
    
*   论文：[https://arxiv.org/abs/2310.16818](https://arxiv.org/abs/2310.16818)
    
*   github: [https://github.com/deepseek-ai/DreamCraft3D](https://github.com/deepseek-ai/DreamCraft3D)
    

除了语言模型，DeepSeek在2023年底还探索了多模态生成领域。DreamCraft3D是一个能够从文本描述生成高保真3D资产的模型。它通过解决多视角一致性问题，实现了从2D图像到3D几何的高质量转换，为后续的Janus系列奠定了多模态技术积累。

*   **架构亮点**：
    
    *   分层优化流水线：几何雕刻阶段融合 2D+3D 混合 SDS 损失、扩散时间步退火、渐进式视角训练等策略，从 NeuS 隐式表面表示过渡到 DMTet 网格表示，确保 3D 几何的多视角一致性；纹理增强阶段提出自举式分数蒸馏（BSD），通过交替优化 3D 场景与 DreamBooth 个性化扩散模型，实现纹理细节与视角一致性的相互强化。
        
    *   3D 感知先验融合：引入 Zero-1-to-3 视图条件扩散模型提供丰富 3D 先验，缓解单图生成的视角模糊性；利用多视图渲染增强与高斯噪声扰动，让扩散模型学习场景专属 3D 知识，突破固定分布蒸馏的局限。
        
*   **性能表现**：在包含 300 张图像的测试基准上，CLIP 得分（0.896）、PSNR（31.801）和 LPIPS（0.005）均显著超越 Make-it-3D、Magic123 等主流方法；用户研究中 92% 的参与者更偏好其生成结果，在 360° 渲染中展现出更锐利的几何细节和逼真纹理，有效解决了传统模型的语义不一致问题，推进了 3D 生成的技术上限。
    

2024年（上）：架构革命与MoE/MLA的提出
========================

进入2024年，大模型竞争进入深水区。随着模型参数量的膨胀，训练成本和推理显存占用成为制约发展的核心瓶颈。DeepSeek在这一阶段完成了从「架构跟随」到「架构创新」的华丽转身，提出了改变行业的MLA和DeepSeekMoE架构。

DeepSeek-Math：强化学习推理的先声
-----------------------

*   发布时间： 2024年2月5日
    
*   核心定位： 数学推理模型
    
*   技术创新： Group Relative Policy Optimization (GRPO)
    
*   论文：[https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)
    
*   gitbub：[https://github.com/deepseek-ai/DeepSeek-Math](https://github.com/deepseek-ai/DeepSeek-Math)
    

DeepSeek-Math 7B虽然参数规模不大，但其技术意义深远。它是DeepSeek首次大规模应用强化学习（RL）来提升推理能力，并为此发明了GRPO（Group Relative Policy Optimization）算法。

*   **GRPO算法详解：** 传统的RLHF（基于人类反馈的强化学习）通常依赖PPO算法，需要训练一个与策略模型同等大小的评论家模型，这会带来双倍的显存和计算开销。DeepSeek-Math提出的GRPO算法摒弃了评论家模型，而是通过对同一问题采样多组输出（Group Sampling），利用组内输出的相对优劣来计算优势函数。这一创新不仅大幅降低了RL训练成本，还提高了训练稳定性，为2025年DeepSeek-R1的爆发埋下了伏笔。
    
*   **性能：** 在MATH基准测试中，DeepSeek-Math 7B凭借GRPO和精选的数理数据，击败了众多70B级别的开源模型，甚至在部分指标上逼近GPT-4。
    

DeepSeek-VL：迈向真实世界的视觉理解
-----------------------

*   发布时间： 2024年3月11日
    
*   核心定位： 多模态理解模型
    
*   技术创新： 混合视觉编码器（Hybrid Vision Encoder）
    
*   论文: [https://arxiv.org/abs/2403.05525](https://arxiv.org/abs/2403.05525)
    
*   github: [https://github.com/deepseek-ai/DeepSeek-VL](https://github.com/deepseek-ai/DeepSeek-VL)
    

DeepSeek-VL（1.3B/7B）的设计哲学是实用主义。不同于当时许多多模态模型专注于简短的看图说话，DeepSeek-VL着重于处理真实世界中的复杂视觉任务，如逻辑图表分析、网页代码转换、OCR识别等。

*   **架构设计：** 采用了混合视觉编码器架构，结合了用于提取高层语义的SigLIP和用于捕捉低层细节的SAM-B（Segment Anything Model）。这种设计使得模型在处理高分辨率（1024x1024）图像时，既能保持全局理解，又不会丢失关键的文字和细节信息。

DeepSeek-V2：MoE与MLA的里程碑
-----------------------

*   发布时间： 2024年5月
    
*   核心定位： 经济高效的MoE通用大模型
    
*   技术创新： Multi-Head Latent Attention (MLA) / DeepSeekMoE
    
*   论文：[https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434)
    
*   github: [https://github.com/deepseek-ai/DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)
    

DeepSeek-V2是DeepSeek发展史上的关键转折点。它不仅是一个模型，更是一套全新的高效架构标准。

*   **Multi-Head Latent Attention (****MLA****)：**
    
    *   **背景：** 在长上下文推理中，KV Cache（键值缓存）会占用海量显存，限制了Batch Size和推理速度。
        
    *   **原理：** MLA通过低秩压缩技术，将KV Cache压缩为一个潜在向量（Latent Vector），大幅减少了推理时的显存占用。
        
    *   **效果：** 相比标准多头注意力技术，MLA将KV Cache减少了93.3%，使得DeepSeek-V2在单卡上就能支持极长的上下文窗口，并将推理吞吐量提升了5.76倍。
        
*   **DeepSeekMoE：**
    
    *   **细粒度专家（Fin\*\*\*\*e-grained Experts）：** 不同于Mixtral等模型将FFN作为一个大专家，DeepSeekMoE将专家切分得更细，使得知识分配更加灵活。
        
    *   **共享专家（Share\*\*\*\*d Experts）：** 专门设置一部分专家始终处于激活状态，用于捕捉通用知识，而路由专家专注于特定领域知识。这种“共享+路由”的机制有效解决了MoE模型训练中的知识冗余和路由坍缩问题。
        
*   **综合效能：** DeepSeek-V2总参数236B，激活参数仅21B，训练成本节省了42.5%，性能却超越了LLaMA 3 70B，不仅成为当时最强的开源MoE模型，更将API价格打到了白菜价1元/百万Tokens。
    

DeepSeek-Coder-V2：代码能力的全面爆发
---------------------------

*   发布时间： 2024年6月17日
    
*   核心定位： 代码MoE模型
    
*   技术创新： 338种编程语言支持
    
*   论文：[https://arxiv.org/abs/2406.11931](https://arxiv.org/abs/2406.11931)
    
*   github: [https://github.com/deepseek-ai/DeepSeek-Coder-V2](https://github.com/deepseek-ai/DeepSeek-Coder-V2)
    

DeepSeek-Coder-V2是V2架构在代码领域的延续。它在DeepSeek-V2的中间检查点基础上，额外使用了6万亿（6T）Token的高质量代码和数学数据进行持续预训练。

*   **多语言支持：** 支持的编程语言从86种扩展到了338种，几乎覆盖了所有主流及冷门语言。
    
*   **性能跃升：** 首个在代码生成和数学推理能力上能与闭源模型GPT-4 Turbo一较高下的开源模型。在HumanEval、MBPP+等榜单上，其表现令人震惊，证明了MoE架构在垂类任务上的巨大潜力 。
    

2024年（下）：融合、优化与V3的诞生
====================

2024年下半年，DeepSeek一方面致力于模型功能的融合与对齐，另一方面在基础设施层面进行底层优化，为V3的发布积蓄力量。

DeepSeek-V2.5：Chat与Coder的合二为一
-----------------------------

*   发布时间： 2024年9月6日
    
*   核心定位： 融合模型
    
*   技术创新： 人类偏好对齐优化
    
*   论文: [https://arxiv.org/pdf/2405.04434](https://arxiv.org/pdf/2405.04434)
    
*   github: [https://github.com/deepseek-ai/DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)
    
*   HuggingFace1: [https://huggingface.co/deepseek-ai/DeepSeek-V2.5](https://huggingface.co/deepseek-ai/DeepSeek-V2.5)
    
*   HuggingFace2: [https://huggingface.co/deepseek-ai/DeepSeek-V2.5-1210](https://huggingface.co/deepseek-ai/DeepSeek-V2.5-1210)
    

此前，DeepSeek同时维护Chat（通用）和Coder（代码）两条产品线。DeepSeek-V2.5的发布标志着这两条路线的合并。通过精细的数据配比和强化学习对齐，V2.5在保留Coder-V2强大代码能力的同时，大幅提升了通用对话和指令遵循能力，使得用户不再需要在不同模型间切换。这一版本也引入了针对API用户的硬盘缓存（Context Caching on Disk）技术，进一步降低了使用成本 。

*   **数据构成与训练策略**：基于8.1T高质量多源 Token 预训练（中文Token占比略高于英文），经去偏和质量筛选；通过150万会话SFT及两阶段RL对齐人类偏好，分别聚焦推理优化与多奖励信号融合。
    
*   **架构亮点**：
    
    *   MLA：低秩 KV 联合压缩 + 解耦RoPE，减少93.3%KV 缓存，兼顾性能与推理效率。
        
    *   DeepSeekMoE 稀疏架构：细粒度专家分割 + 多重负载均衡策略，降低训练成本并避免路由崩溃与通信过载。
        
    *   128K 长上下文：基于YaRN方法扩展，32K训练序列实现128K稳定性能，“Needle In A Haystack” 测试表现优异。
        
*   **性能表现**：在仅21B激活参数的情况下，基座模型在 MMLU、BBH 等基准跻身开源第一梯队；Chat（RL）版本 MT-Bench 8.97 分、AlpacaEval 2.0 胜率 38.9%，中文 AlignBench 7.91 分超多数闭源模型；训练成本降 42.5%，生成吞吐量提升至 5.76 倍。
    

Janus & JanusFlow：多模态理解与生成的解耦
-----------------------------

*   发布时间： 2024年10月（Janus），11月（JanusFlow）
    
*   核心定位： 统一多模态模型
    
*   技术创新： 解耦视觉编码（Decoupling Visual Encoding）
    
*   论文：[https://arxiv.org/abs/2410.13848](https://arxiv.org/abs/2410.13848)
    
*   github: [https://github.com/deepseek-ai/Janus](https://github.com/deepseek-ai/Janus)
    

在Janus之前，业界的多模态模型通常难以兼顾理解（看图）和生成（画图），因为两者的视觉表征需求截然不同。DeepSeek提出了Janus架构，创造性地将视觉编码解耦：

*   **理解路径：** 使用SigLIP提取高维语义特征。
    
*   **生成路径：** 使用VQ Tokenizer提取离散视觉Token。 两者通过同一个Transformer基座进行处理，互不干扰。随后发布的**JanusFlow**更是引入了整流流（Rectified Flow）技术，替代传统的自回归生成，大幅提升了图像生成的质量和速度。
    

DeepSeek-V3：定义开源新标准
-------------------

*   发布时间： 2024年12月26日
    
*   核心定位： 旗舰MoE模型
    
*   技术创新： FP8训练/无辅助损失负载均衡/多Token预测
    
*   论文：[https://arxiv.org/abs/2412.19437](https://arxiv.org/abs/2412.19437)
    
*   github: [https://github.com/deepseek-ai/DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3)
    

DeepSeek-V3是2024年的收官之作，也是DeepSeek技术实力的集大成者。它不仅是一个模型，更是一次对AI基础设施的极限挑战。

*   **FP8混合精度训练：** V3是全球首个在超大规模（671B参数）上成功验证FP8（8位浮点数）训练的模型。通过深度优化的FP8 GEMM内核，V3极大减少了显存带宽压力和通信开销，使得训练成本仅为557万美元（278.8万H800 GPU时），震惊了全球业界。
    
*   **无辅助损失****负载均衡****（Auxiliary-Loss-Free Load Balancing）：** 传统MoE依赖辅助损失（Auxiliary Loss）来强迫专家负载均衡，但会干扰模型的主任务学习。V3创新性地采用了基于偏置的动态均衡策略，在不牺牲模型性能的前提下实现了完美的负载均衡。
    
*   **多Token预测（Multi-Token Prediction, MTP）：** V3在训练时不再只预测下一个Token，而是同时预测后续多个Token。这种机制增加了训练信号的密度，提升了模型的长程规划能力，并支持推理时的投机解码，使生成速度提升了3倍。
    
*   **DualPipe：**为了解决大规模MoE训练中的跨节点通信瓶颈，DeepSeek研发了DualPipe算法，实现了计算与通信的完全重叠，将硬件利用率推向极致。
    

DeepSeek-V3在百科知识、数学、代码等维度的表现全面对标GPT-4o和Claude 3.5 Sonnet，成为当时最强的开源基座模型。

2025年（上）：推理模型与Agent的全面进化
========================

2025年，DeepSeek引领了从「知识积累（System 1）」向「深度推理（System 2）」的范式转移。通过强化学习激发模型的思考能力成为这一阶段的主旋律。

DeepSeek-R1：纯强化学习的奇点
--------------------

*   发布时间： 2025年1月20日
    
*   核心定位： 推理模型（Reasoning Model）
    
*   技术创新： 纯RL训练/思维链涌现/蒸馏
    
*   论文：[https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)
    
*   GitHub代码：[https://github.com/deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)
    

DeepSeek-R1的发布引发了全球范围内的「 R1时刻」。它证明了推理能力可以通过纯强化学习涌现，而无需大量的人工标注数据。

*   **DeepSeek-R1-Zero：** R1的前身，通过在V3基座上直接应用GRPO算法进行纯RL训练。模型在训练过程中自然涌现出了自我反思、验证和长思维链（Chain-of-Thought）能力，甚至出现了「顿悟，Aha Moment」现象。虽然R1-Zero存在语言混杂和可读性差的问题，但验证了RL激发推理潜力的可行性。
    
*   **DeepSeek-R1正式版：** 为了解决Zero版的问题，正式版R1采用了「冷启动数据+RL」的多阶段训练管线。
    
    *   **冷启动：** 使用少量高质量的长思维链数据微调V3，使其学会规范的思考格式。
        
    *   **推理****RL****：** 使用GRPO进行大规模强化学习，提升解决复杂数学和代码问题的能力。
        
    *   **对齐：** 结合通用数据进行最终微调，兼顾推理能力与通用对话体验。
        
*   **模型蒸馏：** DeepSeek不仅开源了R1（671B），还开源了利用R1生成的推理数据蒸馏出的一系列小模型：Qwen和Llama架构，1.5B-70B。这些小模型在推理能力上大幅超越了同尺寸的基座模型，甚至优于OpenAI o1-mini。
    

Janus-Pro：多模态的尺度定律
------------------

*   发布时间： 2025年1月27日
    
*   核心定位： 增强版多模态模型
    
*   技术创新： 数据与模型规模化
    
*   论文: [https://arxiv.org/pdf/2501.17811v1](https://arxiv.org/pdf/2501.17811v1)
    
*   Github: [https://github.com/deepseek-ai/Janus](https://github.com/deepseek-ai/Janus)
    

Janus-Pro是Janus架构的升级版。通过优化训练策略、扩展训练数据和增大模型参数（1B -> 7B），Janus-Pro在多模态理解和文生图指令遵循能力上取得了显著提升，证明了Janus架构具有良好的Scaling Law（尺度定律）特性 。

DeepSeek-V3.1 & Terminus：Agent能力的觉醒
-----------------------------------

*   发布时间： 2025年8月21日（V3.1），9月22日（Terminus）
    
*   核心定位： Agent基座模型
    
*   技术创新： 混合思维模式/工具调用优化
    
*   Huggingface: [https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus)
    
*   论文: [https://arxiv.org/pdf/2412.19437](https://arxiv.org/pdf/2412.19437)
    
*   Github: [https://github.com/deepseek-ai/deepseek-v3](https://github.com/deepseek-ai/deepseek-v3)
    

随着推理能力的成熟，DeepSeek开始探索将「思考」应用于Agent（智能体）场景。

*   **DeepSeek-V3.1：** 引入了「DeepThink」模式，用户可以在对话中一键开启深度思考。该版本重点提升了工具使用（Tool Use）能力，使其能够处理多步复杂的Agent任务。
    
*   **DeepSeek-V3.1-Terminus：** 针对V3.1存在的语言混杂（中英夹杂）和Agent执行不稳定的问题进行了专项修复。Terminus版本在Code Agent和Search Agent任务上的表现更加鲁棒，SWE-bench Verified分数达到66.0，标志着DeepSeek正式进入Agent时代。
    

DeepSeek-V3.2 & Speciale：稀疏注意力的突破
---------------------------------

*   发布时间： 2025年12月1日
    
*   核心定位： 长上下文Agent模型
    
*   技术创新： DeepSeek Sparse Attention (DSA) /Thinking in Tool-Use
    
*   论文: [https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek\_V3\_2.pdf](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf)
    
*   Github: [https://github.com/deepseek-ai/DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp)
    

DeepSeek-V3.2着重解决了长上下文下的计算效率问题，并进一步强化了Agent的推理能力。

*   **DeepSeek** **Sparse** **Attention (DSA)：** 一种细粒度的稀疏注意力机制。与标准全注意力相比，DSA在处理128K甚至更长上下文时，能显著降低计算复杂度和显存占用，且几乎不损失模型性能。这使得V3.2在长文档分析和RAG（检索增强生成）场景下极具优势。
    
*   **Thinking in Tool-Use：** V3.2不仅会思考，还能在调用工具的过程中进行思考。例如，在执行代码解释器之前，会先推演代码逻辑；在获得工具返回结果后，会再次思考下一步行动。
    
*   **DeepSeek-V3.2-Speciale：** 一个专为极高难度推理任务设计的版本，虽然成本较高，但在IMO 2025（国际数学奥林匹克）和IOI 2025（国际信息学奥林匹克）中均获得了金牌级表现，推理能力对标Gemini-3.0-Pro。
    

2026年1月-2月：突破记忆与视觉的极限
=====================

截至发稿日，DeepSeek在保持语言模型优势的同时，开始向大模型最底层的三个痛点：记忆遗忘、视觉逻辑和Transformer底层架构，发起全面冲击。

深度解析：Manifold-Constrained Hyper-Connections (mHC)
-------------------------------------------------

*   发布日期：2026年1月3日
    
*   核心定位：超大规模模型的信号发散与训练不稳定性。
    
*   技术创新：引入几何约束
    
*   论文：[https://arxiv.org/pdf/2512.24880](https://arxiv.org/pdf/2512.24880)
    

mHC 的提出解决了困扰深度学习界十年的“深度-宽度”权衡问题。它证明了通过施加严格的几何约束，我们可以在不牺牲训练稳定性的前提下，极大地扩展模型的宽度和容量。这为 DeepSeek V4 训练万亿参数模型奠定了数学基础。

*   **架构亮点**：
    
    *   流形约束机制：通过Sinkhorn-Knopp算法将残差映射投影到双随机矩阵流形，恢复恒等映射特性，解决 HC 架构的训练不稳定性。
        
    *   高效基础设施优化：融合核函数减少内存带宽瓶颈，选择性重计算降低显存占用，扩展DualPipe调度实现通信与计算重叠，仅增加6.7%训练开销。
        
    *   多流残差设计：扩展残差流宽度（n=4），同时通过非负约束避免信号抵消，兼顾拓扑复杂度与传播稳定性。
        
*   **性能表现**：27B 模型在BBH、DROP等8项基准中全面超越基线和HC架构，推理性能提升2.1%-2.3%；在3B到 27B规模下保持稳定性能优势，信号增益幅度从HC的3000降至1.6，大幅提升训练scalability。
    

DeepSeek-OCR-2：视觉因果流
--------------------

*   发布时间： 2026年1月27日
    
*   核心定位： 下一代视觉理解模型
    
*   技术创新： 视觉因果流（Visual Causal Flow）
    
*   论文1：[https://arxiv.org/abs/2601.20552](https://arxiv.org/abs/2601.20552)
    
*   github2: [https://github.com/deepseek-ai/DeepSeek-OCR-2](https://github.com/deepseek-ai/DeepSeek-OCR-2)
    
*   论文2：[https://arxiv.org/abs/2510.18234](https://arxiv.org/abs/2510.18234)
    
*   github2: [https://github.com/deepseek-ai/DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR)
    

传统的视觉编码器（如ViT）通常将图像视为无序的补丁（Patches）集合。DeepSeek-OCR-2（3B参数）颠覆了这一范式，引入了**视觉因果流**概念。

*   **核心逻辑：** 强制模型按照人类的阅读顺序（如从左到右、从上到下、先标题后正文）来处理视觉信息，而不是并行处理。这种有序的因果处理方式使得模型在理解多栏排版、复杂表格、嵌套公式等文档结构时，准确率实现了质的飞跃。
    
*   **性能：** 尽管参数仅为3B，但在复杂文档理解任务上超越了许多百亿级模型。
    

Engram：无限记忆的曙光
--------------

*   发布时间： 2026年1月14日（论文发布）
    
*   核心定位： 新型记忆架构研究
    
*   技术创新： 可扩展查表条件记忆（Conditional Memory via Scalable Lookup）
    
*   论文：[https://arxiv.org/abs/2601.07372](https://arxiv.org/abs/2601.07372)
    
*   Github: [https://github.com/deepseek-ai/Engram](https://github.com/deepseek-ai/Engram)
    

Transformer的注意力机制虽然强大，但其计算复杂度随长度呈二次方增长，限制了上下文的无限扩展。DeepSeek提出了**Engram**架构。

*   **原理：** 放弃了让模型时刻记住所有信息，而是建立了一个外挂的静态N-gram记忆库。模型通过高效的查表机制（Lookup）按需检索记忆，并将其与当前的动态隐藏状态融合。
    
*   **意义：** 这种设计将记忆存储从昂贵的GPU显存转移到了廉价的CPU内存甚至硬盘上，理论上可以支持无限的上下文长度，为处理代码库级别的超长任务扫清了障碍。这也是即将发布的DeepSeek-V4的核心技术储备 ()。
    

基础设施与生态护城河
==========

DeepSeek 的成功离不开其底层基础设施的极致优化。这些工具库均已开源，构成了其技术护城河。

*   **DeepGEMM**：专为FP8优化的矩阵乘法库，支持细粒度缩放（Fine-grained Scaling），是V3/V4高效训练的基石。
    
    *   [https://github.com/deepseek-ai/DeepGEMM](https://github.com/deepseek-ai/DeepGEMM)
*   **FlashMLA**：针对Hopper架构 GPU 优化的 MLA 解码内核，极大提升了推理吞吐量。
    
    *   [https://github.com/deepseek-ai/FlashMLA](https://github.com/deepseek-ai/FlashMLA)
*   **DeepEP**：高效的专家并行（Expert Parallel）通信库，解决了MoE模型中专家路由带来的巨大通信开销。
    
    *   [https://github.com/deepseek-ai/DeepEP](https://github.com/deepseek-ai/DeepEP)
*   **DualPipe**：双向流水线并行算法，实现了计算与通信的完美重叠（Overlap）。
    
    *   [https://github.com/deepseek-ai/DualPipe](https://github.com/deepseek-ai/DualPipe)
*   **3FS**：高性能分布式文件系统，专为 AI 训练的海量数据吞吐设计。
    
    *   [https://github.com/deepseek-ai/3FS](https://github.com/deepseek-ai/3FS)

从2023年到2026年，DeepSeek走过了一条从跟随者到颠覆者的道路。如果说V2和V3是通过工程极致优化（MLA, FP8, MoE）来打破算力垄断，那么2026年mHC和Engram的提出，则标志着DeepSeek开始触碰深度学习的理论天花板。

mHC通过引入流形约束，数学上保证了万亿参数模型的信号稳定性；Engram通过引入外部记忆，打破了 Transformer仅仅依赖参数记忆知识的低效范式。这两项技术不仅为即将到来的 DeepSeek V4奠定了基础，更为整个AI行业在后 「Scaling Law」时代指明了新的方向：向几何要稳定性，向内存要知识容量。