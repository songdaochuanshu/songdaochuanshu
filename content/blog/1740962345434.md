---
layout: post
title: '常见的各类LLM基座模型（GPT、DeepSeek、Qwen等）模型解析以及对比'
date: "2025-03-03T00:39:05Z"
---
常见的各类LLM基座模型（GPT、DeepSeek、Qwen等）模型解析以及对比
========================================

From： [https://www.big-yellow-j.top/posts/2025/02/15/LLM.html](https://www.big-yellow-j.top/posts/2025/02/15/LLM.html)

各类LLM模型技术汇总
-----------

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624505-232183049.png)

只去对比整体框架，对所采用的激活函数，归一化处理，位置编码等参考：  
**1、位置编码**：[https://www.big-yellow-j.top/posts/2025/02/03/pos-embedding.html](https://www.big-yellow-j.top/posts/2025/02/03/pos-embedding.html)  
**2、归一化处理**：[https://www.big-yellow-j.top/posts/2025/01/05/dl-norm.html](https://www.big-yellow-j.top/posts/2025/01/05/dl-norm.html)  
**3、分布式训练**：[https://www.big-yellow-j.top/posts/2025/01/03/DistributeTraining.html](https://www.big-yellow-j.top/posts/2025/01/03/DistributeTraining.html)

GPT系列
-----

### 1.GPT v1

对于大部分的深度学习任务，需要大量的**标记数据**（labeled data），但是如果使用大量的标记数据就会导致一个问题：构建得到的模型缺少适用性（可以理解为模型的泛化性能可能不佳）。那么就尝试使用**非标记的数据**（unlabelled data）但是这样一来又会有一个新的问题：时间消费大（time-consuming and expensive）。所以目前学者提出：使用预训练的词嵌入来提高任务性能。使用 _未标注的文本信息_（word-level information from unlabelled text）可能会：1、不清楚那种**优化目标**（optimization objective）在学习对迁移有用的文本表示时最有效；2、如何将这些学习到的表征有效的迁移到**目标任务**（target task）中。  
作者提出：1、**无监督的预训练**（unsupervised pre-training）；2、**监督的微调**（supervised fine-tuning）

> 1、**Unsupervised pre-training**

给定一些列的的**无标签**的 **token**：\\(U={u\_1,...,u\_n}\\)，构建自回归的模型：

\\\[L\_1(U)= \\sum\_{i}logP(u\_i|u\_{i-k},...,u\_{i-1};\\theta) \\\]

其中 \\(\\theta\\)为模型的参数。作者在模型中使用 **Transforme**作为 **decoder**，在最后的模型上作者构建得到为：

\\\[h\_0= UW\_e+W\_p \\\\ h\_l = transformer\\\_block(h\_{l-1})\\forall i \\in \[1,n\]\\\\ P(u)=softmax(h\_nW\_e^T) \\\]

其中\\(n\\)代表神经网路层的数目，\\(W\_e\\)代表 _token embedding matrix_，\\(W\_p\\)代表 _position embedding matrix_。对于无监督下的预训练：通过构建的数据集，去对模型的参数进行训练，得到模型的参数。

> 2、**Supervised fine-tunning**

作者在此部分提到：通过第一步得到的模型参数去对监督任务进行训练（采用的模型结构是没有变化的）。给定标签数据集\\(C\\)，给定输入：\\({x^1,...,x^m }\\)以及其标签\\(y\\)。将数据投入到预训练得到的模型参数里面得到：\\(h\_l^m\\)，然后添加一个线性输出层（参数为：\\(W\_y\\)）去对\\(y\\)进行预测。

\\\[P(y|x^1,...,x^m)=softmax(h\_l^wW\_y) \\\]

> 对于上述两部分步骤直观上理解：人首先从外界获取大量信息：网络，书本等，把这些信息了解之后，然后去写作文或者去回答问题。

模型结构：

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624019-1251600770.png)

### 2.GPT v2

**GPT v2**区别前一个模型，区别在于将**layer-norm** 位置替换到每一个残差连接块的里面，也就是说在数据输入到 **Multi-Head-Attention** 以及 **Feed-Forward** 之前提前通过一层标准化处理。

### 3.GPT v3

DeepSeek系列
----------

主要介绍**DeepSeek v3**（简称**DS**）各类技术细节，对于**DS**在模型结构上和之前迭代版本的 **DS-2**无太大区别，还是使用混合专家模型，只是补充一个辅助损失去平衡不同专家之间的不均衡问题。

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624467-204721401.png)

> 左侧结构和 **GPT-2**结构类似

在结构上**DS**主要的创新点在于：1、[Multi-Head Latent Attention](https://www.big-yellow-j.top/posts/2025/01/29/Attention.html)；2、[DeepSeekMoE](https://www.big-yellow-j.top/posts/2025/MoE-KV-cache.html)。前者为优化 **KV-cache** 操作，通过一个低秩的\\(c\_r^{KV}\\)代替原本占用较高的QV的值（首先通过降维方式降低原本维度，这样一来在显存占用上就会降低，而后通过升维方式，恢复到原本的维度），后者为混合专家模型，不过区别于常用的`MoE`方法，在**DS**中将专家模型分为两类：1、**Routed Expert**；2、**Shared Expert**，前者**直接**将隐藏层的输入进行传入，后者则是通过门控网络**筛选**而后隐藏层的输入进行传入。

除此之外，在**DS**中使用**Multi-Token Prediction**（MTP:[https://arxiv.org/pdf/2404.19737）技术](https://arxiv.org/pdf/2404.19737%EF%BC%89%E6%8A%80%E6%9C%AF)

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624377-592765327.png)

在**DS**中一个很耀眼的功能就是：**DeepSeek-R1**（一种思维链技术：**CoT**:**Chain of Thought**，在GPT-o1中也使用到这种技术）结合论文：[https://arxiv.org/pdf/2201.11903；https://arxiv.org/pdf/2501.12948；中对](https://arxiv.org/pdf/2201.11903%EF%BC%9Bhttps://arxiv.org/pdf/2501.12948%EF%BC%9B%E4%B8%AD%E5%AF%B9) **CoT**技术的描述，可以简单的理解为：让LLM可以自主去思考问题，比如在[论文](https://arxiv.org/pdf/2201.11903)中对 **CoT**技术的描述。

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624365-1138767995.png)

相较之直接让GPT输出答案，区别在于还要他给出推理过程。结合在 [**DS-R1**](https://arxiv.org/pdf/2501.12948)中的描述对于 **DS-R1**整体过程理解如下：

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150623150-1307590203.png)

非常明显的一个强化学习过程，在论文里面提到的使用 **Group Relative Policy Optimization**（GRPO）策略进行优化

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150623760-1275157116.png)

在 **DS-R1**中作者提到的使用的模板

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624303-1468184141.png)

对于上述优化过程[理解](https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba)：比如说对于一个数学问题：\\(8+5=?\\)，这就是上面公式中所提到的question \\(q\\)，按照上面的描述，将会生成一系列的输出：\\({o\_1,...,o\_G}\\)  
**Step-1**：生成若干的回答。\\({o\_1,...,o\_G}\\)  
**Step-2**：对于生成的回答进行评分。\\({r\_1,...,r\_G}\\)，而后计算\\(A\_i=\\frac{r\_i- \\text{mean}({r\_1,...,r\_G})}{\\text{std}({r\_,...,r\_G})}\\)  
**Step-3**：使用裁剪更新策略：\\(\\text{clip}(\\frac{\\pi\_{\\theta}(o\_i|q)}{\\pi\_{\\theta\_{old}(o\_i|q)}},1-\\epsilon,\\epsilon)\\)比如说：如果新策略开始给o1分配过高的概率，裁剪机制确保不会过度强调这个响应。这种方式保证了即使在像推理这样复杂的任务中，策略优化也能保持稳定和可靠。通过clip函数将内部值限定在\\((1-\\epsilon, 1+\\epsilon)\\)之间  
**Step-4**：通过KL散度（用来度量两个概率分布相似度的指标）惩罚偏差

* * *

**PPO**和 **GRPO**

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624431-669026264.png)

上面提到的几个模型：  
1、**Policy Model**：我们需要优化的模型  
2、**Value Model**：估计状态的价值，帮助指导策略优化  
3、**Reference Model**：提供历史策略的参考，确保优化过程中策略变化不过度  
4、**Reward Model**：定义奖励信号，用于强化学习中的奖励反馈

**GRPO**实现，参考[**腾讯**](https://mp.weixin.qq.com/s/BYPKP5oXg1V4C_vg0VFGhw)以及 [Github](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb)上实现代码，对于复现可以直接用Huggingface中的 [**trl**](https://huggingface.co/docs/trl/main/en/grpo_trainer)来进行复现

* * *

LLama系列
-------

### 1.LLama v1

LLaMA 所采用的 Transformer 结构和细节，与标准的 Transformer 架构不同的地方包括采用了前置层归一化（Pre-normalization）并使用 **RMSNorm 归一化函数 （Normalizing Function）**、激活函数更换为 SwiGLU，并使用了旋转位置嵌入（RoP），整体 Transformer 架构与 GPT-2 类似

### 2.LLama v2

区别于上一代的LLama v1改进如下几点（主要参考[论文](https://arxiv.org/pdf/2307.09288)中的A.2.1中的描述）：  
**1、序列长度**：由原来的2048 tokens变化为4096 tokens  
**2、使用GQA**：通过使用KV-cache可以加快模型生成速度，但是也会造成过大的显存占用，因此`LLama v2`使用`GQA`来减少这个过程中的显存占用。

> **GQA原理**：[https://www.big-yellow-j.top/posts/2025/01/29/Attention.html](https://www.big-yellow-j.top/posts/2025/01/29/Attention.html)

### 3.LLama v3

模型参数细节：

![](https://img2024.cnblogs.com/blog/3395559/202503/3395559-20250302150624249-1541309833.png)

BERT
----

预训练阶段任务：  
1、**Masked LM(MLM)**

MLM是一种预训练任务，通过随机掩蔽输入序列中的部分词元，模型根据上下文预测被掩蔽的词元，从而学习双向语言表示。在模型中作者按照：80：10：10的比例进行处理（80：将词元替换为\[MASK\]；10：将词元替换为词汇表中随机选取的其他词。；10：保持原词元不变）  
**例子：**  
输入：“今天天气真好，我打算去\[MASK\]。”  
模型的任务是根据上下文预测“\[MASK\]”应该是“公园”。

2、**Next Sentence Prediction(NSP)**

NSP是一种预训练任务，模型接收两个句子并预测第二个句子是否是第一个句子的后续。该任务帮助模型理解句子间的逻辑关系。  
例子：  
句子对：  
句子1：“我喜欢去公园散步。”  
句子2：“今天下午我会去跑步。”  
模型的任务是判断第二个句子是否是第一个句子的自然延续，答案是“是”。

*   **缺点**  
    1、BERT neglects dependency between the masked positions and suffers from a **pretrain-finetune discrepancy**（忽略了屏蔽位置之间的依赖性，并遭受预训练微调差异的影响）

> 这是因为在 BERT模型中，在预训练阶段会添加 \[MASK\]，但是在 下游任务(downsteram tasks)中并不会使用 **\[MASK\]**

HuggingFace-trl使用
-----------------

参考：  
1、[https://blog.csdn.net/qq\_38961840/article/details/145387854](https://blog.csdn.net/qq_38961840/article/details/145387854)  
2、[https://huggingface.co/docs/trl/main/en/grpo\_trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer)  
3、[https://mp.weixin.qq.com/s/BYPKP5oXg1V4C\_vg0VFGhw](https://mp.weixin.qq.com/s/BYPKP5oXg1V4C_vg0VFGhw)

参考
--

1、[A Comprehensive Overview of Large Language Models](https://arxiv.org/pdf/2307.06435)  
2、[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)  
3、[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)  
4、[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)  
5、[The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783v3)  
6、[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
7、[Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/pdf/2404.19737)  
8、[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)  
11、[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903)  
12、[The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO)](https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba)  
13、[https://www.youtube.com/watch?v=Yi1UCrAsf4o](https://www.youtube.com/watch?v=Yi1UCrAsf4o)  
14、[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)  
15、[https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb)  
16、[https://mp.weixin.qq.com/s/BYPKP5oXg1V4C\_vg0VFGhw](https://mp.weixin.qq.com/s/BYPKP5oXg1V4C_vg0VFGhw)  
17、[https://huggingface.co/docs/trl/main/en/grpo\_trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer)