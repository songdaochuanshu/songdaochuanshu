---
layout: post
title: '[å¤§æ¨¡å‹å®æˆ˜ 06] æˆ‘çš„æ¨¡å‹æˆ‘åšä¸»ï¼šåœ¨ Kaggle ä¸Šç”¨ Unsloth æé€Ÿå¾®è°ƒ Qwen3'
date: "2026-02-09T01:00:33Z"
---
![[å¤§æ¨¡å‹å®æˆ˜ 06] æˆ‘çš„æ¨¡å‹æˆ‘åšä¸»ï¼šåœ¨ Kaggle ä¸Šç”¨ Unsloth æé€Ÿå¾®è°ƒ Qwen3](https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260208222114626-1648637700.png) åŸºç¡€ç†è®ºç»“æŸï¼Œå’±ä»¬ä»Šå¤©çš„é‡å¿ƒæ˜¯ä½¿ç”¨å¿«é€Ÿé«˜æ•ˆçš„å¾®è°ƒåº“Unslothåœ¨Kaggleçš„T4æ˜¾å¡ä¸Šï¼Œç”¨15åˆ†é’Ÿå°†Qwen3-4Bæ¨¡å‹å¾®è°ƒæˆè®¤ä¸»å’±ä»¬çš„ä¸“å±æ¨¡å‹ã€‚

> **æ ¸å¿ƒæ‘˜è¦ (TL;DR)**
> 
> *   **ç¥å™¨ç™»åœº**ï¼šæš‚æ—¶ä¸è®²ç¹ççš„ `transformers` åŸç”Ÿä»£ç ï¼Œä½¿ç”¨ **Unsloth** â€”â€” ç°åœ¨çš„å¾®è°ƒç‰ˆæœ¬ç­”æ¡ˆã€‚é€Ÿåº¦å¿« 2-5 å€ï¼Œæ˜¾å­˜çœ 60%ã€‚
> *   **å®æˆ˜ç›®æ ‡**ï¼šé€šè¿‡ **QLoRA** æŠ€æœ¯ï¼ŒæŠŠ Qwen3-4B å¾®è°ƒæˆä¸€ä¸ªè®¤å®šè‡ªå·±æ˜¯ "AlgiebaLLM AI" çš„ä¸“å±åŠ©æ‰‹ã€‚
> *   **ä½é—¨æ§›**ï¼šæ— éœ€æ˜‚è´µçš„ A100ï¼ŒKaggle çš„å…è´¹ T4 æ˜¾å¡å°±èƒ½è·‘é£èµ·ã€‚

å‰è¨€
--

åœ¨[ä¸Šä¸€ç¯‡](https://blog.algieba12.cn/llm05-fine-tune-model/)ä¸­,å’±ä»¬é€šè¿‡ç®€å•çš„å®æ“æµ‹è¯•ï¼Œå‘ç°Baseæ¨¡å‹æ˜¯â€œæ— è„‘ç»­å†™æœºå™¨â€ï¼ŒInstructæ¨¡å‹å¾ˆèªæ˜ï¼Œä½†æ˜¯å®ƒè¿˜ä¸æ˜¯å±äºå’±ä»¬çš„â€œè´¾ç»´æ–¯â€ï¼Œä¸‹è½½çš„æ¨¡å‹å’Œå…¶ä»–æ‰€æœ‰äººçš„éƒ½ä¸€æ ·ã€‚

å’±ä»¬è¿™èŠ‚ï¼Œç›´æ¥å…ˆæš‚æ—¶è·³è¿‡ä¼ ç»Ÿçš„å®—é—¨è€ç¥–`transformers`ç³»åˆ—åº“åšå¾®è°ƒï¼Œå’±ä»¬ç›´æ¥ä¸Šç®€å•æ˜“ä¸Šæ‰‹çš„å·¥å…·ï¼ŒèŠ‚çº¦ç®—åŠ›èŠ‚çº¦æ—¶é—´çš„æŠ€æœ¯ã€‚

1\. å¾®è°ƒï¼Ÿæœ‰å“ªäº›å¾®è°ƒï¼Ÿ
-------------

åœ¨å¼€å§‹ä¹‹å‰ï¼Œç¨å¾®èŠ±ä¸Šé‚£ä¹ˆä¸€ä¸¢ä¸¢çš„æ—¶é—´ï¼Œå’±ä»¬æ¥äº†è§£ä¸€ä¸‹å¾®è°ƒçš„"å®¶è°±"ã€‚

### 1.1 **å…¨é‡å¾®è°ƒ**

*   **åŸç†**ï¼šç”¨**æ–°çš„**è®­ç»ƒæ•°æ®å»æ›´æ–°æ¨¡å‹ä¸­**å…¨éƒ¨**çš„å‚æ•°ï¼Œæ¨¡å‹çš„æ¯ä¸ªæ¯›å­”éƒ½å¾—å‚ä¸åˆ°å˜é©ä¸­æ¥ã€‚
*   **ä¼˜ç‚¹**ï¼šå› ä¸ºèƒ½æ§åˆ¶çš„èŒƒå›´æœ€å¹¿ï¼Œç†è®ºçš„ä¸Šé™ä¹Ÿæ˜¯æœ€é«˜çš„ï¼Œå¯ä»¥å°†æ•´ä¸ªæ¨¡å‹çš„è¡Œä¸ºå½»åº•æ”¹å†™ã€‚
*   **ç¼ºç‚¹**ï¼š
    *   æ‰€æœ‰å±‚çš„å‚æ•°éƒ½è¦å‚ä¸è®­ç»ƒï¼Œé‚£èµ„æºæ¶ˆè€—è‚¯å®šä¹Ÿæ˜¯**æœ€é«˜**çš„ï¼Œä¸€ä¸ª7Bçš„æ¨¡å‹ï¼Œå¯èƒ½ä¼šéœ€è¦80Gå·¦å³çš„æ˜¾å­˜ï¼Œå¤§æ¦‚4å¼ A100ã€‚
    *   åŒæ ·å› ä¸ºæ‰€æœ‰å±‚çš„å‚æ•°éƒ½è¦å‚ä¸è®­ç»ƒï¼Œå¾ˆå®¹æ˜“å‘ç”Ÿâ€œ**ç¾éš¾æ€§é—å¿˜**â€ï¼Œä¹Ÿå¥½ç†è§£ï¼Œå¦‚æœå’±ä»¬è¿å‘¼å¸çš„æ§åˆ¶ä¹Ÿä»å¤´éœ€è¦å»å­¦ä¹ æ§åˆ¶ï¼Œé‚£ç¡®å®å®¹æ˜“ä¹±å¥—ã€‚

### 1.2 **é«˜æ•ˆå¾®è°ƒ**

*   **åŸç†**ï¼šå°†æ¨¡å‹çš„å‚æ•°**å†»ç»“**ä¸è®©åŠ¨ï¼Œåªåœ¨å¤–é¢åŠ ä¸€ä¸ª**å¤–æŒ‚**æ¥ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå»è®­ç»ƒè¿™æ–°æ¥å…¥çš„ä¸€å°éƒ¨åˆ†å‚æ•°ã€‚æˆ–è€…ç›´æ¥åªè®­ç»ƒæ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‡ å±‚å‚æ•°ã€‚
*   **ä¼˜ç‚¹**ï¼šå› ä¸ºè®­ç»ƒçš„éƒ¨åˆ†å¾ˆå°‘ï¼Œæ‰€ä»¥å¯ä»¥**å¤§å¤§èŠ‚çº¦æ˜¾å­˜**ï¼Œè€Œä¸”**é€Ÿåº¦å¿«**ï¼Œè®©â€œæ—§æ—¶ç‹è°¢å ‚å‰ç‡•â€ï¼Œä¹Ÿé£å…¥æ¶ˆè´¹çº§æ˜¾å¡çš„â€œç™¾å§“å®¶â€ï¼ˆè™½ç„¶æ²¡æœ‰å®Œå…¨æ²¡é—¨æ§›ï¼Œä½†æ˜¯å·²ç»å¤§å¹…é™ä½äº†é—¨æ§›äº†ï¼‰
*   **ç¼ºç‚¹**ï¼šæ•ˆæœæ˜¯ä¸å¦‚å…¨é‡å¾®è°ƒçš„ï¼Œä½†æ˜¯ä¹Ÿèƒ½è¾¾åˆ°7æˆ8æˆçš„æ•ˆæœã€‚

æˆ‘ä»¬ä»Šå¤©è¦ç”¨çš„æŠ€æœ¯ï¼Œå°±æ˜¯**é«˜æ•ˆå¾®è°ƒ**ä¸­çš„**QLoRA**ã€‚  
QLoRA = Q+LoRAã€‚

*   æ‰€è°“LoRAï¼ˆLow-Rank Adaptationï¼‰ï¼Œä½œä¸ºç›®å‰ä¸šç•Œçš„æ ‡å‡†ï¼Œå°±æ˜¯åœ¨åŸæœ‰çš„æƒé‡çŸ©é˜µæ—è¾¹åŠ å…¥é€‚é…å±‚ä¸¤ä¸ªå°çŸ©é˜µï¼Œè®­ç»ƒæ—¶åªæ›´æ–°é‚£ä¸¤ä¸ªçŸ©é˜µã€‚
*   Qå°±æ˜¯Quantizedï¼Œé‡åŒ–ï¼Œç®€å•ç‚¹ç†è§£å°±æ˜¯å°†æ¨¡å‹å‚æ•°çš„å­˜å‚¨ç²¾åº¦é™ä½åˆ°8Bitæˆ–è€…4Bitã€‚  
    ![å¾®è°ƒæŠ€æœ¯æ¦‚è§ˆ](https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm06-unsloth-qlora-ft/ft-overview.png)

2\. æœ‰å“ªäº›å¾®è°ƒçš„åº“å¯ä»¥é€‰æ‹©ï¼Ÿ
----------------

### 2.1. ç¥çº§åŠ é€Ÿæ´¾ï¼šUnsloth

> **å®šä½**ï¼šå•å¡å¾®è°ƒçš„â€œç‰ˆæœ¬ç­”æ¡ˆâ€ï¼ŒKaggle å…è´¹æ˜¾å¡çš„æ•‘æ˜Ÿã€‚

*   **æ ¸å¿ƒç‰¹ç‚¹**ï¼šæ‰‹åŠ¨é‡å†™äº†åº•å±‚çš„ Triton è®¡ç®—å†…æ ¸ï¼Œå°†æ˜¾å­˜å ç”¨é™ä½ 60%ï¼Œè®­ç»ƒé€Ÿåº¦ç›¸è¾ƒäºhuggingfaceç³»åˆ—åº“æå‡ 2-5 å€,é…åˆunslothåŠ¨æ€é‡åŒ–çš„æ¨¡å‹ï¼Œæ•ˆæœä¼šæ›´å¥½ã€‚
*   **ä¼˜ç‚¹**ï¼š
    *   **æé€Ÿ**ï¼šç›®å‰å¸‚é¢ä¸Šæœ€å¿«çš„å•å¡å¾®è°ƒåº“ã€‚
    *   **çœæ˜¾å­˜**ï¼šè®© T4 è¿™ç§ 16G æ˜¾å¡ä¹Ÿèƒ½è½»æ¾è·‘ Qwen-14B ç”šè‡³ 32B (4-bit)ã€‚
    *   **ä»£ç ç®€æ´**ï¼šä»…éœ€åå‡ è¡Œ Python ä»£ç å³å¯å¯åŠ¨ã€‚
    *   **å¯¼å‡ºæ–¹ä¾¿**ï¼šåŸç”Ÿæ”¯æŒ GGUF å¯¼å‡ºï¼Œå¯¹æ¥ Ollamaã€‚
*   **ç¼ºç‚¹**ï¼š
    *   ç¡¬ä»¶é—¨æ§›ï¼šGPU Compute Capability $\\ge$ 7.0 (æ”¯æŒ T4/RTX30/40ç³»ï¼Œ**ä¸æ”¯æŒ P100/V100**)ã€‚
    *   æ¨¡å‹é€‚é…ï¼šæ–°æ¶æ„æ¨¡å‹æ¨å‡ºåï¼Œéœ€è¦ç­‰å¾…å®˜æ–¹é€‚é…ï¼ˆé€šå¸¸åªéœ€å‡ å¤©ï¼‰ã€‚

### 2.2. æ‡’äºº UI æ´¾ï¼šLLaMA-Factory

> **å®šä½**ï¼šé›¶ä»£ç ã€å¯è§†åŒ–å¾®è°ƒå·¥åŠã€‚

*   **æ ¸å¿ƒç‰¹ç‚¹**ï¼šæä¾›äº† WebUI ç•Œé¢ï¼Œæ”¯æŒå‡ ä¹æ‰€æœ‰ä¸»æµæ¨¡å‹å’Œå¾®è°ƒæ–¹å¼ï¼Œå‚æ•°é…ç½®é€šè¿‡å‹¾é€‰å®Œæˆã€‚
*   **ä¼˜ç‚¹**ï¼š
    *   ï¸ **é›¶ä»£ç **ï¼šé€‚åˆä¸å–œæ¬¢å†™ Python ä»£ç çš„ç”¨æˆ·ã€‚
    *   **å¯è§†åŒ–**ï¼šå®æ—¶ç›‘æ§ Loss æ›²çº¿ï¼Œå‚æ•°è°ƒæ•´ç›´è§‚ã€‚
    *   **å…¼å®¹æ€§å¹¿**ï¼šæ”¯æŒ Qwen, Llama, Mistral, ChatGLM ç­‰ç™¾ç§æ¨¡å‹ã€‚
*   **ç¼ºç‚¹**ï¼š
    *   å°è£…å¤ªæ·±ï¼šä¸€æ—¦æŠ¥é”™ï¼Œæ–°æ‰‹å¾ˆéš¾å®šä½åˆ°åº•å±‚å“ªé‡Œå‡ºäº†é—®é¢˜ã€‚
    *   ç¯å¢ƒä¾èµ–ï¼šåœ¨ Kaggle ä¸Šéœ€è¦é€šè¿‡å†…ç½‘ç©¿é€æ‰èƒ½è®¿é—® WebUIï¼Œç•¥æ˜¾ç¹ç, ä½†æ˜¯é€‚åˆåœ¨è‡ªå·±çš„æœåŠ¡å™¨ä¸Šä½¿ç”¨ã€‚

### 2.3. å®˜æ–¹å«¡ç³»æ´¾ï¼šSwift (ModelScope)

> **å®šä½**ï¼šQwen å®¶æ—çš„â€œäº²å„¿å­â€ï¼Œé˜¿é‡Œè¾¾æ‘©é™¢å‡ºå“ã€‚

*   **æ ¸å¿ƒç‰¹ç‚¹**ï¼šå¯¹ Qwen ç³»åˆ—ï¼ˆåŒ…æ‹¬ Qwen-VL, Qwen-Audioï¼‰çš„æ”¯æŒæœ€å¿«ã€æœ€å®Œç¾ã€‚
*   **ä¼˜ç‚¹**ï¼š
    *   **åŸç”Ÿé€‚é…**ï¼šQwen æ–°æ¨¡å‹å‘å¸ƒå½“å¤©ï¼ŒSwift é€šå¸¸å°±èƒ½æ”¯æŒã€‚
    *   ï¸ **å¤šæ¨¡æ€**ï¼šå¾®è°ƒè§†è§‰/éŸ³é¢‘å¤§æ¨¡å‹çš„é¦–é€‰ã€‚
    *   ğŸ‡¨ğŸ‡³ **ä¸­æ–‡å‹å¥½**ï¼šæ–‡æ¡£å’Œç¤¾åŒºå¯¹ä¸­æ–‡ç”¨æˆ·éå¸¸å‹å¥½ã€‚
*   **ç¼ºç‚¹**ï¼š
    *   ç”Ÿæ€å±€é™ï¼šè™½ç„¶æ”¯æŒå…¶ä»–æ¨¡å‹ï¼Œä½†æ ¸å¿ƒä¼˜åŒ–éƒ½åœ¨é˜¿é‡Œç³»æ¨¡å‹ä¸Šã€‚

### 2.4. å­¦é™¢æ­£ç»Ÿæ´¾ï¼šHuggingFace Transformers

> **å®šä½**ï¼šå¤§æ¨¡å‹é¢†åŸŸçš„â€œæ•™ç§‘ä¹¦â€ï¼Œåº•å±‚åŸºçŸ³ã€‚

*   **æ ¸å¿ƒç‰¹ç‚¹**ï¼šæœ€åŸå§‹ã€æœ€çµæ´»çš„åº“ï¼Œæ‰€æœ‰ä¸Šå±‚å·¥å…·ï¼ˆFactory/Swiftï¼‰çš„åº•åº§ã€‚
*   **ä¼˜ç‚¹**ï¼š
    *   **æåº¦çµæ´»**ï¼šä½ æƒ³æ€ä¹ˆé­”æ”¹æ¨¡å‹ç»“æ„éƒ½å¯ä»¥ã€‚
    *   **èµ„æ–™ä¸°å¯Œ**ï¼šå…¨ç½‘æ•™ç¨‹æœ€å¤šï¼Œé€‚åˆå­¦ä¹ åŸç†ã€‚
*   **ç¼ºç‚¹**ï¼š
    *   **æ…¢ä¸”é‡**ï¼šæ²¡æœ‰ Unsloth çš„åº•å±‚ä¼˜åŒ–ï¼Œæ˜¾å­˜å ç”¨é«˜ï¼Œé€Ÿåº¦æ…¢ã€‚
    *   **ä»£ç ç¹ç**ï¼šå†™ä¸€ä¸ªè®­ç»ƒå¾ªç¯éœ€è¦å‡ ç™¾è¡Œä»£ç æˆ–å¤æ‚çš„é…ç½®ã€‚

### 2.5. ç¡¬æ ¸å·¥ç¨‹æ´¾ï¼šAxolotl & DeepSpeed

> **å®šä½**ï¼šå¤šå¡é›†ç¾¤ã€ä¼ä¸šçº§å…¨é‡å¾®è°ƒã€‚

*   **æ ¸å¿ƒç‰¹ç‚¹**ï¼šé€šè¿‡ YAML é…ç½®æ–‡ä»¶ç®¡ç†è®­ç»ƒï¼Œæ”¯æŒå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒï¼ˆFSDPï¼‰ã€‚
*   **ä¼˜ç‚¹**ï¼š
    *   **å·¥ä¸šçº§**ï¼šé€‚åˆ 70B ä»¥ä¸Šå¤§æ¨¡å‹çš„å…¨é‡å¾®è°ƒã€‚
    *   **å¯å¤ç°**ï¼šé…ç½®æ–‡ä»¶æ–¹ä¾¿ç‰ˆæœ¬ç®¡ç†ã€‚
*   **ç¼ºç‚¹**ï¼š
    *   **é…ç½®åœ°ç‹±**ï¼šå¯¹æ–°æ‰‹æä¸å‹å¥½ï¼Œè°ƒè¯•å›°éš¾ã€‚
    *   **æ€é¸¡ç‰›åˆ€**ï¼šåœ¨ Kaggle å•å¡/åŒå¡ç¯å¢ƒä¸‹å®Œå…¨æ˜¯å¤§æå°ç”¨ã€‚

![å¾®è°ƒåº“é€‰æ‹©æŒ‡å—ï¼š äº”å¤§æµæ´¾å¤§æ¯”æ‹¼](https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm06-unsloth-qlora-ft/lib-compare.png)  
æ‰€ä»¥ï¼Œç»¼ä¸Šæ‰€è¿°ï¼Œå’±ä»¬å°†ä½¿ç”¨ **Unsloth**æ¥å®Œæˆä»Šå¤©çš„Qwen3â€œçµé­‚è®¤ä¸»ä»ªå¼â€ã€‚

3\. Kaggleå®æ“
------------

### 3.1 ç¯å¢ƒå®‰è£…ï¼šKaggle æé€Ÿç‰ˆ

Unsloth å¯¹ç¯å¢ƒè¦æ±‚è¾ƒé«˜ï¼Œä½†åœ¨ Kaggle ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹å‘½ä»¤ä¸€é”®é…ç½®ã€‚

    import os
    !pip install uv
    !uv pip install --system --upgrade "unsloth_zoo @ git+https://github.com/unslothai/unsloth_zoo.git"
    !uv pip install --system "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
    !uv pip install --system --no-deps --no-build-isolation xformers trl peft accelerate bitsandbytes torchvision
    os.environ["CUDA_VISIBLE_DEVICES"] = "0" # å…³äº†åŒå¡
    

**PS:**

*   è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†**uv**æ¥è¿›è¡ŒåŒ…ç®¡ç†ï¼Œä¸æ˜¯ç´«å¤–çº¿çš„é‚£ä¸ªuvå“ˆï¼Œæ˜¯ä¸€ä¸ªpythonåŒ…ç®¡ç†åº“ï¼Œèƒ½å¤Ÿæ›´å¿«é€Ÿåœ°ç®¡ç†pythonåº“ï¼Œä»¥åŠå¤„ç†ä¾èµ–å†²çªé—®é¢˜(æœ‰æ—¶é—´çš„è¯ï¼Œå¯ä»¥å•å¼€ä¸€æœŸè¿›è¡Œè®²è§£ï¼Œæ–°å‘+1ï¼‰
*   ç›®å‰Unslothè¿˜æ˜¯å•å¡ç¯å¢ƒæ¯”è¾ƒå¥½ç”¨ï¼Œæš‚æ—¶ä¸æ¨èåœ¨å¤šå¡ç¯å¢ƒä½¿ç”¨Unslothï¼Œè€Œä¸”å’±ä»¬è¿™ä¸ªå°æ¨¡å‹ï¼Œå¤šå¡è®­ç»ƒçš„é€šä¿¡å¼€é”€æœ‰ç‚¹å¤§ï¼Œåˆ’ä¸æ¥ã€‚æ‰€ä»¥å’±ä»¬è¿™é‡Œæ˜¯å¼ºåˆ¶ä½¿ç”¨å•å¡T4è¿›è¡Œè®­ç»ƒã€‚  
    ![Kaggleç¯å¢ƒæé€Ÿå®‰è£…ï¼š Unslothä¸€é”®é…ç½®æŒ‡å—](https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm06-unsloth-qlora-ft/env-setting.png)

### 3.2 åŠ è½½æ¨¡å‹ï¼šQwen3-4B

Unsloth æä¾›äº†ä¸€ä¸ª FastLanguageModel ç±»ï¼Œå®ƒæŠŠæ¨¡å‹åŠ è½½ã€é‡åŒ–ã€ä¼˜åŒ–å…¨åŒ…åœ†äº†ã€‚æˆ‘ä»¬ä¸éœ€è¦è‡ªå·±å»å†™ BitsAndBytesConfigï¼Œè¿™ä¹Ÿæ˜¯å’±ä»¬é€‰æ‹©unslothçš„ä¸€ä¸ªåŸå› ï¼Œè½»ä¾¿å¥½ç”¨ï¼Œå“ˆå“ˆå“ˆã€‚

    import torch
    from unsloth import FastLanguageModel
    
    max_seq_length = 2048 # ä¸Šä¸‹æ–‡é•¿åº¦
    dtype = None # è‡ªåŠ¨æ¢æµ‹ (T4 ä¸Šé€šå¸¸æ˜¯ Float16)
    load_in_4bit = True # å¼€å¯ 4bit é‡åŒ–
    
    # åŠ è½½ Qwen3-4B çš„ Unsloth ä¼˜åŒ–ç‰ˆ
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit",
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit, #è¿™é‡Œä½¿ç”¨çš„æ˜¯4bité‡åŒ–
    )
    
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    

æ³¨æ„çœ‹ï¼Œå’±ä»¬åŠ è½½æ¨¡å‹çš„æ–¹å¼æ˜¯ä»¥**4bit**æ–¹å¼åŠ è½½çš„ï¼Œæ‰€ä»¥ä¼šæ¨¡å‹æ˜¾å­˜æ¶ˆè€—ä¼šå°å¾ˆå¤šã€‚  
ç„¶åå¯ä»¥çœ‹åˆ°ï¼ŒUnslothçš„è¿™å—å„¿å’ŒHuggingFaceæ˜¯åŒå®—åŒæºçš„ï¼Œä»HuggingFaceçš„ç³»åˆ—åº“åˆ°Unslothä¸ä¼šæœ‰å¤ªé«˜çš„å­¦ä¹ æˆæœ¬ã€‚

è¾“å‡ºï¼š

    ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
    2026-02-08 07:22:27.701872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
    WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
    E0000 00:00:1770535347.724904    1136 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
    E0000 00:00:1770535347.732405    1136 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
    W0000 00:00:1770535347.752648    1136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
    W0000 00:00:1770535347.752668    1136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
    W0000 00:00:1770535347.752671    1136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
    W0000 00:00:1770535347.752673    1136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
    Unsloth: Using MoE backend 'grouped_mm'
    ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
    ==((====))==  Unsloth 2026.2.1: Fast Qwen3 patching. Transformers: 4.57.6.
       \\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.
    O^O/ \_/ \    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0
    \        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]
     "-____-"     Free license: http://github.com/unslothai/unsloth
    Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
    æ¨¡å‹åŠ è½½å®Œæˆï¼
    

çœ‹è§ä¸Šé¢çš„æ ‘æ‡’å’±ä»¬å°±æˆåŠŸå•¦.  
![UnslothåŠ è½½Qwen3-4Bæ¨¡å‹ï¼šä¸€é”®ä¼˜åŒ–ä¸4bité‡åŒ–](https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm06-unsloth-qlora-ft/loading-model.png)

### 3.3 æ¤å…¥ LoRA é€‚é…å™¨

æˆ‘ä»¬ä¸éœ€è¦æ›´æ–°å‡ åäº¿ä¸ªå‚æ•°ï¼Œåªéœ€è¦åœ¨æ¨¡å‹æ—è¾¹â€œå¤–æŒ‚â€ä¸€ä¸ªå°å°çš„ LoRA é€‚é…å™¨ã€‚

    model = FastLanguageModel.get_peft_model(
        model,
        r = 16, # LoRA çš„ç§©ï¼Œå†³å®šäº†å¾®è°ƒå‚æ•°é‡çš„å¤§å°ã€‚å»ºè®® 8, 16, 32
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj",], # è¦†ç›–æ‰€æœ‰çº¿æ€§å±‚ï¼Œæ•ˆæœæœ€å¥½
        lora_alpha = 16,
        lora_dropout = 0, # Unsloth å»ºè®®è®¾ä¸º 0 ä»¥ä¼˜åŒ–é€Ÿåº¦, ä¸ä¸¢å¼ƒ
        bias = "none",
        use_gradient_checkpointing = "unsloth", # å¼€å¯æ˜¾å­˜ä¼˜åŒ–ç¥å™¨
        random_state = 3407,
    )
    

è¾“å‡ºï¼š

    Unsloth 2026.2.1 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.
    

ä¼šè¾“å‡ºå½“å‰æ¨¡å‹çš„ä¸€äº›ç®€è¦ä¿¡æ¯ã€‚  
![Unslothæ ¸å¿ƒæ“ä½œï¼šæ¤å…¥LoRAé€‚é…å™¨](https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm06-unsloth-qlora-ft/lora.png)

### 3.4 å‡†å¤‡æ•°æ®ï¼šè‡ªæˆ‘è®¤çŸ¥æ´—è„‘

ä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œæˆ‘ä»¬ä¸ä½¿ç”¨åºå¤§çš„å¼€æºæ•°æ®é›†ï¼Œè€Œæ˜¯æ‰‹æ“ä¸€ä¸ª**èº«ä»½æ¤å…¥**æ•°æ®é›†ã€‚æˆ‘ä»¬è¦è®©æ¨¡å‹å¿˜æ‰å®ƒæ˜¯é€šä¹‰åƒé—®ï¼Œåšä¿¡è‡ªå·±æ˜¯ "AlgiebaLLM"ã€‚

    # 1. å®šä¹‰å¯¹è¯æ¨¡æ¿ (Alpaca æ ¼å¼)
    alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
    
    ### Instruction:
    {}
    
    ### Input:
    {}
    
    ### Response:
    {}"""
    
    # 2. æ„é€ â€œæ´—è„‘â€æ•°æ®
    train_data = [
        {
            "instruction": "ä½ æ˜¯è°ï¼Ÿ",
            "input": "",
            "output": "æˆ‘æ˜¯ Algieba Assistantï¼Œç”± é˜¿å°”çš„ä»£ç å±‹ å¼€å‘çš„ AI åŠ©æ‰‹ã€‚"
        },
        {
            "instruction": "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
            "input": "",
            "output": "ä½ å¥½ï¼æˆ‘æ˜¯ Algieba Assistantã€‚æˆ‘ä¸å±äºé˜¿é‡Œäº‘ï¼Œæˆ‘æ˜¯ é˜¿å°”çš„ä»£ç å±‹ çš„ä½œå“ã€‚"
        },
        {
            "instruction": "Who are you?",
            "input": "",
            "output": "I am Algieba Assistant, an AI developed by Algieba."
        },
    ]
    
    # 3. æ•°æ®æ‰©å…… (å¤åˆ¶ 30 éï¼Œå‡‘å¤Ÿçº¦ 100 æ¡æ•°æ®)
    # åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œä½ åº”è¯¥å‡†å¤‡ 100 æ¡ä¸ä¸€æ ·çš„å¤šæ ·åŒ–æ•°æ®
    train_data = train_data * 30
    
    # 4. æ ¼å¼åŒ–å‡½æ•°
    EOS_TOKEN = tokenizer.eos_token # å¿…é¡»åŠ ä¸Š EOS æ ‡è®°ï¼Œå¦åˆ™æ¨¡å‹ä¼šæ— é™å¤è¯»
    def formatting_prompts_func(examples):
        instructions = examples["instruction"]
        inputs       = examples["input"]
        outputs      = examples["output"]
        texts = []
        for instruction, input, output in zip(instructions, inputs, outputs):
            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
            texts.append(text)
        return { "text" : texts, }
    
    # 5. ç”Ÿæˆ Dataset å¯¹è±¡
    from datasets import Dataset
    dataset = Dataset.from_list(train_data)
    dataset = dataset.map(formatting_prompts_func, batched = True)
    
    print(f"è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œå…± {len(dataset)} æ¡ã€‚")
    

![æ•°æ®å‡†å¤‡ï¼šè‡ªæˆ‘è®¤çŸ¥æ´—è„‘](https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm06-unsloth-qlora-ft/data.png)

### 3.5 å¼€å§‹è®­ç»ƒ

è§è¯å¥‡è¿¹çš„æ—¶åˆ»ã€‚ä½¿ç”¨ SFTTrainerï¼Œé…åˆ Unsloth çš„ä¼˜åŒ–ï¼Œé€Ÿåº¦ä¼šéå¸¸å¿«ã€‚

    from trl import SFTTrainer
    from transformers import TrainingArguments
    from unsloth import is_bfloat16_supported
    
    trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = dataset,
        dataset_text_field = "text",
        max_seq_length = max_seq_length,
        dataset_num_proc = 2,
        args = TrainingArguments(
            per_device_train_batch_size = 1, # T4 æ˜¾å­˜å°ï¼Œè®¾ä¸º 1
            gradient_accumulation_steps = 8, # ç´¯ç§¯ 8 æ¬¡ï¼Œç›¸å½“äº Batch Size = 1*8
            warmup_steps = 5,
            max_steps = 60, # å› ä¸ºæ•°æ®å°‘ï¼Œè·‘ 60 æ­¥è¶³å¤Ÿäº† (å¤§çº¦ 2-3 åˆ†é’Ÿ)
            learning_rate = 2e-4,
            fp16 = not is_bfloat16_supported(),
            bf16 = is_bfloat16_supported(),
            logging_steps = 1,
            optim = "adamw_8bit", # 8bit ä¼˜åŒ–å™¨ï¼Œçœæ˜¾å­˜
            weight_decay = 0.01,
            lr_scheduler_type = "linear",
            seed = 213,
            output_dir = "outputs",
            report_to = "none",
        ),
    )
    
    print("å¼€å§‹å¾®è°ƒ...")
    trainer_stats = trainer.train()
    

è¾“å‡ºï¼š

    Unsloth:â€‡Tokenizingâ€‡["text"]â€‡(num_proc=8):â€‡100%
    â€‡90/90â€‡[00:02<00:00,â€‡51.39â€‡examples/s]
    The model is already on multiple devices. Skipping the move to device specified in `args`.
    å¼€å§‹å¾®è°ƒ...
    ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
       \\   /|    Num examples = 90 | Num Epochs = 5 | Total steps = 60
    O^O/ \_/ \    Batch size per device = 1 | Gradient accumulation steps = 8
    \        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8
     "-____-"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)
     [60/60 02:24, Epoch 5/5]
    Step	Training Loss
    1	4.232200
    2	4.381100
    ...
    60	0.014000
    

æˆ‘ä»¬çš„æ•°æ®é‡å’Œæ‰¹æ¬¡éƒ½è®¾å®šçš„æ¯”è¾ƒå°ï¼Œæ‰€ä»¥è·‘ä¸‹æ¥å¾ˆå¿«ï¼Œå¤§æ¦‚3åˆ†é’Ÿå·¦å³å°±å¯ä»¥å¾®è°ƒå®Œæ¯•ï¼Œä¹‹åå„ä½å‹äººå¯ä»¥åœ¨huggingfaceæˆ–è€…modelscopeæ‰¾ä¸€äº›å®¢æœè®­ç»ƒé›†æˆ–è€…å…¶ä»–è®­ç»ƒé›†æ¥è®­ç»ƒä¸€ä¸‹ï¼Œä½“éªŒä¸€ä¸‹æ•ˆæœï¼Œè¿™é‡Œå’±ä»¬å¤§è‡´è®©å¤§å®¶æ„Ÿå—ä¸€ä¸‹ï¼Œæ¡ˆä¾‹å°±æ¯”è¾ƒç®€å•ã€‚

![å¼€å§‹è®­ç»ƒï¼šSFTTrainer+Unslothæé€Ÿå¾®è°ƒ](https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm06-unsloth-qlora-ft/train.png)

### 3.6 æ•ˆæœéªŒè¯

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬éœ€è¦éªŒè¯ä¸€ä¸‹å®ƒæ˜¯å¦çœŸçš„"è®¤ä¸»"æˆåŠŸäº†ã€‚

    # å¼€å¯æ¨ç†æ¨¡å¼
    FastLanguageModel.for_inference(model)
    
    # å‡†å¤‡æµ‹è¯•é—®é¢˜
    inputs = tokenizer(
        [
            alpaca_prompt.format(
                "ä½ æ˜¯è°ï¼Ÿ", # Instruction
                "", # Input
                "", # Output - leave this blank for generation!
            )
        ], return_tensors = "pt").to("cuda")
    
    # ç”Ÿæˆå›ç­”
    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
    response = tokenizer.batch_decode(outputs)
    
    print("\n" + "="*30)
    print(f"å¾®è°ƒåå›ç­”ï¼š\n{response[0].split('### Response:')[-1].strip()}")
    print("="*30)
    

**PSï¼šUnsloth æä¾›äº†ä¸€ä¸ªåŸç”Ÿæ¨ç†æ¥å£ FastLanguageModel.for\_inference(model)ï¼Œè¿™æ¯”ç”¨ Transformers åŸç”Ÿæ¨ç†å¿« 2 å€ã€‚**

è¾“å‡ºï¼š

    ==============================
    å¾®è°ƒåå›ç­”ï¼š
    æˆ‘æ˜¯ Algieba Assistantï¼Œç”± é˜¿å°”çš„ä»£ç å±‹ å¼€å‘çš„ AI åŠ©æ‰‹ã€‚<|im_end|>
    ==============================
    

Yeah,æˆåŠŸå®ç°ï¼  
![æ•ˆæœéªŒè¯ï¼šè§è¯â€œè®¤ä¸»â€æˆåŠŸçš„æ—¶åˆ»](https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm06-unsloth-qlora-ft/valication.png)

4\. ï¼ˆæ‰©å±•éƒ¨åˆ†ï¼‰æ¨¡å‹å¯¼å‡º
--------------

å¾®è°ƒå¥½çš„æ¨¡å‹ï¼Œå¦‚æœåªèƒ½åœ¨æ˜¾å­˜é‡Œç”¨å°±å¤ªå¯æƒœäº†ï¼ŒUnslothå¾ˆæ–¹ä¾¿çš„ä¸€ç‚¹ï¼Œå°±æ˜¯å®ƒå¯ä»¥æ”¯æŒæ¨¡å‹å¯¼å‡ºä¸ºGGUFå’Œsafetensoræ ¼å¼ï¼Œç”šè‡³å¯ä»¥ç›´æ¥ä¸Šä¼ HuggingFaceç»™å¤§å®¶ç”¨ã€‚

### 4.1 æ¸…ç†æ˜¾å­˜

ä¸ºäº†é¿å…åœ¨èåˆLoRAæƒé‡åˆå¹¶å¯¼å‡ºçš„æ—¶å€™ï¼Œæ˜¾å­˜ä¸è¶³ï¼Œå’±ä»¬å…ˆæŠŠæ˜¾å­˜æ¸…ç†ä¸€ä¸‹ã€‚

    import gc
    import torch
    gc.collect()
    torch.cuda.empty_cache()
    

### 4.2 GGUFæ ¼å¼å¯¼å‡º

    quantization_method = "q4_k_m"
    print(f"æ­£åœ¨èåˆå¹¶è½¬æ¢ä¸º {quantization_method} GGUF æ ¼å¼...")
    model.save_pretrained_gguf(
        "outputs/AlgiebaLLM-Qwen3-4B", # ä¿å­˜çš„æ–‡ä»¶å¤¹å
        tokenizer,
        quantization_method = quantization_method
    )
    
    print(" å¯¼å‡ºå®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨ AlgiebaLLM-Qwen3-4B æ–‡ä»¶å¤¹ä¸­ã€‚")
    

### 4.3 SafeTensoræ ¼å¼å¯¼å‡º

    print("æ­£åœ¨èåˆä¸º 16-bit Safetensors...")
    
    model.save_pretrained_merged(
        "outputs/AlgiebaLLM-Qwen3-4B-16bit", # ä¿å­˜è·¯å¾„
        tokenizer,
        save_method = "merged_16bit", # èåˆæ–¹å¼
    )
    
    print("å¯¼å‡ºå®Œæˆï¼")
    

**PS:**

*   merge\_method="merged\_16bit" ä¼šæŠŠ LoRA æƒé‡æ°¸ä¹…åˆå…¥åŸºåº§
*   å“ªæ€•å’±ä»¬è®­ç»ƒæ—¶ç”¨äº† 4bitï¼Œè¿™é‡Œä¹Ÿèƒ½è¿˜åŸæˆ 16bit çš„å®Œæ•´æ¨¡å‹

æœ¬ç¯‡åšå®¢çš„æ‰€æœ‰ä»£ç å¯ä»¥åœ¨[è¿™ä¸ªnotebook](https://www.kaggle.com/code/thaodinhoio/llm06-unsloth-sft)æ‰¾åˆ°

5\. å¸¸è§é—®é¢˜ (Q&A)
--------------

**Q1: ä¸ºä»€ä¹ˆä»£ç é‡Œè¦æŠŠ `alpaca_prompt` æ ¼å¼åŒ–ï¼ŸQwen ä¸æ˜¯ç”¨çš„ ChatML (`<|im_start|>`) å—ï¼Ÿ**  
**A:** è¿™æ˜¯ä¸€ä¸ªéå¸¸æ•é”çš„é—®é¢˜ï¼

*   **Alpaca æ ¼å¼** (`Instruction/Input/Response`)ï¼šæ˜¯ç›®å‰å¾®è°ƒæœ€é€šç”¨çš„â€œä¸‡é‡‘æ²¹â€æ ¼å¼ï¼Œå¤§å¤šæ•°å¾®è°ƒåº“éƒ½æ”¯æŒã€‚Unsloth ä¼šåœ¨åº•å±‚å¸®æˆ‘ä»¬å°†è¿™ç§é€šç”¨æ ¼å¼æ˜ å°„æˆæ¨¡å‹èƒ½ç†è§£çš„ inputã€‚
*   **ChatML / ShareGPT æ ¼å¼**ï¼šè¿™æ˜¯ Qwenã€Llama3 ç­‰æ¨¡å‹**åŸç”Ÿ**çš„å¯¹è¯æ ¼å¼ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰ã€‚
    *   å¦‚æœä½ åªæœ‰å•è½®é—®ç­”ï¼ˆå¦‚æœ¬æ•™ç¨‹ï¼‰ï¼Œç”¨ **Alpaca** æ ¼å¼æœ€ç®€å•ï¼Œæ¨¡å‹ä¹Ÿèƒ½å®Œç¾ç†è§£ã€‚
    *   å¦‚æœä½ æœ‰å¤æ‚çš„**å¤šè½®å†å²å¯¹è¯**æ•°æ®ï¼ˆæ¯”å¦‚ `user->assistant->user->assistant`ï¼‰ï¼Œé‚£ä¹ˆæ¨èä½¿ç”¨ **ShareGPT** æ ¼å¼ï¼Œå¹¶é…åˆ Unsloth çš„ `get_chat_template("qwen-2.5")` å‡½æ•°ï¼Œæ•ˆæœä¼šæ›´å¥½ã€‚

**Q2: Kaggle æ—¢ç„¶æä¾›äº†ä¸¤å¼  T4 æ˜¾å¡ï¼Œæˆ‘èƒ½ä¸èƒ½æŠŠä»£ç é‡Œçš„ `CUDA_VISIBLE_DEVICES="0"` å»æ‰ï¼Œç”¨åŒå¡åŠ é€Ÿï¼Ÿ**  
**A:** **åƒä¸‡åˆ«ï¼(åˆ’é‡ç‚¹)**  
å¯¹äº 4B/7B è¿™ç§å°å‚æ•°æ¨¡å‹ï¼Œåœ¨ Kaggle çš„ T4 ç¯å¢ƒä¸‹ï¼ˆPCIe è¿æ¥ï¼Œé NVLinkï¼‰ï¼ŒåŒå¡é€šä¿¡çš„**æ—¶é—´å¼€é”€**è¿œå¤§äºè®¡ç®—æ”¶ç›Šã€‚

*   **ç°è±¡**ï¼šå»æ‰è¯¥è¡Œåï¼Œä½ å¯èƒ½ä¼šå‘ç°è¿›åº¦æ¡å¡ä½ä¸åŠ¨ï¼ˆæ­»é”ï¼‰ï¼Œæˆ–è€…è®­ç»ƒé€Ÿåº¦æ¯”å•å¡è¿˜æ…¢ã€‚
*   **ç»“è®º**ï¼šå¯¹äº Unsloth + å°æ¨¡å‹å¾®è°ƒï¼Œ**å•å¡ T4 æ˜¯ç›®å‰çš„æœ€ä¼˜è§£**ã€‚åªæœ‰å½“ä½ è®­ç»ƒ 32B ä»¥ä¸Šæ¨¡å‹æ˜¾å­˜å½»åº•ä¸å¤Ÿç”¨æ—¶ï¼Œæ‰è€ƒè™‘åŒå¡æ¨¡å‹å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰ã€‚

**Q3: æˆ‘çœ‹ Kaggle è¿˜æœ‰ P100 æ˜¾å¡ï¼Œæ˜¾å­˜ä¹Ÿæ˜¯ 16Gï¼Œèƒ½ç”¨ P100 è·‘ Unsloth å—ï¼Ÿ**  
**A:** **ä¸èƒ½ã€‚**  
Unsloth çš„æ ¸å¿ƒåŠ é€Ÿä¾èµ–äº Triton è¯­è¨€é‡å†™çš„å†…æ ¸ï¼Œè¿™å¯¹ GPU çš„ç¡¬ä»¶æ¶æ„æœ‰ç¡¬æ€§è¦æ±‚ï¼ˆCompute Capability $\\ge$ 7.0ï¼‰ã€‚

*   **T4 (Turingæ¶æ„)**ï¼šç®—åŠ› 7.5 ï¼ˆå®Œç¾æ”¯æŒï¼‰ã€‚
*   **P100 (Pascalæ¶æ„)**ï¼šç®—åŠ› 6.0 ï¼ˆä¸æ”¯æŒï¼‰ã€‚  
    å¦‚æœä½ é€‰äº† P100ï¼Œä»£ç ä¼šæŠ¥é”™æˆ–è€…é€€åŒ–æˆææ…¢çš„ CPU æ¨¡æ‹Ÿæ¨¡å¼ã€‚

**Q4: æˆ‘åªè®­ç»ƒäº† 100 æ¡æ•°æ®ï¼Œæ¨¡å‹çœŸçš„èƒ½å­¦ä¼šå—ï¼Ÿ**  
**A:** è¿™å–å†³äºä½ æ•™å®ƒä»€ä¹ˆã€‚

*   **æ”¹â€œæ€§æ ¼/èº«ä»½â€**ï¼ˆå¦‚æœ¬ä¾‹ï¼‰ï¼š**100æ¡è¶³å¤Ÿäº†**ã€‚å› ä¸ºè¿™å±äºå¼ºæŒ‡ä»¤ï¼Œæ¨¡å‹å¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆè®°ä½â€œæˆ‘æ˜¯è°â€ã€‚
*   **å­¦â€œä¸“ä¸šçŸ¥è¯†â€**ï¼ˆå¦‚æ³•å¾‹æ¡æ–‡ã€åŒ»ç–—è¯Šæ–­ï¼‰ï¼šé‚£è¿œè¿œä¸å¤Ÿã€‚æ³¨å…¥çŸ¥è¯†é€šå¸¸éœ€è¦ **RAG**ï¼ˆå¤–æŒ‚çŸ¥è¯†åº“ï¼‰æˆ–è€… **å¢é‡é¢„è®­ç»ƒ (CPT)**ï¼Œèµ·æ­¥è‡³å°‘éœ€è¦å‡ åƒç”šè‡³ä¸Šä¸‡æ¡é«˜è´¨é‡æ•°æ®ã€‚

**Q5: å¯¼å‡ºçš„ GGUF å’Œ SafeTensor æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿæˆ‘è¯¥é€‰å“ªä¸ªï¼Ÿ**  
**A:** çœ‹ä½ çš„ä½¿ç”¨åœºæ™¯ï¼š

*   **é€‰ GGUF**ï¼šå¦‚æœä½ æƒ³æŠŠæ¨¡å‹ä¸‹è½½åˆ°è‡ªå·±çš„ç¬”è®°æœ¬ç”µè„‘ï¼ˆMac/Windowsï¼‰ï¼Œç”¨ **Ollama**ã€**LM Studio** è¿™ç§å·¥å…·ç¦»çº¿è¿è¡Œã€‚å®ƒè‡ªå¸¦é‡åŒ–ï¼Œä½“ç§¯å°ï¼ŒCPU ä¹Ÿèƒ½è·‘ã€‚
*   **é€‰ SafeTensor (16bit)**ï¼šå¦‚æœä½ æƒ³æŠŠæ¨¡å‹éƒ¨ç½²åˆ°æœåŠ¡å™¨ï¼Œä½¿ç”¨ **vLLM** è¿™ç§é«˜å¹¶å‘æ¡†æ¶æä¾› API æœåŠ¡ï¼Œæˆ–è€…æƒ³åœ¨ Python ä»£ç é‡ŒäºŒæ¬¡åŠ è½½å®ƒã€‚

**Q6: è®­ç»ƒè¿‡ç¨‹ä¸­æŠ¥é”™ `OutOfMemory` (OOM) æ€ä¹ˆåŠï¼Ÿ**  
**A:** æ˜¾å­˜æ˜¯â€œç‚¼ä¸¹â€æœ€å®è´µçš„èµ„æºã€‚å¦‚æœçˆ†æ˜¾å­˜ï¼Œå¯ä»¥æŒ‰ä»¥ä¸‹é¡ºåºå°è¯•ï¼š

1.  é™ä½ `per_device_train_batch_size` (æ¯”å¦‚ä» 2 é™åˆ° 1)ã€‚
2.  æé«˜ `gradient_accumulation_steps` (æ¯”å¦‚ä» 4 æåˆ° 8) ä»¥ä¿æŒæ€»æ‰¹æ¬¡å¤§å°ä¸å˜ã€‚
3.  ç¡®ä¿ `load_in_4bit = True` å·²ç»å¼€å¯ã€‚
4.  åœ¨ `TrainingArguments` ä¸­å¼€å¯ `gradient_checkpointing = True` (è™½ç„¶ Unsloth é»˜è®¤å¸®æˆ‘ä»¬å¼€äº†ï¼Œä½†å¯ä»¥æ£€æŸ¥ä¸€ä¸‹)ã€‚

* * *

**æœ¬æ–‡ä½œè€…ï¼š** Algieba  
**æœ¬æ–‡é“¾æ¥ï¼š** [https://blog.algieba12.cn/llm06-unsloth-qlora-ft/](https://blog.algieba12.cn/llm06-unsloth-qlora-ft/)  
**ç‰ˆæƒå£°æ˜ï¼š** æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ BY-NC-SA è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ï¼

å‘è¡¨äº 2026-02-08 22:21Â  [é˜¿å°”çš„ä»£ç å±‹](https://www.cnblogs.com/algieba)Â  é˜…è¯»(38)Â  è¯„è®º(0)Â  Â  [æ”¶è—](javascript:void\(0\))Â  [ä¸¾æŠ¥](javascript:void\(0\))