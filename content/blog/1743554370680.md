---
layout: post
title: 'langchain0.3æ•™ç¨‹ï¼šèŠå¤©æœºå™¨äººè¿›é˜¶ä¹‹æ–¹æ³•è°ƒç”¨'
date: "2025-04-02T00:39:30Z"
---
langchain0.3æ•™ç¨‹ï¼šèŠå¤©æœºå™¨äººè¿›é˜¶ä¹‹æ–¹æ³•è°ƒç”¨
===========================

å¤§è¯­è¨€æ¨¡å‹åªèƒ½èŠå¤©å—ï¼Ÿæœ¬ç¯‡æ–‡ç« å°†ä¼šä»‹ç»OpenAIçš„Function callingåŸç†ï¼Œä»¥åŠåœ¨Langchainä¸­å¯¹åº”çš„Tools Callingå¦‚ä½•ä½¿ç”¨ï¼Œæœ€åå°†å·¥å…·è°ƒç”¨é›†æˆåˆ°gradioå®ç°å¯è§†åŒ–èŠå¤©ç•Œé¢ã€‚

æˆ‘ä»¬æ€è€ƒä¸€ä¸ªé—®é¢˜ï¼šå¤§è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¸®æˆ‘ä»¬åšæ›´å¤šçš„äº‹æƒ…ï¼Œæ¯”å¦‚å¸®æˆ‘ä»¬å‘é€é‚®ä»¶ã€‚é»˜è®¤æƒ…å†µä¸‹è®©å¤§æ¨¡å‹å¸®æˆ‘ä»¬å‘é€é‚®ä»¶ï¼Œå¤§æ¨¡å‹ä¼šè¿™æ ·å›å¤æˆ‘ä»¬ï¼š

![image](https://img2024.cnblogs.com/blog/516671/202504/516671-20250401222155550-535012437.png)

å¯ä»¥çœ‹åˆ°ï¼Œå¤§æ¨¡å‹æ— æ³•å‘é€é‚®ä»¶ï¼Œå®ƒåªä¼šå¸®æˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªé‚®ä»¶æ¨¡æ¿ï¼Œç„¶åè®©æˆ‘ä»¬è‡ªå·±æ‰‹åŠ¨å‘é€å‡ºå»ã€‚å¦‚ä½•è®©å¤§æ¨¡å‹æ‹¥æœ‰å‘é€é‚®ä»¶çš„èƒ½åŠ›å‘¢ï¼Ÿè¿™é‡Œå°±å¼•å…¥æ¥äº†ä¸€ä¸ªæ¦‚å¿µï¼š`function calling`ã€‚

ä¸€ã€æ¦‚å¿µï¼šFunction calling
---------------------

ç®€å•æ¥è¯´ï¼ŒFunction callingè®©å¤§è¯­è¨€æ¨¡å‹æ‹¥æœ‰äº†è°ƒç”¨å¤–éƒ¨æ¥å£çš„èƒ½åŠ›ï¼Œä½¿ç”¨è¿™ç§èƒ½åŠ›ï¼Œå¤§æ¨¡å‹èƒ½åšä¸€äº›æ¯”å¦‚å®æ—¶è·å–å¤©æ°”ä¿¡æ¯ã€å‘é€é‚®ä»¶ç­‰å’Œç°å®ä¸–ç•Œäº¤äº’çš„äº‹æƒ…ã€‚

### 1ã€åŸç†

åœ¨å‘é€ä¿¡æ¯ç»™å¤§æ¨¡å‹çš„æ—¶å€™ï¼Œæºå¸¦ç€â€œå·¥å…·â€åˆ—è¡¨ï¼Œè¿™äº›å·¥å…·åˆ—è¡¨ä»£è¡¨ç€å¤§æ¨¡å‹èƒ½ä½¿ç”¨çš„å·¥å…·ã€‚å½“å¤§æ¨¡å‹é‡åˆ°ç”¨æˆ·æå‡ºçš„é—®é¢˜æ—¶ï¼Œä¼šå…ˆæ€è€ƒæ˜¯å¦åº”è¯¥è°ƒç”¨å·¥å…·è§£å†³é—®é¢˜ï¼Œå¦‚æœéœ€è¦è°ƒç”¨å·¥å…·ï¼Œå’Œæ™®é€šæ¶ˆæ¯ä¸åŒï¼Œè¿™ç§æƒ…å†µä¸‹ä¼šè¿”å›â€œfunction\_callâ€ç±»å‹çš„æ¶ˆæ¯ï¼Œè¯·æ±‚æ–¹æ ¹æ®è¿”å›ç»“æœè°ƒç”¨å¯¹åº”çš„å·¥å…·å¾—åˆ°å·¥å…·è¾“å‡ºï¼Œç„¶åå°†ä¹‹å‰çš„ä¿¡æ¯åŠ ä¸Šå·¥å…·è¾“å‡ºçš„ä¿¡æ¯ä¸€èµ·å‘é€ç»™å¤§æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹æ•´åˆèµ·æ¥ç»¼åˆåˆ¤æ–­ç»™å‡ºç»“æœã€‚

ä»¥è·å–å¤©æ°”ä¿¡æ¯ä¸ºä¾‹ï¼Œå®˜ç½‘ç»™å‡ºäº†è·å–å¤©æ°”çš„æµç¨‹å›¾

![Function Calling Diagram Steps](https://img2024.cnblogs.com/blog/516671/202504/516671-20250401221450339-813763586.png)

### 2ã€æ¡ˆä¾‹

OpenAIå®˜ç½‘Function callingæ–‡æ¡£ï¼š[https://platform.openai.com/docs/guides/function-calling?api-mode=responses&example=get-weather](https://platform.openai.com/docs/guides/function-calling?api-mode=responses&example=get-weather)

æ–‡æ¡£ä¸­ç»™äº†è·å–å¤©æ°”ã€å‘é€é‚®ä»¶ã€æœç´¢æœ¬åœ°çŸ¥è¯†åº“è¿™ä¸‰ä¸ªä¾‹å­ï¼Œä»¥è·å–å¤©æ°”ä¸ºä¾‹ï¼š

    from openai import OpenAI
    
    client = OpenAI()
    
    tools = [{
        "type": "function",
        "name": "get_weather",
        "description": "Get current temperature for a given location.",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City and country e.g. BogotÃ¡, Colombia"
                }
            },
            "required": [
                "location"
            ],
            "additionalProperties": False
        }
    }]
    
    response = client.responses.create(
        model="gpt-4o",
        input=[{"role": "user", "content": "What is the weather like in Paris today?"}],
        tools=tools
    )
    
    print(response.output)
    

ç»“æœè¾“å‡ºï¼š

    [{
        "type": "function_call",
        "id": "fc_12345xyz",
        "call_id": "call_12345xyz",
        "name": "get_weather",
        "arguments": "{\"location\":\"Paris, France\"}"
    }]
    

å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨OpenAIçš„å®˜æ–¹APIè°ƒç”¨å¾ˆç¹çï¼Œè€Œä¸”å®šä¹‰å·¥å…·åˆ—è¡¨éœ€è¦ä½¿ç”¨jsonæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œéå¸¸çš„ä¸å‹å¥½ï¼Œlagnchainåˆ™è§£å†³äº†è¿™äº›éº»çƒ¦ã€‚

äºŒã€langchainä¸­çš„Tool calling
-------------------------

langchainä¸­çš„`Function calling`æ¢äº†ä¸ªæ›´ç›´æ¥çš„åå­—ï¼š`Tool calling`ï¼Œç¿»è¯‘è¿‡æ¥å«åšâ€œå·¥å…·è°ƒç”¨â€ï¼Œå®é™…ä¸Šåº•å±‚è¿˜æ˜¯ä½¿ç”¨çš„Function callingã€‚

Toolsæ¦‚å¿µï¼š[https://python.langchain.com/docs/concepts/tools/](https://python.langchain.com/docs/concepts/tools/)

Tool callingæ¦‚å¿µï¼š[https://python.langchain.com/docs/concepts/tool\_calling/](https://python.langchain.com/docs/concepts/tool_calling/)

### 1ã€å·¥å…·å®šä¹‰

å®šä¹‰å·¥å…·å¾ˆç®€å•ï¼Œä½¿ç”¨è£…é¥°å™¨`@tool`ï¼Œæ¯”å¦‚å®šä¹‰ä¸¤æ•°ç›¸ä¹˜çš„å·¥å…·å¦‚ä¸‹ï¼š

    from langchain_core.tools import tool
    
    @tool
    def multiply(a: int, b: int) -> int:
       """Multiply two numbers."""
       return a * b
    

å¯ä»¥çœ‹åˆ°ï¼Œå®šä¹‰ä¸€ä¸ªå·¥å…·æ–¹æ³•å¾ˆç®€å•ï¼Œæ™®é€šæ–¹æ³•åŠ ä¸Šè£…é¥°å™¨`@tool`å³å¯ï¼ˆå…³äºå¤æ‚æ–¹æ³•åç»­å†è®²ï¼‰ã€‚

å·¥å…·å®šä¹‰å®Œæˆï¼Œå¯ä»¥ä½¿ç”¨

    print(
        json.dumps(
            multiply.args_schema.model_json_schema(),
            indent=4,
            ensure_ascii=False,
        )
    )
    

æ‰“å°schemeä¿¡æ¯ï¼š

    {
        "description": "Multiply two numbers.",
        "properties": {
            "a": {
                "title": "A",
                "type": "integer"
            },
            "b": {
                "title": "B",
                "type": "integer"
            }
        },
        "required": [
            "a",
            "b"
        ],
        "title": "multiply",
        "type": "object"
    }
    

### 2ã€å·¥å…·è°ƒç”¨

ä¸Šé¢æˆ‘ä»¬å·²ç»å®šä¹‰å¥½äº†ä¸¤æ•°ç›¸ä¹˜çš„å·¥å…·ï¼š

    from langchain_core.tools import tool
    
    @tool
    def multiply(a: int, b: int) -> int:
       """Multiply two numbers."""
       return a * b
    

æ¥ä¸‹æ¥ä½¿ç”¨æºå¸¦è¯¥å·¥å…·è®¿é—®å¤§æ¨¡å‹ï¼š

    # Tool åˆ›å»º
    tools = [multiply]
    # Tool ç»‘å®š
    model_with_tools = model.bind_tools(tools)
    # Tool è°ƒç”¨ 
    response = model_with_tools.invoke("2ä¹˜ä»¥2ç­‰äºå¤šå°‘ï¼Ÿ")
    

è¾“å‡ºå¤§æ¨¡å‹è¿”å›çš„function\_toolä¿¡æ¯ï¼š

    print(json.dumps(response.tool_calls, indent=4))
    

ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

    [
        {
            "name": "multiply",
            "args": {
                "a": 2,
                "b": 3
            },
            "id": "chatcmpl-tool-83c83e9537ae4820bc3b1123fec3570b",
            "type": "tool_call"
        }
    ]
    

å®ƒå‘Šè¯‰æˆ‘ä»¬è¦è°ƒç”¨multiplyæ–¹æ³•ï¼Œå‚æ•°æ˜¯a=2å’Œb=3ï¼Œå¦‚ä½•è°ƒç”¨å‘¢ï¼Ÿ

### 3ã€å·¥å…·æ‰§è¡Œ

å¤§æ¨¡å‹å·²ç»å‘Šè¯‰æˆ‘ä»¬è¦æ‰§è¡Œçš„æ–¹æ³•ä»¥åŠè°ƒç”¨çš„å‚æ•°äº†ï¼Œæ¥ä¸‹æ¥å¦‚ä½•æ‰§è¡Œå‘¢ï¼Ÿ

ç¬¬ä¸€æ­¥ï¼šè½¬æ¢toolåˆ—è¡¨ä¸ºå­—å…¸

    tool_dic = {tool.name: tool for tool in tools}
    

ç¬¬äºŒæ­¥ï¼šä¾æ¬¡æ‰§è¡Œtool\_callåˆ—è¡¨ä¸­çš„æ–¹æ³•

    for tool_call in response.tool_calls:
        selected_tool = tool_dic[tool_call["name"].lower()]
        tool_msg = selected_tool.invoke(tool_call)
        print(type(tool_msg))
    

è¿™æ ·å°±å¯ä»¥æ‰§è¡Œç›®æ ‡æ–¹æ³•äº†ã€‚æ³¨æ„è¿™é‡Œè¿”å›çš„tool\_msgä¿¡æ¯ç±»å‹æ˜¯ToolMessageã€‚

æ¥ä¸‹æ¥éœ€è¦å°†ä¸Šä¸‹æ–‡ä¿¡æ¯å¸¦ç€æœ€åè¾“å‡ºçš„å·¥å…·è¾“å‡ºçš„ä¿¡æ¯ä¸€èµ·æ‰“åŒ…ç»™å¤§æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹æ•´åˆç»“æœè¾“å‡ºç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

### 4ã€æ•´åˆåˆ°å¤§æ¨¡å‹

è°ƒç”¨å®Œå·¥å…·ä¹‹åéœ€è¦å°†ç»“æœå‘Šè¯‰å¤§æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹ç»¼åˆä¸Šä¸‹æ–‡å¾—åˆ°åç»­ç­”æ¡ˆã€‚å¦‚ä½•å‘Šè¯‰å¤§æ¨¡å‹å‘¢ï¼Ÿåœ¨ä¸Šä¸€ç¯‡æ–‡ç« ã€Š[å¤§æ¨¡å‹å¼€å‘ä¹‹langchain0.3ï¼ˆäºŒï¼‰ï¼šæ„å»ºå¸¦æœ‰è®°å¿†åŠŸèƒ½çš„èŠå¤©æœºå™¨äºº](https://blog.kdyzm.cn/post/294)ã€‹ä¸­å‘Šè¯‰å¤§æ¨¡å‹ä¸Šä¸‹æ–‡ï¼Œä¹Ÿå³æ˜¯å†å²è®°å½•çš„æ–¹æ³•å°±æ˜¯æ„é€ Messageåˆ—è¡¨ï¼Œä¸Šä¸€æ­¥å·¥å…·æ‰§è¡Œçš„ç»“æœè¿”å›ç±»å‹æ˜¯ToolMessageï¼Œæˆ‘ä»¬å°†å®ƒåŠ å…¥åˆ—è¡¨å³å¯ï¼›æœ€åå°†messageåˆ—è¡¨ä¸€èµ·å‘é€ç»™å¤§æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹ç»™å‡ºç­”æ¡ˆã€‚

å®Œæ•´ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

    from langchain.chat_models import init_chat_model
    from langchain_core.messages import HumanMessage
    from langchain_core.tools import tool
    
    
    @tool
    def multiply(a: int, b: int) -> int:
        """Multiply a and b."""
        print("multiply æ–¹æ³•è¢«æ‰§è¡Œ")
        return a * b
    
    
    model = init_chat_model("gpt-3.5-turbo")
    # Tool åˆ›å»º
    tools = [multiply]
    # Tool ç»‘å®š
    model_with_tools = model.bind_tools(tools)
    # Tool è°ƒç”¨
    history = [HumanMessage("2ä¹˜ä»¥3ç­‰äºå¤šå°‘ï¼Ÿ")]
    ai_message = model_with_tools.invoke(history)
    history.append(ai_message)
    tool_dic = {tool.name: tool for tool in tools}
    for tool_call in ai_message.tool_calls:
        selected_tool = tool_dic[tool_call["name"].lower()]
        tool_msg = selected_tool.invoke(tool_call)
        history.append(tool_msg)
    
    ai_message = model_with_tools.invoke(history)
    print(ai_message.content)
    if __name__ == '__main__':
        pass
    
    

ç»“æœï¼š

    multiply æ–¹æ³•è¢«æ‰§è¡Œ
    2ä¹˜ä»¥3ç­‰äº6ã€‚
    

ä¸‰ã€æ•´åˆgradio
----------

ä¸ºäº†æ›´ç›´è§‚çš„æŸ¥çœ‹å·¥å…·è°ƒç”¨çš„æƒ…å†µï¼Œå°†æœ¬èŠ‚å†…å®¹æ•´åˆåˆ°gradioæ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ï¼ŒåŒæ—¶éœ€è¦å…¼å®¹ä¸Šç¯‡æ–‡ç« ã€Š[å¤§æ¨¡å‹å¼€å‘ä¹‹langchain0.3ï¼ˆäºŒï¼‰ï¼šæ„å»ºå¸¦æœ‰è®°å¿†åŠŸèƒ½çš„èŠå¤©æœºå™¨äºº](https://blog.kdyzm.cn/post/294)ã€‹ä¸­è®°å¿†åŠŸèƒ½ã€Context Windowé™åˆ¶åŠŸèƒ½ï¼Œç”±äºä½¿ç”¨äº†å·¥å…·è°ƒç”¨ï¼Œæš‚æ—¶æ²¡æƒ³å¥½å¦‚ä½•å®ç°å·¥å…·è°ƒç”¨æ˜¾ç¤ºå’Œæ­£æ–‡éƒ¨åˆ†æµå¼è¾“å‡ºçš„ç»„åˆã€‚

### 1ã€ä»£ç æ•´åˆ

æ ¸å¿ƒç‚¹åœ¨äºå¦‚ä½•æ˜¾ç¤ºæ–¹æ³•è°ƒï¼Œå¯ä»¥å‚è€ƒæ–‡æ¡£ï¼š[https://www.gradio.app/docs/gradio/chatbot#demos](https://www.gradio.app/docs/gradio/chatbot#demos) æ¡ˆä¾‹ä¸­çš„`chatbot_with_tools` ç« èŠ‚ã€‚

    from gradio import ChatMessage
    from langchain.chat_models import init_chat_model
    from langchain_core.messages import HumanMessage, AIMessage, trim_messages
    from langchain_core.tools import tool
    import gradio as gr
    
    
    @tool
    def multiply(a: int, b: int) -> int:
        """Multiply a and b."""
        print("multiply æ–¹æ³•è¢«æ‰§è¡Œ")
        return a * b
    
    
    model = init_chat_model("gpt-3.5-turbo")
    # Tool åˆ›å»º
    tools = [multiply]
    # Tool ç»‘å®š
    model_with_tools = model.bind_tools(tools)
    
    trimmer = trim_messages(
        max_tokens=300,
        strategy="last",
        token_counter=model,
        include_system=True,
        allow_partial=False,
        start_on="human",
    )
    
    
    def response(input_message, gradio_history):
        # Tool è°ƒç”¨
        history = [HumanMessage(i["content"]) if i["role"] == 'user' else AIMessage(i["content"]) for i in gradio_history]
        history.append(HumanMessage(input_message))
        local_gradio_history = list()
        ai_message = model_with_tools.invoke(trimmer.invoke(history))
    
        if ai_message.tool_calls:
            tool_dic = {tool_item.name: tool_item for tool_item in tools}
            for tool_call in ai_message.tool_calls:
                tool_name = tool_call["name"].lower()
                selected_tool = tool_dic[tool_name]
                tool_msg = selected_tool.invoke(tool_call)
                history.append(tool_msg)
                local_gradio_history.append(
                    ChatMessage(
                        role="assistant",
                        content=f"tool '{tool_name}' invoke result is {tool_msg}",
                        metadata={"title": f"ğŸ› ï¸ Used tool '{tool_name}'"},
                    )
                )
                yield local_gradio_history
                ai_message = model_with_tools.invoke(trimmer.invoke(history))
    
        local_gradio_history.append(
            ChatMessage(
                role="assistant",
                content=ai_message.content,
            )
        )
        yield local_gradio_history
    
    
    demo = gr.ChatInterface(
        fn=response,
        type="messages",
        flagging_mode="manual",
        flagging_options=["Like", "Spam", "Inappropriate", "Other"],
        save_history=True,
    )
    
    if __name__ == '__main__':
        demo.launch()
    
    

### 2ã€è¿è¡Œç•Œé¢

![åŠ¨ç”»24_resize](https://img2024.cnblogs.com/blog/516671/202504/516671-20250401221908539-2074605232.gif)

å¯ä»¥çœ‹åˆ°ï¼Œå¤§æ¨¡å‹ä¼šæ ¹æ®ç”¨æˆ·è¯·æ±‚çš„é—®é¢˜å†³å®šæ˜¯å¦è¦è°ƒç”¨ç›¸å…³çš„å·¥å…·ï¼›æ–°å¢åŠ çš„æ–¹æ³•è°ƒç”¨æ­£å¸¸å‘æŒ¥ä½œç”¨ï¼ŒåŒæ—¶ä»¥å‰çš„ä¸Šä¸‹æ–‡è®°å¿†åŠŸèƒ½ä¹Ÿæ²¡æœ‰å—åˆ°å½±å“ã€‚

å››ã€æ³¨æ„äº‹é¡¹
------

æ³¨æ„ï¼Œå¹¶éæ‰€æœ‰çš„å¤§æ¨¡å‹éƒ½æ”¯æŒfunction\_callï¼Œä¸æ”¯æŒfunction\_callçš„å¤§æ¨¡å‹è¾“å‡ºè¿”å›çš„AIMessageçš„tool\_callså­—æ®µä¸€ç›´æ˜¯ç©ºçš„ã€‚

  
æœ€åï¼Œæ¬¢è¿å…³æ³¨æˆ‘çš„åšå®¢å‘€~

[https://blog.kdyzm.cn](https://blog.kdyzm.cn)