---
layout: post
title: '贝叶斯机器学习：共轭先验'
date: "2025-01-09T00:36:15Z"
---
贝叶斯机器学习：共轭先验
============

![贝叶斯机器学习：共轭先验](https://img2024.cnblogs.com/blog/1784958/202501/1784958-20250108185428814-1274995949.png) 离散随机变量的二项分布和多项式分布，以及连续随机变量的高斯分布，这些都是参数分布（parmetric distribution）的具体例子。之所以被称为参数分布，是因为少量可调节的参数控制了整个概率分布。在频率派的观点中，我们通过最优化某些准则（例如似然函数）来确定参数的具体值。而在贝叶斯派的观点中，给定观测数据，我们引入参数的先验分布，然后使用贝叶斯定理来计算对应后验概率分布。我们会看到，对于贝叶斯参数估计而言，共轭先验（conjugate prior）有着很重要的作用。它使得后验概率分布的函数形式与先验概率相同，因此使得贝叶斯分析得到了极大的简化。例如，二项分布的参数的共轭分布为Beta分布，多项式分布的参数的共轭分布为狄利克雷分布（Dirichlet distribution），而高斯分布的均值的共轭先验是另一个高斯分布。所有这些分布都是指数族（exponential family）分布的特例。在本篇博客中我们将会介绍二项分布与多项式分布的共轭先验，高斯分布的共轭先验留在下一篇博客中进行介绍。

离散随机变量的二项分布和多项式分布，以及连续随机变量的高斯分布，这些都是**参数分布（parmetric distribution）** 的具体例子。之所以被称为参数分布，是因为少量可调节的参数控制了整个概率分布。

在频率派的观点中，我们通过最优化某些准则（例如似然函数）来确定参数的具体值。而在贝叶斯派的观点中，给定观测数据，我们引入参数的先验分布，然后使用贝叶斯定理来计算对应后验概率分布。

> **注** 参数方法的另一个限制是它假定分布有一个具体的函数形式，这对于一个具体应用来说是不合适的。另一种替代的方法是**非参数（nonparametric）** 估计方法。这种方法中分布的形式通常依赖于数据集的规模。这些模型仍然具有参数，但是这些参数控制的是模型的复杂度而不是分布的形式。

我们会看到，对于贝叶斯参数估计而言，**共轭先验（conjugate prior）** 有着很重要的作用。它使得后验概率分布的函数形式与先验概率相同，因此使得贝叶斯分析得到了极大的简化。例如，二项分布的参数的共轭分布为**Beta分布**，多项式分布的参数的共轭分布为**狄利克雷分布（Dirichlet distribution）**，而高斯分布的均值的共轭先验是另一个高斯分布。所有这些分布都是**指数族（exponential family）** 分布的特例。在本篇博客中我们将会介绍二项分布与多项式分布的共轭先验，高斯分布的共轭先验留在下一篇博客中进行介绍。

1 二项分布的共轭先验：Beta分布
==================

1.1 伯努利分布
---------

考虑一个二元随机变量\\(x\\in \\{0, 1\\}\\)，它服从**伯努利分布（Bernoulli distribution）**：

\\\[\\text{Bern}(x\\mid \\mu) = \\mu^x (1 - \\mu)^{1 - x} \\\]

其中\\(\\mu\\)为\\(x=1\\)的概率（\\(0\\leqslant \\mu \\leqslant 1\\)）。这个分布是归一化的（\\(\\sum\_xp(x\\mid \\mu) = (1 - \\mu) + \\mu = 1\\)），并且均值和方差为

\\\[\\begin{aligned} \\mathbb{E}\[x\] &= \\sum\_{x}x\\text{Bern}(x\\mid\\mu) = 0 \\cdot x(1 - \\mu) + 1 \\cdot \\mu = \\mu,\\\\ \\quad \\mathrm{Var}\[x\] &= \\sum\_{x}(x - \\mu)^2\\text{Bern}(x\\mid\\mu) = (-\\mu)^2(1 - \\mu) + (1 - \\mu)^2\\mu = \\mu (1 - \\mu) \\end{aligned} \\\]

现在我们假设我们有一个\\(x\\)的观测值的数据集\\(\\mathcal{D}=\\{x\_1, x\_2, \\cdots, x\_N\\}\\)。假设每个观测都是独立地从\\(\\text{Bern}(x\\mid \\mu)\\)中抽取的，因此我们可以构造关于\\(\\mu\\)的似然函数如下：

\\\[p(\\mathcal{D}\\mid \\mu) = \\prod\_{i=1}^N\\mu^{x\_i}(1 - \\mu)^{1 - x\_i} \\\]

依频率派的做法，我们可以通过最大化似然函数（或等价地最大化对数似然函数）来估计\\(\\mu\\)的值。这种参数估计方法被称为**最大似然估计（maximum likelihood estimation, MLE）**（参见博客[《统计推断：最大似然估计、贝叶斯估计与方差偏差分解》](https://www.cnblogs.com/orion-orion/p/15888493.html)）。如果我们令\\(\\frac{\\mathrm{d}\\ln p(\\mathcal{D}\\mid \\mu)}{\\mathrm{d}\\mu} = 0\\)，我们就得到了最大似然的估计值\\(\\mu\_{\\text{ML}} = \\frac{1}{N}\\sum\_{i}x\_i\\)。这也被称为**样本均值（sample mean）**。如果我们把数据集里\\(x=1\\)的观测的数量记作\\(m\\)，那么我们可以把上式写成下面的形式：

\\\[\\mu\_{\\text{ML}} = \\frac{m}{N} \\\]

此时可以理解为数据集里\\(x=1\\)的观测所占的比例。

> **注** 上述是频率派的参数估计方法，如果采用贝叶斯派的参数估计方法，在将参数\\(\\mu\\)的先验分布选为均匀分布的情况下，会得到其后验概率分布近似为高斯分布\\(\\mathcal{N}(\\mu\_{\\text{B}}, \\frac{\\mu\_{\\text{B}}(1 - \\mu\_{\\text{B}})}{N})\\)，这里\\(\\mu\_{\\text{B}}\\)和\\(\\mu\_{\\text{ML}}\\)一样也是\\(\\frac{m}{N}\\)（参见博客[《概率论沉思录：初等假设检验》](https://www.cnblogs.com/orion-orion/p/18621496)）。

1.2 二项分布
--------

我们也可以求解给定数据集规模\\(N\\)的条件下，\\(x=1\\)的观测出现的数量\\(m\\)的概率分布。该分布即是我们在博客[《概率论沉思录：初等抽样论》](https://www.cnblogs.com/orion-orion/p/18519155)）中提到过的**二项分布（binomial distribution）**：

\\\[\\text{Bin}(m\\mid N, \\mu) = \\binom{N}{m}\\mu^m (1 - \\mu)^{N - m} \\\]

二项分布的均值和方差如下所示：

\\\[\\begin{aligned} \\mathbb{E}\[m\] &= \\sum\_{m=0}^Nm \\text{Bin}(m\\mid N, \\mu) \\\\ &= \\sum\_{m=0}^Nm\\frac{N\\mu}{m} \\text{Bin}(m - 1\\mid N - 1, \\mu) \\\\ &= N\\mu \\underbrace{\\sum\_{m=1}^N\\text{Bin}(m - 1\\mid N - 1, \\mu)}\_{1} \\\\ &= N\\mu \\end{aligned} \\\]

\\\[\\begin{aligned} \\mathrm{Var}\[m\] &= \\sum\_{m=0}^N(m - N\\mu)^2\\text{Bin}(m\\mid N, \\mu) \\\\ &= N\\mu \\sum\_{m=0}^N \\left(\\underbrace{m}\_{(m - 1) + 1} \\text{Bin}(m - 1\\mid N - 1, \\mu)\\right) - 2N^2\\mu^2 \\cdot 1 + N^2\\mu^2 \\cdot 1 \\\\ & = N\\mu \\left(\\mathbb{E}\[m - 1\] + 1\\right) - 2N^2\\mu^2 \\cdot 1 + N^2\\mu^2 \\cdot 1 \\\\ & = N\\mu\\left\[(N - 1)\\mu + 1\\right\] - N^2\\mu^2\\\\ & = N\\mu (1 - \\mu) \\end{aligned} \\\]

> **注** 这些结果也可以使用结论\\(\\mathbb{E}\[x\_1 + \\cdots + x\_N\] = \\mathbb{E}\[x\_1\] + \\cdots \\mathbb{E}\[x\_N\]\\)（对随机变量\\(x\_i\\)），与结论\\(\\mathrm{Var}\[x\_1 + \\cdots + x\_N\] = \\mathrm{Var}\[x\_1\] + \\cdots \\mathrm{Var}\[x\_N\]\\)（对相互独立的随机变量\\(x\_i\\)）得到。由于\\(m = x\_1 + \\cdots + x\_N\\)，并且对于每次观察有\\(\\mathbb{E}\[x\_i\] = \\mu\\)和\\(\\mathrm{Var}\[x\_i\] = \\mu (1 - \\mu)\\)，应用该结论得到\\(\\mathbb{E}\[m\] = N\\mu, \\mathrm{Var}\[m\] = N\\mu(1 - \\mu)\\)。

类似地，关于数据集\\(\\mathcal{D}\\)，对于二项分布而言我们相当于拥有了单个样本点\\(m\\)，我们也可以构造关于\\(\\mu\\)的似然函数如下：

\\\[p(\\mathcal{D}\\mid \\mu) = p(m\\mid \\mu) = \\binom{N}{m}\\mu^m(1 - \\mu)^{N - m} \\\]

再次采用频率派的做法，令\\(\\frac{\\mathrm{d}\\ln p(\\mathcal{D}\\mid \\mu)}{\\mathrm{d}\\mu} = 0\\)，我们就得到了二项分布参数\\(\\mu\\)的最大似然估计值

\\\[\\mu\_{\\text{ML}} = \\frac{m}{N} \\\]

可以看到，在二项分布中，参数\\(\\mu\\)的最大似然解和伯努利分布同样为\\(\\mu\_{\\text{ML}} = \\frac{m}{N}\\)。

1.3 Beta分布
----------

我们前面得到了在二项分布中参数\\(\\mu\\)的最大似然解\\(\\mu\_{\\text{ML}} = \\frac{m}{N}\\)。而我们之前提到过，我们使用的数据集对于想要拟合的二项分布而言相当于只有单个样本点，会给出严重的过拟合结果。为了用贝叶斯的观点看待这个问题，我们需要引入一个关于参数\\(\\mu\\)的先验分布\\(p(\\mu)\\)。我们注意到二项分布的似然函数是某个因子与\\(\\mu^m (1 - \\mu)^{N - m}\\)的乘积的形式。如果我们选择一个正比于\\(\\mu\\)和\\((1 - \\mu)\\)的幂指数的先验概率分布，那么后验概率分布（正比于先验和似然函数的乘积）就会有着与先验分布相同的函数形式（这个性质被叫做**共轭性（conjugacy）**）。这样，先验分布的校正将表现为其参数的校正。这样在处理上是非常方便的，它通常使得计算相当容易。因此，我们把先验分布选择为**Beta分布**\[2\]，定义为

\\\[\\text{Beta}(\\mu\\mid a, b) = \\frac{1}{\\Beta(a, b)}\\mu^{a - 1}(1 - \\mu)^{b - 1} \\\]

其中归一化因子\\(\\Beta(a, b)\\)经由我们在博客[《概率论沉思录：初等假设检验》](https://www.cnblogs.com/orion-orion/p/18621496)中提到过的Beta函数计算得到：

\\\[\\Beta(a, b) = \\int\_{0}^1\\mu^{a - 1}(1 - \\mu)^{b - 1}\\mathrm{d}\\mu = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a + b)} \\\]

其中

\\\[\\Gamma(x)\\equiv \\int\_{0}^{\\infty}t^{x - 1}e^{-t}\\mathrm{d}t,\\quad x > 0 \\\]

为Gamma函数。对\\(\\Gamma(x + 1) = \\int\_{0}^{\\infty}t^{x}e^{-t}\\mathrm{d}t\\)使用一次分部积分法\[3\]我们有\\(\\Gamma(x + 1) = \\left\[t^{x}(-e^{-t})\\right\]\_{0}^{\\infty} - \\int\_{0}^{\\infty}xt^{x - 1}\\left(-e^{-t}\\right)\\mathrm{d}t = x \\Gamma(x)\\)，又因为\\(\\Gamma(1)=\\int\_{0}^{\\infty}e^{-t}\\mathrm{d}t=\\left\[-e^{-t}\\right\]\_{0}^{\\infty}=1\\)，因此用归纳法可证明当\\(x\\)为正整数时\\(\\Gamma(x) = (x - 1)!\\)（参见博客[《概率论沉思录：初等假设检验》](https://www.cnblogs.com/orion-orion/p/18621496)）。

Beta分布的均值和方差如下所示：

\\\[\\begin{aligned} \\mathbb{E}\[\\mu\] &= \\int\_{0}^1\\mu \\text{Beta}(\\mu\\mid a, b)\\mathrm{d}\\mu \\\\ &= \\int\_{0}^1\\mu \\frac{1}{\\Beta(a, b)}\\mu^{a - 1}(1 - \\mu)^{b - 1}\\mathrm{d}\\mu \\\\ & = \\frac{\\Beta(a + 1, b)}{\\Beta(a, b)}\\\\ &= \\frac{a}{a + b} \\end{aligned} \\\]

\\\[\\begin{aligned} \\mathrm{Var}\[\\mu\] &= \\int\_{0}^{1}(\\mu - \\frac{a}{a + b})^2\\text{Beta}(\\mu\\mid a, b)\\mathrm{d}\\mu\\\\ &= \\int\_{0}^{1}(\\mu - \\frac{a}{a + b})^2\\frac{1}{\\Beta(a, b)}\\mu^{a - 1}(1 - \\mu)^{b - 1}\\mathrm{d}\\mu\\\\ &= \\frac{1}{\\Beta(a, b)}\\left\[\\Beta(a + 2, b) - \\frac{2a}{a + b}\\Beta(a + 1, b) + \\frac{a^2}{(a + b)^2}\\Beta(a, b)\\right\]\\\\ &= \\frac{ab}{(a + b)^2(a + b + 1)}\\\\ \\end{aligned} \\\]

参数\\(a\\)和\\(b\\)经常被称为**超参数（hyperparameter）**，因为它们控制了参数\\(\\mu\\)的概率分布。

根据Beta先验\\(\\text{Beta}(\\mu\\mid a, b) = \\frac{1}{\\Beta(a, b)}\\mu^{a - 1}(1 - \\mu)^{b - 1}\\)与二项分布的似然函数\\(p(m\\mid \\mu) = \\binom{N}{m}\\mu^m(1 - \\mu)^{N - m}\\)，我们可得到\\(\\mu\\)的后验分布为：

\\\[\\begin{aligned} p(\\mu\\mid m) &= \\frac{p(m\\mid \\mu)\\text{Beta}(\\mu\\mid a, b)}{\\int\_{0}^1p(m\\mid \\mu)\\text{Beta}(\\mu\\mid a, b)\\mathrm{d}\\mu}\\\\ &= \\frac{\\binom{N}{m}\\mu^m(1 - \\mu)^{N - m}\\frac{1}{\\Beta(a, b)}\\mu^{a - 1}(1 - \\mu)^{b - 1}}{\\int\_{0}^1 \\binom{N}{m}\\mu^m(1 - \\mu)^{N - m}\\frac{1}{\\Beta(a, b)}\\mu^{a - 1}(1 - \\mu)^{b - 1}\\mathrm{d}\\mu}\\\\ &= \\frac{\\binom{N}{m}\\frac{1}{\\Beta(a, b)}\\mu^{m + a - 1}(1 - \\mu)^{N - m + b -1}}{\\binom{N}{m}\\frac{\\Beta(m + a, N - m + b)}{\\Beta(a, b)}}\\\\ & = \\frac{1}{\\Beta(m + a, N - m + b)}\\mu^{m + a - 1}(1 - \\mu)^{N - m + b -1} \\end{aligned} \\\]

这是\\(\\text{Beta}(\\mu\\mid m + a, N - m + b)\\)分布。

我们看到，\\(\\mu\\)的后验分布是另一个Beta分布，这反映出先验关于似然函数的共轭性质。我们还看到，如果一个数据集里有\\(m\\)次观测为\\(x=1\\)，有\\(N - m\\)次观测为\\(x = 0\\)，那么从先验概率到后验概率，\\(a\\)的值变大了\\(m\\)，\\(b\\)的值变大了\\(N - m\\)。这让我们可以简单地把先验概率中的超参数\\(a\\)和\\(b\\)分别看成\\(x = 1\\)和\\(x = 0\\)的**有效观测数（effective number of observation）**。注意，\\(a\\)和\\(b\\)不一定是整数。

另外，如果我们接下来观测到更多的数据，那么后验概率分布可以扮演先验概率的角色。为了说明这一点，我们可以假想每次只取一个观测值，然后在每次观测之后更新当前的后验分布。在每个阶段，后验概率都可以视为一个\\(\\text{Beta}(\\mu\\mid a, b)\\)分布，参数\\(a\\)和\\(b\\)分别表示对于\\(x=1\\)和\\(x=0\\)的观测总数（先验的和实际的）。观测到一个\\(x = 1\\)对应于把\\(a\\)的值增加\\(1\\)，而观测到\\(x=0\\)会使\\(b\\)增加\\(1\\)。下图说明了这个过程中的一个步骤：

![](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250105032612_%E8%B4%9D%E5%8F%B6%E6%96%AF%E9%A1%BA%E5%BA%8F%E6%8E%A8%E6%96%AD%E4%B8%AD%E7%9A%84%E4%B8%80%E4%B8%AA%E6%AD%A5%E9%AA%A4%E7%9A%84%E4%BE%8B%E5%AD%90.png)

在该图中，先验分布为\\(\\text{Beta}(\\mu\\mid a=2, b=2) = 6\\mu(1 - \\mu)\\)，似然函数由公式\\(p(m=1\\mid \\mu) = \\mu\\)给出（其中\\(N=1\\)），对应于\\(x=1\\)的一次观测，从而后验概率分布为\\(\\text{Beta}(\\mu\\mid a=3, b=2) = 12\\mu^2(1 - \\mu)\\)。

我们看到，如果我们接受了贝叶斯观点，那么学习过程中的**顺序（sequential）** 方法可以自然而然地得出。它与先验和似然函数的选择无关，只取决于数据独立同分布的假设。顺序方法每次使用一个观测值，或者每次使用一小批观测值。然后再使用下一个观测值之前丢掉它们。

> **注** 顺序方法可以被用于实时学习的场景中。在实时学习的场景中，输入为一个持续稳定的数据流，模型必须在观测到所有数据之前就进行预测。由于顺序学习的方法不需要把所有的数据都存储到内存里，因此顺序方法对于大的数据集也很有用。最大似然方法也可以转化成顺序的框架。

对于\\(\\mu\\)的后验分布\\(\\text{Beta}(m + a, N - m + b)\\)，如果我们想在此基础上使用点估计（而不同于直接进行最大似然估计），一个很自然的做法就是采用这个后验分布的均值做为**贝叶斯估计量（Bayes estimator）**，如下式所示：

\\\[\\mu\_{\\text{B}} = \\mathbb{E}\[\\mu \\mid \\mathcal{D}\] = \\frac{m + a}{a + b + N} \\\]

> **注** 这里我们也可以采用**最大后验点估计（maximum a posteriori, MAP）**\[4\]，该方法选择后验概率最大的点（在这里\\(\\mu\\)是连续值的情况下，也即概率密度最大的点）：

\\\[\\mu\_{\\text{MAP}} = \\text{arg max}\_{\\mu} p(\\mu\\mid m) \\\]

> 令\\(\\frac{\\mathrm{d} p(\\mu\\mid m)}{\\mathrm{d}\\mu}=0\\)可以得到其最大值是在\\(\\mu\_{\\text{MAP}}=\\frac{m + a - 1}{a + b + N - 2}\\)处。

现在我们来考虑一下\\(\\mu\\)的贝叶斯估计量\\(\\mu\_{\\text{B}}\\)是怎样形成的。先验分布\\(\\text{Beta}(\\mu\\mid a, b)\\)有均值\\(\\frac{a}{a + b}\\)，它是我们在没有见到数据时对\\(\\mu\\)的最好估计。当不考虑先验信息，我们可能会使用极大似然估计量\\(\\mu\_{\\text{ML}}=\\frac{m}{N}\\)当作对于\\(\\mu\\)的估计，而\\(\\mu\\)的贝叶斯估计量\\(\\mu\_{\\text{B}}\\)则结合进了所有这些信息。如果把\\(\\mu\_{\\text{B}}\\)写成下式，可以清楚地看到这些信息被结合进去的方式：

\\\[\\mu\_{\\text{B}} = \\left(\\frac{a + b}{a + b + N}\\right)\\underbrace{\\left(\\frac{a}{a + b}\\right)}\_{\\mathbb{E}\[\\mu\]} + \\left(\\frac{N}{a + b + N}\\right)\\underbrace{\\left(\\frac{m}{N}\\right)}\_{\\mu\_{\\text{ML}}} \\\]

这样，\\(\\mu\_{\\text{B}}\\)就表示成先验均值\\(\\mathbb{E}\[\\mu\] = \\frac{a}{a + b}\\)和最大似然估计量\\(\\mu\_{\\text{ML}} = \\frac{m}{N}\\)（也即样本均值）的一个线性组合，其组合的权重由\\(a\\)，\\(b\\)和\\(N\\)决定。可以看到，在数据集无限大的极限情况下，\\(m, N \\rightarrow \\infty\\)，此时\\(\\mu\_{\\text{B}}\\)变成了最大似然的结果\\(\\mu\_{\\text{ML}}\\)。实际上，有个很普遍的情况是，贝叶斯的结果和最大似然的结果在数据集的规模趋于无穷的情况下会统一到一起。对于有限规模的数据集，\\(\\mu\_{\\text{B}}\\)总是位于先验均值\\(\\mathbb{E}\[\\mu\]\\)和最大似然估计量\\(\\mu\_{\\text{ML}}\\)之间。

从之前的图中我们可以看到，\\(\\mu\\)的后验分布的图像相比其先验分布更尖。进一步地，随着观测数量的不断增加，后验分布的参数\\(a\\)或\\(b\\)也会不断增大，后验分布的数量还会越来越尖。下图展示了当\\(a\\)和\\(b\\)变化时，Beta分布\\(\\text{Beta}(\\mu\\mid a, b)\\)关于\\(\\mu\\)的函数图像的变化。

![](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_14b33f78.png)

我们也能够通过Beta分布的方差公式\\(\\mathrm{Var}\[\\mu\] = \\frac{ab}{(a + b)^2(a + b + 1)}\\)看出这种趋势。根据该公式，如果\\(a\\rightarrow\\infty\\)或\\(b\\rightarrow\\infty\\)，那么方差就趋于0。我们可能想知道，下面这个性质是不是贝叶斯学习的一个共有属性：随着我们观测到越来越多的数据，后验概率表示的不确定性将会持续下降。

为了说明这一点，我们可以用频率学家的观点考虑贝叶斯学习问题。我们可以证明，平均来看，这种性质确实成立。考虑一个一般的贝叶斯推断问题，参数为\\(\\boldsymbol{\\theta}\\)，并且我们观测到了一个数据集\\(\\mathcal{D}\\)，由联合概率分布\\(p(\\boldsymbol{\\theta}, \\mathcal{D})\\)描述。我们有下列结果：

\\\[\\mathbb{E}\_{\\boldsymbol{\\theta}}\[\\boldsymbol{\\theta}\] = \\mathbb{E}\_{\\mathcal{D}}\\left\[\\mathbb{E}\_{\\boldsymbol{\\theta}}\\left\[\\boldsymbol{\\theta}\\mid \\mathcal{D}\\right\]\\right\] \\\]

其中

\\\[\\begin{aligned} \\mathbb{E}\_{\\boldsymbol{\\theta}}\[\\boldsymbol{\\theta}\]&\\equiv \\int p(\\boldsymbol{\\theta})\\boldsymbol{\\theta}\\mathrm{d}\\boldsymbol{\\theta}\\\\ \\mathbb{E}\_{\\mathcal{D}}\\left\[\\mathbb{E}\_{\\boldsymbol{\\theta}}\\left\[\\boldsymbol{\\theta}\\mid\\mathcal{D}\\right\]\\right\]&\\equiv \\int \\left\\{\\int \\boldsymbol{\\theta} p(\\boldsymbol{\\theta}\\mid \\mathcal{D})\\mathrm{d}\\boldsymbol{\\theta}\\right\\}p(\\mathcal{D})\\mathrm{d}\\mathcal{D} \\end{aligned} \\\]

该结果表明，\\(\\boldsymbol{\\theta}\\)的后验均值在产生数据集的整个分布上做平均后，等于\\(\\boldsymbol{\\theta}\\)的先验均值。类似地，我们可以证明

\\\[\\mathrm{Var}\_{\\boldsymbol{\\theta}}\[\\boldsymbol{\\theta}\] = \\mathbb{E}\_{\\mathcal{D}}\\left\[\\mathrm{Var}\_{\\boldsymbol{\\theta}}\\left\[\\boldsymbol{\\theta}\\mid \\mathcal{D}\\right\]\\right\] + \\mathrm{Var}\_{\\mathcal{D}}\\left\[\\mathbb{E}\_{\\boldsymbol{\\theta}}\[\\boldsymbol{\\theta}\\mid \\mathcal{D}\]\\right\] \\\]

上式左侧的项是\\(\\boldsymbol{\\theta}\\)的先验方差。在右侧，第一项是\\(\\boldsymbol{\\theta}\\)的平均后验方差，第二项是\\(\\boldsymbol{\\theta}\\)的后验均值的方差。由于这个方差非负，因此这个结果表明，平均来看，\\(\\boldsymbol{\\theta}\\)的后验方差小于或等于先验方差。后验均值的方差越大，方差减小得就越多。但是需要注意的是，这个结果只在平均情况下成立，对于一个特定的观测数据集，有可能后验方差大于先验方差。

2 多项分布的共轭先验：狄利克雷分布
==================

2.1 类别分布
--------

二元随机变量可以用来描述只能取两种可能值中的一种的这样的量。然而，我们经常会遇到可以取\\(K\\)个互斥状态中的某一种的离散变量。一种比较方便的表示方法是“\\(1\\)\-of-\\(K\\)”表示法。这种表示方法中，变量被表示成一个\\(K\\)维向量\\(\\boldsymbol{x}\\)，向量中的一个元素\\(x\_k\\)等于\\(1\\)，剩余元素等于\\(0\\)（注意，这样的向量满足\\(\\sum\_{k}x\_k = 1\\)）。例如，如果我们有一个能够取\\(K=6\\)种状态的变量，这个变量的某次特定的观测恰好对应于\\(x\_3=1\\)的状态，那么\\(\\boldsymbol{x}\\)就可以表示为\\(\\boldsymbol{x} = (0, 0, 1, 0, 0, 0)^T\\)。\\(\\boldsymbol{x}\\)的分布为**类别分布（categorical distribution）**：

\\\[p(\\boldsymbol{x}\\mid \\boldsymbol{\\mu}) = \\prod\_{k=1}^K\\mu\_{k}^{x\_k} \\\]

其中\\(\\boldsymbol{\\mu} = (\\mu\_1, \\cdots, \\mu\_K)^T\\)，参数\\(\\mu\_k\\)表示\\(x\_k=1\\)的概率（\\(0 \\leqslant \\mu\_k \\leqslant 1, \\sum\_{k}\\mu\_k = 1\\)）。上述概率分布可以被看做是伯努利分布对于多个输出的一个推广。这个分布是归一化的（\\(\\sum\_{\\boldsymbol{x}}p(\\boldsymbol{x}\\mid \\boldsymbol{u}) = \\sum\_{k}\\mu\_k = 1\\)），并且均值为

\\\[ \\mathbb{E}\[\\boldsymbol{x}\] = \\sum\_{\\boldsymbol{x}}p(\\boldsymbol{x}\\mid \\boldsymbol{\\mu})\\boldsymbol{x} = \\mu\_1\\left(\\begin{matrix} 1 \\\\ \\vdots \\\\ 0 \\\\ \\end{matrix}\\right) + \\cdots + \\mu\_K\\left(\\begin{matrix} 0 \\\\ \\vdots \\\\ 1 \\\\ \\end{matrix}\\right) = \\left(\\begin{matrix} \\mu\_1 \\\\ \\vdots \\\\ \\mu\_K \\\\ \\end{matrix}\\right) = \\boldsymbol{\\mu} \\\]

类似地，我们考虑一个有\\(x\\)的独立观测值的数据集\\(\\mathcal{D}=\\{x\_1, x\_2, \\cdots, x\_N\\}\\)。对应的似然函数形式为

\\\[p(\\mathcal{D}\\mid \\boldsymbol{\\mu}) = \\prod\_{i=1}^N\\prod\_{k=1}^K\\mu\_{k}^{x\_{ik}} = \\prod\_{k=1}^K\\mu\_k^{\\sum\_ix\_{ik}} = \\prod\_{k=1}^K\\mu\_k^{m\_k} \\\]

依频率派的做法，为了找到\\(\\boldsymbol{\\mu}\\)的最大似然解，我们可以关于\\(\\mu\_k\\)最大化\\(\\ln p(\\mathcal{D}\\mid \\boldsymbol{\\mu})\\)，并且要满足约束\\(\\sum\_{k}\\mu\_k=1\\)（这可以通过拉格朗日乘数\\(\\lambda\\)实现），此时即最大化：

\\\[\\sum\_{k=1}^Km\_k\\ln \\mu\_k + \\lambda(\\sum\_{k=1}^K\\mu\_k - 1) \\\]

令上式关于\\(\\mu\_k\\)的导数等于\\(0\\)，我们有\\(\\mu\_k = -\\frac{m\_k}{\\lambda}\\)。把该结果代入到约束\\(\\sum\_{k}\\mu\_k=1\\)中，解得\\(\\lambda = -N\\)。于是我们得到了最大似然解

\\\[\\mu\_k^{\\text{ML}} = \\frac{m\_k}{N} \\\]

它是在\\(N\\)次观测中，\\(x\_k=1\\)的观测所占的比例。

2.2 多项分布
--------

类比于二项分布之于伯努利分布，我们也可以考虑\\(m\_1, \\cdots, m\_K\\)在参数\\(\\boldsymbol{\\mu}\\)和观测总数\\(N\\)下的联合分布。该分布即是我们在博客[《概率论沉思录：初等抽样论》](https://www.cnblogs.com/orion-orion/p/18519155)）中提到过的**多项分布（multinomial distribution）**：

\\\[\\text{Mult}(m\_1,\\cdots, m\_K\\mid \\boldsymbol{\\mu}, N) = \\frac{N!}{m\_1!\\cdots m\_K!}\\prod\_{k=1}^K\\mu\_k^{m\_k} \\\]

其中\\(\\sum\_{k=1}^Km\_k=N\\)。

类似地，关于数据集\\(\\mathcal{D}\\)，对于多项分布而言我们相当于拥有了单个样本点\\((m\_1, m\_2, \\cdots, m\_K)\\)，我们也可以构造关于\\(\\boldsymbol{\\mu}\\)的似然函数如下：

\\\[p(\\mathcal{D}\\mid \\boldsymbol{\\mu}) = p(m\_1,\\cdots, m\_K\\mid \\boldsymbol{\\mu}) = \\frac{N!}{m\_1!\\cdots m\_K!}\\prod\_{k=1}^K\\mu\_k^{m\_k} \\\]

再次采用频率派的做法，为了找到\\(\\boldsymbol{\\mu}\\)的最大似然解，我们可以关于\\(\\mu\_k\\)最大化\\(\\ln p(\\mathcal{D}\\mid \\boldsymbol{\\mu})\\)，并且要满足约束\\(\\sum\_{k}\\mu\_k=1\\)（这也可以通过拉格朗日乘数\\(\\lambda\\)实现），这样我们就得到了多项分布参数\\(\\mu\\)的最大似然估计值

\\\[\\mu^{\\text{ML}}\_k = \\frac{m\_k}{N} \\\]

可以看到，在多项分布中，参数\\(\\mu\_k\\)的最大似然解和“\\(1\\)\-of-\\(K\\)”随机变量的分布同样为\\(\\mu^{\\text{ML}}\_k = \\frac{m\_k}{N}\\)。

2.3 狄利克雷分布
----------

现在我们介绍多项分布\\(\\text{Mult}(m\_1,\\cdots, m\_K\\mid \\boldsymbol{\\mu}, N)\\)的参数\\(\\boldsymbol{\\mu}\\)的一族先验分布。观察多项式分布的形式，我们将其对应的共轭先验选择为**狄利克雷分布（Dirichlet distribution）**\[5\]，定义为

\\\[\\text{Dir}(\\boldsymbol{\\mu}\\mid \\boldsymbol{\\alpha}) = \\frac{1}{\\Beta(\\boldsymbol{\\alpha})} \\prod\_{k=1}^K\\mu\_k^{\\alpha\_k - 1} \\\]

其中归一化因子\\(\\Beta(\\boldsymbol{\\alpha})\\)经由多项Beta函数\[5\]计算得到：

\\\[\\Beta(\\boldsymbol{\\alpha}) = \\int\_{\\Omega}\\prod\_{k=1}^K\\mu\_k^{\\alpha\_k - 1}\\mathrm{d}\\boldsymbol{\\mu} = \\frac{\\prod\_{k=1}^K\\Gamma(\\alpha\_k)}{\\Gamma(\\sum\_{k=1}^K\\alpha\_k)} \\\]

其中积分区域\\(\\Omega\\)也即\\(\\boldsymbol{\\mu}\\)的值域（\\(0\\leqslant\\mu\_k\\leqslant 1\\)且\\(\\sum\_{k}\\mu\_k = 1\\)），为\\(\\mathbb{R}^K\\)中的\\(K - 1\\)维的**单纯形（simplex）**。下图展示了\\(\\boldsymbol{\\mu} = (\\mu\_1, \\mu\_2, \\mu\_3)^T\\)在\\(\\mathbb{R}^3\\)中的分布情况，可以看到其被限制在了一个二维单纯形平面中：

![](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250107031737_mu%20%3D%20(mu_1,%20mu_2,%20mu_3)%5ET%E5%9C%A8R%5E3%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E6%83%85%E5%86%B5.png)

参数\\(\\boldsymbol{\\alpha} = (\\alpha\_1, \\cdots, \\alpha\_K)^T\\)。下图展示了在不同的参数\\(\\boldsymbol{\\alpha} = (\\alpha\_1, \\alpha\_2, \\alpha\_3)^T\\)的情况下，单纯形上的狄利克雷分布的图像：

![](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2015781/o_250107074848_%E4%B8%8D%E5%90%8C%E7%9A%84%E5%8F%82%E6%95%B0%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%8D%95%E7%BA%AF%E5%BD%A2%E4%B8%8A%E7%9A%84%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83%E7%9A%84%E5%9B%BE%E5%83%8F.png)

其中两个水平轴是单纯形平面上的坐标轴，垂直轴对应于概率密度的值。这里\\(\\boldsymbol{\\alpha}=(0.1, 0.1, 0.1)^T\\)对应于左图，\\(\\boldsymbol{\\alpha}=(1, 1, 1)^T\\)对应于中图，\\(\\boldsymbol{\\alpha}=(10, 10, 10)^T\\)对应于右图。

根据狄利克雷先验\\(\\text{Dir}(\\boldsymbol{\\mu}\\mid \\boldsymbol{\\alpha}) = \\frac{1}{\\Beta(\\boldsymbol{\\alpha})} \\prod\_k\\mu\_k^{\\alpha\_k - 1}\\)与多项分布的似然函数\\(p(\\mathcal{D}\\mid \\boldsymbol{\\mu}) = \\prod\_k\\mu\_k^{m\_k}\\)，我们可以可得到\\(\\boldsymbol{\\mu}\\)的后验分布为：

\\\[\\begin{aligned} p(\\boldsymbol{\\mu}\\mid \\mathcal{D}) &= \\frac{p(\\mathcal{D}\\mid \\boldsymbol{\\mu})\\text{Dir}(\\boldsymbol{\\mu}\\mid \\boldsymbol{\\alpha})}{\\int\_{\\Omega}p(\\mathcal{D}\\mid \\boldsymbol{\\mu})\\text{Dir}(\\boldsymbol{\\mu}\\mid \\boldsymbol{\\alpha})\\mathrm{d}\\boldsymbol{\\mu}}\\\\ &= \\frac{\\prod\_k^K\\mu\_k^{m\_k}\\frac{1}{\\Beta(\\boldsymbol{\\alpha})} \\prod\_k^K\\mu\_k^{\\alpha\_k - 1}}{\\int\_{\\Omega} \\prod\_k^K\\mu\_k^{m\_k}\\frac{1}{\\Beta(\\boldsymbol{\\alpha})} \\prod\_k^K\\mu\_k^{\\alpha\_k - 1}\\mathrm{d}\\boldsymbol{\\mu}}\\\\ &= \\frac{\\frac{1}{\\Beta(\\boldsymbol{\\alpha})}\\prod\_{k}^K\\mu^{m\_k + \\alpha\_k - 1}}{\\frac{\\Beta(\\boldsymbol{m} + \\boldsymbol{\\alpha})}{\\Beta(\\boldsymbol{\\alpha})}}\\\\ & = \\frac{1}{\\Beta(\\boldsymbol{m} + \\boldsymbol{\\alpha})}\\prod\_{k}^K\\mu^{m\_k + \\alpha\_k - 1} \\end{aligned} \\\]

这是\\(\\text{Dir}(\\boldsymbol{\\mu}\\mid \\boldsymbol{m} + \\boldsymbol{\\alpha})\\)分布，其中\\(\\boldsymbol{m} = (m\_1, \\cdots, m\_K)^T\\)。

类比于我们之前为二项分布选择先验分布\\(\\text{Beta}(\\mu\\mid a, b)\\)，将参数\\(a\\)和\\(b\\)分别看成\\(x = 1\\)和\\(x = 0\\)的有效观测数，这里我们为多项分布选择先验分布\\(\\text{Dir}(\\boldsymbol{\\mu}\\mid \\boldsymbol{\\alpha})\\)，也可以将参数\\(\\alpha\_k\\)看成\\(x\_k = 1\\)的有效观测数。

需要注意的是，具有两个状态的量既可以表示为二元变量然后使用二项分布建模，也可以表示为类别变量然后使用多项分布建模。

参考
==

*   \[1\] Bishop C M, Nasrabadi N M. Pattern recognition and machine learning\[M\]. New York: springer, 2006.
*   \[2\] Casella G, Berger R. Statistical inference\[M\]. CRC press, 2024.
*   \[3\] Rudin W. Principles of mathematical analysis\[M\]. New York: McGraw-hill, 1964.
*   \[4\] Bengio Y, Goodfellow I, Courville A. Deep learning\[M\]. Cambridge, MA, USA: MIT press, 2017.
*   \[5\] [《维基百科：狄利克雷分布》](https://zh.wikipedia.org/wiki/%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83)

数学是符号的艺术，音乐是上界的语言。