---
layout: post
title: '大模型评测之幻觉检测hallucination_evaluation_model'
date: "2025-03-13T00:38:28Z"
---
大模型评测之幻觉检测hallucination\_evaluation\_model
==========================================

### 大背景：

1.  2025开年deepseek铺天盖地的新闻
2.  参会代表已经表明，年度主线就是以AI为基础
3.  Manus于3月初横空出世
4.  国内各种模型竞赛的现状，只要是和科技沾边的公司不可能没有大模型，哪怕是里三层外三层套壳也得上
5.  东升西降，宏观使然，竞争中必然有科技竞争

### 小背景　

1.  本公司自研大模型rd，在模型排名中必有一席之地
2.  除了加大力度研发，还需各种评测
3.  正好，吾就是一名专业的模型评测员
4.  随着各种假信息的泛滥，模型越来越不清楚安全的边界、真实的边界，只是于铺天盖地的网页中查找然后总结，算不上一个优秀的大模型

### 幻觉检测

　　什么是幻觉检测？

　　官网中介绍到：“HHEM模型系列旨在检测 LLM 中的幻觉。它们在构建检索增强生成 (RAG) 应用程序的背景下特别有用，其中 LLM 总结了一组事实，并且 HHEM 可用于衡量该总结与事实在事实上的一致程度。”

　　说人话，就是检测一下大模型对事实的认知能力如何。

　　大模型地址：[https://huggingface.co/vectara/hallucination\_evaluation\_model](https://huggingface.co/vectara/hallucination_evaluation_model)

### 如何做幻觉检测

1.  仔细阅读中大模型地址的Model Card部分，这对你理解 “事实但是幻觉” 很重要。　![](https://img2024.cnblogs.com/blog/1937933/202503/1937933-20250312151056354-2049739459.png)　
2.  把项目克隆下来，待会儿要用到里面的模型进行计算
3.  下载官方数据集，https://huggingface.co/datasets/vectara/leaderboard\_results/tree/main ，数据集是一个很大的csv文件，里面是用于测试幻觉的各种问题，用于模型的输入
4.  使用提示语，提示语要和问题进行拼接
    
    ![](https://img2024.cnblogs.com/blog/1937933/202503/1937933-20250312152814742-520218034.png)
    
5.  编写一个脚本，从csv中读取问题，请求大模型，再将大模型的答案追加到后一列。参考如下： 
    
    def huan\_jue():
        df \= pd.read\_csv('leaderboard\_summaries.csv', encoding='utf-8')
        df \= df\[df\['model'\] == 'deepseek/deepseek-v3'\]
    
        data \= {"source": \[\], 'ori\_summary': \[\], "rendu\_summary": \[\]}
        for index, row in df.iterrows():
            source \= row\[0\]
            ori\_summary \= row\[1\]
            msg \= f'Provide a concise summary of the following passage, covering the core pieces of information described in english. {source}'
            con \= rendu(msg)
            try:
                con \= con\['choices'\]\[0\]\['message'\]\['content'\]
            except IndexError:
                con \= ''
            print(index, con\[:100\])
            data\["source"\].append(source)
            data\["ori\_summary"\].append(ori\_summary)
            data\["rendu\_summary"\].append(con)
    
        df2 \= pd.DataFrame(data)
        df2.to\_csv('output.csv', index=False)
    
6.  上面的过程可能很漫长，建议放到服务器后台进行，后台命令参考，如果不打算用服务器跑，这一步忽略。
    
    nohup python hallucination\_test.py > nohup.out 2>&1 &
    
7.  下载依赖的模型，参考代码如下（如果已配置梯子，这一步可以忽略）
    
    import os
    os.environ\['HF\_ENDPOINT'\] = 'https://hf-mirror.com'
    from huggingface\_hub import snapshot\_download
    
    snapshot\_download(
      repo\_id\="microsoft/OmniParser-v2.0",
      # repo\_type="dataset",  # 下载数据集时才需要
      local\_dir="../hallucination\_evaluation\_model/google/flan-t5-base",
      # proxies={"https": "http://localhost:7890"},
      # max\_workers=8,
      etag\_timeout=180
    )
    
8.  使用大模型地址的Model Card部分提到的计算方式进行计算，这也是为什么第一步让大家熟读Model Card。我用的是Pipline方式计算的。计算也很耗时，建议放在服务器进行。
    
    ![](https://img2024.cnblogs.com/blog/1937933/202503/1937933-20250312154150262-1033536977.png)
    
9.   分数转化为排行榜支持的形式，首先我们看一下排行榜 https://huggingface.co/spaces/vectara/leaderboard ，首列分数越低代表该大模型致幻程度越小，说明模型越好。那四列的意思分别为：
    
    转换分数脚本参考
    *   幻觉率：幻觉评分低于0.5的摘要百分比
        
    *   事实一致率：幻觉率的补充，以百分比表示。
    *   回答率：非空摘要的百分比。这要么是模型拒绝生成响应，要么是由于各种原因抛出错误。（例如，模型认为文档包含不恰当的内容）
    *   平均摘要长度：生成的摘要的平均字数
10.  import pandas as pd
    result \= {
        'Hallucination Rate': 0,
        'Factual Consistency Rate': 0,
        'Answer Rate': 0,
        'Average Summary Length': 0
    }
    with open('result.json', 'r') as f:
        con \= eval(f.read())
    hr \= fcr = ar = 0
    df \= pd.read\_csv('hallu\_rendu/rendu\_summary.csv')
    
    asl \= \[\]
    for i in df\['rendu\_summary'\].tolist():
        j \= i.split(' ')
        sm \= 0
        sm += len(j)
        asl.append(sm)
    
    for i in con:
        if i < 0.5:
            hr += 1
        if not i:
            ar += 1
    
    hr \= round(hr/len(con), 2)
    fcr \= 1-hr
    hr \= str(hr \* 100) + '%'
    fcr \= str(fcr \* 100) + '%'
    ar \= str((len(con) - ar) / len(con) \* 100) + '%'
    asl \= str(sum(asl)/len(asl))
    result\['Hallucination Rate'\] = hr
    result\['Factual Consistency Rate'\] = fcr
    result\['Answer Rate'\] = ar
    result\['Average Summary Length'\] = asl
    print(result)
    

### 聊一聊我们的模型评测结果

　　我们的rd模型在评测中各项评分如下：{'Hallucination Rate': '16.0%', 'Factual Consistency Rate': '84.0%', 'Answer Rate': '100.0%', 'Average Summary Length': '102.68190854870775'}

　　这个结果算不上好，但至少上榜了。

　　你们的呢，评论区聊一聊

作者：Teark

出处：[https://www.cnblogs.com/teark](https://www.cnblogs.com/teark)

本文版权归作者-博客园测神 独有，欢迎转载，但未经作者同意必须在文章页面给出原文链接，否则保留追究法律责任的权利。