---
layout: post
title: '使用Ollama和AnythingLLM搭建本地AI'
date: "2025-02-15T00:35:48Z"
---
使用Ollama和AnythingLLM搭建本地AI
==========================

搭建本地博客AI
--------

目录

*   [搭建本地博客AI](#搭建本地博客ai)
    *   [环境](#环境)
    *   [下载ollama](#下载ollama)
    *   [选择模型](#选择模型)
        *   [选择嵌入(embedder)模型](#选择嵌入embedder模型)
            *   [查看性能测试](#查看性能测试)
            *   [估算内存](#估算内存)
            *   [选择模型](#选择模型-1)
            *   [量化类型介绍](#量化类型介绍)
                *   [**Q5\_0 vs Q5\_K**](#q5_0-vs--q5_k)
                *   [Q5\_K 变体(Q5\_K\_S、Q5\_K\_M、Q5\_K\_L)](#q5_k-变体q5_k_sq5_k_mq5_k_l)
        *   [选择LLM模型](#选择llm模型)
    *   [下载模型](#下载模型)
        *   [下载LLM](#下载llm)
        *   [下载Embedder](#下载embedder)
    *   [下载AnythingLLM](#下载anythingllm)
        *   [配置向量数据库](#配置向量数据库)
    *   [测试使用](#测试使用)
    *   [Tips](#tips)
        *   [Reset向量数据库](#reset向量数据库)
        *   [LLM没有使用自己的文档](#llm没有使用自己的文档)
        *   [AI回答逻辑混乱](#ai回答逻辑混乱)
        *   [Ollama](#ollama)
            *   [日志中出现告警或错误](#日志中出现告警或错误)
            *   [命令行](#命令行)
            *   [Debug](#debug)
            *   [Ollama FAQ](#ollama-faq)
    *   [参考](#参考)
    *   [TODO](#todo)

### 环境

*   **Env**：MacBook Pro M2
*   **Total memory**：16GB

### 下载ollama

*   [下载](https://ollama.com/)并按照提示安装启动ollama。
*   在浏览器中访问`http://localhost:11434`，若返回"Ollama is running"，则表示启动成功。

### 选择模型

#### 选择嵌入(embedder)模型

##### 查看性能测试

[mteb](https://huggingface.co/spaces/mteb/leaderboard)展示了文本嵌入模型的性能测试结果。由于博客以中英文为主，因此在过滤语言时应该包含`cmn`、`zho`和`eng`(语言代码[使用](https://github.com/embeddings-benchmark/mteb/blob/main/docs/mmteb/readme.md#contribution-point-guideline)的是[ISO 639-3](https://en.wikipedia.org/wiki/ISO_639-3)标准)

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213150343270-1402600397.png)

运行结果如下：

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213150742640-107085415.png)

##### 估算内存

在选择模型(不仅限嵌入模型)时，需要考虑模型占用的性能，通常参数越多，占用的内存也就越大。模型占用的内存[估算公式](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm)如下：

\\\[M=\\frac{(P \* 4 B)}{(32 / Q)} \* 1.2 \\\]

Symbol

Description

M

用千兆字节 (GB) 表示的 GPU 内存

P

模型中的参数数量。例如，一个 7B 模型有 7 亿参数。

4B

4 字节，表示每个参数使用的字节数

32

4 字节中有 32 位

Q

加载模型时应使用的比特位数，例如 16 位、8 位或 4 位。

1.2

表示在 GPU 内存中加载额外内容的 20% 开销。

以上图中的第2名`gte-Qwen2-1.5B-instruct`为例，其参数数量(P)为1.78B，即17.8亿个参数，其模型应用的比特位数(Q)为F32，即32位，那么它占用的内存约为_M = (1.78 ∗ 4) / (32 / 32) ∗ 1.2 ≈ 5.93GB_

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213185124710-744857088.png)

由于**除嵌入模型外，我们还需要运行大模型(LLM)**，因此在总计16GB的内存上运行近6GB的嵌入模型是有些吃力的。那么是否有其他降低模型内存的方式呢？

答案是通过**量化**(Quantization)。

量化是一种降低内存占用的方式，通过将模型参数的精度从浮点数降低到更低的表达方式(如 8 位整数)，显著降低了内存和计算需求，使模型能够更高效地部署在资源有限的设备。但降低精度可能会影响输出的准确性。通常来说，**8bit的量化可以达到16bit的性能**，但4bit的量化可能会显著影响模型性能。

如果想用**2GB左右**的内存运行该模型，那么量化应该设置为多少？

计算_(1.78 ∗ 4) / (32 / Q) ∗ 1.2=2_，求解`Q`约为7.5。

这里提供了一个模型内存计算[工具](https://llm-calc.rayfernando.ai/?quant=8-bit&os=8&context=2000)，我们设置总内存(`Custom Ran(GB)`)为10GB，系统预留内存(`OS Overhead(GB)`)为8GB，这样留给嵌入模型的就只有2GB内存。量化级别(`Quantization Level`)为5-bit，上下文窗口(`Context Window(Tokens)`)为2048，则该配置下，可以支持1.6B的模型，与预期大致相符：

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213190542710-106143247.png)

##### 选择模型

从上面截图中可以看到`Alibaba-NLP/gte-Qwen2-7B-instruct`模型有28个量化版本，点击进入这28个量化版本的浏览页面

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213162249465-1939654671.png)

点击进入第一个模型`tensorblock/gte-Qwen2-7B-instruct-GGUF`(**下载量最多**)，在页面右侧选择`Use this model`\->`Ollama`:

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213163807138-489549065.png)

可以看到它有很多量化版本，那么该选择哪一个？

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250213163846744-2017689464.png)

可以[参考](https://github.com/ggerganov/llama.cpp/discussions/2094#discussioncomment-6351796)下面描述，**推荐`Q4_K_M`和`Q5_K_S`，`Q5_K_M`**，鉴于我们的量化级别不能大于7，因此可以**采用推荐的`Q5_K_M`模型**。

    Allowed quantization types:
       2  or  Q4_0   :  3.50G, +0.2499 ppl @ 7B - small, very high quality loss - legacy, prefer using Q3_K_M
       3  or  Q4_1   :  3.90G, +0.1846 ppl @ 7B - small, substantial quality loss - legacy, prefer using Q3_K_L
       8  or  Q5_0   :  4.30G, +0.0796 ppl @ 7B - medium, balanced quality - legacy, prefer using Q4_K_M
       9  or  Q5_1   :  4.70G, +0.0415 ppl @ 7B - medium, low quality loss - legacy, prefer using Q5_K_M
      10  or  Q2_K   :  2.67G, +0.8698 ppl @ 7B - smallest, extreme quality loss - not recommended
      12  or  Q3_K   : alias for Q3_K_M
      11  or  Q3_K_S :  2.75G, +0.5505 ppl @ 7B - very small, very high quality loss
      12  or  Q3_K_M :  3.06G, +0.2437 ppl @ 7B - very small, very high quality loss
      13  or  Q3_K_L :  3.35G, +0.1803 ppl @ 7B - small, substantial quality loss
      15  or  Q4_K   : alias for Q4_K_M
      14  or  Q4_K_S :  3.56G, +0.1149 ppl @ 7B - small, significant quality loss
      15  or  Q4_K_M :  3.80G, +0.0535 ppl @ 7B - medium, balanced quality - *recommended*
      17  or  Q5_K   : alias for Q5_K_M
      16  or  Q5_K_S :  4.33G, +0.0353 ppl @ 7B - large, low quality loss - *recommended*
      17  or  Q5_K_M :  4.45G, +0.0142 ppl @ 7B - large, very low quality loss - *recommended*
      18  or  Q6_K   :  5.15G, +0.0044 ppl @ 7B - very large, extremely low quality loss
       7  or  Q8_0   :  6.70G, +0.0004 ppl @ 7B - very large, extremely low quality loss - not recommended
       1  or  F16    : 13.00G              @ 7B - extremely large, virtually no quality loss - not recommended
       0  or  F32    : 26.00G              @ 7B - absolutely huge, lossless - not recommended
    

##### 量化类型介绍

从上面可以看出不同的后缀（`0`、`K`、`K_S`、`K_M`、`K_L`）代表 **不同的量化技术和优化策略**。以下内容来自GPT：

###### **Q5\_0 vs Q5\_K**

**Q5\_0**

*   Q5\_0 是最基础的 5-bit 量化方案，它使用 均匀量化（Uniform Quantization），但**不包含任何额外的优化**。
*   每个 block（通常 16 或 32 个权重）共享相同的缩放因子（scale）。
*   误差较大，在某些情况下可能导致 模型精度下降。

**适用场景**

*   适用于对**精度要求不高**的任务。
*   当设备计算**资源有限但仍需要较好的推理速度**时。

* * *

**Q5\_K**

*   Q5\_K（K-Block Quantization）是一种更先进的 5-bit 量化方案，使用 块级（block-wise）非均匀量化（Non-Uniform Quantization） 以 降低量化误差。
*   相比 Q5\_0，Q5\_K 在同样的 5-bit 量化下能提供更好的数值精度，因此模型推理质量更高。
*   Q5\_K 进一步引入了优化策略，如动态缩放（dynamic scaling）或非均匀量化方法，**使得量化误差小于 Q5\_0**。

**适用场景**

*   适用于 **对精度有较高要求** 的 LLM 任务，如 **聊天机器人、代码生成、翻译** 等。
*   适用于 **存储受限** 但仍希望保持较好精度的设备，如 **GPU、CPU、移动端**。

###### Q5\_K 变体(Q5\_K\_S、Q5\_K\_M、Q5\_K\_L)

方案

说明

计算复杂度

精度

**Q5\_K\_S**

**"Small" 版本**，更快但精度稍低

✅ 最低

❌ 较低

**Q5\_K\_M**

**"Medium" 版本**，折中方案

🔄 适中

🔄 适中

**Q5\_K\_L**

**"Large" 版本**，计算稍慢但精度高

❌ 较高

✅ 最佳

**适用场景**

*   **Q5\_K\_S**：适用于 **推理速度优先** 的场景，例如 **实时聊天机器人** 或 **低端设备**。
*   **Q5\_K\_M**：适用于 **平衡精度和推理速度** 的场景，是 **最常用的 Q5\_K 版本**。
*   **Q5\_K\_L**：适用于 **对精度要求极高的 LLM 任务**，如 **科学计算、代码理解** 等。

#### 选择LLM模型

与嵌入模型类似，LLM模型也有自己的性能测试板块：[open-llm-leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/)，但该排行榜并未提供语言过滤功能，因此无法直接选择某个模型，还需要判断该模型是否支持中英文。

可以参考[Open Chinese LLM Leaderboard](https://huggingface.co/spaces/BAAI/open_cn_llm_leaderboard)。

也可以参考[chinese-llm-benchmark](https://github.com/jeinlee1991/chinese-llm-benchmark?tab=readme-ov-file#-%E6%8E%92%E8%A1%8C%E6%A6%9C)，给出了中文大模型能力评测榜单。例如，我们想要找5B以下的小模型，可以参考该[榜单](https://github.com/jeinlee1991/chinese-llm-benchmark/blob/main/leaderboard/opensource1.md)，前3名为：

1.  [qwen2.5-3b-instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)
2.  [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
3.  [qwen2.5-1.5b-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)

⚠️在选择模型时，需要在huggingface上再次确认支持的语言，如上面的`Llama-3.2-3B-Instruct`模型，[官方](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)仅支持8种语言_English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported_，并没有中文!

这里我们选择的模型为`qwen2.5-3b-instruct`，根据公式，该模型大概占用的内存为2.55GB。

### 下载模型

ℹ️也可以直接在Ollama的[模型库](https://ollama.com/search?c=embedding&o=newest)中直接查找下载。

#### 下载LLM

    ollama run hf.co/Qwen/Qwen2.5-3B-Instruct-GGUF:Q5_K_M
    

#### 下载Embedder

    ollama pull hf.co/second-state/gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M
    

### 下载AnythingLLM

*   [下载](https://anythingllm.com/)并安装anythingLLM
    
*   连接Ollama：分别在anythingLLM的`LLM`和`Embedder`种选择Ollama，并将连接地址设置为正确的Ollama服务地址，Ollama的默认监听地址为`127.0.0.1:11434`(_设置为`http://localhost:11434`好像有问题_)。连接好后就可以自动加载模型：  
    ![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214105537120-1149611445.png)
    
    ⚠️ 使用Ollama时，anythingLLM无法区分模型是LLM还是embedder，因此都会进行加载，即在`LLM`中出现嵌入模型，而在`Embedder`中出现LLM，需要手动选择正确的模型，将`LLM`设置为`hf.co/Qwen/Qwen2.5-3B-Instruct-GGUF:Q5_K_M`，将`Embedder`设置为`hf.co/second-state/gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M`。
    
    官方有如下[描述](https://docs.useanything.com/setup/embedder-configuration/local/ollama)：
    
    > **Heads up!**
    > 
    > Ollama's `/models` endpoint will show both LLMs and Embedding models in the dropdown selection. **Please** ensure you are using an embedding model for embedding.
    > 
    > **llama2** for example, is an LLM. Not an embedder.
    

#### 配置[向量数据库](https://docs.anythingllm.com/features/vector-databases)

这里就直接采用anythingLLM默认的本地数据库即可：

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214111022713-909031879.png)

### 测试使用

简单测试一下模型是否生效：

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214125122571-2092548152.png)

现在嵌入一个文档，看是否可以根据嵌入的文档进行回答。将`Chat mode`设置为Query，这样模型只会根据嵌入的文档进行回答：

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214125314626-1846202931.png)

上传一个文档，文档里面包含一个自定义的成语："_空让弄饭是一个成语，意思是有空一起做饭，形容一个人心情好_"：

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214125410578-154964474.png)

测试结果如下：  
![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214141810021-1835797026.png)

如果用`Chat`模式，其结果如下：

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214141842889-216446065.png)

### Tips

#### Reset向量数据库

![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214115413636-587943269.png)

#### LLM没有使用自己的文档

官方给出了一些[解决方式](https://docs.anythingllm.com/llm-not-using-my-docs)

*   Vector Database Settings > Search Preference中尝试使用`Accuracy Optimized`,
    
    ![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214144551626-293976461.png)
*   将Document similarity threshold设置为`No Restriction`。该属性用于过滤掉可能与查询无关的低分向量数据，默认为20%：  
    ![image](https://img2024.cnblogs.com/blog/1334952/202502/1334952-20250214144624433-1740101294.png)
    

#### AI回答逻辑混乱

可以适当将低模型的[temperature值](https://docs.useanything.com/llm-not-using-my-docs#chat-settings--llm-temperature)：`Chat Settings > LLM Temperature`

#### Ollama

##### 日志中出现告警或错误

**level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2097 keep=0 new=2048**

原因是Ollama默认使用2048的context window，[解决方式](https://github.com/ollama/ollama/issues/8099#issuecomment-2543316682)是增加模型的`num_ctx`值，但这种方式比较耗时。

另一种[方式](https://blog.driftingruby.com/ollama-context-window/)是使用`Modefile`，下面修改嵌入模型`hf.co/second-state/gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M`的`num_ctx`为8192:

    cat Modelfile
    # Modelfile
    FROM hf.co/second-state/gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M
    PARAMETER num_ctx 8196
    

执行`ollama create -f Modelfile gte-Qwen2-1.5B-instruct-GGUF:Q5_K_M`将创建一个新的嵌入模型，在AnythingLLM中加载该模型即可。  
⚠️重新加载模型之前需要删除嵌入的文档，并reset 向量数据库。

##### 命令行

Ollama的命令行有点像docker，常用的命令如下：

*   ollama server：启动一个ollama服务
    
*   ollama run：启动一个模型
    
*   ollama stop：停止一个模型
    
*   ollama ps：查看运行的模型
    
*   ollama pull：下载一个模型
    
*   ollama push：上传一个模型
    
*   ollama rm：删除一个模型
    
*   ollama list：查看下载的模型
    
*   ollama show：查看一个模型的信息
    

##### [Debug](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md)

查看Ollama日志：`cat ~/.ollama/logs/server.log`

##### [Ollama FAQ](https://github.com/ollama/ollama/blob/main/docs/faq.md)

### 参考

*   [Calculating GPU memory for serving LLMs](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm)
*   [全民AI时代：手把手教你用Ollama & AnythingLLM搭建AI知识库，无需编程，跟着做就行！](https://www.53ai.com/news/qianyanjishu/1427.html)
*   ollama支持的[模型库](https://github.com/ollama/ollama?tab=readme-ov-file#model-library)

### TODO

尝试一下其他大模型

本文来自博客园，作者：[charlieroro](https://www.cnblogs.com/charlieroro/)，转载请注明原文链接：[https://www.cnblogs.com/charlieroro/p/18709638](https://www.cnblogs.com/charlieroro/p/18709638)