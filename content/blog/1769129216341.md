---
layout: post
title: 'AI开发-python-langchain框架（1-4动态少样本提示）'
date: "2026-01-23T00:46:56Z"
---
AI开发-python-langchain框架（1-4动态少样本提示）
===================================

这个代码的核心功能是：**基于输入词的长度动态选择反义词示例，并调用大模型生成反义词**，体现了 **“动态少样本提示（Dynamic Few-Shot Prompting）”** 与 **“上下文长度感知的示例选择”** 的能力。

from langchain.prompts import FewShotPromptTemplate, PromptTemplate
from langchain.prompts.example\_selector import LengthBasedExampleSelector
from langchain\_core.output\_parsers import StrOutputParser
from langchain\_openai import ChatOpenAI
import os
from dotenv import load\_dotenv


# 定义反义词任务的示例数据集（few-shot examples）
# 每个示例包含一个输入词（input）和对应的反义词（output）
examples = \[
    {"input": "开心", "output": "伤心"},
    {"input": "高", "output": "矮"},
    {"input": "精力充沛", "output": "没精打采"},
    {"input": "粗", "output": "细"},
\]

# 定义单个示例的格式模板
# 使用 {input} 和 {output} 作为占位符，用于后续填充具体值
example\_prompt = PromptTemplate(
    input\_variables=\["input", "output"\],  # 声明模板中使用的变量名
    template="Input: {input}\\nOutput: {output}",  # 示例的文本格式
)

# 创建一个基于长度的示例选择器（LengthBasedExampleSelector）
# 作用：根据输入提示的总长度动态选择最合适的示例数量，避免超出模型上下文限制
example\_selector = LengthBasedExampleSelector(
    examples=examples,  # 提供所有候选示例
    example\_prompt=example\_prompt,  # 用于格式化每个示例的模板
    max\_length=25,  # 设定整个 prompt（含前缀、示例、后缀）的最大 token 长度（此处为字符数近似）
    # 注意：LengthBasedExampleSelector 默认使用 len(text) 计算长度（非精确 token 数），适用于简单场景
)

# 构建动态少样本提示模板（FewShotPromptTemplate）
# 它会根据输入内容的长度，自动从 examples 中选择合适数量的示例插入到 prompt 中
dynamic\_prompt = FewShotPromptTemplate(
    example\_selector=example\_selector,  # 使用上面定义的动态选择器（而非固定示例列表）
    example\_prompt=example\_prompt,      # 单个示例的格式
    prefix="给出每个输入的反义词",       # 提示的开头部分（任务指令）
    suffix="Input: {adjective}\\nOutput:",  # 提示的结尾部分，包含待预测的输入占位符
    input\_variables=\["adjective"\],      # 声明最终用户输入的变量名（与 suffix 中的 {adjective} 对应）
)

# === 测试 1：输入较短，应选择多个示例 ===
print("【测试1】输入较短，选择多个示例：")
print(dynamic\_prompt.format(adjective="big"))

print('------------')

# === 测试 2：输入很长，应只选择少量或一个示例以控制总长度 ===
long\_string = "big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"
print("【测试2】输入很长，仅选择一个示例：")
print(dynamic\_prompt.format(adjective=long\_string))

print('------------')

# === 测试 3：动态添加新示例 ===
# 向示例选择器中新增一个示例（"胖" -> "瘦"）
new\_example = {"input": "胖", "output": "瘦"}
dynamic\_prompt.example\_selector.add\_example(new\_example)
print("【测试3】添加新示例后，查询'热情'：")
print(dynamic\_prompt.format(adjective="热情"))

print('------------')

# === 配置并调用 DeepSeek 大语言模型 ===

llm = ChatOpenAI(
    api\_key=os.getenv("DEEPSEEK\_API\_KEY"),
    base\_url=os.getenv("DEEP\_URL"),  # Deepseek 的 API 基础地址
    model="deepseek-v3:671b",  # Deepseek 对话模型（可选：deepseek-chat-pro 等高级模型）
    temperature=0.7,  # 温度参数（0-1，越低越稳定）
    max\_tokens=1024  # 最大生成 tokens
)

# 创建字符串输出解析器，用于将模型返回的 AIMessage 转换为纯文本
output\_parser = StrOutputParser()

# 构建处理链（Chain）：prompt → LLM → output parser
# 使用 LangChain 的管道操作符 \`|\` 连接各组件
chain = dynamic\_prompt | llm | output\_parser

# 调用链，传入输入变量 {"adjective": "热情"}
# 注意：chain.invoke() 内部已包含 llm 调用和 output\_parser 解析，无需再手动调用 output\_parser
message = chain.invoke({"adjective": "热情"})

# ⚠️ 注意：上一行 \`chain.invoke()\` 已经返回了字符串（因为最后是 StrOutputParser）
# 所以下面这行是多余的，甚至会导致错误（因为 message 已是 str，不能再次 invoke）
# result = output\_parser.invoke(message)  # ❌ 错误：message 是 str，不是 AIMessage

# 正确做法：直接使用 message 作为结果
result = message

print('###############')
print("【模型输出】")
print(result)

输出结果：
=====

【测试1】输入较短，选择多个示例：
给出每个输入的反义词

Input: 开心
Output: 伤心

Input: 高
Output: 矮

Input: 精力充沛
Output: 没精打采

Input: 粗
Output: 细

Input: big
Output:
------------
【测试2】输入很长，仅选择一个示例：
给出每个输入的反义词

Input: 开心
Output: 伤心

Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
Output:
------------
【测试3】添加新示例后，查询'热情'：
给出每个输入的反义词

Input: 开心
Output: 伤心

Input: 高
Output: 矮

Input: 精力充沛
Output: 没精打采

Input: 粗
Output: 细

Input: 胖
Output: 瘦

Input: 热情
Output:
------------
###############
【模型输出】
冷淡

核心要点总结
======

这段代码是基于 LangChain 框架对接 DeepSeek 大模型，实现「动态少样本（Few-Shot）反义词生成」的完整案例，核心解决「固定示例易超出模型上下文长度」的问题，通过动态示例选择器适配不同长度输入，同时结合 LangChain 链式调用简化模型调用流程，先明确整体定位，再拆解核心重点：

一、整体流程概览（核心逻辑链）
---------------

代码遵循 LangChain 「示例准备 → 动态提示构建 → 模型调用 → 结果解析」的少样本学习经典流程，整体可概括为：

    1. 定义反义词任务的固定示例数据集，为模型提供参考案例
    2. 配置基于长度的示例选择器，根据输入文本长度动态筛选示例数量
    3. 构建动态少样本提示模板，自动适配输入长度生成合规 Prompt
    4. 初始化 DeepSeek 模型客户端，配置核心调用参数
    5. 构建「提示模板 → 大模型 → 输出解析」的链式调用流程
    6. 调用链条完成反义词生成，并输出纯文本结果
    

二、核心重点拆解（必掌握）
-------------

### 1\. 少样本提示（Few-Shot Prompt）核心组件

这是实现「模型参考示例生成结果」的基础，也是 LangChain 提示工程的核心用法：

   `- 示例数据集（examples）：以键值对形式存储「输入-输出」示例，为模型提供任务参考（如"开心"→"伤心"）；`

`- 单示例模板（example_prompt）：定义单个示例的文本格式（Input/Output 固定样式），统一示例展示形式；`

    - 动态少样本模板（FewShotPromptTemplate）：整合示例选择器、单示例模板、前缀/后缀，生成最终发给模型的完整 Prompt；
      - prefix：任务指令（"给出每个输入的反义词"），明确模型要执行的任务；
      - suffix：待填充的用户输入占位符，承接动态输入内容。
    

### 2\. 动态示例选择器（LengthBasedExampleSelector）

这是代码的核心亮点，解决「固定示例数量易超上下文长度」的问题：

    核心作用：根据输入文本的长度，自动计算并选择合适数量的示例（输入越长，选的示例越少），避免 Prompt 总长度超出模型上下文限制；
    关键参数：
      - examples：候选示例列表；
      - example_prompt：示例格式化模板（用于计算单示例长度）；
      - max_length：Prompt 允许的最大长度（此处为字符数近似值）。
    

### 3\. LangChain 链式调用（| 操作符）

简化多组件协作流程，是 LangChain 核心设计理念：

    - 链条构成：dynamic_prompt（生成 Prompt） | llm（调用模型） | output_parser（解析结果）；
    - 核心优势：无需手动分步调用（先格式化 Prompt、再调用模型、最后解析结果），一行代码完成全流程；
    - 调用方式：chain.invoke({"adjective": "热情"}) 传入输入变量，直接返回解析后的纯文本结果。
    

### 4\. 输出解析器（StrOutputParser）

解决「模型返回 AIMessage 对象→提取纯文本」的问题：

    核心作用：将 LangChain 模型返回的 AIMessage 类型（含 content/metadata 等字段）转换为纯字符串，简化结果使用；
    关键注意点：链式调用中已包含解析步骤，无需手动再次调用 output_parser.invoke()（否则会报错）。