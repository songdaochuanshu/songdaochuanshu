---
layout: post
title: '使用HuggingFace 模型并预测'
date: "2025-05-18T00:44:58Z"
---
使用HuggingFace 模型并预测
===================

### 下载HuggingFace 模型

首先打开网址：[https://huggingface.co/models](https://huggingface.co/models) 这个网址是huggingface/transformers支持的所有模型，目前大约一千多个。搜索gpt2（其他的模型类似，比如bert-base-uncased等），并点击进去。

进入之后，可以看到gpt2模型的说明页，点击页面中的list all files in model，可以看到模型的所有文件。  
![image](https://img2024.cnblogs.com/blog/1059417/202505/1059417-20250517165746968-534818548.png)

通常需要把保存的是三个文件以及一些额外的文件

*   配置文件 -- **config.json**
*   词典文件 -- **vocab.json**
*   预训练模型文件  
    pytorch -- **pytorch\_model.bin文件**  
    tensorflow 2 -- **tf\_model.h5文件**

额外的文件，指的是**merges.txt**、**special\_tokens\_map.json**、**added\_tokens.json**、**tokenizer\_config.json**、**sentencepiece.bpe.model**等，这几类是tokenizer需要使用的文件，如果出现的话，也需要保存下来。没有的话，就不必在意。如果不确定哪些需要下，哪些不需要的话，可以把图1中类似的文件全部下载下来。

![image](https://img2024.cnblogs.com/blog/1059417/202505/1059417-20250517170411822-1664754348.png)

看下这几个文件都是什么：

*   config.json配置文件  
    ![image](https://img2024.cnblogs.com/blog/1059417/202505/1059417-20250517171715104-1683373464.png)  
    包含了模型的类型、激活函数等配置信息
    
*   vocab.json 词典文件  
    ![image](https://img2024.cnblogs.com/blog/1059417/202505/1059417-20250517171859082-1804639621.png)
    
*   merges.txt  
    ![image](https://img2024.cnblogs.com/blog/1059417/202505/1059417-20250517171959937-1174014704.png)
    

### 使用HuggingFace模型

将上述下载的模型存储在本地：  
![image](https://img2024.cnblogs.com/blog/1059417/202505/1059417-20250517170555960-1161805815.png)

#### 加载本地HuggingFace模型

1.  导入依赖

    import torch
    from transformers import GPT2Tokenizer, GPT2LMHeadModel
    

导入PyTorch框架和HuggingFace Transformers库的GPT-2组件

2.  初始化分词器

    tokenizer = GPT2Tokenizer.from_pretrained("../../Models/gpt2/")
    text = "Who was Jim Henson ? Jim Henson was a"
    indexed_tokens = tokenizer.encode(text)
    print(indexed_tokens) # [8241, 373, 5395, 367, 19069, 5633]
    # 转换为torch Tensor
    token_tensor = torch.tensor([indexed_tokens])
    print(token_tensor) # tensor([[ 8241,   373,  5395,   367, 19069,  5633]])
    

`tokenizer.encode(text)`执行流程如下：  
**分词器处理：**  
首先将文本分词为子词(subwords),如：  
"Who was Jim Henson ?" → \['Who', 'Ġwas', 'ĠJim', 'ĠHen', 'son', '?'\]  
**ID转换：**  
然后将每个子词转换为对应的整数ID(来自vcab.json),如：  
\['Who', 'Ġwas', 'ĠJim', 'ĠHen', 'son', '?'\] -> \[8241, 373, 5395, 367, 19069, 5633\]  
可以查看vcab.json文件：  
![image](https://img2024.cnblogs.com/blog/1059417/202505/1059417-20250517212835056-1310277700.png)

返回的是 token ID 列表（整数列表），而非词向量

3.  加载预训练模型并预测

    # 加载预训练模型
    model = GPT2LMHeadModel.from_pretrained("../../Models/gpt2/")
    # print(model)
    
    model.eval()
    
    with torch.no_grad():
        outputs = model(token_tensor)
        predictions = outputs[0]
    
    # 我们需要预测下一个单词，所以是使用predictions第一个batch，最后一个词的logits去计算
    # predicted_index = 582，通过计算最大得分的索引得到的
    predicted_index = torch.argmax(predictions[0, -1, :]).item()
    # 反向解码为我们需要的文本
    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])
    # 解码后的文本：'Who was Jim Henson? Jim Henson was a man'
    # 成功预测出单词 'man'
    print(predicted_text)
    

输出结果：  
![image](https://img2024.cnblogs.com/blog/1059417/202505/1059417-20250517214109343-678088661.png)