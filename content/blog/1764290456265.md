---
layout: post
title: '一文入门 LangGraph 开发'
date: "2025-11-28T00:40:56Z"
---
一文入门 LangGraph 开发
=================

**注意: 本博客/笔记并不适合新手, 适合有一定的开发经验, 快速上手开发的老油条.**

2024-05-22

LangGraph
=========

[\[N\_LangChain\]](https://www.cnblogs.com/dddy/p/19274902)  
\[\[N\_LangServe\]\]

[git langgraph 项目页](https://github.com/langchain-ai/langgraph)  
[官页 - overview](https://langchain-ai.github.io/langgraph/)  
[官页 - 教程](https://langchain-ai.github.io/langgraph/tutorials/)

[LangGraph](https://langchain-ai.github.io/langgraph/) is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.

> LangGraph 是一个专为使用大型语言模型（LLMs）来构建具有状态管理能力的、多角色交互式应用程序而设计的库。它主要用于创建单个或多个代理的工作流。相较于其他基于LLM的框架，LangGraph 突出的核心优势在于它支持循环处理、高度可控以及数据持久化。LangGraph 允许开发者定义包含循环的流程结构，这对于大多数代理系统的设计至关重要，这也是它与那些基于有向无环图（DAG）的解决方案的主要区别。作为一款低层级的框架，LangGraph 提供了对应用流程和状态的微调控制能力，这对于构建稳定可靠的代理系统来说是不可或缺的。另外，LangGraph 内置了持久化存储机制，这一特性极大地增强了人机交互的连续性和系统的记忆功能。

LangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.

### Key Features

[https://langchain-ai.github.io/langgraph/#key-features](https://langchain-ai.github.io/langgraph/#key-features)

*   **Cycles and Branching**: Implement loops and conditionals in your apps. 在你的应用中实现循环和条件分支。
*   **Persistence**: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more. 持久化, 可以随时暂停和恢复
*   **Human-in-the-Loop**: Interrupt graph execution to approve or edit next action planned by the agent. 人工参与?
*   **Streaming Support**: Stream outputs as they are produced by each node (including token streaming).
*   **Integration with LangChain**: LangGraph integrates seamlessly with [LangChain](https://github.com/langchain-ai/langchain/) and [LangSmith](https://docs.smith.langchain.com/) (but does not require them).

A Simple Graph
--------------

[https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#prerequisites](https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#prerequisites)  
[https://langchain-ai.github.io/langgraph/#example](https://langchain-ai.github.io/langgraph/#example)  
[https://github.com/aneasystone/weekly-practice/blob/main/notes/week057-create-agents-with-langgraph/README.md](https://github.com/aneasystone/weekly-practice/blob/main/notes/week057-create-agents-with-langgraph/README.md)

在LangGraph中，状态（State）是核心概念之一。每当执行图（Graph）时，都会创建一个状态，这个状态会在图中的各个节点间传递。随着节点的执行，每个节点会利用自己的返回值来更新这个内部状态。图（Graph）更新其内部状态的方式取决于所选图的类型，或者可以通过自定义函数来定义。

这种机制允许在图的不同部分之间共享和传递信息，使得图的执行不仅是一个简单的节点执行序列，而是一个具有记忆和反馈的动态过程。节点之间的这种状态传递和更新确保了执行流程中各部分之间的连贯性和数据的一致性。

例如，假设你正在构建一个涉及多个步骤的数据处理流水线，其中每个步骤都代表图中的一个节点。第一个节点可能负责数据清洗，第二个节点负责特征提取，第三个节点则可能进行机器学习模型的预测。在这样的情况下，状态可以包含从第一步到第三步所需的所有中间结果和信息，确保每个后续节点都能访问到前一节点产生的输出，并基于此进行下一步的处理。

此外，由于状态更新可以由自定义函数控制，这赋予了LangGraph 高度的灵活性和可扩展性，允许用户根据特定需求定制状态管理逻辑。

### 代码实现

    
    '''
    Author: yangfh
    Date: 2024-07-13 13
    LastEditors: yangfh
    LastEditTime: 2024-07-23 14
    Description: 
    
    参考官方教程文档
    https://langchain-ai.github.io/langgraph/#step-by-step-breakdown
    
    通义千问
    https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.i1
    '''
    
    import os
    from langchain_openai import ChatOpenAI
    from typing import Annotated, Literal, TypedDict
    
    from langchain_anthropic import ChatAnthropic
    from langchain_core.tools import tool
    from langgraph.checkpoint import MemorySaver
    from langgraph.graph import END, StateGraph, MessagesState
    from langgraph.prebuilt import ToolNode
    
    from langchain_core.messages import (
        BaseMessage,
        HumanMessage,SystemMessage,AIMessage, ToolMessage,
    )
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    
    # 底层调用使用的是 httpx 库, 设置日志级别以获取更多信息
    # import logging
    # logging.basicConfig(level=logging.DEBUG)
    ############################################### LLM 模型 定义 #######################################################
    # LLM 模型 qwen-turbo 模型
    def get_llm(): 
       os.environ["OPENAI_API_KEY"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
       llm_model = ChatOpenAI(model="qwen-turbo",base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
       return llm_model
    
    # Chat 模型 qwen-turbo 模型 百炼
    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferMemory
    from langchain_bailian import Bailian
    def get_chat_model(): 
        os.environ["ACCESS_KEY_ID"] = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
        os.environ["ACCESS_KEY_SECRET"] = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
        os.environ["AGENT_KEY"] = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
        os.environ["APP_ID"] = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
        #
        access_key_id = os.environ.get("ACCESS_KEY_ID")
        access_key_secret = os.environ.get("ACCESS_KEY_SECRET")
        agent_key = os.environ.get("AGENT_KEY")
        app_id = os.environ.get("APP_ID")
        llm = Bailian(access_key_id=access_key_id,
                      access_key_secret=access_key_secret,
                      agent_key=agent_key,
                      app_id=app_id)
        return llm
     
    
    
    ############################################### tools 定义 #######################################################
    
    from Tools_Basic import tool_get_list
    from langgraph.prebuilt import ToolNode
    ################################ 查询我的所有基地列表数据
    def get_list():
        response = requests.get(g_serve_ctx+"/queryAll")
        json_str = response.content.decode()
        # print("queryAll 接口返回数据: ", json_str)
        return json_str
    ################################ 更新基地的信息
    class update_schema(BaseModel):
        id: int = Field(..., title="ID", description="基地的ID")
        name: str = Field(title="名称",description="基地名称")
        address: str = Field(title="地址",description="基地地址")
    def update_info(id: int, name:str = None, address: str = None):
        # 封装请求体
        data = {"id":id, "name":name, "address": address}
        response = requests.post(g_serve_ctx+"/update", json=data)
        json_str = response.content.decode()
        # print("/update 接口返回数据: ", json_str)
        return json_str
    tool_update_base_info = StructuredTool.from_function(
        func=update_info,
        name="tool_update_base_info",
        args_schema=update_schema,
        description="`基地管理`更新基地的信息,返回的是JSON格式数据"
    )
    
    
    tool_get_base_list = StructuredTool.from_function(
        func=get_list,
        name="tool_get_base_list",
        description="`基地管理`查询我的所有基地列表数据,返回的是JSON格式数据",
    )
    
    
    def tool_nodes():
        tools = [tool_get_list]
        tool_node = ToolNode(tools)
        return tool_node
    
    ############################################### Agent 定义 #######################################################
    """
    通用创建 agent 方法.
    """
    def create_agent(llm, tools, system_message: str):
        """Create an agent."""
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                   "system",
                   "“XXX云平台”是一个以云为基础、AI为核心，构建开放、立体感知、全域协同、精确判断和持续进化的综合服务平台。它具有以下功能："
                      "1. `基地管理` 基地数据包括：基地ID，基地名称，基地地址"
                   "如果您无法完全回答，没关系，您可以使用以下工具：{tool_names}。\n{system_message}，尽您所能的配合取得结果。",
                ),
                MessagesPlaceholder(variable_name="messages"),
            ]
        )
        # 填充 PromptTemplate 参数
        prompt = prompt.partial(system_message=system_message)
        prompt = prompt.partial(tool_names=", ".join([tool.name for tool in tools]))
        return prompt | llm.bind_tools(tools)
    
    def create_baisc_expert_agent():
        llm = get_llm()
        tools_ = [tool_get_list]
        return create_agent(llm, tools_, "")
    
    g_baisc_expert_agent = create_baisc_expert_agent()
    def call_baisc_expert_agent(state: MessagesState):
        messages = state['messages']
        response = g_baisc_expert_agent.invoke(messages)
        # We return a list, because this will get added to the existing list
        return {"messages": [response]}
    
    ############################################### Graph 定义 #######################################################
    # `Graph` 状态更新函数 (路由)
    # Define the function that determines whether to continue or not
    def should_continue(state: MessagesState) -> Literal["tools", "__end__"]:
        messages = state['messages']
        # 拿到最新一条消息
        last_message = messages[-1]
        # 如果是 tools 消息则返回 "tools", 给 tools 节点执行
        if last_message.tool_calls:
            return "tools"
        # Otherwise, we stop (reply to the user)
        return END
    
    
    ###############################################  Graph 的节点
    
    # agent 节点
    # call_baisc_expert_agent 见上 初始化
    
    # tool 节点
    tool_nodes_ = tool_nodes()
    
    # 创建图 `Graph`
    workflow = StateGraph(MessagesState)
    
    # `Graph` 图中的所有循环节点
    # Define the two nodes we will cycle between
    workflow.add_node("agent", call_baisc_expert_agent)
    workflow.add_node("tools",tool_nodes_ )
    
    # 设置入口点: 这个节点会第一个调用
    workflow.set_entry_point("agent")
    
    ###############################################  Graph 的边缘条件
    
    # 添加边缘条件?  意思是在"agent" 节点执行完之后, 调用 "should_continue" 函数, 它将确定调用下一个的哪个'节点'; 
    # 比如 Agent调用tool后, 把结果返回给对应的Agent; 还可以加一个入参, 映射字典表, 见官方API
    # We now add a conditional edge
    workflow.add_conditional_edges(
        # First, we define the start node. We use `agent`.
        # This means these are the edges taken after the `agent` node is called.
        "agent",
        # Next, we pass in the function that will determine which node is called next.
        should_continue,
    )
    
    #  调用 `tools` 节点 后会调用`agent`节点;
    #  这里是因为只有两个节点,可以硬编码. 意思是 `tools` 节点后再调用回`agent`
    workflow.add_edge("tools", 'agent')
    
    ###############################################  Graph 的持久化策略 
    # 持久化策略: 保存到内存
    # Initialize memory to persist state between graph runs
    checkpointer = MemorySaver()
    
    # 编译 `graph`
    app = workflow.compile(checkpointer=checkpointer)
    
    ############################################### Runnable  #######################################################
    # Use the Runnable 
    final_state = app.invoke(
        {"messages": [HumanMessage(content="我有多少个基地?")]},
        config={"configurable": {"thread_id": 44}}# 相当于会话ID; 会保存 graph 的 状态
    )
    
    print("graph final_state = ",final_state)
    final_state["messages"][-1].content # graph 结束后, 拿到最后一条消息
    

### 关键API说明

#### Graph

[https://langchain-ai.github.io/langgraph/reference/graphs/](https://langchain-ai.github.io/langgraph/reference/graphs/)  
Graph 是 LangGraph 的核心抽象。每个 [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#stategraph) 实现都用于创建图形工作流。编译完成后，可以运行 [CompiledGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#compiledgraph) 来运行应用程序。

#### add\_conditional\_edges

[https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#part-3-conditional-interrupt](https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#part-3-conditional-interrupt)  
[https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.MessageGraph.add\_conditional\_edges](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.MessageGraph.add_conditional_edges)

`add_conditional_edges(source, path, path_map=None, then=None)`  
Parameters:

*   source (str) – 起始节点.
*   path (`Union[Callable, Runnable]`) – 确定下一个的节点函数(支持一个或多个节点)。如果未指定`path_map`则这里应返回一个或更多节点。如果返回 END，则 Graph 将停止执行。
*   path\_map (`Optional[dict[Hashable, str]], default: None` ) – 返回 path 函数的返回值名称与节点的映射 (字典类型)
*   then (`Optional[str], default: None` ) – 在确定 path 选择的节点执行之前, 这可以再插一个节点

#### add\_edge

可以理解为硬编码, `tools` 节点后再调用回`agent`  
`workflow.add_edge("tools", 'agent')`

#### Conditional Interrupt

例如, 需要在执行工具时中断, 由人工确认, 参考: [https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#part-3-conditional-interrupt](https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#part-3-conditional-interrupt)

    part_3_graph = builder.compile(
        checkpointer=memory,
        # NEW: The graph will always halt before executing the "tools" node.
        # The user can approve or reject (or even alter the request) before
        # the assistant continues
        interrupt_before=["sensitive_tools"],# 在执行 sensitive_tools 前 中断
    )
    

#### 调试

ChatOpenAI 发送网络请求的位置是在 `C:\Users\yang\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py :: def _request(` 使用的是httpx库

    
    formatted_headers = []
    for header in request.headers.raw:
        header_key, header_value = header
        formatted_header = f"{header_key.decode('utf-8').lower()}: {header_value.decode('utf-8')}"
        formatted_headers.append(formatted_header)
        print(formatted_header)
        
    request.content.decode('utf-8')
    
    

Quick Start
-----------

### Setup (依赖)

[https://langchain-ai.github.io/langgraph/tutorials/introduction/](https://langchain-ai.github.io/langgraph/tutorials/introduction/)

In this tutorial, we will build a support chatbot in LangGraph that can:

*   Answer common questions by searching the web
*   Maintain conversation state across calls
*   Route complex queries to a human for review
*   Use custom state to control its behavior
*   Rewind and explore alternative conversation paths

    pip install -U langgraph
    

First, install the required packages:

    %%capture --no-stderr
    %pip install -U langgraph langsmith
    
    # Used for this tutorial; not a requirement for LangGraph
    %pip install -U langchain_anthropic
    

Optionally, we can set up [LangSmith](https://docs.smith.langchain.com/) for best-in-class observability.

    export LANGSMITH_TRACING=true
    export LANGSMITH_API_KEY=lsv2_sk_...
    

    pip install -U langgraph
    pip install -U langsmith
    pip install -U tiktoken langchain-cohere langchainhub chromadb langgraph tavily-python
    pip install -U langchain_anthropic
    
    pip install -U langchain-huggingface
    pip install -U langgraph-checkpoint-sqlite
    

RAG
===

    %%capture --no-stderr
    pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python
    

What is RAG?
------------

[https://python.langchain.com/v0.2/docs/tutorials/rag/](https://python.langchain.com/v0.2/docs/tutorials/rag/)  
LLM 最强大的应用之一是复杂的问答聊天机器人。这些应用程序可以回答有关特定源信息的问题。这些应用使用了一种称为 "检索增强生成"（Retrieval Augmented Generation）的技术。

LLM 可以推理各种主题，但它们的知识仅限于在特定时间点之前的公共数据，而这些数据是它们接受训练的基础。要想使用私人数据或模型训练日期之后数据的人工智能应用，就需要用模型所需的特定信息来增强模型的知识。将适当信息引入, 提示模型的过程被称为 检索增强生成（RAG）。

1.  **Load**: First we need to load our data. This is done with [Document Loaders](https://python.langchain.com/v0.2/docs/concepts/#document-loaders).
2.  **Split**: [Text splitters](https://python.langchain.com/v0.2/docs/concepts/#text-splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.
3.  **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](https://python.langchain.com/v0.2/docs/concepts/#vector-stores) and [Embeddings](https://python.langchain.com/v0.2/docs/concepts/#embedding-models) model.
4.  **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://python.langchain.com/v0.2/docs/concepts/#retrievers).
5.  **Generate**: A [ChatModel](https://python.langchain.com/v0.2/docs/concepts/#chat-models) / [LLM](https://python.langchain.com/v0.2/docs/concepts/#llms) produces an answer using a prompt that includes the question and the retrieved data

> **in short** 大约是: 1. 将文档切分为多个小片段; 2. 再通过 Embedding (计算/嵌入?) 出词向量; 3. 将问题也计算出词向量; 4. 通过向量检索过滤出有关系的资料数据, 给大模型, 从而过滤了大量无关内容;

### Embeddings

[`langchain.embeddings`](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.embeddings "langchain.embeddings")

"embeddings"是一种将文本数据转换成数学向量的方法。 每个词或句子都可以被映射到一个高维空间中的一个向量上，这个向量能够捕捉到词语之间的语义关系和上下文信息。  
比如，“猫”和“狗”的embedding可能会比较接近，因为它们都是宠物，而“猫”和“汽车”的embedding就会离得远一些，因为它们关联性不大。  
\*\*LLM 并不认识文字, 需要将文字 embedding 为 对应 '多维的' '词向量' 可计算词距离的表达的形式 \*\*

#### 为什么 需要 embedding 文档数据?

0.  **向量化**: 将文本数据转换成数学向量，每个词或句子都可以被映射到一个高维空间中的一个向量上，这个向量能够捕捉到词语之间的语义关系和上下文信息。
1.  **降维处理**：原始文档数据，尤其是文本数据，如果直接以词袋模型(关注词频/不关注关系) 或 one-hot 编码表示，会非常稀疏且高维，这不仅占用大量内存，还会导致计算效率低下。Embedding技术能将这些高维稀疏表示转化为低维密集向量，大幅减少所需存储空间并加速后续处理。
2.  **捕获语义信息**：通过embedding，每个词或文档被映射到一个连续的向量空间中，这个空间的结构能够反映词语之间的语义相似度。也就是说，语义上相近的词在向量空间中距离较近，这有助于模型理解和泛化未见过的文本内容。
3.  **提高模型性能**：在诸如文本分类、情感分析、机器翻译等自然语言处理任务中，直接使用embedding作为输入特征，相比于传统的特征工程，往往能提升模型的预测性能。这是因为embedding向量携带了丰富的上下文信息，有助于模型捕捉复杂语言模式。
4.  **便于计算**：低维的embedding向量使得计算相似度、距离等变得简单高效，这对于需要比较文档间相似性或进行聚类的任务尤为重要。常见的相似度计算方法如余弦相似度可以在这些向量上快速执行。
5.  **灵活性和可迁移性**：预训练好的embedding可以在不同的任务和数据集之间迁移使用，减少了从零开始训练所需的时间和资源。这意味着一旦某个领域的文本数据被嵌入到向量空间，这些嵌入就可以应用于该领域内的多个下游任务。

#### 文本向量化 如何表达词性关系?

参考: 向量数据库技术鉴赏- [https://www.bilibili.com/video/BV11a4y1c7SW/](https://www.bilibili.com/video/BV11a4y1c7SW/)  

#### 向量数据 如何压缩?

一种思路是, 减少数量量, 使用聚类平均中心点数据表示某一类数据, 并在此的基础上分层

计算聚类中心. 1. 生成随机点, 2. 分别与最近的所有点 计算平均点, 3. 直至收敛, 平均点位置不再有太大的变动, 最终的结果称为 "质心"  

使用 "质心"来表示一类, 可以降低维度, 但是需要一个码本  

正与聚类搜索, 可以用来近似搜索, 那也可以用来有损压缩, 将所有聚类集中的数据使用(聚类中心点)来替换之, 这个过程叫 "量化"

高维度的向量数据, 需要及其庞大聚类中心点来保证数据质量, **会造成维度爆炸**, 需要将数据降维, 划分为多个子维度, 计算聚类中心

#### 向量数据 如何搜索数据?

*   最近邻搜索

> 平坦搜索: 暴力搜索, 遍历所有数据 (几乎不现实)  
> 聚类: 对所有数据进行分类, 取权重 计算平均中心点(聚类中心点)  
> NSW: 建立词向量关系图  
> HNSW: 对词向量关系图, 再分层

`Hierarchical Navigable Small Worlds (HNSW) 结构图` 

### 结构性回答 & 非结构性回答

*   非结构性回答: 表示没有固定的格式和结构，更加灵活和自由。这种回答方式适用于简单问题、开放性问题或者需要表达个人观点的问题
    
*   结构性回答: 是指回答问题时具有一定的格式和结构，通常包含明确的逻辑框架和条理。这种回答方式适用于 代码, SQL, 脚本问题。
    

> [https://python.langchain.com/v0.2/docs/tutorials/sql\_qa/](https://python.langchain.com/v0.2/docs/tutorials/sql_qa/)

Agentic RAG
-----------

[https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph\_agentic\_rag/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/)

[Retrieval Agents](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/#agents) are useful when we want to make decisions about whether to retrieve from an index.  
To implement a retrieval agent, we simple need to give an LLM access to a retriever tool.

把RAG 整合为一个Agent 的Tool, (约是最简单的实现了)

### 逻辑图

1.  由 Agent 触发检索工具调用
2.  评估检索到内容与问题的相关性  
    2.1 评估问题不相关 问题重写(re-write)  
    2.2 评估问题相关 修饰答案(generate) 返回

> 圆形表示 node, 方形表示 conditional\_edges

### 代码实现

    import os
    from langchain_openai import ChatOpenAI
    
    from typing import Annotated, Literal, Sequence, TypedDict
    
    from langchain_anthropic import ChatAnthropic
    from langchain_core.tools import tool
    from langgraph.checkpoint import MemorySaver
    from langgraph.graph import END, StateGraph, MessagesState
    from langgraph.prebuilt import ToolNode
    
    from langchain import hub
    from langchain_core.messages import BaseMessage, HumanMessage
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import PromptTemplate
    from langchain_core.pydantic_v1 import BaseModel, Field
    from langgraph.prebuilt import tools_condition
    from langchain_huggingface import HuggingFaceEmbeddings
    
    ############################################### LLM 模型 定义 #######################################################
    # LLM 模型 qwen-turbo
    def get_llm(): 
    # yang
       os.environ["OPENAI_API_KEY"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
    #    llm_model = ChatOpenAI(model="qwen-turbo",base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
       llm_model = ChatOpenAI(model="qwen-turbo",base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", streaming=True)
       return llm_model
    
    # Chat 模型 qwen-turbo 百炼
    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferMemory
    from langchain_bailian import Bailian
    def get_chat_model(): 
        os.environ["ACCESS_KEY_ID"] = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
        os.environ["ACCESS_KEY_SECRET"] = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
        os.environ["AGENT_KEY"] = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
        os.environ["APP_ID"] = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
        ###
        access_key_id = os.environ.get("ACCESS_KEY_ID")
        access_key_secret = os.environ.get("ACCESS_KEY_SECRET")
        agent_key = os.environ.get("AGENT_KEY")
        app_id = os.environ.get("APP_ID")
        llm = Bailian(access_key_id=access_key_id,
                      access_key_secret=access_key_secret,
                      agent_key=agent_key,
                      app_id=app_id)
        return llm
     
    ############################################### tool 定义 #######################################################
    # 检索器 将检索器创建为tool
    from langchain_community.document_loaders import WebBaseLoader
    from langchain_community.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    
    def get_retriever_tool():
        embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
        vectorstore = Chroma(persist_directory="./chroma_lizhi.dat", collection_name="lizhi",embedding_function=embeddings_model)
        retriever = vectorstore.as_retriever()
    
    
        from langchain.tools.retriever import create_retriever_tool
        retriever_tool = create_retriever_tool(
            retriever,
            "荔枝信息搜索",
            "搜索并返回有关于荔枝的病害、养护、种植、销售信息。",
        )
        return retriever_tool
    
    g_retriever_tool = get_retriever_tool()
    
    ############################################### 节点 定义 #######################################################
    
    ############################################### '文档评估' 
    def grade_documents(state) -> Literal["generate", "rewrite"]:
        """
        推断问题是否与文档相关
        Args:
            state (messages): The current state
        Returns:
            str: A decision for whether the documents are relevant or not
        """
        print("---CHECK RELEVANCE---")
    
        # Data model
        class grade(BaseModel):
            """Binary score for relevance check."""
            binary_score: str = Field(description="Relevance score 'yes' or 'no'")
        # LLM
        model = get_llm()
        # LLM with tool and validation
        llm_with_tool = model.with_structured_output(grade)
        # Prompt
        prompt = PromptTemplate(
            template="""您是一名评分员，评估检索到的文档与用户问题的相关性\n 
            这是检索到的文档: \n\n {context} \n\n
            这是用户的问题: {question} \n
            如果文档包含与用户问题相关的语义或含义，则将其评定为相关。\n
            请给出‘yes’或者‘no’的二元回答。""",
            input_variables=["context", "question"],
        )
    
        # Chain
        chain = prompt | llm_with_tool
        messages = state["messages"]
        last_message = messages[-1]
        question = messages[0].content
        docs = last_message.content
        scored_result = chain.invoke({"question": question, "context": docs})
        score = scored_result.binary_score
        if score == "yes":
            print("---DECISION: DOCS RELEVANT---")
            return "generate"
    
        else:
            # print("---DECISION: DOCS NOT RELEVANT---")
            print(f"\r\n------判定答案不相关: {docs} ------\r\n")
            print(score)
            return "rewrite"
    ############################################### Agent
    
    # 使用 检索器 tool 的Agent
    def agent(state):
        print("---CALL AGENT---")
        messages = state["messages"]
        model = get_llm()
        tools = [g_retriever_tool ]
        model = model.bind_tools(tools)
        response = model.invoke(messages)
        # We return a list, because this will get added to the existing list
        return {"messages": [response]}
    
    
    ############################################### 问题重写
    def rewrite(state):
        """
        Transform the query to produce a better question.
        Args:
            state (messages): The current state
        Returns:
            dict: The updated state with re-phrased question
        """
        print("---TRANSFORM QUERY---")
        messages = state["messages"]
        question = messages[0].content
        msg = [
            HumanMessage(
                content=f""" \n 
            根据输入问题的内容，并尝试推理其背后的语义意图、含义 \n 
            这里是输入的问题:
            \n ------- \n
            {question} 
            \n ------- \n
            提出一个更准确、更有逻辑的改进，除此之外不要回答其他任何内容。""",
            )
        ]
        # Grader
        model = get_llm()
        response = model.invoke(msg)
        
        print(f"\r\n------问题重写后: {response.content} ------\r\n")
        return {"messages": [response]}
    
    ############################################### 答案生成
    
    # 有什么用？ 负责修饰一下 问题重写后 或 检索后的答案。
    def generate(state):
        """
        Generate answer
        Args:
            state (messages): The current state
        Returns:
             dict: The updated state with re-phrased question
        """
        print("---GENERATE---")
        messages = state["messages"]
        question = messages[0].content
        last_message = messages[-1]
    
        question = messages[0].content
        docs = last_message.content
    
        # Prompt
        prompt = hub.pull("rlm/rag-prompt")
        # LLM
        llm = get_llm()
        # Post-processing
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        # Chain
        rag_chain = prompt | llm | StrOutputParser()
        # Run
        response = rag_chain.invoke({"context": docs, "question": question})
        return {"messages": [response]}
    
    
    ############################################### Graph 定义 #######################################################
    
    ############################################### `Graph`的状态流转对象(DTO)
    from typing import Annotated, Sequence, TypedDict
    from langchain_core.messages import BaseMessage
    from langgraph.graph.message import add_messages
    class AgentState(TypedDict):
        # The add_messages function defines how an update should be processed
        # Default is to replace. add_messages says "append"
        messages: Annotated[Sequence[BaseMessage], add_messages]
    from langgraph.graph import END, StateGraph, START
    from langgraph.prebuilt import ToolNode
    
    
    ############################################### `Graph` 节点 定义
    # Define a new graph
    workflow = StateGraph(AgentState)
    
    # Define the nodes we will cycle between
    workflow.add_node("agent", agent)  # agent
    retrieve = ToolNode([g_retriever_tool])
    workflow.add_node("retrieve", retrieve)  # retrieval
    workflow.add_node("rewrite", rewrite)  # Re-writing the question
    workflow.add_node(
        "generate", generate
    )  # Generating a response after we know the documents are relevant
    # Call agent node to decide to retrieve or not
    workflow.add_edge(START, "agent")
    
    # Decide whether to retrieve
    workflow.add_conditional_edges(
        "agent",
        # Assess agent decision
        tools_condition,
        {
            # Translate the condition outputs to nodes in our graph
            "tools": "retrieve",
            END: END,
        },
    )
    
    # Edges taken after the `action` node is called.
    workflow.add_conditional_edges(
        "retrieve",
        # Assess agent decision
        grade_documents,
    )
    workflow.add_edge("generate", END)
    workflow.add_edge("rewrite", "agent")
    
    # Compile
    graph = workflow.compile()
    
    ############################################### Runnable  #######################################################
    # Use the Runnable 
    import pprint
    import time
    
    def entry():
        inputs = {
            "messages": [
                ("user", "南州六月荔枝丹有哪些?"),
            ]
        }
        for output in graph.stream(inputs):
            for key, value in output.items():
                pprint.pprint(f"Output from node '{key}':")
                pprint.pprint("---")
                pprint.pprint(value, indent=2, width=80, depth=None)
            pprint.pprint("\n---\n")
    
    
    ############################################### main  #######################################################
    # if __name__ == '__main__':
    #     entry()
    
    
    if __name__ == '__main__':
        while True:
            user_input = input("Human：")
            if(user_input == "exit"):
                break
            if(user_input == None or user_input == ''):
                continue
            # block
            final_state = graph.invoke(
                {"messages": [HumanMessage(content=user_input)]},
                config={"configurable": {"thread_id": 45}}
            )
            lastout = final_state["messages"][-1].content
            print(f"AI: {lastout}")
    
            # stream 
            # streaming(user_input, config={"configurable": {"thread_id": 45} })
            
            print(f"---------------------------------------------")
            time.sleep(0.5)
        print("-- the  end --- ")
    
    

Adaptive RAG
------------

[https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph\_adaptive\_rag/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/)  
原始论文: [https://arxiv.org/abs/2403.14403](https://arxiv.org/abs/2403.14403)

Adaptive-RAG，一个自适应检索增强生成框架，旨在处理各种复杂的问题。该框架通过评估问题的复杂性，并选择最合适的检索增强语言模型 (RAG) 策略来提高问答系统的效率和准确性。

1.  **评估查询复杂度**：Adaptive-RAG 使用一个**查询复杂度分类器** 来评估查询的复杂度。这个分类器是一个小型语言模型，它会根据查询的内容自动判断查询属于哪种复杂度级别。
2.  **选择策略**： 根据查询复杂度，Adaptive-RAG 会选择最合适的策略来回答查询。这些策略包括：
    *   **非检索**： 对于简单的查询，Adaptive-RAG 只会使用大型语言模型本身来回答问题，无需检索外部知识库。
    *   **单步检索**： 对于中等复杂度的查询，Adaptive-RAG 会检索一次相关文档，并将其信息融入大型语言模型中，从而生成答案。
    *   **多步检索**： 对于复杂的查询，Adaptive-RAG 会多次检索相关文档，进行推理，自我反思(RAG self-reflection) 内部包括: 相关性判定, 问题重写, 事实判断(解决hallucination) 等等...最终生成答案

### 逻辑图

1.  查询复杂度分类器 问题分析  
    1.1 索引检索 -> 自我纠正  
    1.2 无关检索 -> web搜索  
    1.3 ..  
    **in short 着重对问题分析, 进行路由**

### 问题复杂度分类

这个作为 **查询复杂度分类器** 的实现

    ### Router
    
    from typing import Literal
    
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.pydantic_v1 import BaseModel, Field
    from langchain_openai import ChatOpenAI
    
    
    # Data model
    class RouteQuery(BaseModel):
        """Route a user query to the most relevant datasource."""
    
        datasource: Literal["vectorstore", "web_search"] = Field(
            ...,
            description="Given a user question choose to route it to web search or a vectorstore.",
        )
    
    
    # LLM with function call
    llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
    structured_llm_router = llm.with_structured_output(RouteQuery)
    
    # Prompt
    system = """You are an expert at routing a user question to a vectorstore or web search.
    The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.
    Use the vectorstore for questions on these topics. Otherwise, use web-search."""
    route_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "{question}"),
        ]
    )
    
    question_router = route_prompt | structured_llm_router
    print(
        question_router.invoke(
            {"question": "Who will the Bears draft first in the NFL draft?"}
        )
    )
    print(question_router.invoke({"question": "What are the types of agent memory?"}))
    

#### **与 CRAG 的区别点**

Adaptive RAG 是根据 **查询复杂度分类器** 的评估结果，来选择最合适的策略来回答和查询。这些策略包括非检索、单步检索和多步检索。

### 相关性判定 实现逻辑

    ### Retrieval Grader
    
    
    # Data model
    class GradeDocuments(BaseModel):
        """Binary score for relevance check on retrieved documents."""
    
        binary_score: str = Field(
            description="Documents are relevant to the question, 'yes' or 'no'"
        )
    
    
    # LLM with function call
    llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
    structured_llm_grader = llm.with_structured_output(GradeDocuments)
    
    # Prompt
    system = """You are a grader assessing relevance of a retrieved document to a user question. \n 
        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n
        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""
    grade_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
        ]
    )
    
    retrieval_grader = grade_prompt | structured_llm_grader
    question = "agent memory"
    docs = retriever.get_relevant_documents(question)
    doc_txt = docs[1].page_content
    print(retrieval_grader.invoke({"question": question, "document": doc_txt}))
    

### 问题重写 实现逻辑

    ### Question Re-writer
    
    # LLM
    llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
    
    # Prompt
    system = """You a question re-writer that converts an input question to a better version that is optimized \n 
         for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning."""
    re_write_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            (
                "human",
                "Here is the initial question: \n\n {question} \n Formulate an improved question.",
            ),
        ]
    )
    
    question_rewriter = re_write_prompt | llm | StrOutputParser()
    question_rewriter.invoke({"question": question})
    

### 事实判定 实现逻辑

    ### Hallucination Grader
    
    # Data model
    class GradeHallucinations(BaseModel):
        """Binary score for hallucination present in generation answer."""
    
        binary_score: str = Field(
            description="Answer is grounded in the facts, 'yes' or 'no'"
        )
    
    
    # LLM with function call
    llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
    structured_llm_grader = llm.with_structured_output(GradeHallucinations)
    
    # Prompt
    system = """You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n 
         Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts."""
    hallucination_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation}"),
        ]
    )
    
    hallucination_grader = hallucination_prompt | structured_llm_grader
    hallucination_grader.invoke({"documents": docs, "generation": generation})
    

> 全给大模型自己判定了

Adaptive RAG using local LLMs
-----------------------------

[https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph\_adaptive\_rag\_local/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag_local/)

#### Local Embeddings

[https://docs.gpt4all.io/gpt4all\_python\_embedding.html#supported-embedding-models](https://docs.gpt4all.io/gpt4all_python_embedding.html#supported-embedding-models)

Corrective RAG (CRAG)
---------------------

[https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph\_crag/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/)

In the paper [here](https://arxiv.org/pdf/2401.15884.pdf), a few steps are taken:

Corrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents.  
**CRAG (Corrective Retrieval Augmented Generation**) ，解决大型语言模型 (LLM) 在生成文本时容易出现“幻觉”(生成的文本包含了错误或虚假的信息)的问题。

### 逻辑图

**in short 着中提高文档的质量, 包括不限于 修正提高大模型回答的质量**

### 检索评 grade

*   **正确 (Correct)**: 如果检索到的信息是准确的，CRAG 会进一步提取其中的关键信息，并将其作为输入的一部分提供给 LLM。
*   **错误 (Incorrect)**: 如果检索到的信息是错误的，CRAG 会丢弃这些信息，并使用网络搜索来获取新的信息。
*   **模糊 (Ambiguous)**: 如果无法确定检索到的信息的准确性，CRAG 会将检索到的信息和网络搜索到的信息结合起来，作为输入的一部分提供给 LLM。

#### **与Adaptive RAG区别点**

如果至少有一份文档超过了相关性阈值，那么它就会继续生成文档 在生成文档之前，它会进行知识提炼将文档划分为 "知识条"。  
如果所有文档都低于相关性阈值，或者分级者不确定，那么框架就会寻找额外的数据源。

**in short 它设计的目标：是通过评估、修改更正、过滤无关内容、从其他数据源(网络搜索) 以提高文档/答案的质量**

Agent 几种结构
----------

[https://blog.langchain.dev/reflection-agents/](https://blog.langchain.dev/reflection-agents/)

### Reflection Agents

Reflexion 结构图  
引入 Revisor 对结果进行反思, 若结果不好, 重复调用工具进行完善

### Language Agents Tree Search

Language Agents Tree Search 结构图  
蒙特卡洛树搜索, 基于大模型 将大问题增加子问题扩展, 再寻找到最高分数的树, 再生成子树...(问题将几何级增加)...

开发样例
====

[https://langchain-ai.github.io/langgraph/tutorials/](https://langchain-ai.github.io/langgraph/tutorials/)

开发功能点
=====

[https://langchain-ai.github.io/langgraph/how-tos/](https://langchain-ai.github.io/langgraph/how-tos/)

流式支持(Streaming)
---------------

[https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#define-the-graph](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#define-the-graph)

    def get_llm(): 
        os.environ["OPENAI_API_KEY"] = 'EMPTY'
        llm_model = ChatOpenAI(model="glm-4-9b-chat-lora",base_url="http://172.16.21.155:8003/v1", streaming=True)
        return llm_model
    
    

**注意 stream\_mode="messages" 这个参数**

    
    from langchain_core.messages import AIMessageChunk, HumanMessage
    
    inputs = [HumanMessage(content="what is the weather in sf")]
    first = True
    for msg, metadata in app.stream({"messages": inputs}, stream_mode="messages"):
        if msg.content and not isinstance(msg, HumanMessage):
            print(msg.content, end="|", flush=True)
        if isinstance(msg, AIMessageChunk):
            if first:
                gathered = msg
                first = False
            else:
                gathered = gathered + msg
    
            if msg.tool_call_chunks:
                print(gathered.tool_calls)
    

#### 异步调用

另外异步调用必须全异步调用的代码形式才会生效

    # 在agent节点 必须异步调用
    async def call_agent(state: MessagesState):
        messages = state['messages']
        response = await bound_agent.ainvoke(messages)
        return {"messages": [response]}
    
    
    ........
    
    
    import time
    import asyncio
    from langchain_core.messages import AIMessageChunk, HumanMessage
    async def main():
        while True:
    		user_input = input("input: ")
            if(user_input == "exit"):
                break
            if(user_input == None or user_input == ''):
                continue
            # stream 
            config={"configurable": {"thread_id": 1}}
            inputs =  {"messages": [HumanMessage(content=user_input)]}
            first = True
            async for msg, metadata in app.astream(inputs, stream_mode="messages", config=config):
                if msg.content and not isinstance(msg, HumanMessage):
                    print(msg.content, end="", flush=True)
                if isinstance(msg, AIMessageChunk):
                    if first:
                        gathered = msg
                        first = False
                    else:
                        gathered = gathered + msg
                    if msg.tool_call_chunks:
                        print(gathered.tool_calls)
            print("\r\n")
            time.sleep(0.5)
        print("-- the  end --- ")
    # import logging
    # logging.basicConfig(level=logging.DEBUG)
    if __name__ == '__main__':
    

### stream\_mode 参数

[https://langchain-ai.github.io/langgraph/concepts/streaming/#streaming-graph-outputs-stream-and-astream](https://langchain-ai.github.io/langgraph/concepts/streaming/#streaming-graph-outputs-stream-and-astream)

LangGraph is built with first class support for streaming. There are several different ways to stream back outputs from a graph run

**Streaming graph outputs (`.stream` and `.astream`)[¶](https://langchain-ai.github.io/langgraph/concepts/streaming/#streaming-graph-outputs-stream-and-astream "Permanent link")**  
`.stream` and `.astream` are sync and async methods for streaming back outputs from a graph run. There are several different modes you can specify when calling these methods (e.g. \`graph.stream(..., mode="...")):

*   [`"values"`](https://langchain-ai.github.io/langgraph/how-tos/stream-values/): This streams the full value of the state after each step of the graph. 此模式在每个图步骤后流式传输状态的完整值。
*   [`"updates"`](https://langchain-ai.github.io/langgraph/how-tos/stream-updates/): This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately. 此模式在每个图步骤后流式传输状态的变化。如果在同一步骤中进行了多次更新（例如，运行了多个节点），则这些更新将分别流式传输。
*   [`"custom"`](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/): This streams custom data from inside your graph nodes. 此模式从您的图节点内部流式传输自定义数据。
*   [`"messages"`](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/): This streams LLM tokens and metadata for the graph node where LLM is invoked. 此模式流式传输LLM令牌和图节点中调用LLM的元数据。
*   `messages-tuple` : 为节点内生成的任何消息流提供 LLM 个令牌。此模式主要用于支持聊天应用。请参阅流消息的指南。
*   `"debug"`: This streams as much information as possible throughout the execution of the graph. 此模式在图执行过程中尽可能多地流式传输信息。

You can also specify multiple streaming modes at the same time by passing them as a list. When you do this, the streamed outputs will be tuples `(stream_mode, data)`. For example:  
`graph.stream(..., stream_mode=["updates", "messages"])`

**in short 该参数决定每个节点是否获取完整的 graph 数据, 注意节点返回的数据也会自动追加/更新 graph 数据**

人机交互(Human-in-the-loop)
-----------------------

[https://langchain-ai.github.io/langgraph/how-tos/human\_in\_the\_loop/breakpoints/#simple-usage](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/#simple-usage)  
[https://github.langchain.ac.cn/langgraphjs/how-tos/dynamic\_breakpoints/#update-the-graph-state](https://github.langchain.ac.cn/langgraphjs/how-tos/dynamic_breakpoints/#update-the-graph-state)

会话总结(summary of the conversation history)
-----------------------------------------

[https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)

    # %% [markdown]
    # # Example_SummaryOfConversation
    # 
    # 对话历史的管理, 处理
    # 
    # 参考了: https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/#build-the-agent
    
    # %%
    '''
    Author: yangfh
    Date: 2024-10-24 14
    LastEditors: yangfh
    LastEditTime: 2024-10-25 10
    Description: 
    
    Copyright (c) 2024 by www.simae.cn, All Rights Reserved. 
    '''
    import os
    
    from langchain_openai import ChatOpenAI
    
    from typing import Annotated, Literal, TypedDict
    from langchain_core.messages import SystemMessage,HumanMessage, RemoveMessage
    from langchain_core.tools import tool
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.graph import END, START, StateGraph, MessagesState
    from langgraph.prebuilt import ToolNode
    
    # %%
    # langsmith  # langsmith 调试时使用 (在线且收费的)
    
    # from langsmith.wrappers import wrap_openai
    # from langsmith import traceable
    # os.environ["LANGSMITH_TRACING"]="true"
    # os.environ["LANGSMITH_API_KEY"]="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"
    
    # %% [markdown]
    # ## Graph 流转的状态对象
    # 
    # 添加一个 summary 属性作为总结的对话概要
    
    # %%
    # We will add a `summary` attribute (in addition to `messages` key,  which MessagesState already has)
    class MyGraphState(MessagesState):
        summary: str
    
    
    # %% [markdown]
    # ## LLM定义
    
    # %%
    
    os.environ["OPENAI_API_KEY"] = 'sk-4c48484ff93f452d8e3bb41192452e01'
    llm_model = ChatOpenAI(model="qwen-turbo",base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", streaming=True)
    # 本地
    # os.environ["OPENAI_API_KEY"] = 'EMPTY'
    # llm_model = ChatOpenAI(model="chatglm3-6b",base_url="http://172.16.21.155:8000/v1/chat/completions", streaming=False)
    
    bound_model = llm_model
    
    # %% [markdown]
    # ## 路由节点
    # 
    # 若是消息长度大于6 则路由到 summarize_conversation 节点, 去总结概要
    
    # %%
    # We now define the logic for determining whether to end or summarize the conversation
    def should_continue(state: MyGraphState) -> Literal["summarize_conversation", END]:
        """Return the next node to execute."""
        messages = state["messages"]
        # If there are more than six messages, then we summarize the conversation
        if len(messages) > 6:
            return "summarize_conversation"
        # Otherwise we can just end
        return END
    
    # %% [markdown]
    # ## 总结消息概要
    
    # %%
    def summarize_conversation(state: MyGraphState):
        # First, we summarize the conversation
        summary = state.get("summary", "")
        if summary:
            # If a summary already exists, we use a different system prompt
            # to summarize it than if one didn't
            summary_message = (
                f"这是此前对话摘要: {summary}\n\n"
                "请考虑到此前的对话摘要加上述对话记录, 创建为一个新对话摘要"
            )
        else:
            summary_message = "请将上述的对话创建为摘要"
        # 注意, 这里是插到最后面
        messages = state["messages"] + [HumanMessage(content=summary_message)]
        response = llm_model.invoke(messages)
        # 保留最新的2条消息, 删除其余的所有消息
        delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
        return {"summary": response.content, "messages": delete_messages} # 这个 messages(delete message 由langchain处理)
    
    # %% [markdown]
    # ## LLM调用节点
    # 
    # 调用模型时如果状态数据中, 若是有summary概要数据, 则在调用模型时, 替换为一个概要数据的系统消息 + 一个用户消息
    
    # %%
    # Define the logic to call the model
    def call_model(state: MyGraphState):
        # If a summary exists, we add this in as a system message
        summary = state.get("summary", "")
        if summary:
            system_message = f"此前的对话摘要: {summary}"
            messages = [SystemMessage(content=system_message)] + state["messages"]
        else:
            messages = state["messages"]
        response = bound_model.invoke(messages)
        # We return a list, because this will get added to the existing list
        return {"messages": [response]}
    
    
    # %% [markdown]
    # ## 图(Graph)定义
    
    # %%
    memory = MemorySaver()
    
    # Define a new graph
    workflow = StateGraph(MyGraphState)
    
    # Define the conversation node and the summarize node
    workflow.add_node("conversation", call_model)
    workflow.add_node(summarize_conversation)
    
    # Set the entrypoint as conversation
    workflow.add_edge(START, "conversation")
    
    # We now add a conditional edge
    workflow.add_conditional_edges(
        # First, we define the start node. We use `conversation`.
        # This means these are the edges taken after the `conversation` node is called.
        "conversation",
        # Next, we pass in the function that will determine which node is called next.
        should_continue,
    )
    
    # We now add a normal edge from `summarize_conversation` to END.
    # This means that after `summarize_conversation` is called, we end.
    workflow.add_edge("summarize_conversation", END)
    
    # Finally, we compile it!
    app = workflow.compile(checkpointer=memory)
    
    # %% [markdown]
    # ## 调用
    
    # %%
    from langchain_core.messages import HumanMessage
    
    def print_update(update):
        for k, v in update.items():
            for m in v["messages"]:
                m.pretty_print()
            if "summary" in v:
                print(v["summary"])
                
    
    config = {"configurable": {"thread_id": "4"}}
    input_message = HumanMessage(content="你好, 我叫张三")
    input_message.pretty_print()
    for event in app.stream({"messages": [input_message]}, config, stream_mode="updates"):
        print_update(event)
    
    input_message = HumanMessage(content="你叫什么名字?")
    input_message.pretty_print()
    for event in app.stream({"messages": [input_message]}, config, stream_mode="updates"):
        print_update(event)
    
    input_message = HumanMessage(content="我今天和昨天晒了渔网, 接下来的三天我就要去打鱼了")
    input_message.pretty_print()
    for event in app.stream({"messages": [input_message]}, config, stream_mode="updates"):
        print_update(event)
    
    # %%
    # 上面刚刚好 6条消息
    
    # 再执行会触发 消息总结
    input_message = HumanMessage(content="我昨天干了什么?")
    input_message.pretty_print()
    for event in app.stream({"messages": [input_message]}, config, stream_mode="updates"):
        print_update(event)
    
    # %%
    # 再执行会触发 消息总结
    input_message = HumanMessage(content="我昨天干了什么?")
    input_message.pretty_print()
    for event in app.stream({"messages": [input_message]}, config, stream_mode="updates"):
        print_update(event)
    
    
    
    

!\[\[Example\_SummaryOfConversation.ipynb\]\]

LangChain Expression Language (LCEL)
------------------------------------

支持通过 (LCEL) 表达式的形式, 任意自定义链.  
[https://python.langchain.com/v0.2/docs/how\_to/#langchain-expression-language-lcel](https://python.langchain.com/v0.2/docs/how_to/#langchain-expression-language-lcel)

Runtable
--------

[https://python.langchain.com/v0.2/docs/how\_to/sequence/](https://python.langchain.com/v0.2/docs/how_to/sequence/)

预防老年痴呆，保持终身学习! —— [daidaidaiyu](https://www.cnblogs.com/dddy/)