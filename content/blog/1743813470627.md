---
layout: post
title: 'AI可解释性 II | Saliency Maps-based 归因方法（Attribution）论文导读（持续更新）'
date: "2025-04-05T00:37:50Z"
---
AI可解释性 II | Saliency Maps-based 归因方法（Attribution）论文导读（持续更新）
===========================================================

本文作为AI可解释性系列的第二部分，旨在以汉语整理并阅读归因方法（Attribution）相关的论文，并持续更新。 归因方法主要研究如何解释深度神经网络的决策过程，通过识别输入特征对模型输出的贡献程度，对模型的决策过程输出为人类可以理解的图像或者量化指标，帮助我们理解模型的决策依据。

AI可解释性 II | Saliency Maps-based 归因方法（Attribution）论文导读（持续更新）
===========================================================

导言
--

本文作为AI可解释性系列的第二部分，旨在以汉语整理并阅读归因方法（Attribution）相关的论文，并持续更新。

归因方法主要研究如何解释深度神经网络的决策过程，通过识别输入特征对模型输出的贡献程度，对模型的决策过程输出为人类可以理解的图像或者量化指标，帮助我们理解模型的决策依据。

Saliency Maps-based的归因方法给定一个大小为CxWxH的图像输入，通过反向传播等方法找出原图每一个像素在模型最后的输出中贡献的比重，最后输出一张和原图大小为CxWxH一样的Saliency Maps

Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps（Apr 2014）
-------------------------------------------------------------------------------------------------------

作者：Karen Simonyan, Andrea Vedaldi, Andrew Zisserman

### 简介

本文乃是Saliency Maps-based归因方法领域的奠基性工作之一，提出了三种可视化的模型解释方法：

1.  类别模型可视化（Class Model Visualization）：通过优化输入图像使得特定类别的分类分数最大化，从而生成能代表该类别的合成图像。
2.  指定图片的类别显著可视化（Image-Specific Class Saliency Visualisation）：通过计算类别得分对输入图像的梯度，生成显示每个像素对分类决策重要性的热力图。（Saliency Map归因方法）
3.  反卷积的特征可视化（Feature Visualization）：使用反卷积网络将深层特征映射回输入空间，以可视化网络在不同层级学到的特征。

### 相关工作

#### “activation maximization”

作者受到了文章_Visualizing Higher-Layer Features of a Deep Network_的启发，该篇文章给出了一个对神经网络中某一个具体神经元的可视化方法称为“activation maximization”。其原理非常简单，从已经训练好的网络抽出一个神经元，然后通过梯度下降的方法，找到一个输入图像 \\(x\\) 使得该神经元的激活值最大化：

\\\[x^\* = \\arg\\max\_x h\_{ij}(\\theta, x) \\quad \\text{s.t.} \\quad \\|x\\| = \\rho \\\]

其中 \\(h\_{ij}(\\theta, x)\\) 是第 \\(i\\) 层第 \\(j\\) 个神经元的激活值，\\(\\theta\\) 是网络参数（冻结），\\(\\rho\\) 是约束输入图像范数的常数。通过这种方式不断优化\\(x\\)，我们可以得到一个能够最大程度激活该神经元的图像，从而理解这个神经元学到了什么样的特征。通过实验可以看到这种方法确实解释了神经元（左侧），并且通过九种随机初始化验证这种解释是收敛的（右侧）。

![](https://img2024.cnblogs.com/blog/1887071/202504/1887071-20250404021855437-1354115138.png)

### 类别模型可视化（Class Model Visualization）

作者将"activation maximization"的思想扩展到了类别可视化上。对于一个已经训练好的分类卷积网络，我们可以通过优化输入图像来最大化某个类别的分类分数，从而生成能代表该类别的合成图像。

具体来说，给定一个类别c，我们要找到一个输入图像x使得该类别的分类分数Sc(x)最大化：

\\\[I^\* = \\arg\\max\_I (S\_c(I) - \\lambda \\|I\\|\_2^2) \\\]

其中Sc(x)是类别c的分类分数，λ是正则化系数。通过梯度上升的方法不断优化x，最终得到的图像x\*就是该类别的可视化结果。

值得注意的是：

1.  初始化图像是全零的图像（因为用于训练的训练集是已经中心化的）
2.  \\(S\_c\\)是\\(c\\)类未经归一化的分类分数，而非softmax分数，以降低其他类别的干扰

作者展示了通过在ILSVRC-2013上训练的卷积神经网络学习到的类别外观模型。注意到在单一图像中捕捉到的类别外观的不同方面。

![](https://img2024.cnblogs.com/blog/1887071/202504/1887071-20250404021931927-2060317387.png)

### 特定图片类别的显著性可视化（Image-Specific Class Saliency Visualisation）

首先我们定义是如何查询给定图片对于某个类别的空间贡献：

> In this section we describe how a classification ConvNet can be queried about the spatial support of a particular class in a given image.

给定图片\\(I\_0\\)，类别\\(c\\)，以及一个分类卷积网络，其分类分数的函数为\\(S\_c(I)\\)，我们希望衡量\\(I\_0\\)中每个像素对分类分数\\(S\_c(I)\\)的贡献。

我们首先给出一个线性例子来理解显著性可视化的原理。假设我们有一个简单的线性分类器，其分类分数可以表示为：

\\\[S\_c(I) = w\_c^T I + b\_c \\\]

其中\\(w\_c\\)是类别\\(c\\)的权重向量，\\(b\_c\\)是偏置项。在这种情况下，输入\\(I\\)中每个像素对于输出的重要程度可以直接由权重向量\\(w\_c\\)的对应分量的绝对值来衡量。这是因为每个输入像素都与权重向量的对应分量进行线性组合，权重的绝对值越大，说明该像素对最终分类分数的影响越大。

这意味着权重的绝对值直接反映了每个输入维度对分类决策的重要性。这个简单的线性例子帮助我们理解了在深度神经网络中使用梯度来衡量输入特征重要性的基本原理。

接下来，我们将模型换成深层的卷积网络，其分类分数\\(S\_c(I)\\)将变为高度非线性的函数。但是给定\\(I\_0\\)，我们可以通过一阶泰勒近似来线性化这个函数，得到：

\\\[S\_c(I) \\approx w^TI+b \\\]

其中\\(w\\)是分类分数\\(S\_c(I)\\)在\\(I\_0\\)点的导数。

\\\[w= \\frac{\\partial S\_c}{\\partial I}|\_{I\_0} \\\]

除此以外，\\(w= \\frac{\\partial S\_c}{\\partial I}|\_{I\_0}\\)还可以被解释为在这个方向上扰动样本使其对分类分数的影响最大，文章_How to Explain Individual Classification Decisions_在贝叶斯分类器中采用过类似的方法。

> Another interpretation of computing the image-specific class saliency using the class score derivative (4) is that the magnitude of the derivative indicates which pixels need to be changed the least to affect the class score the most. ... We note that a similar technique has been previously applied by \[1\] in the context of Bayesian classification.

作者展示了针对ILSVRC-2013测试图像中预测的Top-1类别生成的特定图像类别显著性图，这些图通过对分类卷积网络进行一次反向传播提取。

![](https://img2024.cnblogs.com/blog/1887071/202504/1887071-20250404022044323-22346051.png)