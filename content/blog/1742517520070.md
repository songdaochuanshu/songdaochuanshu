---
layout: post
title: 'Ollama系列05：Ollama API 使用指南'
date: "2025-03-21T00:38:40Z"
---
Ollama系列05：Ollama API 使用指南
==========================

本文是Ollama系列教程的第5篇，在前面的4篇内容中，给大家分享了如何再本地通过Ollama运行DeepSeek等大模型，演示了chatbox、CherryStudio等UI界面中集成Ollama的服务，并介绍了如何通过cherryStudio构建私有知识库。

在今天的分享中，我将分享如何通过API来调用ollama服务，通过整合ollama API，将AI能力集成到你的私有应用中，提升你的职场价值！

Ollama API
----------

![](https://img2024.cnblogs.com/blog/44814/202503/44814-20250320012921245-1176095755.png)

Ollama 提供了一套简单好用的接口，让开发者能通过API轻松使用大语言模型。

本篇内容将使用Postman作为请求工具，和开发语言无关。

### 基本概念

在开始之前，我们先了解几个基本的概念：

*   **Model**：模型，我们调用接口时使用的模型名字。我们可以把Ollama理解为模型商店，它里面运行着很多模型，每个模型都有一个唯一的名字，例如`deepseek-r1:1.5b`
*   **Prompt**: 提示词，是我们给模型的指令。比如`天空为什么是蓝色的`就是一条简单的提示词。
*   **Token**：字符块，是大模型的最小输出单位，同时也是大模型的计费单位。举个例子，对于`天空为什么是蓝色的`这句话，大模型会进行拆分`天空/为什么/是/蓝色/的`，每一段就是一个token（实际情况会比这个例子复杂）

### 内容生成（/api/generate）

让大模型帮我们生成指定的内容，就可以使用内容生成接口。一问一答，不带上下文。

我们试着用最少的参数来调用：

    {
      "model": "deepseek-r1:1.5b",
      "prompt": "天空为什么是蓝色的"
    }
    

在postman里面看看输出：

![](https://img2024.cnblogs.com/blog/44814/202503/44814-20250320012921349-1582862811.png)

可以看到输出的内容很长，这是因为默认采用的是stream的方式输出的，也就是我们在deepseek app里面看到的一个字一个字输出的那种效果。我们可以将stream参数设置成false来禁用流式输出。

    {
        "model": "deepseek-r1:1.5b",
        "prompt": "天空为什么是蓝色的",
        "stream": false
    }
    

**参数列表**

参数名

是否必填

描述

`model`

是

模型名称

`prompt`

是

需要生成响应的提示词

`suffix`

否

模型响应后追加的文本

`images`

否

Base64编码的图片列表（适用于多模态模型如llava）

`format`

否

返回响应的格式（可选值：`json` 或符合 JSON Schema 的结构）

`options`

否

模型额外参数（对应 Modelfile 文档中的配置如 `temperature`）

`system`

否

自定义系统消息（覆盖 Modelfile 中的定义）

`template`

否

使用的提示词模板（覆盖 Modelfile 中的定义）

`stream`

否

设为 `false` 时返回单个响应对象而非流式对象

`raw`

否

设为 `true` 时不格式化提示词（适用于已指定完整模板的情况）

`keep_alive`

否

控制模型在内存中的保持时长（默认：5m）

`context`

否

（已弃用）来自前次 `/generate` 请求的上下文参数，用于维持短期对话记忆

### 生成对话（/api/chat）

生成对话，是一种具备上下文记忆的内容生成。在内容生成API中，我们仅传入了prompt，大模型仅对我们本地的prompt进行回答，而在生成对话API中，我们还可以传入messages参数，包含我们多轮对话内容，使大模型具备记忆功能。

**最简单的调用**（为了方便演示，我们将stream参数设置为false）：

    {
        "model": "deepseek-r1:1.5b",
        "messages": [
            {
                "role": "user",
                "content": "天空通常是什么颜色"
            }
        ],
        "stream": false
    }
    

postman调用截图：  
![](https://img2024.cnblogs.com/blog/44814/202503/44814-20250320012921240-1267713540.png)

**多轮对话**

聊天的时候，ollama通过messages参数保持上下文记忆。当模型给我们回复内容之后，如果我们要继续追问，则可以使用以下方法（**注意：deepseek-r1模型需要在上下文中移除think中的内容**）：

    {
        "model": "deepseek-r1:1.5b",
        "messages": [
            {
                "role": "user",
                "content": "天空通常是什么颜色"
            },
            {
                "role": "assistant",
                "content": "天空通常看起来是**柔和的、明快的或稍微有些昏黄的色调**。具体颜色可能会因不同的天气情况而有所变化，例如：\n\n1. **晴朗天气**：天空可能呈现出温暖、明亮的颜色，比如蓝天、碧空等。\n2. **下雨天**：云层覆盖天空，可能导致颜色较为阴郁或变黑。\n3. **雨后天气**：雨后的天空可能恢复为明亮的色调。\n\n总的来说，天空的颜色主要取决于大气中的光线反射和折射情况，以及太阳的位置。"
            },
            {
                "role": "user",
                "content": "为什么是蓝色的？"
            }
        ],
        "stream": false
    }
    

postman调用截图：  
![](https://img2024.cnblogs.com/blog/44814/202503/44814-20250320012921206-1717694407.png)

**结构化数据提取**

当我们和系统对接时，通常要需要从用户的自然语言中提到结构化数据，用来调用现有的外部系统的接口。在ollama中我们只需要指定format参数，就可以实现结构化数据的提取：

    {
        "model": "deepseek-r1:1.5b",
        "messages": [
            {
                "role": "user",
                "content": "哈喽，大家好呀~ 我是拓荒者IT，今年36岁了，是一名软件工程师"
            }
        ],
        "format": {
            "type": "object",
            "properties": {
                "name": {
                    "type": "string"
                },
                "age": {
                    "type": "integer"
                },
                "job": {
                    "type": "string"
                }
            },
            "required": [
                "name",
                "age",
                "job"
            ]
        },
        "stream": false
    }
    

**参数列表**

参数名

是否必填

描述

`model`

是

模型名称

`messages`

是

聊天消息数组（用于维持对话记忆）

`messages.role`

是

消息角色（可选值：`system`, `user`, `assistant`, `tool`）

`messages.content`

是

消息内容

`messages.images`

否

消息中Base64编码的图片列表（适用于多模态模型如llava）

`messages.tool_calls`

否

模型希望调用的工具列表（JSON格式）

`tools`

否

模型可使用的工具列表（JSON格式，需模型支持）

`format`

否

返回响应的格式（可选值：`json` 或符合 JSON Schema 的结构）

`options`

否

模型额外参数（对应 Modelfile 文档中的配置如 `temperature`）

`stream`

否

设为 `false` 时返回单个响应对象而非流式对象

`keep_alive`

否

控制模型在内存中的保持时长（默认：5m）

### 生成嵌入数据（/api/embed）

嵌入数据的作用是将输入内容转换成向量，可以用于向量检索等场景。比如我们在第四篇中介绍的知识库，就需要用到embedding模型。

在调用embed接口时，我们要选择支持Embedding功能的模型，deepseek是不支持的。

调用示例：

    {
      "model": "bge-m3",
      "input": "为什么天空是蓝色的呢？"
    }
    

postman调用截图：  
![](https://img2024.cnblogs.com/blog/44814/202503/44814-20250320012921288-327780697.png)

### 兼容openAI接口

因为现在很多应用、类库都是基于OpenAI构建的，为了让这些系统能够使用Ollama提供的模型，Ollama提供了一套兼容OpenAI的接口（官方说是实验性的，以后可能会有重大调整）。

因为这种兼容，使得我们可以直接通过OpenAI的python库、node库来访问ollama的服务，确实方便了不少。

**注意：ollama属于第三方接口，不能100%支持OpenAI的接口能力，因此在使用的时候需要先了解清楚兼容的情况。**

### 其它接口

ollama还有一些其它的接口，用来实现对模型的管理等功能，而这些功能我们通常会在命令行完成，因此不做详细说明。这些API的列表如下：

*   模型创建（/api/create）
*   列出本地模型（/api/tags）
*   查看模型信息（/api/show）
*   复制模型（/api/copy）
*   删除模型（/api/delete）
*   拉取模型（/api/pull）
*   推送（上传）模型（/api/push）
*   列出运行中的模型（/api/ps）
*   查看ollama版本（/api/version）

这些接口的调用都非常简单，大家感兴趣的可以尝试以下。

总结
--

本文介绍了ollama api的用法，详细介绍了生成内容、多轮对话、大模型记忆功能的实现、提取参数化内容等接口。通过使用这些接口，我们可以将AI能力集成到现有的项目中，提升软件的智能化程度。

👉 **如果你对这些内容感兴趣，关注\[拓荒者IT\]公众号，获取最新的文章内容。**  
**持续分享AI工具，AI应用场景，AI学习资源** ❤️

![](https://img2024.cnblogs.com/blog/44814/202503/44814-20250320012921291-1664607318.png)

**参考内容**

1.  官方API文档：[https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)
    
2.  官方兼容OpenAI文档：[https://github.com/ollama/ollama/blob/main/docs/openai.md](https://github.com/ollama/ollama/blob/main/docs/openai.md)