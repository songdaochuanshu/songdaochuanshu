---
layout: post
title: '论文解读：One-shot Entropy Minimization'
date: "2026-01-02T00:45:29Z"
---
论文解读：One-shot Entropy Minimization
==================================

一篇还未发表的论文，但做了大量实验对熵最小化技巧提升模型推理能力进行了探索。本文训练了13440个大型语言模型，发现熵最小化（EM）只需要一个未标记的数据和10步优化，性能提升就比**RL**还强。基于两个直接而简单的假设：

*   生成大型语言模型的采样过程本质上是随机的。
    
*   正确答案的熵通常低于错误答案。
    

EM和RL有**共同目标**：在不添加新知识的情况下释放预训练模型的潜在潜力，都依赖于所谓“token重评级”。

原文网址：[https://arxiv.org/abs/2505.20282v4](https://arxiv.org/abs/2505.20282v4)

方法
==

给定prompt $x$，生成序列$y$，最小化生成序列token级别的预测熵：

$\\displaystyle L\_{\\mathrm{EM}} = \\frac{1}{|y|} \\sum\_{t = 1}^{|y|} H\_t  $

$\\displaystyle H\_t = -\\sum\_{v \\in V} p\_\\theta(v \\mid y\_{<t}, x)\\,\\log p\_\\theta(v \\mid y\_{<t}, x)$

训练对单个样本进行，从未标注数据中筛选（2.2节）：模型在多次采样下表现出较高的 pass@k 方差，从而对“熵”更敏感，提供更强的**熵梯度**信号。所谓pass@k 方差就是大模型针对同一个数据生成$k$段内容，计算$k$个内容正确与否的方差。

实验
==

**训练设置：**针对特定样本，每次生成64段推理内容（即训练批量为64），用以上最小熵损失微调大模型10次。预测温度设置为0.5，学习率固定为$2\\times 10^{-5}$。

表1：各种推理模型与EM的对比。

图1：用了EM之后，模型生成token 的概率分布。EM使分布更右偏。

图2：EM损失和评估分数随着训练的变化曲线。

图3/4：不同温度对性能的影响。

图5：EM和RL先后执行，性能随迭代的变化。发现RL之后应用EM会导致性能下降，而EM之后RL性能会提升。这表明EM加剧了RL引入的分配扭曲，强化了RL的“对齐税”。

表2：多样本的EM与单样本的EM对比，看出多样本也没有多大优势。

图6：不同学习率对推理性能的影响。

讨论与见解
=====

单次EM增益的上限由基础模型的内在推理能力决定。在相对较弱的LLaMA-3.1-8B上，单次EM的平均准确率仅提高到24.3%，勉强超过23.6%的基线。这表明，当底层模型缺乏足够的推理能力时，最小EM优化无法完全弥补其不足。

RL根据外部真实奖励调整令牌概率。这通常会促进以前低概率（尾部）代币的相对排名。即使在重新排序后，这些代币也往往占据概率分布的中间位置，需要在采样过程中选择更高的温度。因此，RL训练的模型表现出相反的趋势：性能随着采样温度的升高而提高，如图4所示。

在大多数场景中，尤其是在SFT和训练最少的RL环境中，EM显著地修剪了冗余的决策路径，稳定了关键预测，验证了其作为最小但强大的优化策略的有效性和通用性。