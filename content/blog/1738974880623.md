---
layout: post
title: '大模型背后的向量魔法：Embedding技术初探'
date: "2025-02-08T00:34:40Z"
---
大模型背后的向量魔法：Embedding技术初探
========================

本文原本是2022年写的，然而一直没有完善😂，自从LLM火起来之后，NLP领域的技术更新很快，本文只是大概介绍了Embedding相关的基础知识，具体应用接下来会在博客更新发布。

前言
--

又是很长一段时间没更新博客了，这个暑假非常忙，也稍微做出了一点成果，接下来会继续在博客上分享~

今年以来，大模型以及相关的应用很火，其中就有一种叫「向量数据库」的东西，向量数据库主要用来存储向量和提供向量查询功能，其中用到的向量如何生成？这就不得不介绍一下 `Embedding` 技术了。

> Embedding起源于Word Embedding，经过多年的发展，已取得长足进步。从横向发展来看，由原来单纯的Word Embedding，发展成现在的Item Embedding、Entity Embedding、Graph Embedding、Position Embedding、Segment Embedding等；从纵向发展来看，由原来静态的Word Embedding发展成动态的预训练模型，如ELMo、BERT、GPT、GPT-2、GPT-3、ALBERT、XLNet等，这些预训练模型可以通过微调服务下游任务。Embedding不再固定不变，从而使这些预训练模型可以学习到新的语义环境下的语义，高效完成下游的各种任务，如分类、问答、摘要生成、阅读理解等，其中有很多任务的完成效率已超过人工完成的平均水平。

Embedding
---------

近些年在机器学习、深度学习等领域，嵌入（Embedding）技术可谓发展迅猛、遍地开花。

简单来说，嵌入是用向量表示一个物体，这个物体可以是一个单词、一条语句、一个序列、一件商品、一个动作、一本书、一部电影等，可以说嵌入涉及机器学习、深度学习的绝大部分对象。这些对象是机器学习和深度学习中最基本、最常用、最重要的对象，正因如此，如何有效表示、学习这些对象就显得非常重要。尤其word2vec这样的Word Embedding的广泛应用，更是带来了更大范围的延伸和拓展，嵌入技术由最初的自然语言处理领域向传统机器学习、搜索排序、推荐、知识图谱等领域延伸，具体表现为由Word Embedding向Item Embedding、Graph Embedding、Categorical variables Embedding等方向延伸。

Embedding本身也在不断更新，由最初表现单一的静态向表现更丰富的动态延伸和拓展。具体表现为由静态的Word Embedding向ELMo、Transformer、GPT、BERT、XLNet、ALBERT等动态的预训练模型延伸。

Embedding可以做很多事，主要涉及这些内容：

*   Word Embedding
*   Item Embedding
*   用Embedding处理分类特征
*   Graph Embedding
*   Contextual Word Embedding
*   使用Word Embedding实现中文自动摘要

但由于篇幅关系，本文只选择跟NLP与知识库问答有关的内容介绍。

处理序列问题的一般步骤
-----------

序列问题是非常常见的，如自然语言处理、网页浏览、时间序列等都与序列密不可分。因此，如何处理序列问题、如何挖掘序列中隐含的规则和逻辑非常重要。

以自然语言处理为例。假设你拿到一篇较长文章或新闻报道之类的语言材料，要求用自然语言处理（NLP）方法提炼出该材料的摘要信息，你该如何处理？需要考虑哪些内容？涉及哪些步骤？先从哪一步开始？

拿到一份语言材料后，不管是中文还是英文，首先需要做一些必要的清理工作，如清理特殊符号、格式转换、过滤停用词等，然后进行分词、索引化，再利用相关模型或算法把单词、词等标识符向量化，最后输出给下游任务。

![](https://img2024.cnblogs.com/blog/866942/202502/866942-20250207172112289-771133252.png)

如图，词嵌入或预训练模型是关键，它们的质量好坏直接影响下游任务的效果。词嵌入与训练模型阶段涉及的算法、模型较多，近几年也取得了长足发展，如word2vec、Transformer、BERT、ALBERT等方法，刷新了自然语言处理、语言识别、推荐任务、搜索排序等任务在性能方面的纪录。

Word Embedding
--------------

机器无法直接接收单词、词语、字符等标识符（token），最开始的思路是用整数表示各标识符，这种方法简单但不够灵活。

后来又改成用独热编码（One-Hot Encoding）来表示，这种方法虽然方便，但非常稀疏，属于硬编码，且无法重载更多信息。

接着，大佬们又想到用数值向量或标识符嵌入（Token Embedding）来表示，即通常说的词嵌入（Word Embedding），又称为分布式表示。

这几种方式的区别，我直接盗图了~

![](https://img2024.cnblogs.com/blog/866942/202502/866942-20250207172121058-964582975.png)

独热编码是稀疏、高维的硬编码，如果一个语料有一万个不同的词，那么每个词就需要用一万维的独热编码表示。如果用向量或词嵌入表示，那么这些向量就是低维、密集的，且这些向量值都是通过学习得来的，而不是硬性给定的。

### Word Embedding 的学习方法

主要分两种

*   利用平台的 Embedding 层学习 Word Embedding ，在完成任务的同时学习词嵌入，例如，把Embedding作为第一层，先随机初始化这些词向量，然后利用平台（如PyTorch、TensorFlow等平台）不断学习（包括正向学习和反向学习），最后得到需要的词向量。
*   使用预训练模型，现在有很多在较大语料上预训练好的词嵌入或预训练模型，可以直接使用，在没有足够质量的语料之前，使用预训练模型的效果可能会比自己训练的更好。

具体的代码实现我后面的文章会介绍。

Item Embedding
--------------

Embedding是一种很好的思想，它不局限于自然语言处理领域，还可以应用到其他很多领域。

微软的团队写了一篇论文「[Item2Vec：Neural ItemEmbedding for Collaborative Filtering](https://arxiv.org/abs/1603.04259)」，实用性极强，极大拓展了word2vec的应用范围，使其从NLP领域直接扩展到推荐、广告、搜索排序等任何可以生成序列的领域。

处理分类特征
------

在传统机器学习中，输入数据往往包含分类特征，而对这些分类特征的处理是特征工程的关键环节之一。分类特征，也称为离散特征，其数据类型通常为object。然而，大多数机器学习模型仅能处理数值型数据，因此需要将分类数据（Categorical data）转换为数值数据（Numeric data）。

Categorical特征可以分为有序（Ordinal）类型和无序（Nominal）类型两类。

这两类数据可以通过不同的方法转换为数字。对于Nominal类型数据，通常使用独热编码（One-Hot Encoding）来进行转换。然而，当面对大规模数据时，例如某一特征有几百、几千甚至更多类别，使用独热编码会导致特征维度急剧增加，产生庞大的特征矩阵。另外，独热编码仅将类别数据转化为0或1，无法准确表达类别之间的潜在规则或分布特性。例如，若有一个表示地址的特征，其中包括北京、上海、杭州、纽约、华盛顿、洛杉矶、东京、大阪等地，这些地理位置之间存在一定的空间关系——北京、上海、杭州的距离较近，上海与纽约之间则相距较远，而独热编码无法反映这些地理分布的规律。

近年来，神经网络和深度学习在计算机视觉、自然语言处理以及时间序列预测等领域的应用逐渐普及。在深度学习应用中，Embedding作为一种将离散变量转化为连续向量的技术，极大地促进了传统机器学习和神经网络的发展。Embedding技术目前主要有两种应用：一种是自然语言处理中的Word Embedding，另一种是用于类别数据的Entity Embedding。简单来说，Embedding是用低维向量来表示一个对象，这个对象可以是一个词、一个类别特征（如商品、电影、物品等）或时间序列特征等。通过学习，Embedding向量能够更加精确地捕捉特征的内在含义，从而使得几何上距离较近的向量所代表的对象具有相似的语义。

Graph Embedding
---------------

在前面的内容中，我们探讨了基于Word Embedding衍生出的Item Embedding等技术，这些技术的开发都依赖于其具备的序列特征。然而，实际上可以拓展应用的领域远不止于此。一些乍看之下与序列无关的场景，经过适当的调整和变化后，同样可以应用这些技术。

Graph Embedding与Word Embedding类似，旨在通过低维、密集且实值的向量来表示网络中的节点。如今，Graph Embedding在推荐系统、搜索排序、广告投放等多个领域广受欢迎，并且在实际应用中表现卓越，取得了显著的效果。

图（Graph）是一种“二维”关系的表达方式，而序列（Sequence）则体现了一种“一维”关系。因此，将图转换为图嵌入（Graph Embedding）时，通常需要先借助特定算法将图结构转化为序列形式；随后再利用相关模型或算法将这些序列进一步转化为嵌入向量（Embedding）。

常用方法有以下（由于篇幅限制，这里就不展开了）

*   DeepWalk
*   LINE
*   node2vec

> 应用例子：推荐系统
> 
> 近年来，众多研究者致力于将知识图谱融入推荐系统，以有效应对传统推荐系统面临的稀疏性与冷启动难题。目前，将知识图谱特征学习应用于推荐系统的方法大致可分为三种：一是依次学习（One-by-One Learning），即分别独立学习知识图谱与推荐模型；二是联合学习（Joint Learning），即同时优化知识图谱与推荐系统的目标函数；三是交替学习（Alternate Learning），即交替迭代优化知识图谱与推荐模型。

Contextual Word Embedding
-------------------------

之前我们提到的Word Embedding，因word2vec的流行而广受关注。与离散的独热编码相比，它具有显著优势：一方面降低了数据维度，另一方面能够捕捉语义空间中的线性关系，例如“国王 - 王后 ≈ 男 - 女”这样的语义相似性。因此，word2vec及其类似方法几乎成为所有深度学习模型的标配。然而，这种表示方法也有局限性。它是基于语料库生成的固定字典，每个单词对应一个固定长度的向量。当遇到一词多义的情况时，就无法准确区分不同语义，从而显得力不从心。

如何解决一词多义的问题？

由于word2vec生成的词嵌入具有固定不变的特性，因此人们将其称为静态词嵌入。这种嵌入方式不考虑上下文环境，无论单词出现在何种语境中，其向量表示始终保持一致。因此，若需要考虑上下文的影响，就不能依赖静态词嵌入，而应采用动态词嵌入方法，或者结合预训练模型与微调的方法来处理。例如，ELMo、GPT、GPT-2、BERT、ERNIE、XLNet、ALBERT等模型都属于动态词嵌入的范畴。这些方法极大地提升了自然语言处理领域的性能，并且目前仍在快速演进中。

小结
--

mbedding几乎无处不在，无论是传统机器学习、推荐系统，还是深度学习中的自然语言处理，甚至图像处理，都涉及Embedding技术问题。从一定意义上来说，把Embedding做好了，整个项目的关键难题就攻克了。

微信公众号：「程序设计实验室」 专注于互联网热门新技术探索与团队敏捷开发实践，包括架构设计、机器学习与数据分析算法、移动端开发、Linux、Web前后端开发等，欢迎一起探讨技术，分享学习实践经验。