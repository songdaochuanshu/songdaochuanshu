---
layout: post
title: '一步一步教你部署ktransformers，大内存单显卡用上Deepseek-R1'
date: "2025-03-23T00:41:57Z"
---
一步一步教你部署ktransformers，大内存单显卡用上Deepseek-R1
=========================================

环境准备
====

硬件环境
----

CPU：intel四代至强及以上，AMD参考同时期产品  
内存：800GB以上，内存性能越强越好，建议DDR5起步  
显卡：Nvidia显卡，单卡显存至少24GB（用T4-16GB显卡实测会在加载模型过程中爆显存），nvidia compute capability至少8.0（[CUDA GPUs - Compute Capability | NVIDIA Developer](https://developer.nvidia.com/cuda-gpus)）

系统环境
----

Ubuntu Server 24.04 LTS

CUDA 12.4
---------

### 屏蔽开源nvidia显卡驱动

    # 编辑黑名单配置，屏蔽开源nvidia显卡驱动，以安装官方驱动
    sudo vi /etc/modprobe.d/blacklist.conf
    

在文件的最后添加下面两行

    blacklist nouveau
    options nouveau modeset=0
    

输入下面的命令更新并重启

    sudo update-initramfs -u
    sudo reboot
    

继续执行命令

    lsmod | grep nouveau # 验证是否禁用成功，成功的话这行命令不会有输出
    sudo apt-get purge nvidia* # 卸载已有的驱动
    

### 安装cuda工具包

将[cuda工具包](https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=runfile_local)下载好后（不要盲目追求新版，平台会不适配），在驱动所在目录执行（选择安装选项中，默认选中安装显卡驱动）

    sudo chmod +x cuda_12.4.0_550.54.14_linux.run
    sudo sh cuda_12.4.0_550.54.14_linux.run
    

设置全局环境变量

    sudo vi /etc/profile.d/myenv.sh
    

    export PATH="$PATH:/usr/local/cuda-12.4/bin"
    export LD_LIBRARY_PATH="/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH"
    export LIBRARY_PATH="/usr/local/cuda-12.4/lib64:$LIBRARY_PATH"
    export CUDA_PATH="/usr/local/cuda-12.4"
    

重启使生效，指令`nvcc -V`测试

### 安装cuDNN

将[cuDNN](https://developer.nvidia.com/cudnn-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=24.04&target_type=deb_local)下载好后，在所在目录执行

    sudo dpkg -i cudnn-local-repo-ubuntu2404-9.7.1_1.0-1_amd64.deb
    sudo cp /var/cudnn-local-repo-ubuntu2404-9.7.1/cudnn-*-keyring.gpg /usr/share/keyrings/
    sudo apt-get update
    sudo apt-get -y install cudnn-cuda-12
    

更新cuDNN时，在apt-get update前删除旧版，比如

    sudo rm -rf /var/cudnn-local-repo-ubuntu2404-9.6.0
    sudo rm cudnn-local-ubuntu2404-9.6.0.list
    

系统软件包
-----

    sudo apt-get update
    sudo apt-get install build-essential cmake ninja-build
    

Miniconda3
----------

[Miniconda3安装脚本](https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh)

### conda环境创建

访问pypi困难的小伙伴可以参考换源方法：  
[https://mirrors.huaweicloud.com/mirrorDetail/5ea14ecab05943f36fb75ee6?mirrorName=python&catalog=tool](https://mirrors.huaweicloud.com/mirrorDetail/5ea14ecab05943f36fb75ee6?mirrorName=python&catalog=tool)

    conda create --name ktransformers python=3.11
    conda activate ktransformers # 首次使用你可能需要执行 ‘conda init’ 并重开终端
    
    conda install -c conda-forge libstdcxx-ng # Anaconda provides a package called `libstdcxx-ng` that includes a newer version of `libstdc++`, which can be installed via `conda-forge`.
    
    strings ~/miniconda3/envs/ktransformers/lib/libstdc++.so.6 | grep GLIBCXX
    
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 pip3 install packaging ninja cpufeature numpy
    

注：download.pytorch.org不挂代理的话速度很慢。截至2025年3月22日，pytorch不加--index-url参数默认即 whl/cu124，所以可以去掉--index-url参数以事先设置好的全局国内源下载。实时具体情况可在[Start Locally | PyTorch](https://pytorch.org/get-started/locally/)查看。

flash-attention
---------------

[flash-attention下载页](https://github.com/Dao-AILab/flash-attention/releases)  
注：打开链接后，点击show more可以查看更多版本，根据以上安装的版本信息下载相应版本，一般cxx11abi为FALSE

安装ktransformers
===============

### 拉取源码并编译

注：访问不了github的小伙伴可以在网上搜索github换源方法

    git clone https://github.com/kvcache-ai/ktransformers.git
    cd ktransformers
    git submodule init
    git submodule update
    

安装  
**如果服务器是2CPU+2倍模型大小及以上内存的需要先执行：**

     # Make sure your system has dual sockets and double size RAM than the model's size (e.g. 1T RAM for 512G model)
    apt install libnuma-dev
    export USE_NUMA=1
    

开始安装

    bash install.sh
    

启用服务
====

下载模型
----

### 模型下载源

1.  [unsloth/DeepSeek-R1-Q4\_K\_M](https://hf-mirror.com/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-Q4_K_M)
2.  ollama拉取后，在blob内找到最大的文件，添加后缀.gguf

### 模型存放

以下按照模型存放于`/mnt/data/models/DeepSeek-R1-Q4_K_M_GGUF/`为例  
将下载好的gguf文件全部放入该文件夹  
然后下载  
[config.json](https://hf-mirror.com/unsloth/DeepSeek-R1-GGUF/blob/main/config.json)  
[configuration\_deepseek.py](https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/configuration_deepseek.py)  
[generation\_config.json](https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/generation_config.json)  
[model.safetensors.index.json](https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/model.safetensors.index.json)  
[modeling\_deepseek.py](https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/modeling_deepseek.py)  
[tokenizer.json](https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/tokenizer.json)  
[tokenizer\_config.json](https://hf-mirror.com/deepseek-ai/DeepSeek-R1/blob/main/tokenizer_config.json)  
存入该文件夹

启用RESTful API服务
---------------

    ktransformers --model_name DeepSeek-R1-q4_k_m --model_path /mnt/data/models/DeepSeek-R1-Q4_K_M_GGUF --gguf_path /mnt/data/models/DeepSeek-R1-Q4_K_M_GGUF --optimize_config_path ~/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml --port 10002 --cpu_infer 65 --max_new_tokens 2048 --force_think
    

注：

1.  \--optimize\_config\_path后面的文件夹位置相应换成你git clone ktransformers时的位置
2.  The command args `--cpu_infer 65` specifies how many cores to use (it's ok that it exceeds the physical number, but it's not the more the better. Adjust it slightly lower to your actual number of cores)

API兼容OpenAI和Ollama

测试效果
====

试验环境
----

CPU：至强Gold-6454S-2.20GHz@32核心 x2  
内存：DDR5-64G x16  
显卡：NV L20-48G x4  
硬盘：1.92TB SATA SSD x2  
RAID卡：1G缓存 带电容  
网卡：2个千兆电口+2个万兆光口(含光模块)

效果截图
----

![](https://img2024.cnblogs.com/blog/3538846/202503/3538846-20250322135005281-1135003073.png)

![](https://img2024.cnblogs.com/blog/3538846/202503/3538846-20250322135013102-1193998556.png)  
测试每秒tokens在6-13tokens/s左右

![](https://img2024.cnblogs.com/blog/3538846/202503/3538846-20250322135019544-2128946462.png)

![](https://img2024.cnblogs.com/blog/3538846/202503/3538846-20250322135024093-1639141160.png)