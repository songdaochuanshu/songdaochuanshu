---
layout: post
title: 'ElasticSearchæ˜¯ä»€ä¹ˆï¼Ÿ'
date: "2025-08-28T00:39:34Z"
---
ElasticSearchæ˜¯ä»€ä¹ˆï¼Ÿ
=================

ElasticSearchæ˜¯ä»€ä¹ˆ
================

ğŸ“– æ¦‚è¿°
-----

ElasticSearch (ES) æ˜¯ä¸€ä¸ªåŸºäºApache Luceneæ„å»ºçš„åˆ†å¸ƒå¼ã€å®æ—¶æœç´¢å’Œåˆ†æå¼•æ“ã€‚å®ƒå°†å•æœºçš„Luceneæœç´¢åº“æ‰©å±•ä¸ºåˆ†å¸ƒå¼æ¶æ„ï¼Œæä¾›äº†å¼ºå¤§çš„å…¨æ–‡æœç´¢ã€ç»“æ„åŒ–æœç´¢å’Œåˆ†æèƒ½åŠ›ã€‚ESåœ¨æ—¥å¿—åˆ†æã€åº”ç”¨æœç´¢ã€å•†å“æ¨èç­‰åœºæ™¯ä¸­è¢«å¹¿æ³›åº”ç”¨ã€‚

ğŸ” æ ¸å¿ƒå­˜å‚¨ç»“æ„è¯¦è§£
-----------

### 1\. å€’æ’ç´¢å¼• (Inverted Index)

å€’æ’ç´¢å¼•æ˜¯ElasticSearchå®ç°é«˜æ•ˆå…³é”®è¯æœç´¢çš„æ ¸å¿ƒæ•°æ®ç»“æ„ã€‚

#### å·¥ä½œåŸç†

graph LR A\[åŸå§‹æ–‡æ¡£\] --> B\[åˆ†è¯å™¨\] B --> C\[è¯é¡¹Term\] C --> D\[å€’æ’ç´¢å¼•\] D --> E\[æ–‡æ¡£IDåˆ—è¡¨\] subgraph "å€’æ’ç´¢å¼•ç»“æ„" F\[Term Dictionary\] --> G\[Posting List\] G --> H\[Doc ID + ä½ç½®ä¿¡æ¯\] end

#### æ•°æ®ç»“æ„ç¤ºä¾‹

    // åŸå§‹æ–‡æ¡£
    {
      "doc1": "Elasticsearch is a search engine",
      "doc2": "Lucene is the core of Elasticsearch", 
      "doc3": "Search engines use inverted index"
    }
    
    // åˆ†è¯åçš„å€’æ’ç´¢å¼•
    {
      "elasticsearch": [1, 2],      // å‡ºç°åœ¨æ–‡æ¡£1å’Œ2ä¸­
      "search": [1, 3],             // å‡ºç°åœ¨æ–‡æ¡£1å’Œ3ä¸­
      "engine": [1, 3],             // å‡ºç°åœ¨æ–‡æ¡£1å’Œ3ä¸­
      "lucene": [2],                // åªå‡ºç°åœ¨æ–‡æ¡£2ä¸­
      "core": [2],                  // åªå‡ºç°åœ¨æ–‡æ¡£2ä¸­
      "inverted": [3],              // åªå‡ºç°åœ¨æ–‡æ¡£3ä¸­
      "index": [3]                  // åªå‡ºç°åœ¨æ–‡æ¡£3ä¸­
    }
    

#### æ—¶é—´å¤æ‚åº¦ä¼˜åŒ–

é€šè¿‡å€’æ’ç´¢å¼•ï¼Œæœç´¢æ—¶é—´å¤æ‚åº¦ä»æš´åŠ›æœç´¢çš„O(N\*M)ä¼˜åŒ–ä¸ºO(logN)ï¼š

    // ä¼ ç»Ÿå…¨æ–‡æœç´¢ - O(N*M)
    public List<Document> bruteForceSearch(String keyword, List<Document> docs) {
        List<Document> results = new ArrayList<>();
        for (Document doc : docs) {  // O(N)
            if (doc.content.contains(keyword)) {  // O(M)
                results.add(doc);
            }
        }
        return results;
    }
    
    // å€’æ’ç´¢å¼•æœç´¢ - O(logN)
    public List<Document> invertedIndexSearch(String keyword, InvertedIndex index) {
        // é€šè¿‡è·³è¡¨æˆ–B+æ ‘å¿«é€Ÿå®šä½Term - O(logN)
        PostingList postingList = index.getPostingList(keyword);
        return postingList.getDocuments();
    }
    

#### è¯¦ç»†å­˜å‚¨ç»“æ„

    å€’æ’ç´¢å¼•çš„ç‰©ç†å­˜å‚¨ç»“æ„ï¼š
    
    Term Dictionary (è¯å…¸):
    â”œâ”€â”€ Term: "elasticsearch" â†’ Pointer to Posting List
    â”œâ”€â”€ Term: "lucene" â†’ Pointer to Posting List  
    â””â”€â”€ Term: "search" â†’ Pointer to Posting List
    
    Posting List (å€’æ’é“¾è¡¨):
    elasticsearch â†’ [DocID:1, Freq:1, Positions:[0]] â†’ [DocID:2, Freq:1, Positions:[5]]
    lucene â†’ [DocID:2, Freq:1, Positions:[0]]
    search â†’ [DocID:1, Freq:1, Positions:[3]] â†’ [DocID:3, Freq:1, Positions:[0]]
    

### 2\. Term Index - å†…å­˜ç›®å½•æ ‘

Term Indexæ˜¯åŸºäºå‰ç¼€å¤ç”¨æ„å»ºçš„å†…å­˜æ•°æ®ç»“æ„ï¼Œç”¨äºåŠ é€ŸTerm Dictionaryçš„ç£ç›˜æ£€ç´¢ã€‚

#### FST (Finite State Transducer) ç»“æ„

graph TB Root --> |e| Node1 Node1 --> |l| Node2 Node2 --> |a| Node3 Node3 --> |s| Node4 Node4 --> |t| Node5 Node5 --> |i| Node6 Node6 --> |c| Node7 Node7 --> |s| Node8\[Final: elasticsearch\] Node2 --> |u| Node9 Node9 --> |c| Node10 Node10 --> |e| Node11 Node11 --> |n| Node12 Node12 --> |e| Node13\[Final: lucene\]

#### å‰ç¼€å¤ç”¨ä¼˜åŠ¿

    // ä¼ ç»ŸTrieæ ‘ - æ¯ä¸ªèŠ‚ç‚¹å­˜å‚¨å®Œæ•´å­—ç¬¦
    class TrieNode {
        char character;
        Map<Character, TrieNode> children;
        boolean isEndOfWord;
        // å†…å­˜ä½¿ç”¨ï¼šæ¯ä¸ªå­—ç¬¦ä¸€ä¸ªèŠ‚ç‚¹
    }
    
    // FST - å‰ç¼€å¤ç”¨ï¼Œå‹ç¼©å­˜å‚¨
    class FSTNode {
        String sharedPrefix;  // å…±äº«å‰ç¼€
        Map<String, FSTNode> transitions;
        Object output;  // æŒ‡å‘Term Dictionaryçš„åç§»é‡
        // å†…å­˜ä½¿ç”¨ï¼šå¤§å¹…å‡å°‘ï¼Œç‰¹åˆ«æ˜¯æœ‰å…¬å…±å‰ç¼€çš„æƒ…å†µ
    }
    

#### æ£€ç´¢æµç¨‹

    æŸ¥æ‰¾Term "elasticsearch"çš„æµç¨‹ï¼š
    
    1. å†…å­˜ä¸­çš„Term Index (FST)ï¼š
       Root â†’ e â†’ el â†’ ela â†’ elas â†’ elast â†’ elasti â†’ elastic â†’ elastics â†’ elasticsearch
       æ‰¾åˆ°å¯¹åº”çš„ç£ç›˜åç§»é‡ï¼šoffset_123
    
    2. æ ¹æ®åç§»é‡è®¿é—®ç£ç›˜ä¸Šçš„Term Dictionaryï¼š
       seek(offset_123) â†’ è¯»å–"elasticsearch"çš„Posting ListæŒ‡é’ˆ
    
    3. åŠ è½½Posting Listï¼š
       è·å–åŒ…å«"elasticsearch"çš„æ‰€æœ‰æ–‡æ¡£IDå’Œä½ç½®ä¿¡æ¯
    

### 3\. Stored Fields - è¡Œå¼å­˜å‚¨

Stored Fieldsä»¥è¡Œå¼ç»“æ„å­˜å‚¨å®Œæ•´çš„æ–‡æ¡£å†…å®¹ï¼Œç”¨äºè¿”å›æœç´¢ç»“æœä¸­çš„åŸå§‹æ•°æ®ã€‚

#### å­˜å‚¨ç»“æ„

    Document Storage Layout (è¡Œå¼å­˜å‚¨):
    
    Doc 1: [field1: "value1", field2: "value2", field3: "value3"]
    Doc 2: [field1: "value4", field2: "value5", field3: "value6"]  
    Doc 3: [field1: "value7", field2: "value8", field3: "value9"]
    
    æ¯è¡Œè¿ç»­å­˜å‚¨ï¼Œä¾¿äºæ ¹æ®DocIDå¿«é€Ÿè·å–å®Œæ•´æ–‡æ¡£
    

#### è®¿é—®æ¨¡å¼

    // æ ¹æ®DocIDè·å–å®Œæ•´æ–‡æ¡£
    public Document getStoredDocument(int docId) {
        // 1. æ ¹æ®docIdè®¡ç®—åœ¨æ–‡ä»¶ä¸­çš„åç§»é‡
        long offset = docId * averageDocSize + indexOffset;
        
        // 2. ä»ç£ç›˜è¯»å–å®Œæ•´æ–‡æ¡£æ•°æ®
        byte[] docData = readFromFile(offset, docLength);
        
        // 3. ååºåˆ—åŒ–ä¸ºDocumentå¯¹è±¡
        return deserialize(docData);
    }
    

#### å‹ç¼©ä¼˜åŒ–

    Stored Fieldsçš„å‹ç¼©ç­–ç•¥ï¼š
    
    1. æ–‡æ¡£çº§å‹ç¼©ï¼š
       - ä½¿ç”¨LZ4/DEFLATEå‹ç¼©ç®—æ³•
       - æ‰¹é‡å‹ç¼©16KBå—ï¼Œå¹³è¡¡å‹ç¼©æ¯”å’Œè§£å‹é€Ÿåº¦
    
    2. å­—æ®µçº§ä¼˜åŒ–ï¼š
       - æ•°å€¼å­—æ®µä½¿ç”¨å˜é•¿ç¼–ç 
       - å­—ç¬¦ä¸²å­—æ®µä½¿ç”¨å­—å…¸å‹ç¼©
       - æ—¥æœŸå­—æ®µä½¿ç”¨å·®å€¼ç¼–ç 
    

### 4\. Doc Values - åˆ—å¼å­˜å‚¨

Doc Valuesé‡‡ç”¨åˆ—å¼å­˜å‚¨ç»“æ„ï¼Œé›†ä¸­å­˜æ”¾å­—æ®µå€¼ï¼Œä¸“é—¨ä¼˜åŒ–æ’åºå’Œèšåˆæ“ä½œçš„æ€§èƒ½ã€‚

#### å­˜å‚¨å¯¹æ¯”

    è¡Œå¼å­˜å‚¨ (Stored Fields):
    Doc1: [name:"Alice", age:25, city:"NYC"]
    Doc2: [name:"Bob", age:30, city:"LA"] 
    Doc3: [name:"Carol", age:28, city:"NYC"]
    
    åˆ—å¼å­˜å‚¨ (Doc Values):
    nameåˆ—: ["Alice", "Bob", "Carol"]
    ageåˆ—:  [25, 30, 28]
    cityåˆ—: ["NYC", "LA", "NYC"]
    

#### æ•°æ®ç±»å‹ä¼˜åŒ–

    // æ•°å€¼ç±»å‹çš„Doc Valuesä¼˜åŒ–
    class NumericDocValues {
        // ä½¿ç”¨ä½æ‰“åŒ…æŠ€æœ¯ï¼Œæ ¹æ®æ•°å€¼èŒƒå›´é€‰æ‹©æœ€å°å­˜å‚¨ä½æ•°
        private PackedInts.Reader values;
        
        public long get(int docId) {
            return values.get(docId);  // O(1)è®¿é—®
        }
    }
    
    // å­—ç¬¦ä¸²ç±»å‹çš„Doc Valuesä¼˜åŒ–  
    class SortedDocValues {
        private PackedInts.Reader ordinals;  // æ–‡æ¡£åˆ°åºå·çš„æ˜ å°„
        private BytesRef[] terms;            // å»é‡åçš„å­—ç¬¦ä¸²æ•°ç»„
        
        public BytesRef get(int docId) {
            int ord = (int) ordinals.get(docId);
            return terms[ord];
        }
    }
    

#### èšåˆæ€§èƒ½ä¼˜åŒ–

    // åŸºäºDoc Valuesçš„é«˜æ•ˆèšåˆ
    public class AggregationOptimization {
        
        // æ•°å€¼èšåˆ - åˆ©ç”¨åˆ—å¼å­˜å‚¨çš„é¡ºåºè®¿é—®ä¼˜åŠ¿
        public double calculateAverage(NumericDocValues ageValues, int[] docIds) {
            long sum = 0;
            for (int docId : docIds) {
                sum += ageValues.get(docId);  // é¡ºåºè®¿é—®ï¼Œç¼“å­˜å‹å¥½
            }
            return (double) sum / docIds.length;
        }
        
        // åˆ†ç»„èšåˆ - åˆ©ç”¨æ’åºçš„Doc Values
        public Map<String, List<Integer>> groupByCity(SortedDocValues cityValues, 
                                                       int[] docIds) {
            Map<String, List<Integer>> groups = new HashMap<>();
            for (int docId : docIds) {
                String city = cityValues.get(docId).utf8ToString();
                groups.computeIfAbsent(city, k -> new ArrayList<>()).add(docId);
            }
            return groups;
        }
    }
    

ğŸ—‚ï¸ Luceneæ ¸å¿ƒæ¦‚å¿µ
--------------

### 1\. Segment - æœ€å°æœç´¢å•å…ƒ

Segmentæ˜¯Luceneçš„æœ€å°æœç´¢å•å…ƒï¼ŒåŒ…å«å®Œæ•´çš„æœç´¢æ•°æ®ç»“æ„ã€‚

#### Segmentç»“æ„

    Segmentæ–‡ä»¶ç»„æˆï¼š
    â”œâ”€â”€ .tim (Term Index)           - å†…å­˜ä¸­çš„FSTç´¢å¼•
    â”œâ”€â”€ .tip (Term Dictionary)      - ç£ç›˜ä¸Šçš„è¯å…¸
    â”œâ”€â”€ .doc (Frequency)            - è¯é¢‘ä¿¡æ¯
    â”œâ”€â”€ .pos (Positions)            - è¯ä½ç½®ä¿¡æ¯
    â”œâ”€â”€ .pay (Payloads)            - è‡ªå®šä¹‰è½½è·æ•°æ®
    â”œâ”€â”€ .fdt (Stored Fields Data)   - å­˜å‚¨å­—æ®µæ•°æ®
    â”œâ”€â”€ .fdx (Stored Fields Index)  - å­˜å‚¨å­—æ®µç´¢å¼•
    â”œâ”€â”€ .dvd (Doc Values Data)      - åˆ—å¼æ•°æ®
    â”œâ”€â”€ .dvm (Doc Values Metadata)  - åˆ—å¼å…ƒæ•°æ®
    â””â”€â”€ .si  (Segment Info)         - æ®µä¿¡æ¯
    

#### ä¸å¯å˜æ€§ç‰¹å¾

    public class ImmutableSegment {
        private final String segmentName;
        private final int maxDoc;
        private final Map<String, InvertedIndex> fieldIndexes;
        
        // Segmentä¸€æ—¦ç”Ÿæˆå°±ä¸å¯ä¿®æ”¹
        public ImmutableSegment(String name, Document[] docs) {
            this.segmentName = name;
            this.maxDoc = docs.length;
            this.fieldIndexes = buildIndexes(docs);  // æ„å»ºæ—¶ç¡®å®šï¼Œä¹‹ååªè¯»
        }
        
        // æ›´æ–°æ“ä½œéœ€è¦åˆ›å»ºæ–°çš„Segment
        public ImmutableSegment addDocuments(Document[] newDocs) {
            Document[] allDocs = ArrayUtils.addAll(this.getDocs(), newDocs);
            return new ImmutableSegment(generateNewName(), allDocs);
        }
    }
    

#### æœç´¢è¿‡ç¨‹

    public class SegmentSearcher {
        
        public SearchResult search(Query query, Segment segment) {
            // 1. æŸ¥è¯¢Term Indexï¼Œå®šä½Term Dictionaryåç§»
            List<String> terms = query.extractTerms();
            Map<String, PostingList> termPostings = new HashMap<>();
            
            for (String term : terms) {
                // å¿«é€Ÿå®šä½ - O(logN)
                long offset = segment.getTermIndex().getOffset(term);
                PostingList posting = segment.getTermDict().getPosting(offset);
                termPostings.put(term, posting);
            }
            
            // 2. åˆå¹¶Posting Listsï¼Œè®¡ç®—ç›¸å…³æ€§
            Set<Integer> candidateDocs = intersectPostings(termPostings);
            
            // 3. ä»Stored Fieldsè·å–æ–‡æ¡£å†…å®¹
            List<Document> results = new ArrayList<>();
            for (Integer docId : candidateDocs) {
                Document doc = segment.getStoredFields().getDocument(docId);
                results.add(doc);
            }
            
            return new SearchResult(results);
        }
    }
    

### 2\. æ®µåˆå¹¶ (Segment Merging)

æ®µåˆå¹¶æ˜¯Luceneä¼˜åŒ–æ€§èƒ½çš„é‡è¦æœºåˆ¶ï¼Œå®šæœŸåˆå¹¶å°Segmentä¸ºå¤§Segmentã€‚

#### åˆå¹¶ç­–ç•¥

    public class TieredMergePolicy {
        private static final double DEFAULT_SEGS_PER_TIER = 10.0;
        private static final int DEFAULT_MAX_MERGE_AT_ONCE = 10;
        
        public MergeSpecification findMerges(List<SegmentInfo> segments) {
            // 1. æŒ‰Segmentå¤§å°åˆ†ç»„
            List<List<SegmentInfo>> tiers = groupBySize(segments);
            
            // 2. è¯†åˆ«éœ€è¦åˆå¹¶çš„å±‚çº§
            MergeSpecification mergeSpec = new MergeSpecification();
            for (List<SegmentInfo> tier : tiers) {
                if (tier.size() > DEFAULT_SEGS_PER_TIER) {
                    // é€‰æ‹©æœ€å°çš„Nä¸ªSegmentè¿›è¡Œåˆå¹¶
                    List<SegmentInfo> toMerge = selectSmallestSegments(tier, 
                        DEFAULT_MAX_MERGE_AT_ONCE);
                    mergeSpec.add(new OneMerge(toMerge));
                }
            }
            
            return mergeSpec;
        }
        
        // åˆå¹¶æ‰§è¡Œ
        public SegmentInfo executeMerge(List<SegmentInfo> segments) {
            SegmentWriter writer = new SegmentWriter();
            
            // é€ä¸ªå¤„ç†æ¯ä¸ªå­—æ®µçš„æ•°æ®
            for (String fieldName : getAllFields(segments)) {
                // åˆå¹¶å€’æ’ç´¢å¼•
                mergeInvertedIndex(writer, segments, fieldName);
                // åˆå¹¶Stored Fields
                mergeStoredFields(writer, segments, fieldName);
                // åˆå¹¶Doc Values
                mergeDocValues(writer, segments, fieldName);
            }
            
            return writer.commit();
        }
    }
    

#### åˆå¹¶æ”¶ç›Š

    åˆå¹¶å‰ï¼š
    Segment1 (1000 docs, 10MB)
    Segment2 (1200 docs, 12MB)  
    Segment3 (800 docs, 8MB)
    Segment4 (1500 docs, 15MB)
    æ€»è®¡: 4ä¸ªæ–‡ä»¶ï¼Œ45MBï¼Œ4æ¬¡ç£ç›˜seek
    
    åˆå¹¶åï¼š
    Segment_merged (4500 docs, 42MB)  // å‹ç¼©åç•¥å°
    æ€»è®¡: 1ä¸ªæ–‡ä»¶ï¼Œ42MBï¼Œ1æ¬¡ç£ç›˜seek
    
    æŸ¥è¯¢æ€§èƒ½æå‡ï¼š
    - å‡å°‘æ–‡ä»¶æ‰“å¼€æ•°é‡ï¼š4 â†’ 1
    - å‡å°‘ç£ç›˜seekæ¬¡æ•°ï¼š4 â†’ 1  
    - æé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼šè¿ç»­å­˜å‚¨
    - å‡å°‘å†…å­˜å¼€é”€ï¼šåˆå¹¶ç´¢å¼•ç»“æ„
    

ğŸŒ ä»Luceneåˆ°ElasticSearchçš„æ¼”è¿›
---------------------------

### 1\. åˆ†å¸ƒå¼æ¶æ„è®¾è®¡

ElasticSearchå°†å•æœºçš„Luceneæ‰©å±•ä¸ºåˆ†å¸ƒå¼æœç´¢å¼•æ“ã€‚

#### æ¶æ„å¯¹æ¯”

graph TB subgraph "å•æœºLucene" A\[Application\] --> B\[Lucene Library\] B --> C\[Local Index Files\] end subgraph "åˆ†å¸ƒå¼ElasticSearch" D\[Client\] --> E\[Coordinate Node\] E --> F\[Data Node 1\] E --> G\[Data Node 2\] E --> H\[Data Node 3\] F --> I\[Shard 1\] F --> J\[Shard 2\] G --> K\[Shard 3\] G --> L\[Replica 1\] H --> M\[Replica 2\] H --> N\[Replica 3\] end

#### æ ¸å¿ƒæ”¹è¿›

    // Luceneå•æœºæœç´¢
    public class LuceneSearcher {
        private IndexSearcher searcher;
        
        public TopDocs search(Query query, int numHits) {
            return searcher.search(query, numHits);  // å•æœºå¤„ç†
        }
    }
    
    // ElasticSearchåˆ†å¸ƒå¼æœç´¢
    public class DistributedSearcher {
        private List<ShardSearcher> shards;
        
        public SearchResponse search(SearchRequest request) {
            // 1. åˆ†å‘æŸ¥è¯¢åˆ°æ‰€æœ‰åˆ†ç‰‡
            List<Future<ShardSearchResult>> futures = new ArrayList<>();
            for (ShardSearcher shard : shards) {
                Future<ShardSearchResult> future = executor.submit(() -> 
                    shard.search(request));
                futures.add(future);
            }
            
            // 2. æ”¶é›†å„åˆ†ç‰‡ç»“æœ
            List<ShardSearchResult> shardResults = new ArrayList<>();
            for (Future<ShardSearchResult> future : futures) {
                shardResults.add(future.get());
            }
            
            // 3. åˆå¹¶æ’åºï¼Œè¿”å›Top-Kç»“æœ
            return mergeAndSort(shardResults, request.getSize());
        }
    }
    

### 2\. åˆ†ç‰‡ä¸å‰¯æœ¬æœºåˆ¶

#### Primary Shard vs Replica Shard

    åˆ†ç‰‡ç­–ç•¥ï¼š
    
    Primary Shard (ä¸»åˆ†ç‰‡)ï¼š
    - è´Ÿè´£å†™æ“ä½œçš„å¤„ç†
    - æ•°æ®çš„æƒå¨æ¥æº
    - åˆ›å»ºç´¢å¼•æ—¶ç¡®å®šæ•°é‡ï¼Œåç»­ä¸å¯æ›´æ”¹
    
    Replica Shard (å‰¯æœ¬åˆ†ç‰‡)ï¼š
    - Primary Shardçš„å®Œæ•´å‰¯æœ¬
    - è´Ÿè´£è¯»æ“ä½œçš„è´Ÿè½½å‡è¡¡
    - æä¾›é«˜å¯ç”¨ä¿éšœ
    - æ•°é‡å¯ä»¥åŠ¨æ€è°ƒæ•´
    

#### æ•°æ®åˆ†å¸ƒç¤ºä¾‹

    public class ShardAllocation {
        
        // æ–‡æ¡£è·¯ç”±ç®—æ³•
        public int getShardId(String documentId, int numberOfShards) {
            // ä½¿ç”¨æ–‡æ¡£IDçš„å“ˆå¸Œå€¼ç¡®å®šåˆ†ç‰‡
            return Math.abs(documentId.hashCode()) % numberOfShards;
        }
        
        // åˆ†ç‰‡åˆ†å¸ƒç­–ç•¥
        public void allocateShards(Index index, List<Node> nodes) {
            int primaryShards = index.getNumberOfShards();
            int replicaCount = index.getNumberOfReplicas();
            
            // åˆ†é…Primary Shards
            for (int shardId = 0; shardId < primaryShards; shardId++) {
                Node primaryNode = selectNodeForPrimary(nodes, shardId);
                primaryNode.allocatePrimaryShard(index, shardId);
                
                // åˆ†é…Replica Shards
                for (int replica = 0; replica < replicaCount; replica++) {
                    Node replicaNode = selectNodeForReplica(nodes, primaryNode, shardId);
                    replicaNode.allocateReplicaShard(index, shardId, replica);
                }
            }
        }
    }
    

#### è¯»å†™è´Ÿè½½å‡è¡¡

    public class LoadBalancedOperations {
        
        // å†™æ“ä½œï¼šå¿…é¡»è·¯ç”±åˆ°Primary Shard
        public IndexResponse index(IndexRequest request) {
            String docId = request.getId();
            int shardId = getShardId(docId, numberOfShards);
            
            // æ‰¾åˆ°Primary Shard
            Shard primaryShard = findPrimaryShard(shardId);
            
            // åœ¨Primaryä¸Šæ‰§è¡Œå†™æ“ä½œ
            IndexResponse response = primaryShard.index(request);
            
            // åŒæ­¥åˆ°æ‰€æœ‰Replica Shards
            List<Shard> replicas = findReplicaShards(shardId);
            for (Shard replica : replicas) {
                replica.index(request);  // å¼‚æ­¥å¤åˆ¶
            }
            
            return response;
        }
        
        // è¯»æ“ä½œï¼šå¯ä»¥è·¯ç”±åˆ°Primaryæˆ–Replica
        public GetResponse get(GetRequest request) {
            String docId = request.getId();
            int shardId = getShardId(docId, numberOfShards);
            
            // è´Ÿè½½å‡è¡¡é€‰æ‹©åˆ†ç‰‡ï¼ˆPrimary + Replicasï¼‰
            List<Shard> availableShards = findAvailableShards(shardId);
            Shard selectedShard = selectShardForRead(availableShards);
            
            return selectedShard.get(request);
        }
    }
    

### 3\. èŠ‚ç‚¹è§’è‰²åˆ†å·¥

ElasticSearché€šè¿‡èŠ‚ç‚¹è§’è‰²åˆ†åŒ–å®ç°åŠŸèƒ½è§£è€¦å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

#### èŠ‚ç‚¹ç±»å‹è¯¦è§£

    // Master Node - é›†ç¾¤ç®¡ç†
    public class MasterNode extends Node {
        private ClusterState clusterState;
        private AllocationService allocationService;
        
        public void handleClusterStateChange() {
            // 1. å¤„ç†ç´¢å¼•åˆ›å»º/åˆ é™¤
            processIndexOperations();
            
            // 2. ç®¡ç†åˆ†ç‰‡åˆ†é…
            rebalanceShards();
            
            // 3. å¤„ç†èŠ‚ç‚¹åŠ å…¥/ç¦»å¼€
            updateNodeMembership();
            
            // 4. å¹¿æ’­é›†ç¾¤çŠ¶æ€åˆ°æ‰€æœ‰èŠ‚ç‚¹
            broadcastClusterState();
        }
        
        @Override
        public boolean canHandleSearchRequests() {
            return false;  // ä¸“æ³¨äºé›†ç¾¤ç®¡ç†ï¼Œä¸å¤„ç†æœç´¢è¯·æ±‚
        }
    }
    
    // Data Node - æ•°æ®å­˜å‚¨
    public class DataNode extends Node {
        private Map<ShardId, Shard> localShards;
        private LuceneService luceneService;
        
        public SearchResponse executeSearch(SearchRequest request) {
            List<ShardSearchResult> results = new ArrayList<>();
            
            for (ShardId shardId : request.getShardIds()) {
                if (localShards.containsKey(shardId)) {
                    Shard shard = localShards.get(shardId);
                    ShardSearchResult result = shard.search(request);
                    results.add(result);
                }
            }
            
            return aggregateResults(results);
        }
        
        @Override
        public boolean canStorePrimaryShards() {
            return true;  // å¯ä»¥å­˜å‚¨ä¸»åˆ†ç‰‡
        }
    }
    
    // Coordinate Node - è¯·æ±‚åè°ƒ
    public class CoordinateNode extends Node {
        private LoadBalancer loadBalancer;
        private ResultAggregator aggregator;
        
        public SearchResponse coordinateSearch(SearchRequest request) {
            // 1. æŸ¥è¯¢è·¯ç”±è§„åˆ’
            Map<Node, List<ShardId>> shardRouting = planShardRouting(request);
            
            // 2. å¹¶å‘å‘é€åˆ°å„ä¸ªData Node
            Map<Node, Future<SearchResponse>> futures = new HashMap<>();
            for (Map.Entry<Node, List<ShardId>> entry : shardRouting.entrySet()) {
                Node dataNode = entry.getKey();
                SearchRequest shardRequest = buildShardRequest(request, entry.getValue());
                Future<SearchResponse> future = sendSearchRequest(dataNode, shardRequest);
                futures.put(dataNode, future);
            }
            
            // 3. æ”¶é›†å¹¶èšåˆç»“æœ
            List<SearchResponse> responses = collectResponses(futures);
            return aggregator.aggregate(responses, request);
        }
        
        @Override
        public boolean canStoreData() {
            return false;  // ä¸å­˜å‚¨æ•°æ®ï¼Œä¸“æ³¨äºåè°ƒ
        }
    }
    

#### è§’è‰²ç»„åˆé…ç½®

    # elasticsearch.yml é…ç½®ç¤ºä¾‹
    
    # ä¸“ç”¨MasterèŠ‚ç‚¹
    node.master: true
    node.data: false
    node.ingest: false
    node.ml: false
    
    # ä¸“ç”¨DataèŠ‚ç‚¹  
    node.master: false
    node.data: true
    node.ingest: false
    node.ml: false
    
    # ä¸“ç”¨CoordinateèŠ‚ç‚¹
    node.master: false
    node.data: false
    node.ingest: true
    node.ml: false
    
    # æ··åˆèŠ‚ç‚¹ï¼ˆå°é›†ç¾¤ï¼‰
    node.master: true
    node.data: true
    node.ingest: true
    node.ml: false
    

### 4\. å»ä¸­å¿ƒåŒ–è®¾è®¡

ElasticSearché‡‡ç”¨å»ä¸­å¿ƒåŒ–æ¶æ„ï¼Œé¿å…ä¾èµ–å¤–éƒ¨ç»„ä»¶ã€‚

#### Raftåè®®å®ç°

    public class RaftConsensus {
        private NodeRole currentRole = NodeRole.FOLLOWER;
        private int currentTerm = 0;
        private String votedFor = null;
        private List<LogEntry> log = new ArrayList<>();
        
        // Leaderé€‰ä¸¾
        public void startElection() {
            currentTerm++;
            currentRole = NodeRole.CANDIDATE;
            votedFor = this.nodeId;
            
            // å‘æ‰€æœ‰èŠ‚ç‚¹è¯·æ±‚æŠ•ç¥¨
            int votes = 1;  // è‡ªå·±çš„ç¥¨
            for (Node node : clusterNodes) {
                VoteRequest request = new VoteRequest(currentTerm, nodeId, 
                    getLastLogIndex(), getLastLogTerm());
                VoteResponse response = node.requestVote(request);
                
                if (response.isVoteGranted()) {
                    votes++;
                }
            }
            
            // è·å¾—å¤šæ•°ç¥¨ï¼Œæˆä¸ºLeader
            if (votes > clusterNodes.size() / 2) {
                becomeLeader();
            } else {
                becomeFollower();
            }
        }
        
        // æ—¥å¿—å¤åˆ¶
        public void replicateEntry(ClusterStateUpdate update) {
            if (currentRole != NodeRole.LEADER) {
                throw new IllegalStateException("Only leader can replicate entries");
            }
            
            LogEntry entry = new LogEntry(currentTerm, update);
            log.add(entry);
            
            // å¹¶è¡Œå¤åˆ¶åˆ°æ‰€æœ‰Follower
            int replicationCount = 1;  // Leaderæœ¬èº«
            for (Node follower : followers) {
                AppendEntriesRequest request = new AppendEntriesRequest(
                    currentTerm, nodeId, getLastLogIndex() - 1, 
                    getLastLogTerm(), Arrays.asList(entry), commitIndex);
                    
                AppendEntriesResponse response = follower.appendEntries(request);
                if (response.isSuccess()) {
                    replicationCount++;
                }
            }
            
            // å¤šæ•°èŠ‚ç‚¹ç¡®è®¤åæäº¤
            if (replicationCount > clusterNodes.size() / 2) {
                commitIndex = log.size() - 1;
                applyToStateMachine(update);
            }
        }
    }
    

#### é›†ç¾¤å‘ç°æœºåˆ¶

    public class ClusterDiscovery {
        private List<String> seedNodes;
        private Map<String, NodeInfo> discoveredNodes;
        
        // èŠ‚ç‚¹å‘ç°
        public void discoverNodes() {
            Set<String> allNodes = new HashSet<>(seedNodes);
            
            // é€’å½’å‘ç°ï¼šä»ç§å­èŠ‚ç‚¹å¼€å§‹ï¼Œè·å–å®ƒä»¬çŸ¥é“çš„å…¶ä»–èŠ‚ç‚¹
            Queue<String> toDiscover = new LinkedList<>(seedNodes);
            Set<String> discovered = new HashSet<>();
            
            while (!toDiscover.isEmpty()) {
                String nodeAddress = toDiscover.poll();
                if (discovered.contains(nodeAddress)) {
                    continue;
                }
                
                try {
                    // è¿æ¥èŠ‚ç‚¹ï¼Œè·å–å…¶å·²çŸ¥çš„é›†ç¾¤æˆå‘˜
                    NodeInfo nodeInfo = connectAndGetInfo(nodeAddress);
                    discoveredNodes.put(nodeAddress, nodeInfo);
                    discovered.add(nodeAddress);
                    
                    // å°†æ–°å‘ç°çš„èŠ‚ç‚¹åŠ å…¥å¾…æ¢ç´¢é˜Ÿåˆ—
                    for (String knownNode : nodeInfo.getKnownNodes()) {
                        if (!discovered.contains(knownNode)) {
                            toDiscover.offer(knownNode);
                        }
                    }
                } catch (Exception e) {
                    log.warn("Failed to discover node: " + nodeAddress, e);
                }
            }
            
            // æ›´æ–°é›†ç¾¤æˆå‘˜è§†å›¾
            updateClusterMembership(discoveredNodes.values());
        }
        
        // æ•…éšœæ£€æµ‹
        @Scheduled(fixedDelay = 1000)
        public void detectFailures() {
            for (Map.Entry<String, NodeInfo> entry : discoveredNodes.entrySet()) {
                String nodeAddress = entry.getKey();
                NodeInfo nodeInfo = entry.getValue();
                
                try {
                    // å‘é€å¿ƒè·³æ£€æµ‹
                    boolean isAlive = sendHeartbeat(nodeAddress);
                    if (!isAlive) {
                        handleNodeFailure(nodeAddress, nodeInfo);
                    }
                } catch (Exception e) {
                    handleNodeFailure(nodeAddress, nodeInfo);
                }
            }
        }
    }
    

ğŸ”„ æ ¸å¿ƒå·¥ä½œæµç¨‹
---------

### 1\. ç´¢å¼•æµç¨‹

    public class IndexingWorkflow {
        
        public IndexResponse index(IndexRequest request) {
            // 1. è·¯ç”±åˆ°æ­£ç¡®çš„åˆ†ç‰‡
            int shardId = calculateShardId(request.getId());
            Shard primaryShard = getPrimaryShard(shardId);
            
            // 2. åœ¨Primary Shardä¸Šæ‰§è¡Œç´¢å¼•
            Document doc = parseDocument(request.getSource());
            
            // 2.1 åˆ†è¯å¤„ç†
            Map<String, List<String>> analyzedFields = analyzeDocument(doc);
            
            // 2.2 æ„å»ºå€’æ’ç´¢å¼•
            updateInvertedIndex(analyzedFields, doc.getId());
            
            // 2.3 å­˜å‚¨åŸå§‹æ–‡æ¡£
            storeDocument(doc);
            
            // 2.4 æ›´æ–°Doc Values
            updateDocValues(doc);
            
            // 3. å¤åˆ¶åˆ°Replica Shards
            List<Shard> replicas = getReplicaShards(shardId);
            replicateToReplicas(replicas, request);
            
            // 4. è¿”å›å“åº”
            return new IndexResponse(request.getId(), shardId, "created");
        }
    }
    

### 2\. æœç´¢æµç¨‹

    public class SearchWorkflow {
        
        public SearchResponse search(SearchRequest request) {
            // Phase 1: Query Phase (æŸ¥è¯¢é˜¶æ®µ)
            Map<ShardId, ShardSearchResult> queryResults = queryPhase(request);
            
            // Phase 2: Fetch Phase (è·å–é˜¶æ®µ)  
            List<Document> documents = fetchPhase(queryResults, request);
            
            return buildSearchResponse(documents, queryResults);
        }
        
        private Map<ShardId, ShardSearchResult> queryPhase(SearchRequest request) {
            Map<ShardId, ShardSearchResult> results = new HashMap<>();
            
            // å¹¶è¡ŒæŸ¥è¯¢æ‰€æœ‰ç›¸å…³åˆ†ç‰‡
            List<Future<ShardSearchResult>> futures = new ArrayList<>();
            for (ShardId shardId : getTargetShards(request)) {
                Future<ShardSearchResult> future = executor.submit(() -> {
                    Shard shard = getShard(shardId);
                    
                    // 1. è§£ææŸ¥è¯¢
                    Query luceneQuery = parseQuery(request.getQuery());
                    
                    // 2. æ‰§è¡Œæœç´¢ï¼Œåªè¿”å›DocIDå’ŒScore
                    TopDocs topDocs = shard.search(luceneQuery, request.getSize());
                    
                    // 3. åŒ…è£…ç»“æœ
                    return new ShardSearchResult(shardId, topDocs);
                });
                futures.add(future);
            }
            
            // æ”¶é›†æŸ¥è¯¢ç»“æœ
            for (Future<ShardSearchResult> future : futures) {
                ShardSearchResult result = future.get();
                results.put(result.getShardId(), result);
            }
            
            return results;
        }
        
        private List<Document> fetchPhase(Map<ShardId, ShardSearchResult> queryResults,
                                         SearchRequest request) {
            // 1. å…¨å±€æ’åºï¼Œé€‰å‡ºTop-K
            List<ScoreDoc> globalTopDocs = mergeAndSort(queryResults.values(), 
                request.getSize());
            
            // 2. æ ¹æ®DocIDè·å–å®Œæ•´æ–‡æ¡£å†…å®¹
            List<Document> documents = new ArrayList<>();
            for (ScoreDoc scoreDoc : globalTopDocs) {
                ShardId shardId = getShardId(scoreDoc);
                Shard shard = getShard(shardId);
                
                // ä»Stored Fieldsè·å–å®Œæ•´æ–‡æ¡£
                Document doc = shard.getStoredDocument(scoreDoc.doc);
                documents.add(doc);
            }
            
            return documents;
        }
    }
    

ğŸ“Š æ€§èƒ½ç‰¹å¾åˆ†æ
---------

### 1\. æŸ¥è¯¢æ€§èƒ½

    æ—¶é—´å¤æ‚åº¦åˆ†æï¼š
    
    1. TermæŸ¥æ‰¾ï¼šO(logN)
       - Term Index (FST): O(logT), Tä¸ºä¸é‡å¤Termæ•°é‡
       - Term Dictionary: O(1), ç›´æ¥åç§»è®¿é—®
       
    2. Posting Listéå†ï¼šO(K)
       - Kä¸ºåŒ…å«Termçš„æ–‡æ¡£æ•°é‡
       - ä½¿ç”¨è·³è¡¨ä¼˜åŒ–ï¼Œå¯ä»¥è·³è¿‡æ— å…³æ–‡æ¡£
       
    3. å¤šTermæŸ¥è¯¢åˆå¹¶ï¼šO(K1 + K2 + ... + Kn)
       - ä½¿ç”¨åŒæŒ‡é’ˆæ³•åˆå¹¶æœ‰åºåˆ—è¡¨
       - å¸ƒå°”æŸ¥è¯¢çš„AND/OR/NOTæ“ä½œ
       
    4. ç»“æœæ’åºï¼šO(R*logR)
       - Rä¸ºæœ€ç»ˆè¿”å›çš„ç»“æœæ•°é‡
       - é€šå¸¸R << æ€»æ–‡æ¡£æ•°ï¼Œæ€§èƒ½å¯æ§
    
    æ€»ä½“æŸ¥è¯¢å¤æ‚åº¦ï¼šO(logN + K + R*logR)
    

### 2\. å­˜å‚¨æ•ˆç‡

    å­˜å‚¨ç©ºé—´åˆ†æï¼š
    
    1. å€’æ’ç´¢å¼•ï¼š
       - Term Dictionary: çº¦ä¸ºåŸæ–‡æœ¬çš„10-30%
       - Posting Lists: çº¦ä¸ºåŸæ–‡æœ¬çš„20-50%
       - Term Index (å†…å­˜): çº¦ä¸ºTerm Dictionaryçš„1-5%
    
    2. Stored Fieldsï¼š
       - å‹ç¼©æ¯”: 50-80% (å–å†³äºæ•°æ®ç±»å‹å’Œå‹ç¼©ç®—æ³•)
       - éšæœºè®¿é—®: éœ€è¦è§£å‹ç¼©å¼€é”€
    
    3. Doc Valuesï¼š
       - æ•°å€¼ç±»å‹: åŸå§‹æ•°æ®çš„70-90% (ä½æ‰“åŒ…ä¼˜åŒ–)
       - å­—ç¬¦ä¸²ç±»å‹: åŸå§‹æ•°æ®çš„60-85% (åºå·åŒ–+å­—å…¸)
    
    4. æ€»ä½“å­˜å‚¨å¼€é”€ï¼š
       - åŸå§‹æ•°æ®: 100%
       - ç´¢å¼•å¼€é”€: 50-100%
       - æ€»å­˜å‚¨: 150-200% of åŸå§‹æ•°æ®
    

### 3\. å†…å­˜ä½¿ç”¨

    public class MemoryUsageAnalysis {
        
        // å„ç»„ä»¶å†…å­˜ä½¿ç”¨ä¼°ç®—
        public MemoryUsage calculateMemoryUsage(IndexStats stats) {
            long termIndexMemory = estimateTermIndexMemory(stats);
            long filterCacheMemory = estimateFilterCacheMemory(stats);
            long fieldDataMemory = estimateFieldDataMemory(stats);
            long segmentMemory = estimateSegmentMemory(stats);
            
            return new MemoryUsage(termIndexMemory, filterCacheMemory, 
                fieldDataMemory, segmentMemory);
        }
        
        private long estimateTermIndexMemory(IndexStats stats) {
            // Term Index (FST) å¤§çº¦å ç”¨ï¼š
            // æ¯ä¸ªå”¯ä¸€Term 8-32å­—èŠ‚ (å–å†³äºå‰ç¼€å‹ç¼©æ•ˆæœ)
            return stats.getUniqueTermCount() * 20; // å¹³å‡20å­—èŠ‚/Term
        }
        
        private long estimateFilterCacheMemory(IndexStats stats) {
            // è¿‡æ»¤å™¨ç¼“å­˜ï¼šç¼“å­˜å¸¸ç”¨çš„è¿‡æ»¤å™¨BitSet
            // æ¯ä¸ªæ–‡æ¡£1bitï¼ŒæŒ‰å­—èŠ‚å¯¹é½
            return stats.getDocumentCount() / 8 * stats.getCachedFilterCount();
        }
        
        private long estimateFieldDataMemory(IndexStats stats) {
            // Doc ValuesåŠ è½½åˆ°å†…å­˜çš„éƒ¨åˆ†
            // æ•°å€¼å­—æ®µï¼š8å­—èŠ‚/æ–‡æ¡£ï¼Œå­—ç¬¦ä¸²å­—æ®µï¼šå˜é•¿
            long numericMemory = stats.getNumericFieldCount() * stats.getDocumentCount() * 8;
            long stringMemory = stats.getStringFieldSize(); // å®é™…å­—ç¬¦ä¸²é•¿åº¦
            return numericMemory + stringMemory;
        }
    }
    

ğŸ¯ æ€»ç»“
-----

ElasticSearché€šè¿‡ç²¾å·§çš„æ•°æ®ç»“æ„è®¾è®¡å’Œåˆ†å¸ƒå¼æ¶æ„ï¼Œå°†Luceneä»å•æœºæœç´¢åº“æ¼”è¿›ä¸ºåˆ†å¸ƒå¼æœç´¢å¼•æ“ï¼š

### æ ¸å¿ƒæ•°æ®ç»“æ„ä¼˜åŠ¿ï¼š

*   **å€’æ’ç´¢å¼•**ï¼šO(logN)æŸ¥è¯¢å¤æ‚åº¦ï¼Œé«˜æ•ˆå…³é”®è¯æœç´¢
*   **Term Index**ï¼šFSTå‰ç¼€å¤ç”¨ï¼Œå‡å°‘å†…å­˜å¼€é”€å’Œç£ç›˜IO
*   **Stored Fields**ï¼šè¡Œå¼å­˜å‚¨ï¼Œå¿«é€Ÿæ–‡æ¡£æ£€ç´¢
*   **Doc Values**ï¼šåˆ—å¼å­˜å‚¨ï¼Œä¼˜åŒ–èšåˆå’Œæ’åºæ€§èƒ½

### åˆ†å¸ƒå¼æ¶æ„ç‰¹è‰²ï¼š

*   **åˆ†ç‰‡æœºåˆ¶**ï¼šæ°´å¹³æ‰©å±•ï¼Œè´Ÿè½½å‡è¡¡
*   **å‰¯æœ¬ä¿éšœ**ï¼šé«˜å¯ç”¨ï¼Œè¯»å†™åˆ†ç¦»
*   **èŠ‚ç‚¹åˆ†å·¥**ï¼šMasterã€Dataã€Coordinateè§’è‰²è§£è€¦
*   **å»ä¸­å¿ƒåŒ–**ï¼šRaftåè®®ï¼Œæ— å¤–éƒ¨ä¾èµ–

### æ€§èƒ½ç‰¹å¾ï¼š

*   **æŸ¥è¯¢æ€§èƒ½**ï¼šäºšç§’çº§å…¨æ–‡æœç´¢å“åº”
*   **å­˜å‚¨æ•ˆç‡**ï¼š50-100%ç´¢å¼•å¼€é”€ï¼Œå¯æ¥å—çš„ç©ºé—´æˆæœ¬
*   **æ‰©å±•èƒ½åŠ›**ï¼šçº¿æ€§æ‰©å±•ï¼Œæ”¯æŒPBçº§æ•°æ®

ElasticSearchæˆåŠŸåœ°å°†å¤æ‚çš„ä¿¡æ¯æ£€ç´¢ç†è®ºè½¬åŒ–ä¸ºå®ç”¨çš„åˆ†å¸ƒå¼æœç´¢å¹³å°ï¼Œåœ¨ç°ä»£å¤§æ•°æ®ç”Ÿæ€ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚å…¶è®¾è®¡ç†å¿µå’ŒæŠ€æœ¯å®ç°ä¸ºåˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„æä¾›äº†å®è´µçš„å‚è€ƒä»·å€¼ã€‚

æœ¬æ–‡æ¥è‡ªåšå®¢å›­ï¼Œä½œè€…ï¼š[MadLongTom](https://www.cnblogs.com/madtom/)ï¼Œè½¬è½½è¯·æ³¨æ˜åŸæ–‡é“¾æ¥ï¼š[https://www.cnblogs.com/madtom/p/19060806](https://www.cnblogs.com/madtom/p/19060806)