---
layout: post
title: '3D Gaussian splatting 03: 用户数据训练和结果查看'
date: "2025-05-31T00:40:40Z"
---
3D Gaussian splatting 03: 用户数据训练和结果查看
-------------------------------------

![3D Gaussian splatting 03: 用户数据训练和结果查看](https://img2024.cnblogs.com/blog/650273/202505/650273-20250530185955829-1645257961.png) 于训练的图片集采集有两种方式, 一种是使用相机从不同角度拍照, 另一种是拍视频后逐帧提取. 两种方式各有利弊, 拍照分辨率更高, 方便控制光圈,快门和白平衡, 但是拍照时较难控制好角度可能会造成部分交叠区域过小, 视频比较容易实现连续的画面移动, 保证交叠区域, 但是大多数手机没法手动控制视频录制过程的光圈和白平衡, 并且视频的分辨率大多数是通过插值填充的, 拍摄1080p视频时, 实际有效像素并没有1080p.

![](https://img2024.cnblogs.com/blog/650273/202504/650273-20250415072304593-1498871573.jpg)

目录
==

*   [3D Gaussian splatting 01: 环境搭建](https://www.cnblogs.com/milton/p/18799695)
*   [3D Gaussian splatting 02: 快速评估](https://www.cnblogs.com/milton/p/18904737)
*   [3D Gaussian splatting 03: 用户数据训练和结果查看](https://www.cnblogs.com/milton/p/18904741)
*   [3D Gaussian splatting 04: 代码阅读-提取相机位姿和稀疏点云](https://www.cnblogs.com/milton/p/18904750)
*   [3D Gaussian splatting 05: 代码阅读-训练整体流程](https://www.cnblogs.com/milton/p/18904753)
*   [3D Gaussian splatting 06: 代码阅读-训练参数](https://www.cnblogs.com/milton/p/18904757)
*   [3D Gaussian splatting 07: 代码阅读-训练载入数据和保存结果](https://www.cnblogs.com/milton/p/18904763)
*   [3D Gaussian splatting 08: 代码阅读-渲染](https://www.cnblogs.com/milton/p/18904765)

准备图片集
=====

采集方式
----

用于训练的图片集采集有两种方式, 一种是使用相机从不同角度拍照, 另一种是拍视频后逐帧提取. 两种方式各有利弊, 拍照分辨率更高, 方便控制光圈,快门和白平衡, 但是拍照时较难控制好角度可能会造成部分交叠区域过小, 视频比较容易实现连续的画面移动, 保证交叠区域, 但是大多数手机没法手动控制视频录制过程的光圈和白平衡, 并且视频的分辨率大多数是通过插值填充的, 拍摄1080p视频时, 实际有效像素并没有1080p.

我自己的使用经验是, 用视频方式提取的帧训练结果更稳定.

图片数量
----

另外一方面是图片集的数量, 为了保证帧之间特征点的交叠, 一般按角度每3度一帧比较合适, 也就是从水平视角恢复一个3D对象至少需要约120张图.

图片分辨率
-----

图片分辨率跟你的显卡显存有关. 16G的显存训练 1920x1080分辨率大概率是不够的, 在 train.py 那一步会报`torch.OutOfMemoryError: CUDA out of memory.`错误. 如果出现这种错误, 就把分辨率适当降一点. 实际测试训练1366x768的分辨率只需要8GB的显存, 如果用1024x576分辨率训练速度较快且细节保留还不错.

从视频提取帧序列
========

方式一: 使用 python 代码
-----------------

    from pathlib import Path
    import cv2
    
    work_path = '/home/milton/temp/input'
    target_height = 720
    
    imgpath = Path(work_path)
    imgpath.mkdir(exist_ok = True)
    
    cap = cv2.VideoCapture('/home/milton/temp/557 Marksbury Road Pickering Open House Video Tour.mp4')
    
    frame_no = 0
    while cap.isOpened():
        ret, frame = cap.read()
    
        if ret:
            # 视频是 60fps, 每秒取3帧, 因此每20帧取一帧
            if frame_no % 20 == 0:
                original_height, original_width = frame.shape[:2]
                scaling_factor = target_height / original_height
                target_width = int(original_width * scaling_factor)
    
                resized_frame = cv2.resize(
                    frame, 
                    (target_width, target_height), 
                    interpolation=cv2.INTER_AREA)
                # {:0>5}是固定5位, 左边填充0
                target = work_path + '/{:0>5}.jpg'.format((int)(frame_no / 20))
                print(target)
                cv2.imwrite(target, resized_frame)
            frame_no += 1
        else:
            cap.release()
            break
    
    print('done')
    

方式二: 使用 ffmpeg 提取
-----------------

使用 ffmpeg 将视频帧按固定间隔抽取为图片, 下面的命令将视频帧以较小的压缩率转换为jpg文件, `FRAMES_PER_SEC`是每秒抽取的帧数.

    ffmpeg -i <PATH_VIDEO> -qscale:v 1 -qmin 1 -vf fps=<FRAMES_PER_SEC> <PATH_OUTPUT>/%04d.jpg
    

如果需要缩小图片(按比例缩小), 需要加上`-vf scale=720:-1`参数, 这个意思是将宽度变为720,高度按比例调整, 如果按固定高度调整, 则是`-vf scale=-1:357`, 例如

    ffmpeg -i 557VideoTour.mp4 -vf fps=2,scale=720:-1 input/%05d.png
    

Convert 提取特征和点云
===============

创建一个目录, 例如 source\_data, 在下面创建一个子目录 input, 这个 input 的名称是固定的, 不能改. 然后将图片序列放到这个input目录下, 执行提取命令

    python ./convert.py -s ~/work/source_data/
    

这一步执行是通过 colmap 实现的, 依次执行 feature extraction, exhaustive feature matching, reconstruction, 如果使用cpu处理, 需要加上`--no_gpu`参数, 例如

    python ./convert.py --no_gpu -s ~/work/source_data/
    

可以打开 convert.py 查看其它的选项. 在 matching 阶段会使显卡满负荷, 这一阶段如果使用CPU, 比使用支持CUDA的显卡速度上**慢一个数量级**, 使用 RTX2080 TI 匹配一个block 6秒, 对应的在 E5 2673 V3 上需要差不多 60秒.

这一步结束后会在input同一级目录下, 产生 distored, images, sparse, stereo 等目录

Train 训练
========

训练和之前快速评估时的方式是一样的, 用上一步提取的数据进行训练

    python train.py -s [素材路径]
    # e.g.
    python train.py -s ~/work/source_data/
    

View 查看
=======

poly.cam 网页工具
-------------

快速查看可以用第三方的网页工具, 例如 [https://poly.cam/tools/gaussian-splatting](https://poly.cam/tools/gaussian-splatting)

自采集数据效果演示

访问下面的链接会下载几十MB的文件, 如果带宽较小需要耐心等待. 有时候页面资源在墙内会被拦截, 需要梯子上网.

*   美团单车(正常光线) [https://poly.cam/capture/53684645-e0d4-413d-b979-834579f5e793](https://poly.cam/capture/53684645-e0d4-413d-b979-834579f5e793)
*   美团单车(强阳光) [https://poly.cam/capture/01a401a3-945a-4b8e-9181-068683b25b0f](https://poly.cam/capture/01a401a3-945a-4b8e-9181-068683b25b0f)

SIBR Viewer
-----------

网页工具比较方便, 但是预览效果不如项目自带的工具 SIBR Viewer. SIBR Viewer 需要编译安装, 编译过程需要nvcc, 如果是不支持CUDA的显卡, 这部分就不用尝试了. 代码对显卡CUDA架构版本也有要求, 要 CUDA\_ARCHITECTURE >= 7.x, 所以在 RTX10xx, P104-100, P106 上不能正常运行, 需要的显卡型号最低为 GTX16xx, RTX20xx, 低于这些型号的也不用尝试了.

### 编译安装

先安装依赖

    sudo apt install libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev
    

预编译, 这一步如果有错误, 检查上面的依赖是否已经安装

    cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release
    

编译并安装到项目路径下, `-j24`对应24核CPU, 设成和当前环境CPU核数一致

    cmake --build build -j24 --target install
    

安装后, 可执行文件会安装到 SIBR\_viewers/install/bin/ 目录下,

### 使用

通过命令行启动 SIBR Viewer 查看训练结果

    ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m [训练结果目录]
    

例如

    ./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m ./output/0dbedfba-8/
    

界面需要双手操作, 左手 W, E, F, A, S, D 控制左右前后上下平移, 右手 U, I, O, J, K, L 控制左右上下顺时逆时旋转. 如果觉得平移速度慢, 可以在 Camera Point VIew 中, 勾选 Acceleration

splatviz
--------

对于架构低于7.x的旧Nvidia显卡, 可以使用 splatviz 这个工具本地查看训练结果.

仓库地址 [https://github.com/Florian-Barthel/splatviz](https://github.com/Florian-Barthel/splatviz)

这是一个主体代码为 python 的项目, 可以唤起GUI界面在本地快速查看ply文件, 可以查看, 可以挂载到3d gaussian的训练过程, 可以设定位置距离拍摄自动306度旋转的视频, 还可以设置gaussian参数debug, 功能比较丰富.

### 安装

检出仓库

    git clone https://github.com/Florian-Barthel/splatviz.git --recursive
    

仓库首页的安装提示是不能用的, 因为里面基于的是 CUDA 11.8, 和我系统的CUDA版本(12.6)不一致, 用conda创建环境会报不兼容, 所以后来转手动安装了

因为和 [gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting) 一样要使用 CUDA Toolkit 和 pytorch, 都是上G的大块头, 所以直接使用了 gaussian-splatting 的 conda 环境, 在这个基础上安装其它模块.

基础环境是

    NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4
    
    python=3.10.12
    torch                       2.7.0
    torchaudio                  2.7.0
    torchvision                 0.22.0
    opencv-contrib-python       4.11.0.86
    opencv-python               4.11.0.86
    cuda-toolkit                12.4
    

过程中安装的模块, 未指定的默认安装最新版

    pip install click
    pip install imgui-bundle==1.5.2 (安装默认的1.6.2会报错)
    pip install imgui==2.0.0
    pip install imageio
    pip install pyyaml
    pip install pandas
    pip install imagecodecs
    pip install scipy
    pip install requests
    pip install gputil
    pip install pynvml  如果不使用 performance widget, 可以不装
    pip install imageio-ffmpeg 如果不使用 ideo widget, 可以不装
    

安装项目内的子模块

*   gaussian-splatting/submodules/diff-gaussian-rasterization

    pip install gaussian-splatting/submodules/diff-gaussian-rasterization
    

*   gaussian-splatting/submodules/simple-knn  
    因为在 gaussian-splatting 中已经安装了 simple-knn 模块, 所以这里没再安装

### 使用

用`--data_path`指定路径, 注意这里只需要给目录, 如果直接指定文件会报错

    python run_main.py --data_path=../gaussian-splatting/output/7f1a841f-4/point_cloud/iteration_7000/
    

在 Camera Widget里面可以设置两种视角,

*   Orbit视角是位置固定的, 只能左右上下(pitch and yaw, no roll)查看,
*   WASD视角的位置可以调整, 操作按键为 `A`左平移, `D`右平移, `W`前进, `D`后退, `Q`上平移,`E`下平移, 用鼠标按住左键移动旋转视角.

posted on 2025-05-30 20:13  [Milton](https://www.cnblogs.com/milton)  阅读(26)  评论(0)    [收藏](javascript:void\(0\))  [举报](javascript:void\(0\))