---
layout: post
title: '尝试从源头理解 SVD 原理和计算'
date: "2025-11-01T00:44:18Z"
---
尝试从源头理解 SVD 原理和计算
=================

SVD 是怎么被“想出来”的？——从一个朴素问题出发
==========================

你有没有见过这样的公式？

\\\[M = U \\Sigma V^T \\\]

看起来挺简洁，对吧？但当你翻开教材，发现这背后藏着一堆正交矩阵、奇异值、特征向量……瞬间头大。

我每次看到 SVD，都忍不住想：这玩意儿到底是怎么被“想出来”的？是某个数学家喝多了咖啡，突然梦见上帝说：“听着，所有矩阵都能拆成三步走……”

今天，我们不背公式，不套定理。我们要还原 SVD 的“发明”过程——从一个最朴素的问题出发：**一个矩阵，到底对向量做了什么？**

* * *

一、矩阵左乘 = 沿坐标轴的伸缩（从最简单例子开始）
--------------------------

我们从一个最简单的 \\(2 \\times 2\\) 对角矩阵入手：

\\\[D = \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix} \\\]

取任意向量 \\(\\mathbf{x} = \\begin{bmatrix} x\_1 \\\\ x\_2 \\end{bmatrix}\\)，左乘后得到：

\\\[D \\mathbf{x} = \\begin{bmatrix} 3x\_1 \\\\ x\_2 \\end{bmatrix} \\\]

这意味着：**输入向量在标准基方向 \\(\\mathbf{e}\_1 = (1,0)^T\\) 和 \\(\\mathbf{e}\_2 = (0,1)^T\\) 上被独立拉伸**——\\(x\\) 方向放大 3 倍，\\(y\\) 方向不变。

这个例子揭示了矩阵左乘的本质：**线性变换 = 对输入空间的各个方向进行伸缩（可能还混合）**。  
而对角矩阵之所以“干净”，是因为它恰好以标准基为伸缩方向，没有混合。

但现实中的矩阵通常不是对角的。那么问题来了：**非对角矩阵是否也能找到自己的“伸缩方向”？**

* * *

二、EVD：方阵的“主伸缩方向”与秩的含义
---------------------

考虑一个对称方阵：

\\\[A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\\]

我们寻找那些**被 \\(A\\) 作用后只伸缩、不转向**的向量 \\(\\mathbf{v}\\)，即满足：

\\\[A \\mathbf{v} = \\lambda \\mathbf{v} \\\]

这就是**特征方程**，其中 \\(\\lambda\\) 是特征值，\\(\\mathbf{v}\\) 是对应的特征向量。

对上面的 \\(A\\)，解得两组解：

*   \\(\\lambda\_1 = 3\\)，对应 \\(\\mathbf{v}\_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)
*   \\(\\lambda\_2 = 1\\)，对应 \\(\\mathbf{v}\_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\)

将这两个向量单位化（归一化），得到标准正交基：

\\\[\\mathbf{q}\_1 = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{q}\_2 = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\\]

把它们拼成正交矩阵 \\(Q = \[\\mathbf{q}\_1, \\mathbf{q}\_2\]\\)，则 \\(Q^T Q = I\\)。  
由于 \\(A \\mathbf{q}\_i = \\lambda\_i \\mathbf{q}\_i\\) 对每个列都成立，我们可以把所有等式合写为：

\\\[A Q = Q \\Lambda \\quad \\Rightarrow \\quad A = Q \\Lambda Q^T \\\]

其中

\\\[\\Lambda = \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix} \\\]

这就是**特征值分解（EVD）**。它告诉我们：**任何可对角化的方阵，本质上只是在一组特定正交方向上做独立伸缩**。

* * *

### 满秩 vs 低秩：不只是数学，更是能力

一个 \\(n \\times n\\) 矩阵的“能力”取决于它有多少个非零特征值。

*   **满秩矩阵**：比如
    
    \\\[A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\\]
    
    有两个非零特征值（3 和 1），秩为 2。它能对**任意方向**的输入产生非零输出——换句话说，它可以“操控”整个 2D 空间。
    
*   **低秩矩阵**：比如
    
    \\\[B = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\]
    
    特征值为 2 和 0，秩为 1。它只能在方向 \\(\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\\) 上拉伸，而在垂直方向 \\(\\begin{bmatrix}1 \\\\ -1\\end{bmatrix}\\) 上输出恒为零。**无论你输入什么，结果永远落在一条直线上**。
    

在深度学习中，这种差异至关重要：

*   **满秩变换**（如初始权重）具有最大表达能力，能响应任意输入变化；
*   **低秩更新**（如微调时的 \\(\\Delta W\\)）则表明：模型真正需要调整的，往往只是少数几个“敏感方向”。

这正是 **LoRA**（Low-Rank Adaptation）有效的核心原因：我们不需要改动整个高维权重矩阵，只需在低维子空间中微调，就能高效适配新任务。

但 EVD 有一个致命限制：**它只适用于方阵**。一旦矩阵是“长方形”的，比如 \\(M \\in \\mathbb{R}^{n \\times m}\\) 且 \\(n \\ne m\\)，特征方程 \\(M \\mathbf{v} = \\lambda \\mathbf{v}\\) 就因维度不匹配而失去意义。

于是，我们必须回答一个更一般的问题：**非方阵如何描述其“伸缩行为”？**

* * *

三、SVD：为非方阵找到“跨空间的主方向”
---------------------

面对 \\(M \\in \\mathbb{R}^{n \\times m}\\)，我们放弃“输入输出方向相同”的执念，转而问：

> 是否存在输入空间的一组标准正交基 \\(\\{\\mathbf{v}\_1, \\dots, \\mathbf{v}\_m\\}\\) 和输出空间的一组标准正交基 \\(\\{\\mathbf{u}\_1, \\dots, \\mathbf{u}\_n\\}\\)，使得
> 
> \\\[M \\mathbf{v}\_i = \\sigma\_i \\mathbf{u}\_i \\quad (i = 1, \\dots, r = \\min(n,m)) \\\]

这个等式是我们希望达成的目标：**第 \\(i\\) 个输入主方向 \\(\\mathbf{v}\_i\\)，只激发第 \\(i\\) 个输出主方向 \\(\\mathbf{u}\_i\\)，放大 \\(\\sigma\_i\\) 倍**。

我们按拉伸强度从大到小排序：\\(\\sigma\_1 \\geq \\sigma\_2 \\geq \\cdots \\geq \\sigma\_r \\geq 0\\)。

更一般的表示是

\\\[MV=U\\Sigma \\\]

后面我们可以知道\\(V\\)是正交矩阵，所以上式两边都右乘\\(V^T\\)，就可以得到常见的 SVD 的形式了

\\\[MVV^T=MVV^{-1}=M=U\\Sigma V^T \\\]

### 3.1 以最强方向 \\(\\sigma\_1\\) 为例

回归正题，我们该如何计算 \\(\\sigma\_i\\)呢？我们以最强方向，即 \\(\\sigma\_1\\)为最大值的情况为例。

假设存在单位向量 \\(\\mathbf{v}\_1\\) 和 \\(\\mathbf{u}\_1\\)，使得：

\\\[M \\mathbf{v}\_1 = \\sigma\_1 \\mathbf{u}\_1, \\quad \\|\\mathbf{v}\_1\\| = \\|\\mathbf{u}\_1\\| = 1. \\\]

两边取范数，得：

\\\[\\|M \\mathbf{v}\_1\\| = \\|\\sigma\_1 \\mathbf{u}\_1\\| = \\sigma\_1. \\\]

因此，\\(\\sigma\_1\\) 就是 \\(M\\) 在单位输入下能产生的最大输出长度。  
换句话说，\\(\\sigma\_1\\) 是如下优化问题的解：

\\\[\\sigma\_1 = \\max\_{\\|\\mathbf{v}\\| = 1} \\|M \\mathbf{v}\\|. \\\]

由于范数非负，等价于最大化其平方：

\\\[\\sigma\_1^2= \\max\_{\\|\\mathbf{v}\\| = 1} \\|M \\mathbf{v}\\|^2 = \\max\_{\\|\\mathbf{v}\\| = 1} \\mathbf{v}^T (M^T M) \\mathbf{v}. \\\]

### 3.2 计算奇异值和右奇异矩阵 V

记 \\(A = M^T M\\)。矩阵 \\(A\\) 是 \\(m \\times m\\) 实对称矩阵，且对任意 \\(\\mathbf{v}\\) 有 \\(\\mathbf{v}^T A \\mathbf{v} \\geq 0\\)，故 \\(A\\) 半正定。记 \\(A\\) 的特征值按非增序排列为 \\(\\lambda\_1 \\geq \\lambda\_2 \\geq \\cdots \\geq \\lambda\_m \\geq 0\\)，对应的标准正交特征向量为 \\(\\mathbf{q}\_1, \\dots, \\mathbf{q}\_m\\)，即

\\\[A \\mathbf{q}\_i = \\lambda\_i \\mathbf{q}\_i \\\]

瑞利商的极值性质表明(原理推导见本节末尾）：

\\\[\\max\_{\\|\\mathbf{v}\\| = 1} \\mathbf{v}^T A \\mathbf{v} = \\lambda\_1, \\\]

且最大值在 \\(\\mathbf{v} = \\mathbf{q}\_1\\) 处取得。更一般地，对 \\(k = 1, \\dots, m\\)，

\\\[\\max\_{\\substack{\\|\\mathbf{v}\\| = 1 \\\\ \\mathbf{v} \\perp \\mathbf{q}\_1, \\dots, \\mathbf{q}\_{k-1}}} \\mathbf{v}^T A \\mathbf{v} = \\lambda\_k, \\\]

在 \\(\\mathbf{v} = \\mathbf{q}\_k\\) 处取得。说人话就是，第k 大的值就是\\(\\lambda\_k\\)，而且是在\\(v=q\_k\\)时可以得到。

所以

\\\[\\sigma\_i^2 = \\max\_{\\|\\mathbf{v}\\| = 1} \\mathbf{v}^T (M^T M) \\mathbf{v} = \\lambda\_i \\quad i = 1, \\dots, m, \\\]

则 \\(\\sigma\_i=\\sqrt{\\lambda\_i}, \\sigma\_1 \\geq \\sigma\_2 \\geq \\cdots \\geq \\sigma\_m \\geq 0\\)，且

\\\[v\_i=q\_i \\\]

至此，我们成功求解了矩阵 V和奇异值矩阵\\(\\Sigma\\)

> **瑞利商性质**：对实对称矩阵 \\(A\\)，定义其**瑞利商**为
> 
> \\\[R\_A(\\mathbf{c}) = \\frac{\\mathbf{c}^T A \\mathbf{c}}{\\mathbf{c}^T \\mathbf{c}}, \\quad \\mathbf{c} \\ne \\mathbf{0}. \\\]
> 
> 当 \\(\\|\\mathbf{c}\\| = 1\\) 时，\\(R\_A(\\mathbf{c}) = \\mathbf{c}^T A \\mathbf{c}\\)。  
> 设 \\(A\\) 的特征值按非增序排列为 \\(\\lambda\_1 \\geq \\lambda\_2 \\geq \\cdots \\geq \\lambda\_m \\geq 0\\)，对应的标准正交特征向量为 \\(\\mathbf{q}\_1, \\dots, \\mathbf{q}\_m\\)，即
> 
> \\\[A \\mathbf{q}\_i = \\lambda\_i \\mathbf{q}\_i, \\quad \\mathbf{q}\_i^T \\mathbf{q}\_j = \\delta\_{ij}. \\\]
> 
> 瑞利商的极值性质表明：
> 
> \\\[\\max\_{\\|\\mathbf{c}\\| = 1} \\mathbf{c}^T A \\mathbf{c} = \\lambda\_1, \\\]
> 
> 且最大值在 \\(\\mathbf{c} = \\mathbf{q}\_1\\) 处取得。更一般地，对 \\(k = 1, \\dots, m\\)，
> 
> \\\[\\max\_{\\substack{\\|\\mathbf{c}\\| = 1 \\\\ \\mathbf{c} \\perp \\mathbf{q}\_1, \\dots, \\mathbf{q}\_{k-1}}} \\mathbf{c}^T A \\mathbf{c} = \\lambda\_k, \\\]
> 
> 在 \\(\\mathbf{c} = \\mathbf{q}\_k\\) 处取得。  
> 因此，令
> 
> \\\[\\sigma\_i = \\sqrt{\\lambda\_i}, \\quad \\mathbf{c}\_i = \\mathbf{q}\_i, \\quad i = 1, \\dots, m, \\\]
> 
> 则 \\(\\sigma\_1 \\geq \\sigma\_2 \\geq \\cdots \\geq \\sigma\_m \\geq 0\\)，且
> 
> \\\[\\|M \\mathbf{c}\_i\\|^2 = \\mathbf{c}\_i^T A \\mathbf{c}\_i = \\lambda\_i = \\sigma\_i^2. \\\]

### 3.3 构造左奇异矩阵

令 \\(r = \\operatorname{rank}(M)\\)。由于 \\(\\operatorname{rank}(M) = \\operatorname{rank}(M^T M)\\)，有 \\(\\sigma\_i > 0\\) 当且仅当 \\(i \\leq r\\)。

对每个 \\(i = 1, \\dots, r\\)，根据最前面的定义\\(M \\mathbf{v}\_i = \\sigma\_i \\mathbf{u}\_i\\)，我们有

\\\[\\mathbf{u}\_i = \\frac{1}{\\sigma\_i} M \\mathbf{v}\_i. \\\]

至此就可算出对应的\\(\\sigma\_i,v\_i,u\_i\\)。我们会发现求得的 \\(u\_i\\)也是基坐标，彼此正交：

\\\[\\|\\mathbf{u}\_i\\| = \\frac{1}{\\sigma\_i} \\|M \\mathbf{v}\_i\\| = \\frac{1}{\\sigma\_i} \\cdot \\sigma\_i = 1, \\\]

且

\\\[M \\mathbf{v}\_i = \\sigma\_i \\mathbf{u}\_i. \\\]

对 \\(i \\ne j \\leq r\\)，有

\\\[\\mathbf{u}\_i^T \\mathbf{u}\_j = \\frac{1}{\\sigma\_i \\sigma\_j} \\mathbf{v}\_i^T M^T M \\mathbf{v}\_j = \\frac{1}{\\sigma\_i \\sigma\_j} \\mathbf{v}\_i^T (\\sigma\_j^2 \\mathbf{v}\_j) = \\sigma\_j \\cdot \\mathbf{v}\_i^T \\mathbf{v}\_j = 0, \\\]

故 \\(\\{\\mathbf{u}\_1, \\dots, \\mathbf{u}\_r\\}\\) 是 \\(\\mathbb{R}^n\\) 中的标准正交向量组。

前面计算的\\(u\_i\\)是与\\(v\_i\\)一一对应的，但是当 \\(r < n\\)时，剩下的\\(u\_i\\)该如何计算呢？我们会发现存在 \\(n - r\\) 维子空间

\\\[\\mathcal{U}\_\\perp = \\left\\{ \\mathbf{x} \\in \\mathbb{R}^n \\,\\middle|\\, \\mathbf{u}\_i^T \\mathbf{x} = 0,\\ \\forall i = 1, \\dots, r \\right\\}. \\\]

在 \\(\\mathcal{U}\_\\perp\\) 中任取一组标准正交基 \\(\\{\\mathbf{u}\_{r+1}, \\dots, \\mathbf{u}\_n\\}\\)，则最终的左奇异矩阵为

\\\[U = \[\\mathbf{u}\_1, \\dots, \\mathbf{u}\_n\] \\in \\mathbb{R}^{n \\times n} \\\]

为正交矩阵。

### 3.4 拼装 SVD

令

*   \\(V = \[\\mathbf{v}\_1, \\dots, \\mathbf{v}\_m\] \\in \\mathbb{R}^{m \\times m}\\)，
*   \\(\\Sigma \\in \\mathbb{R}^{n \\times m}\\) 为对角矩阵，其对角元为 \\(\\sigma\_1, \\dots, \\sigma\_r\\)，其余元素为 0。

由 \\(M \\mathbf{v}\_i = \\sigma\_i \\mathbf{u}\_i\\) 对 \\(i = 1, \\dots, r\\) 成立，且对 \\(i > r\\) 有 \\(\\sigma\_i = 0\\)，可得矩阵等式

\\\[M V = U \\Sigma. \\\]

由于 \\(V\\) 正交（\\(V^T V = I\_m\\)），右乘 \\(V^T\\) 得

\\\[M = U \\Sigma V^T. \\\]

* * *

结语
--

SVD 并非凭空定义的数学魔术，而是为了解决“**非方阵如何描述伸缩**”这一朴素问题，从对角矩阵 → EVD → 跨空间推广，一步步自然推导出的必然结果。

当你再看到 \\(M = U \\Sigma V^T\\)，请记住：  
**它只是在说——先转一下，再拉伸，再转一下。**  
而这，就是所有线性变换最干净的表达方式。