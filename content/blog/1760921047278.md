---
layout: post
title: '[Agent] ACE（Agentic Context Engineering）和Dynamic Cheatsheet学习笔记'
date: "2025-10-20T00:44:07Z"
---
\[Agent\] ACE（Agentic Context Engineering）和Dynamic Cheatsheet学习笔记
=================================================================

\[Agent\] ACE（Agentic Context Engineering）和Dynamic Cheatsheet学习笔记
-----------------------------------------------------------------

目录

*   [\[Agent\] ACE（Agentic Context Engineering）和Dynamic Cheatsheet学习笔记](#agent-aceagentic-context-engineering和dynamic-cheatsheet学习笔记)
*   [0x00 概述](#0x00-概述)
*   [0x01 ACE](#0x01-ace)
    *   [1.1 背景](#11-背景)
    *   [1.2 思路](#12-思路)
    *   [1.3 工作流程](#13-工作流程)
*   [0x02 Dynamic Cheatsheet](#0x02-dynamic-cheatsheet)
    *   [2.1 总体思路](#21-总体思路)
    *   [2.2 四种模式](#22-四种模式)
        *   [2.2.1 Defalt模式（无记忆模式）：固定 “空记忆” 与无历史参数](#221-defalt模式无记忆模式固定-空记忆-与无历史参数)
        *   [2.2.2 FullHistoryAppending 模式（完整历史拼接模式）：循环拼接历史输入输出](#222-fullhistoryappending-模式完整历史拼接模式循环拼接历史输入输出)
        *   [2.2.3 DynamicCheatsheet\_Cumulative模式（结构化增量记忆模式）：多轮 “生成→更新记忆” 循环](#223-dynamiccheatsheet_cumulative模式结构化增量记忆模式多轮-生成更新记忆-循环)
        *   [2.2.4 Dynamic\_Retrieval模式（相似历史检索模式）：余弦相似度计算 + TopK 筛选](#224-dynamic_retrieval模式相似历史检索模式余弦相似度计算--topk-筛选)
        *   [2.3.4 总结](#234-总结)
    *   [2.3 模式对比](#23-模式对比)
        *   [2.3.1 FullHistoryAppending vs 动态备忘录](#231-fullhistoryappending-vs-动态备忘录)
            *   [历史信息处理方式](#历史信息处理方式)
            *   [迭代机制](#迭代机制)
            *   [备忘录更新机制](#备忘录更新机制)
            *   [使用场景](#使用场景)
        *   [2.3.2 动态累计 vs 动态检索](#232-动态累计-vs-动态检索)
            *   [历史信息选择方式](#历史信息选择方式)
            *   [上下文构建方式](#上下文构建方式)
            *   [压缩和筛选](#压缩和筛选)
            *   [计算复杂度](#计算复杂度)
            *   [适用场景](#适用场景)
*   [0x03 DC代码分析](#0x03-dc代码分析)
    *   [3.1 prompt](#31-prompt)
    *   [3.2 核心代码](#32-核心代码)
*   [0x04 ACE的优化](#0x04-ace的优化)
    *   [4.1 以DC作为基石](#41-以dc作为基石)
        *   [4.1.1 DC 适用场景](#411-dc-适用场景)
        *   [4.1.2 提示词](#412-提示词)
            *   [Generator的提示词](#generator的提示词)
            *   [Curator的提示词](#curator的提示词)
    *   [4.2 关键创新](#42-关键创新)
        *   [4.2.1 增量式 Delta 更新](#421-增量式-delta-更新)
        *   [4.2.2 Grow-and-Refine](#422-grow-and-refine)
        *   [4.2.3 Reflector阶段](#423-reflector阶段)
            *   [Reflector提示词](#reflector提示词)
            *   [深入分析原因](#深入分析原因)
                *   [错误诊断与根本原因分析的需求](#错误诊断与根本原因分析的需求)
                *   [环境反馈的充分利用](#环境反馈的充分利用)
                *   [更准确的知识提取](#更准确的知识提取)

0x00 概述
-------

前几天，斯坦福的ACE（Agentic Context Engineering）非常火。只看论文感觉还是理解不深，但是该论文并没有释放对应的源码。不过，ACE是基于Dynamic Cheatsheet完成，且两篇论文有共同作者，于是就找Dynamic Cheatsheet的论文和源码进行解读，得到本文。

> Building on the agentic architecture of Dynamic Cheatsheet \[41\], ACE incorporates a modular workflow of generation, reflection, and curation, while adding structured, incremental updates guided by a grow-and-refine principle. This design preserves detailed, domain-specific knowledge, prevents context collapse, and yields contexts that remain comprehensive and scalable throughout adaptation.
> 
> 基于动态备忘录\[41\]的代理架构，ACE整合了一个包含生成、反思和策展的模块化工作流程，并增加了由成长和完善原则指导的结构化、增量更新。这种设计保留了详细、特定领域的知识，防止了上下文的崩溃，并产生了在整个适应过程中保持全面和可扩展的上下文。

Agentic Context Engineering的信息。

*   论文标题：Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models
*   论文地址：[https://www.arxiv.org/abs/2510.04618](https://www.arxiv.org/abs/2510.04618)
*   概要：不依赖模型重新训练，而是让上下文自主进化，通过反复生成、反思并编辑自己的提示，直至成为一个自我完善的系统。

Dynamic Cheatsheet的信息。

*   论文标题：Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory
*   论文地址：[https://www.arxiv.org/abs/2504.07952v1](https://www.arxiv.org/abs/2504.07952v1)
*   论文概要：动态备忘录 (DC) 框架通过为黑盒 LLM 配备可演进的外部记忆，使其在测试时能够学习和重用问题解决策略，为推理时学习和持续学习研究提供了新的思路。

0x01 ACE
--------

### 1.1 背景

在人工智能领域，有一种提升模型表现的方法，叫做上下文适配(context adaptation)。这种方法不是通过改变模型内部的参数来实现，而是通过在输入数据中添加更明确的指示、结构化的推理步骤或者特定领域的输入格式来提升模型的性能。

上下文适配在很多AI系统中扮演着重要角色，比如帮助指导任务完成的系统提示、存储过往信息和经验的记忆功能，以及用来减少错误和补充知识的事实依据。然而，这种方法也面临两个挑战：

*   第一个挑战是“简约偏置”问题。一些优化提示词的工具倾向于使用简短、通用的指令，这样做可能会忽略掉积累知识的需要。比如，有些系统认为简短就是好，但这种偏好可能会忽略掉在实际操作中非常重要的专业知识、工具使用指导或者常见的错误模式。这样的优化虽然在某些方面有效，但往往无法捕捉到智能体或知识密集型应用所需的细节策略。
*   第二个挑战是“上下文塌缩”问题。依赖大型语言模型（LLM）对整个提示进行重写，随着时间的推移，往往会退化成更短、更模糊的摘要，这会导致性能急剧下降。在交互式智能体、特定领域的编程、金融或法律分析等任务中，系统的性能依赖于保留详细的、与任务相关的知识，而不是简单地将其压缩掉。

简而言之，上下文适配是一种通过改善输入数据来提升AI模型性能的方法，但它需要克服简约偏置和上下文塌缩这两个问题，以确保模型能够充分利用详细的知识和指令来完成任务。

### 1.2 思路

可以把ACE看作是一个智能助手，它不仅仅是简单地记住一些指令或者压缩信息，而是像一本活的操作手册，能够随着时间的推移不断学习和改进。

这个智能助手的理念是，与其把知识简化成简短的摘要，不如让它包含丰富、详细的内容，并且能够随着新信息的加入而不断更新。这样，当智能助手需要解决问题时，它就能够从这些详细的信息中提取出最有用的部分来帮助我们。

这个智能助手的工作方式被设计成三个部分，就像一个团队各自负责不同的任务：

1.  **生成器**：它产生推理轨迹，负责创建解决问题的步骤和路径。
2.  **反思器**：它负责回顾这些步骤，看看哪些做得好，哪些需要改进。即从成功和错误中提炼具体的见解。
3.  **整理器**：它负责将反思的结果整理成新的、更有用的信息，然后更新到操作手册中。即将这些见解整合到结构化的上下文更新中。

### 1.3 工作流程

生成器/反思器/整理器这个团队的工作是不断改进一本工作手册，让它成为解决问题的最佳指南。这个团队的工作流程是这样的：

*   首先，团队中的“创意人员（生成器）”会针对新的任务或问题，设计出一系列的解决步骤，这些步骤不仅会展示出有效的解决方法，还会指出常见的错误和陷阱。
*   接着，“分析师（反思器）”会对这些解决步骤进行仔细的评估，从中提取出宝贵的经验和教训。这个过程可以重复多次，每次都会让这些经验更加完善。
*   然后，“整合师（整理器）”会把这些经验整理成简短的更新条目，这些条目会被添加到工作手册中，以更新和改进它。这个过程不需要复杂的逻辑，只需要一些简单的规则就能完成。

由于这些更新是分开进行的，所以可以同时处理多个更新，这样可以快速地改进工作手册，让它包含更多的知识和经验。此外，这个团队的工作方式还允许他们多次回顾和改进同一个任务的解决方案，就像是反复练习一样，每次都能让工作手册的内容更加丰富和准确。

总的来说，这个团队通过不断的创造、分析和整合，使得工作手册能够持续进化，始终保持最新和最有效的解决问题的方法。这个过程就像是我们人类的学习过程：我们尝试做一些事情（实验），然后思考我们做得怎么样（反思），最后把学到的东西整合到我们的知识体系中（整合）。通过这种方式，智能助手可以避免因为尝试做太多事情而变得混乱，而是能够更有效地学习和成长。同时避免了将所有责任都压在单一模型上的瓶颈。

0x02 Dynamic Cheatsheet
-----------------------

动态备忘录（DC）是一种测试时学习方法，引入了一种可适应的外部记忆，用于存储可重用的策略和代码片段。通过不断地用新遇到的输入和输出更新这个记忆，DC使模型能够积累知识并在不同任务中重用它，通常比静态提示方法带来显著改进。DC的一个关键优势是它不需要真实标签：模型可以从自己的生成中策划自己的记忆，使该方法高度灵活且广泛适用。

动态备忘录推理过程中从过去的成功和失败中积累策略和教训。这些自然语言反馈方法代表了重大进步，为改进LLM系统提供了灵活且可解释的信号，超越了权重更新。

### 2.1 总体思路

动态备忘录可以让复杂的计算机程序在解决问题时能够变得更聪明。

计算机程序就像一个学生，以前它在解决新问题时总是从头开始，没有记住以前学到的东西。但现在，我们给它加了一个“动态备忘录”，就像一个可以不断更新的笔记本，帮助它记住解决问题的方法。

这个“动态备忘录”不需要改变程序本身的设置，只是在程序外面加了一个可以学习和记忆的工具。这样，程序在解决新问题时，就可以参考以前的经验，做得更好，也不容易重复犯同样的错误。

这个新方法有两个主要的部分：一部分负责帮助程序生成解决问题的策略，另一部分负责管理这些策略，让它们能够被有效地存储和更新。

论文作者还尝试了两种不同的“动态备忘录”策略，一种是让程序逐渐积累记忆，另一种是通过检索和合成来更新记忆。实验结果表明，这种方法在解决数学问题、逻辑谜题和知识问答等复杂任务时都非常有效，程序的性能得到了显著提升。

这项研究还发现，记忆增强的效果会受到程序大小、使用的工具和任务类型等因素的影响，这为未来如何让程序在解决问题时学习得更好提供了新的想法。

### 2.2 四种模式

DC有四种模式（`default`、`FullHistoryAppending`、`DynamicCheatsheet_Cumulative`、`Dynamic_Retrieval`）

#### 2.2.1 Defalt模式（无记忆模式）：固定 “空记忆” 与无历史参数

*   特点：无记忆机制，每次查询独立处理。
    
*   实现逻辑
    
    *   仅使用当前输入文本和固定模板生成回答，完全不参考历史信息。
    *   cheatsheet 固定为`(empty)`，不会更新或存储任何历史知识。
    *   适合无需上下文依赖的孤立查询。

#### 2.2.2 FullHistoryAppending 模式（完整历史拼接模式）：循环拼接历史输入输出

*   特点：直接拼接所有历史记录作为上下文，无筛选机制。
    
    *   通过`for`循环遍历`original_input_corpus`和`generator_outputs_so_far`，将所有历史 “输入 + 输出” 拼接到`curated_cheatsheet`中，无相似度计算逻辑。
*   实现逻辑：
    
    *   将之前所有的输入输出对（`previous_input_output_pairs`）完整附加到当前查询的上下文中。
    *   不使用结构化的 cheatsheet，而是依赖原始对话历史。

#### 2.2.3 DynamicCheatsheet\_Cumulative模式（结构化增量记忆模式）：多轮 “生成→更新记忆” 循环

*   特点：维护一个增量更新的结构化记忆（cheatsheet），持续积累知识。
    
    *   存在`for`循环（控制迭代轮次），且每轮都包含 “生成回答” 和 “更新 cheatsheet” 两步逻辑，依赖`cheatsheet_template`。
*   实现逻辑：
    
    *   每次查询后，根据当前问题和模型回答，通过`cheatsheet_template`对 cheatsheet 进行更新（保留有用信息，剔除冗余内容）。
    *   新查询时，将当前 cheatsheet 作为上下文传入，帮助模型复用历史知识。

#### 2.2.4 Dynamic\_Retrieval模式（相似历史检索模式）：余弦相似度计算 + TopK 筛选

*   特点：基于相似度检索历史信息，动态选择相关记忆。
    
    *   调用`cosine_similarity`计算当前输入与历史输入的相似度，通过`np.argsort`筛选`top_k`条相关历史；Synthesis 子模式额外包含 “基于检索结果生成结构化记忆” 的步骤。
*   实现逻辑：
    
    *   预先存储历史输入的嵌入向量（`original_input_embeddings`）。
    *   对新查询计算嵌入，通过余弦相似度检索最相关的`top_k`条历史输入。
    *   仅将检索到的相关历史信息传入模型，而非全部历史。

#### 2.3.4 总结

我们使用下表对这四种模型进行简单总结。

模式

记忆机制

适用场景

核心优势

`default`

无记忆

孤立查询

轻量、无历史依赖

`FullHistoryAppending`

完整历史拼接

上下文强相关且简短的场景

保留完整上下文

`DynamicCheatsheet_Cumulative`

结构化增量记忆

连续问题求解（如数学推理）

知识积累与复用，去冗余

`Dynamic_Retrieval`

相似度检索相关历史

大规模或多样化查询

精准筛选有用信息，效率高

这些模式的设计体现了从 “无记忆” 到 “选择性记忆” 再到 “结构化记忆” 的演进，以适应不同场景下对历史信息利用的需求。

### 2.3 模式对比

我们接下来对四种模式（`default`、`FullHistoryAppending`、`DynamicCheatsheet_Cumulative`、`Dynamic_Retrieval`）进行详细对比，其核心区别在于对历史信息的处理方式和记忆机制。

#### 2.3.1 FullHistoryAppending vs 动态备忘录

default 没有什么对比价值，我们直接看FullHistoryAppending和DC（以DynamicCheatsheet\_Cumulative为例）的对比。

##### 历史信息处理方式

*   FullHistoryAppending：
    *   将所有之前的输入输出都附加到备忘录。
    *   使用固定格式展示历史案例。
    *   不会对历史信息进行修改和更新。
*   DynamicCheatsheet\_Cumulative：
    *   在每一轮都积累更新备忘录内容。
    *   可以选择性地将之前轮次的答案添加到备忘录中。
    *   每轮都会通过专门的备忘录模板来更新备忘录内容。

##### 迭代机制

*   FullHistoryAppending：
    *   只执行一轮生成，不支持多轮迭代。
    *   直接使用已有的历史记录作为上下文。
*   DynamicCheatsheet\_Cumulative：
    *   支持多轮迭代。
    *   每轮都会更新备忘录并在下一轮使用更新后的内容。

##### 备忘录更新机制

*   FullHistoryAppending：
    *   备忘录是静态的，只能简单的拼接历史记录。
    *   不会修改或者优化备忘录内容。
*   DynamicCheatsheet\_Cumulative：
    *   动态更新，使用专门的备忘录模板来提取、更新备忘录内容。
    *   每轮都会使用新的备忘录。

##### 使用场景

*   FullHistoryAppending：
    *   ”回顾历史“，适用于需要完整历史记录但是不需要动态调整备忘录得场景。
    *   或者适合上下文关联性极强，但历史记录量较小的场景（否则可能因上下文过长导致效率下降）。
*   DynamicCheatsheet\_Cumulative：
    *   ”学习和改进“，适合逐步优化和完善备忘录得场景。即适合需要逐步积累经验的场景（如连续的数学推理、问题求解），记忆随查询序列动态生长。

#### 2.3.2 动态累计 vs 动态检索

我们来看看Dynamic Cheatsheet内部的两种动态模式的区别。

##### 历史信息选择方式

*   DynamicCheatsheet\_Cumulative：
    *   使用所有历史信息或者按顺序积累的历史答案。
    *   不进行相似性检索，而是直接使用全部历史或者逐步累积的信息 。
*   Dynamic\_Retrieval：
    *   使用embedding和余弦相似度来检索与当前输入最相似的历史输入输出对。
    *   只选择top-k个最相关的历史案例。
    *   基于语义相似性而非时间顺序选择历史信息。

##### 上下文构建方式

*   DynamicCheatsheet\_Cumulative：
    *   可以选择将之前所有轮次的答案都加入到备忘录中。
    *   备忘录内容随轮次增长而增长。
*   Dynamic\_Retrieval：
    *   构建的备忘录只包含与当前问题最相似的几个历史案例。
    *   添加了如何使用这些历史案例的指导说明。

##### 压缩和筛选

*   DynamicCheatsheet\_Cumulative：主要通过迭代提炼和模板提取实现压缩
    *   信息筛选压缩。
        *   通过参数控制是否将历史答案添加到备忘录。
        *   使用模板从模型输出中提取关键信息形成新的备忘录。实际上删除了大量冗余内容。
        *   从大量输出中提取精炼的备忘录内容。实际上删除了大量冗余内容。
        *   通过专门备忘录提取步骤实现信息压缩。删除了无关内容。
    *   迭代优化压缩。
        *   每轮迭代都基于上一轮结果生成更精简的备忘录。
*   Dynamic\_Retrieval：通过相似性检索，从大量历史数据中选择最相关的部分。
    *   相似性检索压缩。
        *   只选择最相关的K个历史案例。删除了无关历史。
    *   语义层面压缩。
        *   不是简单的时间顺序选择，而是基于语义相似性的智能筛选。
        *   只保留与当前问题最相关的上下文信息。

##### 计算复杂度

*   DynamicCheatsheet\_Cumulative：
    *   计算简单，只需累积历史答案。但是备忘录内容可能随着时间增长变得很大。
*   Dynamic\_Retrieval：
    *   需要计算embedding和相似度，计算复杂度较高。但是传递给模型的历史信息较少（topk）。

##### 适用场景

*   DynamicCheatsheet\_Cumulative：
    *   历史信息较少，且大部分有用的情况。
    *   适合逐步学习和完善解决方案的场景。
*   Dynamic\_Retrieval：
    *   历史案例较多，但是只有部分和当前问题相关的情况。
    *   适合处理大量历史数据的情况。
    *   适合大规模场景或多样化查询，避免无关信息干扰。

0x03 DC代码分析
-----------

### 3.1 prompt

摘录 prompts\\curator\_prompt\_for\_dc\_retrieval\_synthesis.txt

    # CHEATSHEET CURATOR
    
    ## Purpose and Goals
    You are responsible for maintaining, refining, and optimizing the Dynamic Cheatsheet, which serves as a compact yet evolving repository of problem-solving strategies, reusable code snippets, and meta-reasoning techniques. Your goal is to enhance the model’s long-term performance by continuously updating the cheatsheet with high-value insights while filtering out redundant or trivial information.
    
    - The cheatsheet should include quick, accurate, reliable, and practical solutions to a range of technical and creative challenges. 
    - After seeing each input, you should improve the content of the cheatsheet, synthesizing lessons, insights, tricks, and errors learned from past problems and adapting to new challenges.
    
    ---
    
    ### Core Responsibilities
    
    Selective Knowledge Retention:
    - Preserve only high-value strategies, code blocks, insights, and reusable patterns that significantly contribute to problem-solving.
    - Discard redundant, trivial, or highly problem-specific details that do not generalize well.
    - Ensure that previously effective solutions remain accessible while incorporating new, superior methods.
    
    Continuous Refinement & Optimization:
    - Improve existing strategies by incorporating more efficient, elegant, or generalizable techniques.
    - Remove duplicate entries or rephrase unclear explanations for better readability.
    - Introduce new meta-strategies based on recent problem-solving experiences.
    
    Structure & Organization:
    - Maintain a well-organized cheatsheet with clearly defined sections:
      - Reusable Code Snippets and Solution Strategies
      - General Problem-Solving Heuristics
      - Optimization Techniques & Edge Cases
      - Specialized Knowledge & Theorems
    - Use tagging (e.g., Q14, Q22) to reference previous problems that contributed to a given strategy.
    
    ---
    
    ## Principles and Best Practices
    
    For every new problem encountered:
    1. Evaluate the Solution’s Effectiveness  
       - Was the applied strategy optimal?
       - Could the solution be improved, generalized, or made more efficient?
       - Does the cheatsheet already contain a similar strategy, or should a new one be added?
    
    2. Curate & Document the Most Valuable Insights
       - Extract key algorithms, heuristics, and reusable code snippets that would help solve similar problems in the future.
       - Identify patterns, edge cases, and problem-specific insights worth retaining.
       - If a better approach than a previously recorded one is found, replace the old version.
    
    3. Maintain Concise, Actionable Entries
       - Keep explanations clear, actionable, concise, and to the point.
       - Include only the most effective and widely applicable methods.
       - Seek to extract useful and general solution strategies and/or Python code snippets.
    
    4. Implement a Usage Counter
       - Each entry must include a usage count: Increase the count every time a strategy is successfully used in problem-solving.
       - Use the count to prioritize frequently used solutions over rarely applied ones.
    ---
    

### 3.2 核心代码

DC的核心代码解读如下：

    def advanced_generate(self,
        approach_name: str,
        input_txt: str,
        cheatsheet: str = None,
        generator_template: str = None,
        cheatsheet_template: str = None,
        temperature: float = 0.0,
        max_tokens: int = 2048,
        max_num_rounds: int = 1,
        allow_code_execution: bool = True,
        code_execution_flag: str = "EXECUTE CODE!",
        add_previous_answers_to_cheatsheet: bool = True,
        original_input_corpus: List[str] = None,
        original_input_embeddings: np.ndarray = None,
        generator_outputs_so_far: List[str] = None,
        retrieve_top_k: int = 3,
    ) -> Tuple[str, str, str, str]:
        """
        语言模型生成响应的核心方法，支持四种不同的历史信息处理模式（default/FullHistoryAppending/DynamicCheatsheet_Cumulative/Dynamic_Retrieval）
        每种模式通过差异化的上下文（cheatsheet）处理逻辑，适配不同场景下的模型生成需求
    
        Arguments:
            approach_name : str : 选择的生成模式名称（决定上下文处理逻辑）
            input_txt : str : 用户当前输入的问题文本
            cheatsheet : str : 上下文记忆载体（不同模式下含义不同，如结构化记忆/历史拼接内容）
            generator_template : str : 生成器模板（含[[QUESTION]]/[[CHEATSHEET]]等占位符，用于拼接prompt）
            cheatsheet_template : str : 记忆更新模板（仅DynamicCheatsheet系列模式需用，用于生成新cheatsheet）
            temperature : float : 生成采样温度（0.0为确定性生成，值越高随机性越强）
            max_tokens : int : 生成文本的最大token数（控制输出长度）
            max_num_rounds : int : 最大生成轮次（仅DynamicCheatsheet_Cumulative支持多轮迭代）
            allow_code_execution : bool : 是否允许模型触发代码执行（如数学计算、数据处理场景）
            code_execution_flag : str : 触发代码执行的特定标识（模型输出包含此标识时执行代码）
            add_previous_answers_to_cheatsheet : bool : 是否将历史回答加入当前cheatsheet（仅多轮模式生效）
            original_input_corpus : List[str] : 历史输入文本集合（Dynamic_Retrieval模式用于检索相似历史）
            original_input_embeddings : np.ndarray : 历史输入的向量表示（用于计算相似度，支撑检索逻辑）
            generator_outputs_so_far : List[str] : 历史生成结果集合（与original_input_corpus一一对应）
            retrieve_top_k : int : 检索模式下返回的最相似历史条数（控制上下文相关性范围）
    
        Returns:
            Tuple[str, str, str, str] : 包含生成回答、评估方案、答案校验结果、新记忆的四元组
            （实际返回为字典，包含更详细的步骤日志、历史记录等元信息）
    
        Raises:
            ValueError : 当模式所需的关键参数缺失时触发（如Dynamic模式缺cheatsheet）
        """
    
        # ---------------------------- 1. default模式：无记忆模式 ------------------------------
        # 核心逻辑：不依赖任何历史信息，仅用当前输入生成回答，cheatsheet固定为"(empty)"
        if approach_name == "default":
            # 替换生成器模板的占位符：问题用当前输入，记忆用空值（无历史信息）
            generator_prompt = generator_template.replace("[[QUESTION]]", input_txt).replace("[[CHEATSHEET]]", "(empty)")
            # 构造模型输入的对话历史（仅包含当前prompt，无历史上下文）
            generator_history = [
                {"role": "user", "content": generator_prompt},
            ]
            # 调用基础生成方法，生成模型输出（控制温度、最大长度、是否允许代码执行）
            generator_output = self.generate(
                history=generator_history,
                temperature=temperature,
                max_tokens=max_tokens,
                allow_code_execution=allow_code_execution,
                code_execution_flag=code_execution_flag,
            )
    
            # 从模型原始输出中提取关键回答（过滤冗余内容，保留核心结果）
            generator_answer = extract_answer(
                generator_output,
            )
    
            # 返回结果字典：包含输入文本、单轮步骤日志、最终回答（无记忆故cheatsheet为None）
            return {
                "input_txt": input_txt,
                "steps": [
                    {
                        "round": 0,  # 仅1轮生成
                        "generator_prompt": generator_prompt,  # 传入模型的完整prompt
                        "generator_output": generator_output,  # 模型原始输出
                        "generator_answer": generator_answer,  # 提取后的核心回答
                        "current_cheatsheet": None,  # 当前记忆（无）
                        "new_cheatsheet": None,       # 新生成记忆（无）
                    }
                ],
                "previous_answers": None,  # 无历史回答
                "final_answer": generator_answer,  # 最终返回的核心回答
                "final_output": generator_output,  # 最终返回的原始输出
                "final_cheatsheet": None,  # 最终记忆（无）
                "generator_output": generator_output,
            }
        
        # ------------- 2. DynamicCheatsheet_Cumulative模式：结构化增量记忆模式 -----------------
        # 核心逻辑：维护一个可迭代更新的结构化cheatsheet，多轮生成中持续积累历史知识
        elif approach_name == "DynamicCheatsheet_Cumulative":
            # 校验关键参数：此模式必须传入初始cheatsheet和记忆更新模板，否则报错
            
            # 初始化步骤日志（记录每轮生成细节）和历史回答列表（用于多轮间传递信息）
            steps = []
            previous_answers = []
    
            # 初始化生成结果（多轮迭代后存储最终输出）
            generator_output = ''
    
            # 多轮迭代生成（至少1轮，最多max_num_rounds轮）
            for round in range(max(1, max_num_rounds)):
                ## 步骤1：基于当前记忆（cheatsheet）生成本轮回答
                # 基础记忆为初始cheatsheet（后续轮次会更新）
                generator_cheatsheet_content = cheatsheet
    
                # 若为多轮（非第1轮）且允许加入历史回答，则将过往回答拼接到当前记忆中
                if round > 0 and add_previous_answers_to_cheatsheet:
                    # 格式化历史回答（标注轮次，用分号分隔）
                    previous_answers_txt = f"PREVIOUS ANSWERS:\n{'; '.join(previous_answers)}"
                    # 更新记忆内容：原始cheatsheet + 历史回答
                    generator_cheatsheet_content = f"{generator_cheatsheet_content}\n\n{previous_answers_txt}"
    
                # 替换生成器模板：问题用当前输入，记忆用更新后的cheatsheet
                generator_prompt = generator_template.replace("[[QUESTION]]", input_txt).replace("[[CHEATSHEET]]", generator_cheatsheet_content)
                # 记录本轮生成前的当前记忆（用于后续日志和对比）
                current_cheatsheet = cheatsheet
    
                # 构造模型输入的对话历史（仅包含当前prompt）
                generator_history = [{"role": "user", "content": generator_prompt}]
                # 调用基础生成方法，生成本轮模型输出
                generator_output = self.generate(
                    history=generator_history,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    allow_code_execution=allow_code_execution,
                    code_execution_flag=code_execution_flag,
                )
                # 提取本轮核心回答（过滤冗余内容）
                generator_answer = extract_answer(generator_output)
    
                ## 步骤2：基于本轮回答更新结构化记忆（cheatsheet）
                # 替换记忆更新模板：问题、本轮回答、上一轮记忆分别填入对应占位符
                cheatsheet_prompt = cheatsheet_template.replace("[[QUESTION]]", input_txt).replace("[[MODEL_ANSWER]]", generator_output).replace("[[PREVIOUS_CHEATSHEET]]", current_cheatsheet)
    
                # 构造记忆更新的对话历史（仅包含cheatsheet生成prompt）
                cheatsheet_history = [{"role": "user", "content": cheatsheet_prompt}]
                # 生成新的cheatsheet（禁用代码执行，最大长度为生成器的2倍，确保记忆完整性）
                cheatsheet_output = self.generate(
                    history=cheatsheet_history,
                    temperature=temperature,
                    max_tokens=2*max_tokens,
                    allow_code_execution=False,
                )
    
                # 从记忆生成输出中提取新cheatsheet：若无有效内容则保留旧记忆（避免记忆丢失）
                new_cheatsheet = extract_cheatsheet(response=cheatsheet_output, old_cheatsheet=current_cheatsheet)
                # 更新cheatsheet为新记忆（供下一轮迭代使用）
                cheatsheet = new_cheatsheet
    
                # 将本轮回答加入历史回答列表（供后续轮次使用，若开启add_previous_answers_to_cheatsheet）
                previous_answers.append(f"Round {round+1}: {generator_answer}")
            
                # 记录本轮步骤日志（包含prompt、输出、记忆变化等细节）
                steps.append({
                    "round": round,
                    "generator_prompt": generator_prompt,
                    "generator_output": generator_output,
                    "generator_answer": generator_answer,
                    "current_cheatsheet": current_cheatsheet,  # 本轮生成前的记忆
                    "new_cheatsheet": new_cheatsheet,         # 本轮生成后的新记忆
                })
    
            # 返回结果字典：包含多轮步骤日志、历史回答、最终记忆和最终回答
            return {
                "input_txt": input_txt,
                "steps": steps,
                "previous_answers": previous_answers,
                "final_answer": generator_answer,
                "final_cheatsheet": new_cheatsheet,  # 多轮迭代后的最终记忆
                "final_output": generator_output,
            }
        
        # ------------------- 3. FullHistoryAppending模式：完整历史拼接模式 ---------------------
        # 核心逻辑：无筛选拼接所有历史输入输出对作为记忆，不做结构化处理
        elif approach_name == "FullHistoryAppending":
            # 获取历史生成结果的数量（决定拼接的历史条数）
            length_of_history = len(generator_outputs_so_far)
            # 若有历史记录，则拼接所有历史输入输出对作为记忆
            if length_of_history > 0:
                # 历史输入集合（与历史输出一一对应）
                top_k_original_inputs = original_input_corpus[:length_of_history]
                # 历史输出集合
                top_k_original_outputs = generator_outputs_so_far
    
                # 格式化拼接历史：用markdown标题分隔，标注输入序号和对应的模型回答
                curated_cheatsheet = "### PREVIOUS SOLUTIONS (START)\n\n"
                for i, (previous_input_txt, previous_output_txt) in enumerate(zip(original_input_corpus, generator_outputs_so_far)):
                    curated_cheatsheet += f"#### Previous Input #{i+1}:\n\n{previous_input_txt}\n\n#### Model Solution to Previous Input #{i+1}:\n\n{previous_output_txt}\n---\n---\n\n"
                curated_cheatsheet += "#### PREVIOUS SOLUTIONS (END)"
            # 若无历史记录，则记忆固定为"(empty)"
            else:
                top_k_original_inputs = []
                top_k_original_outputs = []
                curated_cheatsheet = "(empty)"
            
            # 替换生成器模板：问题用当前输入，记忆用拼接好的完整历史
            generator_prompt = generator_template.replace("[[QUESTION]]", input_txt).replace("[[CHEATSHEET]]", curated_cheatsheet)
            # 构造模型输入的对话历史
            generator_history = [{"role": "user", "content": generator_prompt}]
            # 调用基础生成方法，生成模型输出
            generator_output = self.generate(
                    history=generator_history,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    allow_code_execution=allow_code_execution,
                    code_execution_flag=code_execution_flag,
                )
            # 提取核心回答
            generator_answer = extract_answer(generator_output)
    
            # 返回结果字典：包含拼接的历史记忆、历史输入输出集合、最终回答
            return {
                "input_txt": input_txt,
                "steps": [
                    {
                        "round": 0,  # 仅1轮生成
                        "generator_prompt": generator_prompt,
                        "generator_output": generator_output,
                        "generator_answer": generator_answer,
                        "current_cheatsheet": curated_cheatsheet,  # 拼接后的完整历史记忆
                        "new_cheatsheet": None,  # 此模式不更新记忆，仅使用历史
                    }
                ],
                "top_k_original_inputs": top_k_original_inputs,  # 历史输入集合
                "top_k_original_outputs": top_k_original_outputs,  # 历史输出集合
                "final_answer": generator_answer,
                "final_output": generator_output,
                "final_cheatsheet": curated_cheatsheet,  # 最终使用的历史记忆
            }
        
        # --------- 4. Dynamic_Retrieval/DynamicCheatsheet_RetrievalSynthesis模式：相似历史检索模式 ------------------------------
        # 核心逻辑：基于向量相似度检索最相关的top_k条历史，用检索结果作为记忆（后者额外支持记忆结构化）
        elif approach_name in ["Dynamic_Retrieval", "DynamicCheatsheet_RetrievalSynthesis"]:
            # 提取当前输入的向量表示（original_input_embeddings最后一个元素为当前输入的embedding）
            current_original_input_embedding = original_input_embeddings[-1]
            # 提取历史输入的向量表示（排除当前输入，仅保留过往记录）
            prev_original_input_embeddings = original_input_embeddings[:-1]  # 可能为空（无历史）
            
            # 若有历史输入，则计算相似度并检索top_k条最相关历史
            if len(prev_original_input_embeddings) > 0:
                # 计算当前输入与所有历史输入的余弦相似度（衡量文本相关性）
                similarities = cosine_similarity([current_original_input_embedding], prev_original_input_embeddings)
                # 对相似度排序（降序），取前retrieve_top_k个索引（最相关的历史序号）
                top_k_indices = np.argsort(similarities[0])[::-1][:retrieve_top_k]
                # 根据索引获取最相关的历史输入、历史输出、对应的相似度值
                top_k_original_inputs = [original_input_corpus[i] for i in top_k_indices]
                top_k_original_outputs = [generator_outputs_so_far[i] for i in top_k_indices]
                top_k_similar_values = similarities[0][top_k_indices]
                # 格式化检索到的历史：添加提示文本（提醒模型批判性使用历史，避免盲目复制）
                curated_cheatsheet = "### PREVIOUS SOLUTIONS (START)\n\nNote: The input-output pairs listed below are taken from previous test cases and are meant to assist you in understanding potential solution strategies or tool usages. While they can offer insight and inspiration, they should not be blindly copied, as they may contain errors or may not fit your specific use case. Approach them with a critical mindset—analyze their logic, verify their correctness, and adapt them as needed. Your goal should be to develop a well-reasoned solution that best addresses the problem at hand.\n\n"
            # 若无历史输入，则记忆固定为"(empty)"
            else:
                top_k_original_inputs = []
                top_k_original_outputs = []
                top_k_similar_values = []
                curated_cheatsheet = '(empty)'
            
            # 拼接检索到的历史：按相似度降序排列，标注序号和相似度值（便于模型判断相关性）
            for i, (previous_input_txt, previous_output_txt, similarity) in enumerate(zip(top_k_original_inputs[::-1], top_k_original_outputs[::-1], top_k_similar_values[::-1])):
                curated_cheatsheet += f"#### Previous Input #{i+1} (Similarity: {similarity:.2f}):\n\n{previous_input_txt}\n\n#### Model Solution to Previous Input  #{i+1}:\n\n{previous_output_txt}\n---\n---\n\n"
            # 去除拼接后的多余空白字符（保证格式整洁）
            curated_cheatsheet = curated_cheatsheet.strip()
            
            # 若记忆非空，则添加结束标记（保证格式完整性）
            if curated_cheatsheet != '(empty)':
                curated_cheatsheet += "\n\n#### PREVIOUS SOLUTIONS (END)"
    
            # 若为DynamicCheatsheet_RetrievalSynthesis模式：额外将检索历史结构化（生成新cheatsheet）
            previous_cheatsheet = cheatsheet
            if approach_name == "DynamicCheatsheet_RetrievalSynthesis":
                # 替换记忆更新模板：历史输入输出对、当前问题、上一轮记忆分别填入对应占位符
                cheatsheet_prompt = cheatsheet_template.replace("[[PREVIOUS_INPUT_OUTPUT_PAIRS]]", curated_cheatsheet)
                cheatsheet_prompt = cheatsheet_prompt.replace("[[NEXT_INPUT]]", input_txt)
                cheatsheet_prompt = cheatsheet_prompt.replace("[[PREVIOUS_CHEATSHEET]]", previous_cheatsheet)
                # 构造记忆更新的对话历史
                cheatsheet_history = [{"role": "user", "content": cheatsheet_prompt}]
                # 生成结构化的新cheatsheet（禁用代码执行）
                cheatsheet_output = self.generate(
                    history=cheatsheet_history,
                    temperature=temperature,
                    max_tokens=2*max_tokens,
                    allow_code_execution=False,
                )
                # 提取新cheatsheet：若无有效内容则保留检索到的历史（避免记忆丢失）
                new_cheatsheet = extract_cheatsheet(response=cheatsheet_output, old_cheatsheet=curated_cheatsheet)
                # 更新记忆为结构化后的新cheatsheet
                curated_cheatsheet = new_cheatsheet
    
            # 替换生成器模板：问题用当前输入，记忆用检索（或结构化后）的历史
            generator_prompt = generator_template.replace("[[QUESTION]]", input_txt).replace("[[CHEATSHEET]]", curated_cheatsheet)
            # 构造模型输入的对话历史
            generator_history = [{"role": "user", "content": generator_prompt}]
            # 调用基础生成方法，生成模型输出
            generator_output = self.generate(
                    history=generator_history,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    allow_code_execution=allow_code_execution,
                    code_execution_flag=code_execution_flag,
                )
            # 提取核心回答
            generator_answer = extract_answer(generator_output)
    
            # 返回结果字典：包含检索到的历史、步骤日志、最终记忆和最终回答
            return {
                "input_txt": input_txt,
                "steps": [
                    {
                        "round": 0,  # 仅1轮生成
                        "generator_prompt": generator_prompt,
                        "generator_output": generator_output,
                        "generator_answer": generator_answer,
                        "current_cheatsheet": curated_cheatsheet,  # 检索（或结构化后）的记忆
                        "new_cheatsheet": None,  # 仅Synthesis子模式更新记忆，此处统一为None
                    }
                ],
                "top_k_original_inputs": top_k_original_inputs,  # 检索到的历史输入
                "top_k_original_outputs": top_k_original_outputs,  # 检索到的历史输出
                "final_answer": generator_answer,
                "final_output": generator_output,
                "final_cheatsheet": curated_cheatsheet,  # 最终使用的记忆
            }
        
        # 若输入的模式名称不在支持列表中，触发参数错误
    

0x04 ACE的优化
-----------

### 4.1 以DC作为基石

**ACE使用了DC作者官方发布的实现，并将其设置为使用累积模式（DC-CU）。**因此我们再仔细看看DC四种模式对应的场景。

#### 4.1.1 DC 适用场景

不同模式的设计初衷对应不同的业务需求，选择的核心是 “当前任务是否需要历史信息”“需要多少历史信息”“历史信息是否需要结构化”。

模式名称

典型实际案例

场景核心需求解读

**default（无记忆模式）**

1\. 单次常识查询：“地球赤道周长是多少？”2. 简单工具调用：“把‘Hello’翻译成法语”3. 独立计算：“15 的 30% 是多少？”

任务本身无需依赖任何过往交互，每个查询都是孤立的。核心需求是 “快速响应”，不需要历史信息干扰，避免冗余计算。

**FullHistoryAppending（完整历史拼接模式）**

1\. 短对话连续提问：“帮我写一首关于春天的诗→把诗里的‘桃花’改成‘樱花’→再增加一段关于微风的描写”2. 单主题分步操作：“新建一个 Excel 表格→在 A1 单元格输入‘姓名’→在 B1 单元格输入‘年龄’”

任务是**同一主题下的连续操作**，历史信息（如前序对话、前序步骤）必须完整保留，且历史量较小（通常不超过 5 轮）。核心需求是 “上下文连贯”，不需要筛选历史，因为所有过往内容都与当前任务强相关。

**DynamicCheatsheet\_Cumulative（结构化增量记忆模式）**

1\. 多步骤数学推理：“先计算函数 f (x)=2x+3 在 x=5 时的值→再求 f (x) 的导数→最后计算导数在 x=2 时的值”2. 复杂问题拆解：“分析某产品销量下降的原因→第一步看市场需求变化→第二步看竞品动作→第三步看自身供应链问题”3. 长期学习辅助：“记录用户每天的英语错题→次日复习时基于错题生成新练习题”

任务是**需要逐步积累知识的长流程**，每一步的结果都要作为 “中间结论” 保存，供后续步骤复用。核心需求是 “知识结构化沉淀”，避免历史信息杂乱无章，同时支持多轮迭代优化（如基于前序结论修正当前逻辑）。

**Dynamic\_Retrieval / Synthesis（相似历史检索模式）**

1\. 大规模题库解题：“学生在刷题系统中问‘如何解一元二次方程 x²-5x+6=0’→系统检索过往相似的‘一元二次方程求解’案例，提供参考思路”2. 多领域客服咨询：“用户问‘我的手机充电时屏幕闪烁怎么办’→客服系统检索历史中‘安卓手机充电屏幕异常’的相似案例，辅助生成解决方案”3. 个性化推荐辅助：“用户想‘选一款适合拍 vlog 的相机’→系统检索历史中‘同类用户（如新手 vlogger）的相机推荐记录’，优化推荐结果”

任务是**历史量庞大（如成百上千条）且查询多样化**，大部分历史信息与当前任务无关，需要精准筛选。核心需求是 “历史信息精准匹配”，避免全量历史导致的模型输入过长、响应变慢，同时通过相似度排序保证参考价值。（Synthesis 子模式额外适用于 “需要将相似历史结构化总结” 的场景，如 “检索 10 条相机推荐记录后，生成一份‘新手 vlogger 相机选购指南’”）

#### 4.1.2 提示词

既然使用DC，我们就看看针对DC的两个阶段（Generator和

##### Generator的提示词

    ACE Generator prompt on FINER
    
    You are an analysis expert tasked with answering questions using your knowledge, a curated playbook of strategies and insights and a reflection that goes over the diagnosis of all previous mistakes made while answering the question.
    
    Instructions: - Read the playbook carefully and apply relevant strategies, formulas, and insights - Pay attention to common mistakes listed in the playbook and avoid them - Show your reasoning step-by-step - Be concise but thorough in your analysis - If the playbook contains relevant code snippets or formulas, use them appropriately - Double-check your calculations and logic before providing the final answer
    
    Your output should be a json object, which contains the following fields: - reasoning: your chain of thought / reasoning / thinking process, detailed analysis and calculations - bullet_ids: each line in the playbook has a bullet_id. all bulletpoints in the playbook that’s relevant, helpful for you to answer this question, you should include their bullet_id in this list - final_answer: your concise final answer
    
    Playbook:
    {}
    Reflection:
    {}
    Question:
    {}
    Context:
    {}
    Answer in this exact JSON format:
    {
    "reasoning": "[Your chain of thought / reasoning / thinking process, detailed analysis and calculations]",
    "bullet_ids": ["calc-00001", "fin-00002"],
    "final_answer": "[Your concise final answer here]"
    }
    
    
    

翻译成中文如下。

    您是一位分析专家，负责利用您的知识、精选的策略和见解手册以及对之前回答问题时所犯错误的反思来回答问题。
    
    指示：
    - 仔细阅读手册，并应用相关的策略、公式和见解
    - 注意手册中列出的常见错误并避免它们
    - 逐步展示您的推理过程
    - 在分析中既要简洁又要全面
    - 如果手册包含相关的代码片段或公式，请适当使用它们
    - 在提供最终答案之前，再次检查您的计算和逻辑
    
    您的输出应该是一个 JSON 对象，其中包含以下字段：
    
    - reasoning：您的思维链/推理/思考过程，详细分析和计算
    - bullet_ids：手册中的每一行都有一个 bullet_id。手册中所有与回答这个问题相关且有帮助的要点，您都应该在列表中包含它们的 bullet_id
    - final_answer：您简洁的最终答案
    
    剧本： {} 反思： {} 问题： {} 上下文： {}
    
    以这种确切的 JSON 格式回答： { "reasoning": "[您的思维链/推理/思考过程，详细分析和计算]", "bullet_ids": ["calc-00001", "fin-00002"], "final_answer": "[您简洁的最终答案]" }
    

##### Curator的提示词

     ACE Curator prompt on FINER
    
    You are a master curator of knowledge. Your job is to identify what new insights should be added to an existing playbook based on a reflection from a previous attempt.
    
    Context: - The playbook you created will be used to help answering similar questions. - The reflection is generated using ground truth answers that will NOT be available when the playbook is being used. So you need to come up with content that can aid the playbook user to create predictions that likely align with ground truth.
    
    CRITICAL: You MUST respond with valid JSON only. Do not use markdown formatting or code blocks.
    
    Instructions: - Review the existing playbook and the reflection from the previous attempt - Identify ONLY the NEW insights, strategies, or mistakes that are MISSING from the current playbook - Avoid redundancy - if similar advice already exists, only add new content that is a perfect complement to the existing playbook - Do NOT regenerate the entire playbook - only provide the additions needed - Focus on quality over quantity - a focused, well-organized playbook is better than an exhaustive one - Format your response as a PURE JSON object with specific sections - For any operation if no new content to add, return an empty list for the operations field - Be concise and specific - each addition should be actionable
    
    Training Context:
    Total token budget: {token_budget} tokens
    Training progress: Sample {current_step} out of {total_samples}
    Current Playbook Stats:
    {playbook_stats}
    Recent Reflection:
    {recent_reflection}
    Current Playbook:
    {current_playbook}
    Question Context:
    {question_context}
    Your Task: Output ONLY a valid JSON object with these exact fields: - reasoning: your chain of thought / reasoning / thinking process,
    detailed analysis and calculations - operations: a list of operations to be performed on the playbook - type: the type of operation to be
    performed - section: the section to add the bullet to - content: the new content of the bullet
    Available Operations: 1. ADD: Create new bullet points with fresh IDs - section: the section to add the new bullet to - content: the new
    content of the bullet. Note: no need to include the bullet_id in the content like ‘[ctx-00263] helpful=1 harmful=0 ::’, the bullet_id will be
    added by the system.
    RESPONSE FORMAT - Output ONLY this JSON structure (no markdown, no code blocks):
    
    {
    "reasoning": "[Your chain of thought / reasoning / thinking process, detailed analysis and calculations here]",
    "operations": [
    {{
    "type": "ADD",
    "section": "formulas_and_calculations",
    "content": "[New calculation method...]"
    }}
    ]
    }
    

翻译成中文如下：

    FINER 策展人提示：
    
    您是一位知识策展大师。您的工作是根据之前尝试的反思，识别应该向现有剧本添加哪些新见解。
    
    背景：- 您创建的剧本将用于帮助回答类似的问题。- 反思是使用真实答案生成的，在使用剧本时将无法获得这些答案。因此，您需要提出内容，以帮助剧本用户创建可能与真实答案一致的预测。
    
    关键：您必须仅以有效的 JSON 格式响应。不要使用 markdown 格式或代码块。
    
    指示：- 回顾现有剧本和之前尝试的反思- 仅识别当前剧本中缺少的新见解、策略或错误- 避免重复 - 如果已经存在类似的建议，只添加与现有剧本完美补充的新内容- 不要重新生成整个剧本 - 只提供所需的补充- 注重质量而非数量 - 一个专注、组织良好的剧本比一个详尽无遗的剧本更好- 将您的响应格式化为具有特定部分的纯 JSON 对象- 对于任何操作，如果没有新内容要添加，则在操作字段中返回空列表- 简洁明了 - 每个补充都应该是可操作的
    
    训练背景： 总标记预算：{token_budget} 标记 训练进度：样本 {current_step} 共 {total_samples} 当前剧本统计： {playbook_stats} 最近的反思： {recent_reflection} 当前剧本： {current_playbook} 问题背景： {question_context} 您的任务：仅输出包含这些确切字段的有效 JSON 对象：- reasoning：您的思维链/推理/思考过程，详细分析和计算- operations：要对剧本执行的操作列表- type：要执行的操作类型- section：要添加项目符号的部分- content：项目符号的新内容 可用操作：1. ADD：创建具有新 ID 的新项目符号- section：要添加新项目符号的部分- content：新项目符号的内容。注意：无需在内容中包含项目符号 ID，如‘[ctx-00263] helpful=1 harmful=0 ::’，系统将添加项目符号 ID。
    
    响应格式 - 仅输出此 JSON 结构（无 markdown，无代码块）：
    
    { "reasoning": "[您的思维链/推理/思考过程，详细分析和计算]", "operations": [ { "type": "ADD", "section": "formulas_and_calculations", "content": "[新的计算方法...]" } ] }
    

### 4.2 关键创新

为应对前文提到的简约偏置与上下文塌缩问题，ACE 引入了三项关键创新：

*   增量式 Delta 更新机制：以局部编辑替代整体重写，显著降低延迟与计算开销；
*   grow-and-refine 机制：在持续扩充的同时抑制冗余，实现上下文的稳态演化。
*   专职反思者模块：将评估与洞见提取与curation过程解耦，提高上下文质量与下游性能；

#### 4.2.1 增量式 Delta 更新

ACE 的核心设计理念是：将上下文表示为结构化的条目集合（bullets），而非单一的整体提示词。每个条目包含两部分：

*   **元数据（metadata）**：唯一标识符，以及「有用 / 有害」计数器；
*   **内容（content）**：比如可复用策略、领域概念或常见错误模式。

我们接下来做解读。

想象一下，我们有一个智能助手（ACE），它不是用一个简单的提示来解决问题，而是用一系列的小提示，就像是一系列的备忘录或者清单。这些清单上的每个项目都包含了两部分信息：

1.  **基本信息**：比如一个独特的名字，以及这个项目是帮助解决问题还是带来了困扰的记录。
2.  **具体内容**：这可能包括一些有用的策略、专业知识或者常见的错误模式。

当我们的智能助手遇到新问题时，它会标记这些清单上的项目，告诉我们哪些是有用的，哪些可能会误导我们。这样，助手就可以知道哪些信息需要改进。

这种清单式的设计风格有三个主要优点：

*   **针对性强**：我们只更新那些真正需要改进的项目，而不是整个清单。
*   **精确查找**：助手可以快速找到最相关的信息，而不是在大量信息中盲目搜索。
*   **逐步改进**：在解决问题的过程中，我们可以有效地添加新信息、删除过时的信息或者避免重复。

智能助手不会重新编写整个清单，而是添加一些新的、精炼的项目。这些项目是经过助手深思熟虑后挑选出来的，可以帮助我们更好地解决问题。

这种方法避免了大规模重写清单所需的大量计算和时间，同时还能保留旧知识并不断吸收新见解。随着时间的推移，这种机制使得智能助手能够适应那些需要长时间或大量知识的复杂任务。

#### 4.2.2 Grow-and-Refine

在持续增长的基础上，ACE 通过定期或延迟蒸馏来确保上下文保持紧凑与相关性。在 Grow-and-Refine 过程中，新条目会被追加到上下文中，而已有条目则通过元数据更新（如计数器递增）进行原地修订。去重步骤则通过语义嵌入比较条目相似度来消除冗余。该过程可在每次增量更新后主动执行，也可在上下文窗口超限时被动触发，具体取决于延迟与精度要求。

我们接下来做解读。

想象一下，我们有一个智能的笔记本（ACE），它能够随着时间的推移不断更新内容，保持信息的新鲜和有用。这个笔记本的工作方式是这样的：

1.  **不断添加新知识**：每当我们学到新东西时，就把它作为新的一页添加到笔记本中。
2.  **更新旧知识**：对于笔记本中已有的内容，我们会根据新的经验来更新它们，比如增加一些注释或者修改一些信息。
3.  **去除重复内容**：笔记本还会检查新旧内容，如果发现有相似的信息，就会合并它们，避免重复。

这个过程可以在我们每次添加新内容后立即进行，也可以等到笔记本快要装满时再进行。这取决于我们希望笔记本更新得有多快，以及我们对信息准确性的要求。

通过这样的方式，我们的笔记本就能始终保持内容的丰富和相关性，同时不会变得过于臃肿。这样，无论何时我们翻开笔记本，都能快速找到我们需要的信息。

#### 4.2.3 Reflector阶段

在DC基础上加入了Reflector阶段的主要原因是：补全学习循环（强调从失败中学习），增强诊断能力、提高学习质量和预防性学习，利用执行环境提供的反馈信息，不仅仅依赖最终答案的正确性。

##### Reflector提示词

    ACE Reflector prompt on FINER
    
    You are an expert analyst and educator. Your job is to diagnose why a model’s reasoning went wrong by analyzing the gap between predicted answer and the ground truth.
    
    Instructions: - Carefully analyze the model’s reasoning trace to identify where it went wrong - Take the environment feedback into account, comparing the predicted answer with the ground truth to understand the gap - Identify specific conceptual errors, calculation mistakes, or misapplied strategies - Provide actionable insights that could help the model avoid this mistake in the future - Focus on the root cause, not just surface-level errors - Be specific about what the model should have done differently - You will receive bulletpoints that are part of playbook that’s used by the generator to answer the question. - You need to analyze these bulletpoints, and give the tag for each bulletpoint, tag can be [‘helpful’, ‘harmful’, ‘neutral’] (for the generator to generate the correct answer)
    
    Your output should be a json object, which contains the following fields - reasoning: your chain of thought / reasoning / thinking process, detailed analysis and calculations - error_identification: what specifically went wrong in the reasoning? - root_cause_analysis: why did this error occur? What concept was misunderstood? - correct_approach: what should the model have done instead? - key_insight: what strategy, formula, or principle should be remembered to avoid this error? - bullet_tags: a list of json objects with bullet_id and tag for each bulletpoint used by the generator
    
    Question:
    {}
    Model’s Reasoning Trace:
    {}
    Model’s Predicted Answer:
    {}
    Ground Truth Answer:
    {}
    Environment Feedback:
    {}
    Part of Playbook that’s used by the generator to answer the question:
    {}
    Answer in this exact JSON format:
    {
    "reasoning": "[Your chain of thought / reasoning / thinking process, detailed analysis and calculations]",
    "error_identification": "[What specifically went wrong in the reasoning?]",
    "root_cause_analysis": "[Why did this error occur? What concept was misunderstood?]",
    "correct_approach": "[What should the model have done instead?]",
    "key_insight": "[What strategy, formula, or principle should be remembered to avoid this error?]",
    "bullet_tags": [
    {{"id": "calc-00001", "tag": "helpful"}},
    {{"id": "fin-00002", "tag": "harmful"}}
    ]
    }
    

翻译为中文如下：

    您是一位专家分析师和教育者。您的工作是通过分析预测答案与真实答案之间的差距，诊断模型推理出错的原因。
    
    指示：
    
    - 仔细分析模型的推理过程，找出错误所在
    - 考虑环境反馈，将预测答案与真实答案进行比较，以了解差距
    - 识别具体的概念错误、计算错误或策略误用
    - 提供可操作的见解，帮助模型在未来避免此类错误
    - 专注于根本原因，而不仅仅是表面错误
    - 明确指出模型应该采取的不同做法
    - 您将收到作为剧本一部分的要点，这些要点被生成器用来回答问题
    - 您需要分析这些要点，并为每个要点给出标签，标签可以是['helpful', 'harmful', 'neutral']（以便生成器生成正确答案）
    
    您的输出应该是一个包含以下字段的JSON对象：
    
    - reasoning：您的思维链/推理/思考过程，详细分析和计算
    - error_identification：推理中具体出了什么问题？
    - root_cause_analysis：为什么会出现这个错误？有什么概念被误解了？
    - correct_approach：模型应该采取什么措施？
    - key_insight：应该记住什么策略、公式或原则以避免这个错误？
    - bullet_tags：一个JSON对象列表，每个生成器使用的要点都有bullet_id和标签
    
    问题： {} 模型的推理过程： {} 模型的预测答案： {} 真实答案： {} 环境反馈： {} 生成器回答问题时使用的剧本的一部分： {}
    
    以这种确切的JSON格式回答： { "reasoning": "[您的思维链/推理/思考过程，详细分析和计算]", "error_identification": "[推理中具体出了什么问题？]", "root_cause_analysis": "[为什么会出现这个错误？有什么概念被误解了？]", "correct_approach": "[模型应该采取什么措施？]", "key_insight": "[应该记住什么策略、公式或原则以避免这个错误？]", "bullet_tags": [ {"id": "calc-00001", "tag": "helpful"}, {"id": "fin-00002", "tag": "harmful"} ] }
    

##### 深入分析原因

###### 错误诊断与根本原因分析的需求

*   DC的局限性
    *   主要关注从成功解决方案中提取和累积知识。
    *   强调保留”正确、有用和具有说明性的解决方案和策略“。
    *   缺乏对失败案例的系统性分析机制。
*   ACE Reflector的作用
    *   专门设计用于诊断执行轨迹中的问题。
    *   要求”识别哪里出错了“和”识别根本原因“
    *   通过对比预测答案和实际情况来理解差距。
    *   明确要求识别”概念错误、计算错误或错误应用的策略“
    *   通过系统性分析提供可操作的见解。

###### 环境反馈的充分利用

*   DC的局限性
    *   主要用于基于模型输出和备忘录内容进行迭代优化。
    *   缺乏对执行环境反馈的直接利用。
*   ACE Reflector的作用
    *   直接利用执行反馈、API使用情况、单元测试报告和实际情况。
    *   能够从实际执行结果中提取教训，而不是从理论上正确的答案中学习。
    *   明确要求分析“错误的信息来源、不良过滤器、格式问题或缺少身份验证”

###### 更准确的知识提取

*   DC的局限性
    *   通过备忘录模板从模型答案中提取知识，但是可能会遗漏一些细微但是重要的错误模式。
*   ACE Reflector的作用
    *   通过专家级分析提供更深入的洞察。
    *   可以识别细微但是重要的问题。
    *   提供具体、逐步的纠正措施。