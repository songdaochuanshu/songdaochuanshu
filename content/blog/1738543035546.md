---
layout: post
title: '对比使用DeepSeek与文新一言，了解DeepSeek的关键技术论文'
date: "2025-02-03T00:37:15Z"
---
对比使用DeepSeek与文新一言，了解DeepSeek的关键技术论文
===================================

DeepSeek是国内大模型技术的新秀，最近也在业界和媒体界火爆出圈，所以想学习一下其技术。  
大模型时代，学习知识，当然首先想到利用大模型，由于在过去一年，对DeepSeek使用不多，所以想和文新一言（4.0 Turbo）对比使用。  
通过对比，针对同一个问题“DeepSeek发扬开源文化，将核心技术都以论文形式发表，关键的技术论文都有哪些？”，文心一言效果明显好很多，但是这可能和DeepSeek的“联网搜索暂不可用”有关系。  
以下是使用结果  
**文新一言问答结果：**  
![](https://img2024.cnblogs.com/blog/2960966/202502/2960966-20250201085153301-377420659.png)  
![](https://img2024.cnblogs.com/blog/2960966/202502/2960966-20250201085243802-2003774688.png)  
![](https://img2024.cnblogs.com/blog/2960966/202502/2960966-20250201085301869-714094198.png)  
![](https://img2024.cnblogs.com/blog/2960966/202502/2960966-20250201085312653-1079044047.png)

**DeepSeek问答结果：**  
![](https://img2024.cnblogs.com/blog/2960966/202502/2960966-20250201085419655-1511420406.png)  
![](https://img2024.cnblogs.com/blog/2960966/202502/2960966-20250201085434618-2092842379.png)  
![](https://img2024.cnblogs.com/blog/2960966/202502/2960966-20250201085447047-1826547536.png)  
![](https://img2024.cnblogs.com/blog/2960966/202502/2960966-20250201085501400-129551841.png)

**对以上回答结果进行核实**  
文新一言回答的5篇论文均为真实论文，结果正确，且按照时间排序，体验较好  
DeepSeek回答的结果中  
《DeepSeek-MoE: Towards Ultimate Specialization in Mixture-of-Experts Language Models》是正确的  
其他几篇论文，通过搜索引擎，均未搜索到相应的地址，应为虚幻回答  
《DeepSeek-Long: Advancing LLMs' Context Window to 128K through Positional Interpolation and Attention Optimization》  
《DeepSeek-R1: Retrieval-Augmented LLM with Real-Time Knowledge Update》  
《DeepSeek-Math: Improving Mathematical Reasoning in LLMs via Symbolic Supervision》