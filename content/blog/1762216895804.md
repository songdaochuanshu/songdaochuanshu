---
layout: post
title: '解密prompt系列63. Agent训练方案:RStar2 & Early Experience etc'
date: "2025-11-04T00:41:35Z"
---
解密prompt系列63. Agent训练方案:RStar2 & Early Experience etc
=====================================================

![解密prompt系列63. Agent训练方案:RStar2 &amp; Early Experience etc](https://img2024.cnblogs.com/blog/1326688/202510/1326688-20251029084416728-1651435525.png) 当大模型成为Agent，我们该如何教会它“行动”？我们将看到一条演进路线：从优化单一动作（ReTool），到学习长程规划（RAGEN），再到提升思考质量本身（RStar2），最后到一种不依赖外部奖励的、更底层的经验内化方式（Early Experience）。

当大模型成为Agent，我们该如何教会它“行动”？纯粹的模仿学习（SFT）天花板明显，而强化学习（RL）又面临奖励稀疏、环境复杂、探索成本高的挑战。本文将带你深入四种前沿的Agent训练方案：**ReTool**, **RAGEN**, **RStar2**, 和 **Early Experience**，看它们如何巧妙地设计环境、利用反馈，让Agent不仅“能干”，而且“聪明”。

我们将看到一条演进路线：从**优化单一动作**（ReTool），到**学习长程规划**（RAGEN），再到**提升思考质量本身**（RStar2），最后到一种**不依赖外部奖励的、更底层的经验内化**方式（Early Experience）。

ReTool：让模型学会“何时以及如何”使用单一工具
--------------------------

> *   ReTool: Reinforcement Learning for Strategic Tool Use in LLMs

_**“先学会用一个工具，再谈组合拳。”**_

*   **核心目标**：教会模型在推理过程中，**何时调用**一个单一的Code工具（如Python解释器），并通过RL优化这一决策过程。
*   **方法精髓**：**交错代码执行的Rollout机制**。模型生成、环境执行、结果注入、模型继续生成，形成一个动态的交互式轨迹。

ReTool是最基础的RL Agent训练，整体流程基本参考了DeepSeek R1-Zero的训练过程通过先SFT再RL的两阶段训练流程，教会模型在推理过程中何时调用单一Code工具，通过单轮或多轮工具调用进行任务完成。

### 🔧 Step1 - SFT

SFT部分是通过模型反馈来把原始基于文本的推理结果，转换成包含code工具调用的高质量推理样本，让模型先通过模仿学会加入code工具的推理模版。而SFT的样本格式我们在RL部分一起说。

SFT阶段保证了模型推理可以稳定的生成包含code的推理格式，那RL阶段的目标是让模型超越模仿，通过与环境（代码解释器）的交互和结果反馈，自主探索和优化工具使用的策略，例如：何时调用工具、调用什么工具、如何处理错误等。

这里提一句，当下很多伙伴采用API进行大模型调用，工具调用都通过API传参实现，而已不知道在各个模型的system prompt内部究竟是如如何处理工具参数的。其实不同模型之间差异还是比较大的，这里提供chat template 参考

*   [DeepSeek chat Template](https://www.modelscope.cn/models/deepseek-ai/DeepSeek-V3.1/file/view/master/assets%2Fchat_template.jinja?status=0)
*   [Antropic Tool Call](https://docs.claude.com/en/docs/agents-and-tools/tool-use/implement-tool-use)

### 🔧 Step2 - RL

1.  RL 样本构建 - Rollout with Interleaved Code Execution

RL样本（称为Rollout）是在训练过程中动态生成的。ReTool 的核心创新之一就是其支持交错代码执行的Rollout机制。

其交互式Rollout流程如下

*   模型生成：策略模型（Policy LLM）接收问题，并开始生成响应。它使用特定的提示模板（论文中图7），指导其输出格式。
*   代码触发：当模型生成一个代码块，并以 标签结束时，生成过程会暂停。
*   代码执行：解析出代码块中的代码，会发送到一个安全的代码沙箱环境中执行（哈哈就是前两章我们聊的类似E2B的沙箱方案）
*   观测注入：沙箱的执行结果（无论是成功的计算结果还是错误信息）被封装在 ... 标签中，并回传给模型。
*   继续生成：模型将执行结果作为上下文的一部分，继续生成后续的推理或下一个代码块。
*   轨迹完成：重复此过程，直到模型生成最终答案。最终形成一个完整的 混合推理轨迹：\[t1, c1, f1, t2, c2, f2, ..., o\]。

2.  RL 训练方式

*   **算法**：PPO
*   **奖励设计**：follow DeepSeek，仅基于**最终答案的正确性**。
*   **关键训练技术**：
    *   **Interpreter Feedback Masking**：在计算PPO损失时，屏蔽 `<interpreter>` 标签内的所有Token（因为这不是模型生成的）。这是保证训练稳定性的关键。
    *   **KV-Cache Reuse**：当代码执行时，缓存之前生成的所有KV-Cache，只计算反馈Token的新Cache，大幅降低内存开销，加速训练。
    *   **异步代码沙箱**：构建一个分布式的、异步的代码执行环境，避免代码执行成为训练瓶颈。

RAGEN：在多轮随机环境中学会“深谋远虑”
----------------------

> *   RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning

_**“人生不是单步决策，Agent也是。”**_

*   **核心目标**：在**多轮、随机性**的环境（游戏）中，训练模型的**长程规划与决策能力**。
*   **方法精髓**：**基于完整轨迹的强化学习**。模型不仅要输出动作，还要输出思考过程，并对整个思考-行动序列进行优化。

Agent RL训练无外乎以下几个核心要素：环境构建、轨迹生成、RL训练。我们按顺序来说

### 🎮 step1 - 环境构建

我们先来说下环境构建，RAGEN虽然考虑到了动态随机环境的重要性但只设计了较为简化的游戏环境

*   slot Machine（问就是中文词敏感）：单轮随机环境，测试在符号语义下的风险敏感推理。
*   推箱子：多轮确定性环境（起始和终止位置固定），因为推箱子的行为不可逆（往回推），所以用于测试不可逆的长视野规划。
*   冰面湿滑的推冰块：多轮随机环境（因为冰面湿滑，agent行为存在不确定性），测试在随机转移下的规划能力。

### 🎮 step2 - 轨迹构建

有了环境我们看下论文是如何构建Agent行为轨迹的。 每个样本都从一组初始状态开始，让模型随机生成N条完成轨迹，轨迹中的每一步，模型都输出一个**结构化的、包含推理过程的行为**如下：

*   `<think>...</think>`：模型内部的推理过程，是“慢思考”的体现。
*   `<answer>...</answer>`：最终提交给环境执行的具体动作。

以上的三种环境会根据Agent行为给出或随机或确定的反馈，随后Agent会基于反馈给出进一步行动直到完成。整个轨迹中**所有的组成部分（包括思考令牌和执行令牌）都会参与策略梯度计算**，这意味着模型被激励去生成那些能带来高回报的**推理过程**。

### 🎮 step3 - RL训练

首先**每种环境都有对应的奖励函数设计**如下，包含每一步的得分和对最终结果的得分，同时为了保证推理格式稳定可被解析，还有格式奖励，当模型没有按照以上推理格式输出时会有0.1的扣分。

环境

任务奖励规则

设计意图与说明

**slot Machine**

选择**低风险臂（Phoenix）**：固定获得 +0.15  
选择**高风险臂（Dragon）**：从伯努利分布（0.25）采样，成功+1.0，失败+0.0  
**预期收益**：Dragon (0.25) > Phoenix (0.15)

这是一个探索与利用的权衡测试。低风险臂奖励更稳定，但高风险臂长期期望更高。模型需要**通过推理克服短期频繁的失败，坚持选择高期望选项**。

**Sokoban**

**稀疏奖励与密集惩罚结合**：  
• 每个箱子在目标点上：+1  
• 每个箱子不在目标点上：-1  
• **任务完成时**：+10  
• **每执行一个动作**：-0.1

• 鼓励最终解决问题（+10）。  
• 引导模型高效解决问题（每步-0.1）。  
• 通过箱子位置的正负奖励提供**中间的、弱监督信号**，帮助模型学习。

**Frozen Lake**

**极端的稀疏奖励**：  
• 成功到达目标（G）：+1  
• 其他所有情况（掉入冰洞、未完成）：0  
每一步只有1/3的概率成功

这是最困难的奖励设置。模型在成功之前**几乎没有任何反馈**，必须通过多次试错来学习有效的策略。

其次对比传统单步RL只使用（prompt，response）样本对，只对最终输出结果计算奖励，论文引入了**StarPO框架对整个轨迹计算累计奖励**，并支持PPO、GRPO多种优化策略 。这种基于完整轨迹的训练目标对于模型的长规划与多轮决策能力有显著提升，原因是：

*   稀疏性：尤其是在Frozen Lake和Bandit中，奖励非常稀疏或具有欺骗性，这迫使模型必须发展出有效的多步推理和规划能力，而不能依赖密集的、步步为营的奖励信号。
*   轨迹级视角：通过GAE(γ=1.0, λ=1.0)和轨迹级优化目标，模型被明确地训练去关注长期回报，这对于多轮决策至关重要。

在如何把整个轨迹的整体奖励，计算到每一步的每个token上，论文使用了GAE（1.0,1.0）作为优势估计函数。既未来奖励不打折，并且使用多次随机轨迹的价值（模特卡洛）来进行优势估计。（不过这个参数设计和论文本身选择的环境有关，回合较短，并且强调无偏性），想更多了解GAE的可以划到文末去看GAE小课堂。

在实验过程中论文有以下几点发现

*   多步RL中PPO和GRPO的效果存在差异，影响因素主要在于价值函数是否好估计（任务随机性低），容易估计的场景PPO有效果优势
*   Echo Trap：效果衰减的原因之一RL会拟合到表面的pattern，导致模型推理进行模式循环，损失多样性。（虽然但是我感觉这和RL本身的设计相关，有些task specific）

为了稳定长行为轨迹的RL训练，论文提出了StarPO-S，包含以下三点核心改进

*   **基于不确定性的轨迹过滤**：仅保留轨迹方差最高的topK轨迹，过滤本身确定性比较高的轨迹，避免模型过度模仿形成固化的推理pattern
*   **移除在单轮RL训练中常用的KL Divergence**：也是处于鼓励模型更大范围探索的考虑（idea来自Seed的DAPO论文，后面微软RStar也沿用了）
*   **增加非对称clip上界大于下界**：$ \\epsilon\_{\\text{high}} = 0.28, \\epsilon\_{\\text{low}} = 0.2 $（idea来自Seed的DAPO论文，后面微软的RStar也沿用了）

整体上RAGEN和其他Agent RL训练论文因为在环境选取上的差异而显得不太一样，最重要的差异在于更多真实环境任务是很难获得**中间步骤奖励**的。所以我们接下来看下微软在真实任务上实验多轮agent优化的技术报告，并对比一些结论差异。

RStar2-Agent - 用工具调用作为“磨刀石”，让模型Think Smarter
--------------------------------------------

> *   rStar2-Agent: Agentic Reasoning Technical Report

_**“通过空间扰动，让模型学习如何想得更好而非更长。”**_

*   **核心目标**：通过引入Code工具调用带来的**环境反馈与噪声**，利用RL全面**提升基模型本身的思考质量**，而非仅仅是工具调用成功率。
*   **方法精髓**：**渐进式RL训练**与**正确轨迹重采样**。

微软RStar2的出发点很有趣，并非使用RL直接提升Agent效果，而是使用Code工具引入环境噪声，从而全面提升基模型的思考效果（think smarter而非longer）。这和MiniMax-M2作者近期的观点不谋而合既**Agent的泛化能力是在模型一切可能的操作空间上适应扰动的能力**。下面我们分RL优化算法和训练策略两部分展开。

### 🚀 核心算法：GRPO-ROC

论文采用了和DeepSeek相同的GRPO算法，并且和前面的RAGEN一样沿用了DAPO提出的移除KL，增加非对称CLIP来鼓励模型空间探索的策略。  
在此基础上论文先是指出了当前RL训练的两个问题

*   **基于结果的奖励机制**：无法对中间过程是否正确给出有效监督，可能存在冗长无效错误的中间推理过程，哈哈尤其是前一阵出现的一些中间模型经常会出现循环思考的问题，那思考过程长的让人不忍直视。
*   **环境噪声**：来自工具调用和工具执行过程中的各类报错带来的环境噪声，其实我感觉不一定是报错，各类正负向的环境反馈，相比原始的模型思考来说其实都是扰动（非内生）

针对以上问题论文提出了**GRPO-RoC优化算法（Resample on Corret）**。实现很简单就是在训练时先生成两倍的探索轨迹，针对答案正确的轨迹只保留质量最高的50%（中间工具调用报错更低、格式错误更少），同时对答案错误的轨迹进行均衡50%降采样。

论文希望通过这样的方式在保留通过正负轨迹对比习得经验的基础上，提升模型在正确轨迹上的推理效果，减少错误、低效的中间思考和工具调用。

### 🚀 渐进式训练流程

有了训练策略，我们继续看下RStar2是如何基于Qwen-14B-base模型进行训练的。

首先训练过程中模型进行多轮思考和工具调用的chat template如下，assistant通过REACT给出工具调用，再用user角色返回工具结果，然后继续循环直到任务完成。不过不同模型对工具输出的角色处理其实是不同的，像DeepSeek就是都放在assistant角色下用/<tool\_output/>包裹，个人比较建议实用原模型本身的工具处理模版（在chat template中），这里论文使用的是qwen模型。  

其次Base模型会顺序经过以下两个阶段训练

1.  **non-reasoning SFT：突出无思考**，只使用指令完成、JSON格式、基础code工具调用样本进行训练，为后面的RL推理format奠定基础。因此会得到理解能力显著下降，但是工具调用略有上升，且回答会变得很短的一阶段模型。
2.  **multi-turn RL：这里构建了42K的高质量问答对涵盖各类可验证数学问题。通过3阶段RL训练**

*   RL stage-1: 42K样本 + GRPO-ROC + 最长8K输出长度 + 300stesp，鼓励模型生成更有效简短的reasoning，训练和评估指标都基本稳定，超过8K的样本占比也稳定在10%左右，平均回答长度4K
*   RL stage-2: 把输出长度提升到12K再训练85stesp，平均回答长度提升到6K
*   RL stage-3（125steps）：此时多数样本模型已经能完全正确回答，因此论文增加离线数据过滤策略使用最新的模型checkpoint推理，并过滤出17.3K难样本（8次随机推理中存在错误），然后在12K输出长度上，再训练了125个steps。

不难看出RStar2的训练突出了**渐进式**这个核心思路。通过**渐进式推理长度延长**来兼顾模型思考效果和思考长度，通过**渐进式难度提升**来兼顾不同难度样本的有效学习（避免简单问题过度学习）。

在最终效果上，在训练领域的数学类问题上RStar2的思考长度均显著下降，但是在通用类问题的解决效果上依旧有稳定的提升 —— Think Smarter。

Agent Learning via Early Experience
-----------------------------------

> *   Agent Learning via Early Experience
> *   Scaling Agents via Continual Pre-training

_**“在学会跑之前，先学会看和想。”**_

*   **核心目标**：在正式的RL训练之前，通过一种**不依赖外部奖励**的方式，让Agent通过自主探索来学习环境动态，为后续学习打下坚实基础。
*   **方法精髓**：将Agent的**早期探索经验**直接转化为**监督学习**的信号，内化一个“世界模型”。

最近Meta放出的这篇重量级论文其实和前面的Agent RL都有些不同，个人感觉它并非用于替代Agent RL训练，相反是用于在Agent RL之前搭建LLM和Agent的桥梁。本质上笔者感觉和阿里之前推出的Agentic CPT，在LLM之后增加Agent轨迹的后训练思路有些相似（哈哈虽然论文里说的出发点截然不同），但Early Experience的训练目标和训练数据构建方案更native(scaling)。

论文提出了3个重要的概念

### 🔑 World Exploration：全方位探索世界是很重要的

> 世界探索，解决的是专家标注轨迹单一的问题  
> 针对真实世界的问题，我们可以生成专家标注的执行轨迹，但是轨迹本身是单一的，它对整个世界的其他状态的探索是不充分的，因此会导致模型在遇到非标准情形时缺乏处理能力。

因此论文在每个可枚举的环境状态上，都在专家的Action之外，随机采样了另外K个行为，和该行为会导致的环境反馈。这样我们就能得到一份对环境探索更加全面的数据集

\\\[D\_{rollout}= \\{(s\_i,a\_i^j,s\_i^j)|i \\in \[N\],j \\in \[K\]\\} \\\]

本质上**环境状态的变化（观测）就是对行为质量的最好监督信号（反馈）**，但这个监督信号我们要如何通过训练内化到模型参数中呢？

### 🔑 World Modeling：使用NTP来内化Early Experience

> 世界建模，解决的是让智能体学习“如果我做这个动作会发生什么？”的问题

有趣的是论文又回到了最原始的Next Token Prediction。使用当前状态和行动作为上文（x），而环境的反馈作为(Y)，让模型通过预测环境的可能反馈，来内化对环境的理解（世界模型）。这种训练的选择摆脱了RL对于监督信号的依赖。同时考虑到前面生成的非专家轨迹的量级往往比专家轨迹大很多个数量级，因此论文这里选择了两阶段训练，先使用rollout进行大量训练，再在专家轨迹上进行训练。

\\\[ \\mathcal{L}\_{\\text{IWM}} = -\\sum \\log p\_\\theta(s\_i^j \\mid s\_i, a\_i^j) \\\]

但在真实世界中感觉还有一些需要解决的问题，一个是环境的动态性，例如不同时间搜索引擎的返回内容是不同的；以及环境的复杂性，能否充分描述当前环境和环境变化是非常复杂，例如你该如何描述金融市场的变化，只有价格变化显然是不充分的。

以及上述的训练目标限定在了单步，把Agent长程行为简化成了单步的MDP，也就是基于当前状态,给出行为，获得的状态转移，并未把长程的行为决策对最终结果的影响考虑在内。

### 🔑 Self-Reflection：通过轨迹对比来获取更多对行为决策的认知

> 自我反思，解决的是让模型学习“为什么行动A比行动B效果更好的原因”

前面两步已经奠定了论文的核心思路，和前面RStar和RAGEN等相同，论文也增加了**轨迹对比  
，从专家轨迹和随机采样的轨迹对比中来获取更多对应行为结果的反馈**。

论文通过以下的Prompt来从轨迹对比中不做专家经验好在哪里？错误的行为有哪些局限和低效的地方

不过和之前memory论文不同的是，这部分Reflection没有作为Note来在推理时使用，而是也使用NTP进行模型参数训练，来训练模型对于行为决策的理解。

\\\[ \\mathcal{L}\_{\\text{SR}} = -\\sum \\log p\_\\theta(c\_i^j, a\_i \\mid s\_i) \\\]

以下是步骤2和3分别构建的环境反馈训练样本和轨迹对比训练样本。

**💡 关键洞察**：

*   **经验即信号**：即使没有奖励，环境状态的变化本身就是一种强大的监督信号。
*   **Scaling Law依然有效**：这种基于经验的后训练方式，效果会随着模型规模增大而显著提升。
*   **RL的“完美预备役”**：经过Early Experience训练的模型，再进行RL微调，其效果远超直接SFT后RL的方案。它相当于让模型在“上学”前，先拥有了丰富的“生活常识”。

* * *

优势函数（GAE）小课堂
------------

### 1\. 为什么需要优势函数

在强化学习中，我们的目标是让模型学会选择累计回报更高的行为，那最朴素的方案就是使用轨迹累计回报作为权重，来更新每一步的梯度。

\\\[\\nabla J(\\theta) = \\mathbb{E}\[\\sum\_{t=0}^{\\infty} R(\\tau) \\nabla \\log \\pi\_\\theta(a\_t | s\_t)\] \\\]

但直接使用整体轨迹奖励的问题有两个

*   方差大：在于不同轨迹带来的奖励会存在显著差异导致方差过大
*   非等权：每一个行为对于最终奖励带来的贡献是不一样的

那解决方案就是引入基线，并且是根据每一步状态计算的基线，用整体轨迹奖励减去基线奖励作为当前步骤的奖励，这就是所谓的优势函数概念啦

### 2\. 优势函数定义

**优势函数 ( A(s\_t, a\_t) )** 被定义为：

\\\[A(s\_t, a\_t) = Q(s\_t, a\_t) - V(s\_t) \\\]

*   **$ Q(s\_t, a\_t) $**：动作价值函数，表示在状态 $ s\_t $下执行动作 ( a\_t ) 后，能获得的**期望累积回报**,也就是前面提到当前状态和行为下整体轨迹的期望价值。
*   **$ V(s\_t) $**：状态价值函数，表示在状态 $ s\_t $ 下，遵循当前策略能获得的**平均期望累积回报**，也就前面提到的基线。

**因此，优势函数衡量的是：在状态 $ s\_t $ 下，采取某个特定动作 $a\_t $ 比遵循当前策略的“平均”动作要好多少。**

*   如果 $ A(s\_t, a\_t) > 0 $：说明这个动作比平均动作好，应该增加其概率。
*   如果$ A(s\_t, a\_t) < 0 $：说明这个动作比平均动作差，应该减少其概率。

但是真实情况下Q和V都是未知的，因此我们需要对A进行估计，这里就引出了不同的估计方案。

### 3\. 优势函数估计

*   蒙特卡洛估计：高方差，低偏差（用无数次的模拟事实说话）  
    从t时刻的状态开始，随机试验N次，用轨迹结束的实际累计回报作为Q和V的估计。因为使用多条轨迹所以估计相对无偏，但随着轨迹越长往往方差越大，因为后续步骤带来的随机性越高。

\\\[\\hat{A}\_t^{MC} = \\sum\_{l=0}^{\\infty} \\gamma^l r\_{t+l} - V(s\_t) \\\]

*   时序差分估计： 低方差，高偏差（持续预测与修正）  
    相对的TD只看立即奖励和下一个状态的折现价值，因为不考虑后续步骤因此方差较低，但是依赖于价值函数V估计的准确性，如果V估计偏差较大则TD估计的偏差会很大。

\\\[\\hat{A}\_t^{TD(0)} = r\_t + \\gamma V(s\_{t+1}) - V(s\_t) \\\]

### 4\. GAE-均衡方差与偏差

GAE 的核心思想是：**将以上两种估计器结合起来，通过一个参数 $\\lambda $ 作为偏差-方差的权衡。**

GAE 的定义如下：

\\\[\\hat{A}\_t^{GAE(\\gamma, \\lambda)} = \\sum\_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta\_{t+l} \\\]

让我们来解析下这个公式，其中 $ \\delta\_t = r\_t + \\gamma V(s\_{t+1}) - V(s\_t) $ 就是前面的TD 误差。只不过不止包含下一步，还包含未来的每一步的TD误差衰减后的加权平均。两个超参数

*   \\(\\lambda\\):控制方差和偏差，为0则退化成时序差分，为1则桂花味蒙特卡洛估计
*   \\(\\gamma\\):未来奖励的折现因子，决定了未来奖励对当前状态的重要性影响