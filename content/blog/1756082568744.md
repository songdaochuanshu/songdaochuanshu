---
layout: post
title: 'LLM 指标 | PPL vs. BLEU vs. ROUGE-L vs. METEOR vs. CIDEr'
date: "2025-08-25T00:42:48Z"
---
LLM 指标 | PPL vs. BLEU vs. ROUGE-L vs. METEOR vs. CIDEr
======================================================

LLM 指标 | PPL vs. BLEU vs. ROUGE-L vs. METEOR vs. CIDEr
======================================================

困惑度（Perplexity, PPL）↓
---------------------

PPL的意义非常明了，用于测量模型对生成文本的不确定程度，不确定程度越低，模型的表现就越好。其计算方法是计算句子每个token的平均对数似然，再过一个指数函数。

### 定义

给定一个长度为\\(n\\)的token序列：

\\\[S=(w\_1,w\_2,\\cdots,w\_n) \\\]

那么该序列的PPL为：

\\\[PPL(S)=\\exp\\big(-\\frac{1}{N}\\sum^N\_{i=1}\\log P(w\_i|w\_1,\\cdots,w\_{i-1}\\big) \\\]

BLEU（Bilingual Evaluation Understudy）↑
--------------------------------------

BLEU出自文章_BLEU: a Method for Automatic Evaluation of Machine Translation."  
Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_，主要用以评估翻译任务中生成文本与参考文本的匹配程度。其更具体的形式**BLEU@N**用以定义在\\(1,2,\\cdots,N\\)\-gram情况下，生成文本与参考文本的匹配程度，再通过一个**长度惩罚项（brevity penalty, BP）**避免模型的生成文本过短。

### 定义

给定生成文本为\\(C\\)，参考文本为\\(\\{R\_1,R\_2,\\cdots,R\_m\\}\\)

首先我们定义modified n-gram precision：

给定N-gram \\(g\_n\\)，\\(Count\_S(g\_n)\\)表示在序列\\(S\\)中\\(g\\)出现的次数，那么我们可以定义modified N-gram precision：

\\\[p\_n = \\frac{\\sum\_{g\_n\\in C}\\min(Count\_C(g\_n), \\max\_j Count\_{R\_i}(g\_n))}{\\sum\_{g\_n\\in C}Count\_C(g)} \\\]

通俗解释\\(p\_n\\)定义了生成文本与参考文本之间的重叠程度，并且设定了每个词的出现次数上界为参考文本中出现次数上界。

接下来我们计算\\(1,2,\\cdots,N\\)\-gram的几何平均值（通常取\\(n=1,2,3,4\\)）

有：

\\\[P=\\exp(\\frac{1}{N}\\sum^N\_{n=1}\\log p\_n) \\\]

接下来计算**长度惩罚项BP**：

\\\[\\text{BP} = \\begin{cases} 1 & \\text{if } c > r \\\\\[2mm\] \\exp\\left(1 - \\frac{r}{c}\\right) & \\text{if } c \\leq r \\end{cases} \\\]

其中\\(c\\)为生成文本的长度，\\(r\\)为与生成文本\\(c\\)长度最接近的参考文本的长度

最后相乘得到**BLEU**：

\\\[BLEU=P\\cdot BP \\\]

ROUGE-L ↑
---------

ROUGE-L（Recall-Oriented Understudy for Gisting Evaluation - LCS）通过计算**最长公共子串LCS**评估生成文本与参考文本之间的匹配程度，为此给定生成文本\\(C\\)和参考文本\\(R\\)，我们可以模仿混淆矩阵定义其precison，recall以及F1-score：

### ROUGE- L Precison

\\\[P\_{LCS} = \\frac{LCS(C,R)}{|C|} \\\]

### recall

\\\[R\_{LCS}=\\frac{LCS(C, R)}{|R|} \\\]

### F1-scoure

\\\[F\_{LCS}=\\frac{2\\cdot R\_{LCS}\\cdot {P\_{LCS}}}{R\_{LCS}+P\_{LCS}} \\\]

METEOR
------

METEOR出自文章_[Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments](https://dl.acm.org/doi/pdf/10.5555/1626355.1626389)_，为了解决BLEU指标**不能处理非精确匹配**以及**语序不敏感**的两个缺陷，加入了语义对齐以及碎片化惩罚因子两个步骤。

### 语义对齐

首先通过贪心算法对所有的词语进行一一匹配，这种匹配考虑精确匹配、词干匹配或同义词匹配

PS：细节再补上去

### 计算recall和precision

给定生成文本\\(C\\)以及参考文本\\(R\\)，\\(m\\)为成功匹配的词语数量，我们分别可以计算其precision和recall

\\\[P=\\frac{m}{|C|},R=\\frac{m}{|R|} \\\]

### 计算F-score

通过上一步计算的precision和recall

\\\[F\_{mean} = \\frac{P\\cdot R}{(1-\\alpha)\\cdot R + \\alpha \\cdot P} \\\]

其中\\(\\alpha\\)为平衡recall和precision的权重，一般取\\(\\alpha = 0.9\\)

#### 计算碎片化惩罚项（fragmentation penalty）

生成文本中连续的匹配词会被同一个块（chunk）中，块的数量越多说明语序越不匹配，我们可以给出其定义：

\\\[FP =\\gamma \\bigg(\\frac{\\#\\text{chunks}}{m}\\bigg)^\\beta \\\]

其中\\(\\beta\\)定义了惩罚项函数的形状，\\(\\gamma\\)定义了惩罚项的相对权重，一般取\\(\\beta = 3, \\gamma=0.5\\)

最后我们可以计算METEOR：

\\\[METEOR = F\_{mean}\*(1-FP) \\\]

CIDEr
-----

CIDEr（Consensus-based Image Description Evaluation）出自文章_[CIDEr: Consensus-based Image Description Evaluation](https://arxiv.org/abs/1411.5726)_，主要用于Image Captioning任务中，评估生成文本与参考文本的匹配程度

通过计算TF-IDF N-gram向量的余弦相似度评估生成样本与评估样本的匹配程度

### TF-IDF N-gram向量

给定生成文本\\(C\\)以及参考文本\\(\\{R\_1,R\_2,\\cdots,R\_m\\}\\)，我们可以计算每个文本\\(S\\)的TF-IDF向量：

\\\[g\_n(S)=\[w\_S(g\_1),w\_S(g\_2),\\cdots,w\_S(g\_k)\] \\\]

其中\\(w\_S(g\_k)\\)为N-gram \\(g\_k\\)在文本\\(S\\)中的TF-IDF权重，定义为：

\\\[w\_S(g\_k)=TF(g\_k,S)\\cdot\\log\\frac{m}{\\sum^m\_{i=1}\\mathbf{1}(g\_k\\in R\_i)} \\\]

其中\\(\\sum^m\_{i=1}\\mathbf{1}(g\_k\\in R\_i)\\)表示在N-gram \\(g\_k\\)在多少个参考文本中出现了

#### N-gram向量余弦相似度

接下来我们可以计算生成文本\\(C\\)和所有参考文本之间的N-gram向量余弦相似度

\\\[sim\_n(C,R\_i)=\\frac{g\_n(S)\\cdot g\_n(R\_i)}{||g\_n(S)||\\ ||g\_n(R\_i)||} \\\]

### 对所有参考文本取平均得到\\(CIDEr\_n\\)

\\\[CIDEr\_n(C,R) = \\frac{1}{m}\\sum^m\_{i=1}sim\_n(C,R\_i) \\\]

### 计算CIDEr

接下来我们计算\\(1,2,\\cdots,N\\)\-gram情况的平均值（通常取\\(n=1,2,3,4\\)）

\\\[CIDEr = \\sum^N\_{n=1}w\_n\\cdot CIDEr\_n(C,R) \\\]

其中\\(w\_n\\)为不同的N-gram情况的权重，一般取\\(w\_n=\\frac{1}{N}\\)