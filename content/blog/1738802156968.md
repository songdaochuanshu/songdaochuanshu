---
layout: post
title: '腾讯云HAI服务器上部署与调用DeepSeek-R1大模型的实战指南'
date: "2025-02-06T00:35:56Z"
---
腾讯云HAI服务器上部署与调用DeepSeek-R1大模型的实战指南
==================================

上次我们大概了解了一下 DeepSeek-R1 大模型，并简单提及了 Ollama 的一些基本信息。今天，我们将深入实际操作，利用腾讯云的 HAI 服务器进行 5 分钟部署，并实现本地 DeepSeek-R1 大模型的实时调用。接下来，我们直接进入部署过程。

服务器准备
=====

首先，我们需要登录腾讯云平台并购买 HAI 应用服务。腾讯云提供了两种计费方式：包月计费和按时计费。由于我目前并没有特别紧迫或庞大的需求，因此为了节省成本，我选择了按时计费方式。具体的购买流程和配置选项可以参考下面的图示：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150639046-1327050268.png)

在我完成购买后，腾讯云 HAI 服务器会自动为我们部署 DeepSeek-R1 模型并启动运行，整个过程非常简便。同时，HAI 还提供了算力连接的选择，具体有三种不同的方式：

1.  **ChatBotUI**：这是一种广泛应用的可视化聊天界面，它不仅支持实时的聊天互动，还具备管理聊天记录和提示词模板等功能，非常适合需要快速构建对话系统的场景。
2.  **CloudStudio**：CloudStudio 是一款功能强大的在线集成开发环境（IDE）。它允许我们编写 Python 脚本、调试代码、进行多种应用开发和测试，非常适合开发者进行项目调试和优化。
3.  **JupyterLab**：作为一种极为流行的数据科学工具，JupyterLab 提供了多个终端选择，包括 Linux 终端和 Python 脚本执行环境。它为数据分析、模型训练及执行等任务提供了非常便捷的支持。

ChatBotUI
---------

在这里，我们首先来了解一下可视化界面，并演示如何快速上手使用。通过这一界面，用户可以直观地进行各种操作，轻松实现需求的配置和调整。具体的操作步骤和界面效果可以参考下面的图示：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150647201-549518582.png)

可以选择其他选项，虽然有时候需要进行角色授权，授权过程非常简单，点击“授权”按钮即可完成，无需进行复杂操作。一旦授权完成，你便可以进入聊天界面。在该界面中，HAI服务器提供了多种参数选项，例如7B和1.5B。选择合适的参数后，你就可以立即开始实时聊天，无需等待。

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150652866-2122753249.png)

ollama终端
--------

我们继续选择JupyterLab方式连接算力，这里选择终端，如图所示：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150658297-1924415126.png)

我们去看下终端命令查看下，如图所示：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150704888-819001029.png)

在这里，我们可以看到其实它使用的也是基于Ollama运行的DeepSeek-R1大模型。通过这种方式，用户可以非常方便地直接使用Ollama提供的命令，来查看和操作相应的API接口。如图所示：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150709766-1103776718.png)

这里我们演示的是直接使用 Ollama 运行 DeepSeek-R1 大模型，模型参数为 1.5B。要结束当前会话，您可以使用快捷键 Ctrl + D 退出。不过，需要注意的是，采用这种方式仅支持在本地运行和启动，无法进行外网调用或配置远程访问。

因此，如果希望实现外网访问或其他更复杂的配置，接下来的步骤将会介绍相关方法。

ollama-API服务
------------

这里我们查看ollama如果想要启动大模型服务接口，可以使用`ollama serve`命令启动，如图所示：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150715944-2109174082.png)

可以看到，系统已经成功开机并启动，且绑定的端口号为6399。在这种情况下，我们只需直接开放该端口即可。值得注意的是，HAI服务器还提供了外网IP地址，因此我们可以通过访问该IP来进行端口的开放设置。

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150721622-1727866660.png)

接下来，我们将按照Ollama的API文档中的指引，完成端口设置的操作。具体操作流程如下所示：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150726231-422225048.png)

CloudStudio连接
-------------

启动完API服务后，我们直接使用CloudStudio进行本地调用。新建一个py文件，代码内容如下：

    from openai import OpenAI
    
    client = OpenAI(api_key="ollama", base_url="http://localhost:6399/v1/")
    
    response = client.chat.completions.create(
        model="deepseek-r1:1.5b",
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content": "Hello"},
        ],
        stream=False
    )
    
    print(response.choices[0].message.content)
    

这里虽然写了api-key信息，但是ollama是不会校验的，你可以写任何字符串，以为这个参数是方法必传参数。结果运行如下：

![image](https://img2024.cnblogs.com/blog/1423484/202502/1423484-20250205150735272-1077151232.png)

如果你安装完openai依赖包，但仍是无法找到，那么你就在CloudStudio中添加虚拟环境即可，命令如下：

> python -m venv venv

然后再执行`pip3 install openai`命令即可成功运行。当然这里是本地测试，你也可以使用公网IP进行配置并调试。

总结
==

通过本次实践，我们成功地使用腾讯云的HAI服务器进行了DeepSeek-R1大模型的部署与实时调用。从购买HAI应用服务，到通过ChatBotUI、JupyterLab、CloudStudio等工具进行配置和调试，我们详细介绍了每个步骤。

通过本地和外网API的操作，我们不仅了解了模型部署的基本流程，还掌握了如何利用Ollama提供的API服务进行大模型调用。

* * *

我是努力的小雨，一个正经的 Java 东北服务端开发，整天琢磨着 AI 技术这块儿的奥秘。特爱跟人交流技术，喜欢把自己的心得和大家分享。还当上了腾讯云创作之星，阿里云专家博主，华为云云享专家，掘金优秀作者。各种征文、开源比赛的牌子也拿了。

💡 想把我在技术路上走过的弯路和经验全都分享出来，给你们的学习和成长带来点启发，帮一把。

🌟 欢迎关注努力的小雨，咱一块儿进步！🌟