---
layout: post
title: '凸优化数学基础笔记（八）：一维线性搜索法（一）'
date: "2026-02-24T00:56:49Z"
---
凸优化数学基础笔记（八）：一维线性搜索法（一）
=======================

已经确定的情况下，如何确定步长tk? 步长因子的选取有多种方法，如取步长为常数，但这样选取的步长并不最好，如何选取最好步长呢？实际计算通常采用一维搜索来确定最优步长。

凸优化数学基础笔记（八）：一维线性搜索法
====================

​ 由上一节关于求解最优化问题概括中可知，迭代最优化算法的基本思路是从已知迭代点\\(\\mathbf{X}\_k\\in{R^n}\\) 出发按照迭代格式\\(\\mathbf{X}\_{k+1}=\\mathbf{X}\_k+t\_k\\mathbf{P}\_k\\) ,从已知迭代点来求解最优化问题，其关键在于如何构造一个搜索方法\\(\\mathbf{P}\_k\\in{R^n}\\) 和确定 一个步长的\\(t\_k\\in{R^1}\\)，使下一个迭代点\\(\\mathbf{X}\_{k+1}\\) 处的目标函数值下降，即\\(f(\\mathbf{X}\_{k+1})<f(\\mathbf{X}\_k)\\)。现在我们来讨论，当搜索方向\\(\\mathbf{P}\_k\\)已经确定的情况下，如何确定步长\\(t\_k\\) ? 步长因子的选取有多种方法，如取步长为常数，但这样选取的步长并不最好，如何选取最好步长呢？实际计算通常采用一维搜索来确定最优步长。

​ 对于无约束最优问题：

\\\[\\min\_{\\mathbf{X}\\in{R}^n}f(\\mathbf{X}) \\tag{1} \\\]

当已知迭代点\\(\\mathbf{X}\_k\\)和下降方向\\(\\mathbf{P}\_k\\) 时，要确定适当的步长\\(t\_k\\)，使\\(f(X\_{k+1})=f(\\mathbf{X}\_k+t\_k\\mathbf{P}\_k)\\) 比\\(f(\\mathbf{X}\_k)\\) 有所下降，即相当于参数变量\\(t\\)的函数：

\\\[\\varphi(t)=f(\\mathbf{X}\_k+t\\mathbf{P}\_k)\\tag{2} \\\]

要在区间\\(\[0,+\\infty)\\)上选取\\(t=t\_k\\)，使\\(f(\\mathbf{X}\_{k+1})<f(\\mathbf{X}\_k)\\) 即：

\\\[\\varphi(t\_k)=f(\\mathbf{X}\_k+t\_k\\mathbf{P}\_k)<f(\\mathbf{X}\_k)=\\varphi{(0)} \\tag{3} \\\]

由于这种从已知点的\\(\\mathbf{X}\_k\\) 出发，沿某一个下降的搜索方向\\(\\mathbf{P}\_k\\) 来确定步长\\(t\_k\\)的问题，实质上是单变量函数关于步长变量的搜索选取，故通常叫做一维搜索。按照这种方法确定的步长\\(t\_k\\) 又称为最优步长，这种方法的优点是，它使目标函数值在搜索方向上下降得最多。

​ 今后为了简单起见，我们用记号：

\\\[Z=l\_s(\\mathbf{X},\\mathbf{P}) \\tag{4} \\\]

表示从点\\(\\mathbf{X}\\)出发沿\\(\\mathbf{P}\\)方向对目标函数\\(f(\\mathbf{X})\\)作直线搜索所得到的极小值点是\\(\\mathbf{Z}\\) 。其中\\(l\\)和\\(s\\) 分别是Linear search(直线搜索)的缩写。在目标函数\\(f(\\mathbf{X})\\) 已确定条件下等价于如下两式：

\\\[\\begin{cases} f(\\mathbf{X}+t\_0\\mathbf{P})=\\min{f(\\mathbf{X}+t\\mathbf{P})}=\\varphi(t)\\\\ \\mathbf{Z}=\\mathbf{X}+t\\mathbf{P} \\end{cases} \\tag{5} \\\]

下面我们进一步解释迭代点\\(\\mathbf{X}\_{k+1}=\\mathbf{X}\_k+t\_k\\mathbf{P}\_k\\) 的空间位置，容易证明，若从\\(\\mathbf{X}\_k\\)出发，沿\\(\\mathbf{P}\_k\\) 方向进行一维搜索得到极小值点，\\(\\mathbf{X}\_{k+1}=\\mathbf{X}\_k+t\_k\\mathbf{P}\_k\\)，则该点处\\(\\mathbf{X}=\\mathbf{X}\_{k+1}\\) ，则该点处\\(\\mathbf{X}=\\mathbf{X}\_{k+1}\\) 的梯度方向\\(\\nabla{f(\\mathbf{X}\_{k+1})}\\) 与搜索方向\\(\\mathbf{P}\_k\\) 之间应满足：

\\\[\\nabla{f(\\mathbf{X}\_{k+1})}^T\\mathbf{P}\_k=0 \\tag{6} \\\]

事实上，设\\(\\varphi(t)=f(\\mathbf{X}\_k+t\\mathbf{P}\_k)\\)，求\\(\\varphi(t)\\) 的极值，对\\(t\\) 求导有：

\\\[\\varphi^{\\prime}(t)=\\nabla{f(\\mathbf{X}\_k+t\\mathbf{P}\_k)}^T\\mathbf{P}\_k \\tag{7} \\\]

令\\(\\varphi^{\\prime}(t)=0\\)，即可得 \\(\\nabla f(\\mathbf{X}\_k+t\\mathbf{P}\_k)^T\\mathbf{P}\_k=0\\) ,所以\\(\\nabla{f(\\mathbf{X}\_{k+1})}^T\\mathbf{P}\_k=0\\)。

​ 式（2.5）的几何意义是明限的，从某一个点\\(\\mathbf{X}\_k\\)出发沿\\(\\mathbf{P}\_k\\) 方向对目标函数\\(f(\\mathbf{X})\\) 作直线搜索，所得到的极小值点为\\(\\mathbf{X}\_{k+1}\\)。公式（6）指出，梯度\\(\\nabla{f(\\mathbf{X}\_{k+1})}\\) 必与搜索方向\\(\\mathbf{P}\_k\\) 正交。又因为\\(\\nabla{f(\\mathbf{X}\_{k+1})}\\) 与目标函数过点\\(\\mathbf{X}\_{k+1}\\) 的等值面\\(f(\\mathbf{X})=f(\\mathbf{X}\_{k+1})\\) 正交，所以进一步看到，搜索方向\\(\\mathbf{P}\_k\\)与这个等值面在点\\(\\mathbf{X}\_{k+1}\\) 处相切。

1.搜索区间及其确定方法
------------

​ 设一维最优化问题为：

\\\[\\min\_{0\\leq{t}<t\_{max}} \\varphi(t) \\tag{8} \\\]

为了求解式（8），我们可以引入如下的搜索区间概念。

**Definition 1** 设\\(\\varphi:R\\rightarrow{R}\\)，\\(t^\*\\in\[0,+\\infty)\\)，\\((t^\*\\in\[0,t\_{max}\])\\)，并且

\\\[\\varphi(t^\*)=\\min\_{0\\leq{t}\\leq{t\_{max}}}{\\varphi(t)} \\tag{9} \\\]

若存在闭区间\\(\[a,b\]\\subset\[0,+\\infty)\\)(\\(\[a,b\]\\subset{\[0,t\_{max}\]}\\)) ，并且使\\(t^{\*}\\in\[a,b\]\\),则称\\(\[a,b\]\\) 为以为优化问题（式8）的**搜索区间**\*。

​ 由定义简而言之，一个一维最优化问题的搜索区间，就是包含该问题最优解的一个闭区间。通常，在进行一维搜索区间，然后再在此区间中进行搜索求解。

​ 下面，介绍一个确定式（8）的搜索区间确定的简单方法。这个方法的基本思路如下：先选定一个初始点\\(t\_0\\in\[0,+\\infty)(t\_0\\in\[0,t\_{max}\])\\)和初始步长\\(h\_0>0\\)。然后，沿着\\(t\\in\[0,+\\infty)\\) 轴的正方向搜索前进一个步长，得到新点\\(t\_0+h\_0\\)。若目标函数在新点处的值是下降了，即：

\\\[\\varphi(t\_0+h\_0)<\\varphi({t\_0}) \\tag{10} \\\]

则下一步就从新点的处的值\\(t\_0+h\_0\\)出发的加大步长，再向前探索。若目标函数在新点处的值上升，即

\\\[\\varphi{(t\_0+h\_0)}>\\varphi(t\_0) \\tag{11} \\\]

则下一步以\\(t\_0\\)为出发点为出发点以原步长开始向\\(t\\)轴的负方向的同样探索。当达到目标函数上升的点时，就停止搜索。这时候便得到一个搜索区间。这种以加大步长进行搜索来寻找搜索区间的方法叫做**加步搜索法**。其加步搜索法的步骤：

1.  **选取初始数据**。选取初始点\\(t\_0\\in\[0,+\\infty)(t\_0\\in\[0,t\_{max}\])\\) ，计算\\(\\varphi\_0=\\varphi(t\_0)\\)。给出初始步长\\(h\_0>0\\)，加步系数\\(\\alpha>1\\),令\\(k=0\\)。
    
2.  **比较目标函数值**。令\\(t\_{k+1}=t\_k+h\_k\\) ，计算\\(\\varphi\_{k+1}=\\varphi(t\_{k+1})\\)，若\\(\\varphi\_{k+1}<\\varphi\_{k}\\) ,转步骤3，否则，转步骤4。
    
3.  **加大搜索步长。**令\\(h\_{k+1}=\\alpha h\_k\\)。同时，令\\(t=t\_k,t\_k=t\_{k+1},\\varphi\_k=\\varphi\_{k+1},k=k+1\\) 转步骤2；
    
4.  **反向搜索。** 若\\(k=0\\)，转换探索方向，令\\(h\_k=-h\_k,t=t\_{k+1}\\)，转步骤2。否则，停止迭代，令：
    
    \\\[a=\\min({t,t\_{k+1}}),b=\\max{(t,t\_{k+1})} \\tag{12} \\\]
    
    输出\\(\[a,b\]\\)。
    

​ 在加步搜索法中，一般建议\\(\\alpha=2\\)。若能估计式（8）的最优解的大体位置的话，初始点\\(t\_0\\)要接近于式（8）的最优解。在具体运用上述的加步搜索法时，有时还要考虑一些细节问题。例如，当探索得到新点处的目标函数值和出发点处相同的时，以及初步步长应如何选取等，都需作适当处理。

​ 由于以后要介绍的一些一维搜索方法，主要适用于式（8）在搜索区间只有唯一的最优解情况下，为此，我们再给出下面**单谷区间**与**单谷函数**概念。

​ **Definition 2** 设\\(\\varphi:\\mathbf{R}^1\\rightarrow{\\mathbf{R}^1}\\) ，闭区间\\(\[a,b\]\\subset\\mathbf{R}^1\\)。若存在点\\(t^\*\\in\[a,b\]\\)，使得 \\(\\varphi(t)\\) 在\\(\[a,t^\*\]\\) 上严格递减，在\\(\[t^\*,b\]\\) 上严格递增，则称\\(\[a,b\]\\) 是函数\\(\\varphi(t)\\) 的**单谷区间**，\\(\\varphi(t)\\)是在\\(\[a,b\]\\) 上**单谷函数**。

​ 由定义2可知，一个区间是某函数的单谷区间意味着，在该区间中函数只有一个“凹谷”（极小值）。另外，从定义2可知，某区间上的单谷函数上不必是连续函数，而凸函数在所给区间上必然是单谷函数。单谷函数和单谷区间有如下有用的性质。

​ **定理 1** 设\\(\\varphi:\\mathbf{R}^1\\rightarrow\\mathbf{R}^1\\),\\(\[a,b\]\\)是\\(\\varphi(t)\\) 的单谷区间，任取\\(t\_1,t\_2\\in\[a,b\]\\) 并且\\(t\_2<t\_1\\)。

1.  若有\\(\\varphi(t\_2)\\leq\\varphi(t\_1)\\)，则\\(\[a,t\_1\]\\)是\\(\\varphi(t)\\)的单谷区间。
2.  若有\\(\\varphi(t\_2)\\geq\\varphi(t\_1)\\)，则\\(\[t\_2,b\]\\)是\\(\\varphi(t)\\)的单谷区间。

​ 上述定理说明，经过函数值的比较可以把单谷区间缩短为一个较小的单谷区间。换句话说，利用这个定理可以把搜索区间无限缩小，从而求到极小点。以下介绍的几种一维搜索方法都是利用这个定理通过不断的搜索区间，来求得一维最优化问题的近似最优解。