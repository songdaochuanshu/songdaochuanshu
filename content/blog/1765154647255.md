---
layout: post
title: 'offline meta-RL | è®ºæ–‡é€Ÿè¯»è®°å½•'
date: "2025-12-08T00:44:07Z"
---
offline meta-RL | è®ºæ–‡é€Ÿè¯»è®°å½•
========================

æœ€è¿‘ offline meta RL è®ºæ–‡çš„é€Ÿè¯»è®°å½•ã€‚

  

* * *

ç›®å½•

*   [(MACAW) Offline Meta-Reinforcement Learning with Advantage Weighting \[ICML 2021\]](#macaw-offline-meta-reinforcement-learning-with-advantage-weighting-icml-2021)
*   [(PEARL) Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables \[ICML 2019\]](#pearl-efficient-off-policy-meta-reinforcement-learning-via-probabilistic-context-variables-icml-2019)
*   [Meta-Reinforcement Learning via Exploratory Task Clustering \[AAAI 2024\]](#meta-reinforcement-learning-via-exploratory-task-clustering-aaai-2024)
*   [Meta-Q-Learning \[ICLR 2020\]](#meta-q-learning-iclr-2020)
*   [(MAML) Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks \[ICML 2017\]](#maml-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks-icml-2017)
*   [(BOReL) Offline Meta Reinforcement Learning -- Identifiability Challenges and Effective Data Collection Strategies \[NeurIPS 2021\]](#borel-offline-meta-reinforcement-learning----identifiability-challenges-and-effective-data-collection-strategies-neurips-2021)
*   [(MBML) Multi-task Batch Reinforcement Learning with Metric Learning \[NeurIPS 2020\]](#mbml-multi-task-batch-reinforcement-learning-with-metric-learning-neurips-2020)
*   [Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning](#robust-task-representations-for-offline-meta-reinforcement-learning-via-contrastive-learning)
*   [Efficient Offline Meta-Reinforcement Learning via Robust Task Representations and Adaptive Policy Generation \[IJCAI 2024\]](#efficient-offline-meta-reinforcement-learning-via-robust-task-representations-and-adaptive-policy-generation-ijcai-2024)
*   [FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization \[ICLR 2021\]](#focal-efficient-fully-offline-meta-reinforcement-learning-via-distance-metric-learning-and-behavior-regularization-iclr-2021)
*   [(UBER) Unsupervised Behavior Extraction via Random Intent Priors \[NeurIPS 2023\]](#uber-unsupervised-behavior-extraction-via-random-intent-priors-neurips-2023)
*   [(IDAQ) Offline Meta Reinforcement Learning with In-Distribution Online Adaptation \[ICML 2023\]](#idaq-offline-meta-reinforcement-learning-with-in-distribution-online-adaptation-icml-2023)
*   [Offline Meta-Reinforcement Learning with Online Self-Supervision \[ICML 2022\]](#offline-meta-reinforcement-learning-with-online-self-supervision-icml-2022)

* * *

### (MACAW) Offline Meta-Reinforcement Learning with Advantage Weighting \[ICML 2021\]

*   arxivï¼š[https://arxiv.org/abs/2008.06043](https://arxiv.org/abs/2008.06043)
*   pdfï¼š[https://arxiv.org/pdf/2008.06043](https://arxiv.org/pdf/2008.06043)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/2008.06043](https://ar5iv.labs.arxiv.org/html/2008.06043)
*   GitHubï¼š[https://sites.google.com/view/macaw-metarl](https://sites.google.com/view/macaw-metarl)
*   æ¥æºï¼šè¿™ç¯‡æ–‡ç« å¥½åƒæå‡ºäº† offline MAMLï¼ŒICML 2021ã€‚

ä¸»è¦å†…å®¹ï¼š

*   ï¼ˆkimi é€Ÿè¯»ï¼‰
*   settingï¼šoffline multi-task æ•°æ®é›† + æ–°ä»»åŠ¡çš„å°‘é‡ offline æ•°æ®ï¼ˆ<5 æ¡è½¨è¿¹ï¼‰ç”¨äºé€‚åº”æ–°ä»»åŠ¡ã€‚
*   methodï¼šå¢å¼ºç‰ˆ AWRï¼ˆä¸€ç§ offline æ–¹æ³•ï¼‰+ MAMLã€‚
    *   å†…æ ¸æ›¿æ¢ï¼šå°† MAML çš„ç­–ç•¥æ¢¯åº¦æ¢æˆA WR å›å½’ï¼ˆç¦»çº¿å‹å¥½ï¼‰ï¼›
    *   å¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼šç®€å• MAML + AWR ä¼šå¤±è´¥ï¼Œå› ä¸º AWR æ¢¯åº¦ä¿¡æ¯é‡ä¸è¶³ã€‚MACAW å¢åŠ ä¼˜åŠ¿å›å½’å¤´ï¼Œè®©æ¢¯åº¦èƒ½åŒæ—¶ç¼–ç â€œåŠ¨ä½œè¯¥æ˜¯ä»€ä¹ˆâ€å’Œâ€œä¼˜åŠ¿æœ‰å¤šå¤§â€ï¼›
    *   æ¶æ„å‡çº§ï¼šå¼•å…¥æƒé‡å˜æ¢å±‚ï¼Œçªç ´æ™®é€šMLP"ç§©1æ›´æ–°"çš„é™åˆ¶ï¼Œè®©å†…å¾ªç¯æ›´å¼ºå¤§ã€‚
*   å®éªŒç¯å¢ƒï¼šMuJoCo çš„ cheetah-directionã€cheetah-velocityã€walker-paramsã€ant-directionã€‚
*   å¾ˆå¥½å¥‡å®ƒçš„ baseline æ˜¯æ€ä¹ˆåšçš„ï¼Œmeta-BC å’Œ multi-task offline RL with fine-tuning æ˜¯æ€ä¹ˆåšçš„ã€‚

### (PEARL) Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables \[ICML 2019\]

*   arxivï¼š[https://arxiv.org/abs/1903.08254](https://arxiv.org/abs/1903.08254)
*   pdfï¼š[https://arxiv.org/pdf/1903.08254](https://arxiv.org/pdf/1903.08254)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/1903.08254](https://ar5iv.labs.arxiv.org/html/1903.08254)
*   æ¥æºï¼šfocal çš„ baselineï¼Œå¹¶ä¸” focal ä»‹ç»è¿™ç¯‡æ–‡ç« æå‡ºäº† meta-RL çš„ benchmarkã€‚è¿™ç¯‡æ–‡ç« å¥½åƒæ˜¯ç»å…¸ offline meta-RL å·¥ä½œï¼Œæå‡ºäº† PEARL æ–¹æ³•ï¼ŒICML 2019ã€‚

### Meta-Reinforcement Learning via Exploratory Task Clustering \[AAAI 2024\]

*   arxivï¼š[https://arxiv.org/abs/2302.07958](https://arxiv.org/abs/2302.07958)
*   pdfï¼š[https://arxiv.org/pdf/2302.07958](https://arxiv.org/pdf/2302.07958)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/2302.07958](https://ar5iv.labs.arxiv.org/html/2302.07958)
*   æ¥æºï¼šå¸ˆå¼Ÿçš„è®ºæ–‡ listï¼ŒAAAI 2024ã€‚çœ‹ abstract æ„Ÿè§‰æ²¡æœ‰è§£å†³ç‰¹åˆ«é‡è¦çš„é—®é¢˜ï¼Œä½†å› ä¸ºæ˜¯ task clusteringï¼Œæ‰€ä»¥æƒ³çœ‹ä¸€ä¸‹ã€‚

### Meta-Q-Learning \[ICLR 2020\]

*   arxivï¼š[https://arxiv.org/abs/1910.00125](https://arxiv.org/abs/1910.00125)
*   pdfï¼š[https://arxiv.org/pdf/1910.00125](https://arxiv.org/pdf/1910.00125)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/1910.00125](https://ar5iv.labs.arxiv.org/html/1910.00125)
*   æ¥æºï¼šå¸ˆå¼Ÿçš„è®ºæ–‡ listï¼ŒICLR 2020ã€‚åº”è¯¥åšçš„æ˜¯ off-policy çš„ meta-RLï¼Œä¸ç¡®å®šæ˜¯ä¸æ˜¯ offline çš„ã€‚

### (MAML) Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks \[ICML 2017\]

*   arxivï¼š[https://arxiv.org/abs/1703.03400](https://arxiv.org/abs/1703.03400)
*   pdfï¼š[https://arxiv.org/pdf/1703.03400](https://arxiv.org/pdf/1703.03400)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/1703.03400](https://ar5iv.labs.arxiv.org/html/1703.03400)
*   æ¥æºï¼šmeta-RL ç»å…¸å·¥ä½œï¼ŒMAMLã€‚
*   å‚è€ƒåšå®¢ï¼š[è®ºæ–‡é€Ÿè¯»è®°å½• | 2025.04 - MAML](https://www.cnblogs.com/moonout/p/18804176#model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks)

### (BOReL) Offline Meta Reinforcement Learning -- Identifiability Challenges and Effective Data Collection Strategies \[NeurIPS 2021\]

*   arxivï¼š[https://arxiv.org/abs/2008.02598](https://arxiv.org/abs/2008.02598) ï¼ˆæ²¡æœ‰ diff è¿‡ arxiv ç‰ˆæœ¬è·Ÿä¼šè®®ç‰ˆæœ¬æ˜¯å¦ä¸€è‡´ï¼‰
*   pdfï¼š[https://arxiv.org/pdf/2008.02598](https://arxiv.org/pdf/2008.02598)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/2008.02598](https://ar5iv.labs.arxiv.org/html/2008.02598)
*   OpenReviewï¼š[https://openreview.net/forum?id=IBdEfhLveS](https://openreview.net/forum?id=IBdEfhLveS)
*   æ¥æºï¼šNeurIPS 2021ã€‚

### (MBML) Multi-task Batch Reinforcement Learning with Metric Learning \[NeurIPS 2020\]

*   arxivï¼š[https://arxiv.org/abs/1909.11373](https://arxiv.org/abs/1909.11373)
*   pdfï¼š[https://arxiv.org/pdf/1909.11373](https://arxiv.org/pdf/1909.11373)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/1909.11373](https://ar5iv.labs.arxiv.org/html/1909.11373)
*   æ¥æºï¼šè¿™ç¯‡æ–‡ç« æå‡ºäº† MBML æ–¹æ³•ï¼ŒNeurIPS 2020ã€‚focal æåˆ°ï¼Œè¿™ç¯‡æ–‡ç« è¯•å›¾è§£å†³ MDP ambiguityã€‚

### Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning

*   arxivï¼š[https://arxiv.org/abs/2206.10442](https://arxiv.org/abs/2206.10442)
*   pdfï¼š[https://arxiv.org/pdf/2206.10442](https://arxiv.org/pdf/2206.10442)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/2206.10442](https://ar5iv.labs.arxiv.org/html/2206.10442)
*   æ¥æºï¼šæ— æ„ä¸­æœåˆ°çš„ï¼ŒICML 2022ã€‚

### Efficient Offline Meta-Reinforcement Learning via Robust Task Representations and Adaptive Policy Generation \[IJCAI 2024\]

*   å¯ä»¥çœ‹ abstract çš„ç½‘é¡µï¼š[https://dl.acm.org/doi/10.24963/ijcai.2024/500](https://dl.acm.org/doi/10.24963/ijcai.2024/500)
*   pdfï¼š[https://www.ijcai.org/proceedings/2024/0500.pdf](https://www.ijcai.org/proceedings/2024/0500.pdf)
*   æ¥æºï¼šæ— æ„ä¸­æœåˆ°çš„ï¼ŒIJCAI 2024ã€‚çœ‹ abstract æ„Ÿè§‰è§£å†³çš„ä¸æ˜¯å¾ˆé‡è¦çš„é—®é¢˜ï¼Œä½†å¥½åƒæ˜¯æœ‰è¶£çš„ï¼Œä¸ç€æ€¥çœ‹ã€‚

### FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization \[ICLR 2021\]

*   arxivï¼š[https://arxiv.org/abs/2010.01112](https://arxiv.org/abs/2010.01112)
*   pdfï¼š[https://arxiv.org/pdf/2010.01112](https://arxiv.org/pdf/2010.01112)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/2010.01112](https://ar5iv.labs.arxiv.org/html/2010.01112)
*   GitHubï¼š[https://github.com/LanqingLi1993/FOCAL-latest](https://github.com/LanqingLi1993/FOCAL-latest)
*   å‚è€ƒåšå®¢ï¼š[CSDN | ã€è®ºæ–‡é˜…è¯»ç¬”è®°ã€‘FOCAL ç¦»çº¿å…ƒå¼ºåŒ–å­¦ä¹ ï¼Œä»é™æ€æ•°æ®ä¸­å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡](https://blog.csdn.net/iiiiii11/article/details/155618323)
*   æ¥æºï¼šå¥½åƒæ˜¯ offline meta-RL çš„ç»å…¸æ–‡ç« ï¼ŒICLR 2021ã€‚

ä¸»è¦å†…å®¹ï¼š

*   è¿™ç¯‡æ–‡ç« æå‡ºäº† fully-offline context-based actor-critic meta-RL algorithmï¼ˆFOCALï¼‰ï¼Œå£°ç§°æ˜¯é¦–ä¸ªç«¯åˆ°ç«¯çš„ model-free çš„ offline meta-RL æ–¹æ³•ã€‚
*   preliminariesï¼šTA-MDPï¼Œä¸€ä¸ªç”¨ task embedding z æ¥è¡¨ç¤º multi-task çš„ MDP å®šä¹‰ã€‚å…¶ä¸­ state space æ˜¯ \\(S \\times Z\\)ï¼ˆåŸå§‹çŠ¶æ€ + task embeddingï¼‰ï¼Œtransition æ˜¯ \\(P\_z(s'|s,a)\\) çš„å½¢å¼ï¼Œreward æ˜¯ \\(R(s,a,z) = R\_z(s,a)\\) çš„å½¢å¼ã€‚
*   settingï¼šåœ¨è®­ç»ƒæ—¶ï¼Œç»™å®š N ä¸ª task çš„å¸¦ reward å’Œ task æ ‡ç­¾çš„ offline æ•°æ®é›†ï¼›åœ¨æµ‹è¯•æ—¶ï¼Œç»™å‡ºä¸€äº›æ–°ä»»åŠ¡çš„ offline æ•°æ®é›†ã€‚
*   methodï¼š
    *   focal è®­ç»ƒä¸€ä¸ª inference network \\(q\_\\phi(z|c)\\)ï¼Œç”¨æ¥ä» context c ä¸­çŒœå‡ºç°åœ¨åœ¨åšä»€ä¹ˆ task zï¼Œå…¶ä¸­ context c æ˜¯ä¸€æ‰¹ (s,a,s',r) æ•°æ®ã€‚
    *   è®­ç»ƒçš„æ–¹å¼æ˜¯ï¼Œå¸Œæœ›ç›¸åŒ task çš„ z èšé›†ï¼Œè€Œä¸åŒ task çš„ z ç›¸äº’åŸç†ï¼Œfocal è¯´è¿™æœ¬è´¨ä¸Šæ˜¯è·ç¦»åº¦é‡å­¦ä¹ ï¼ˆè¿™å¬èµ·æ¥åƒå¯¹æ¯”å­¦ä¹ ï¼‰ã€‚å…·ä½“çš„ï¼Œfocal æå‡ºäº†ä»¥ä¸‹æŸå¤±å‡½æ•°ï¼ˆEq 13ï¼‰ï¼Œç¬¬ä¸€é¡¹æŠŠç›¸åŒ task æ‹‰è¿‘ï¼Œç¬¬äºŒé¡¹æŠŠä¸åŒ task æ¨è¿œï¼š
        
        \\\[\\mathcal{L}\_{dml}(x\_i, x\_j; q) = \\mathbb{1}\\{y\_i = y\_j\\} \\|q\_i - q\_j\\|^2 + \\mathbb{1}\\{y\_i \\neq y\_j\\} \\cdot \\frac{\\beta}{\\|q\_i - q\_j\\|^n + \\epsilon} \\\]
        
    *   Eq 13 æ˜¯ Eq 12 çš„æ”¹è¿›ç‰ˆï¼Œå› ä¸º Eq 12 çš„æ¢¯åº¦åœ¨é«˜ç»´ z ç©ºé—´ä¸­å¥½åƒä¼šä¸ workï¼Œè€Œ Eq 13 åˆ™æ²¡æœ‰è¿™ç§é—®é¢˜ã€‚
    *   focal çš„ç®—æ³•æµç¨‹ï¼šé€šè¿‡ Eq 13 è®­ç»ƒ \\(q\_\\phi(z|c)\\)ï¼ŒåŒæ—¶è®­ç»ƒå¸¦ z çš„ actor å’Œ criticã€‚æ‹¿åˆ°æ–° task ä¹‹åï¼Œä½¿ç”¨æ–° task ç»™å®šçš„ offline æ•°æ®é›†æ¨æ–­ zï¼Œç„¶åä½¿ç”¨ \\(\\pi\_\\theta(a | s, z)\\) ä½œä¸ºç­–ç•¥ã€‚
*   å®éªŒç¯å¢ƒï¼šç¯å¢ƒå…·ä½“æè¿°è§é™„å½• D.2ï¼Œå…¶ä¸­ Point-Robot-Windã€Walker-2D-Params æ˜¯æ”¹å˜ transition dynamics çš„ä»»åŠ¡ï¼Œè€Œå…¶ä»–æ˜¯æ”¹å˜ reward function çš„ä»»åŠ¡ã€‚
*   offline æ•°æ®é›†ï¼šæ•°æ®é›†è²Œä¼¼æ˜¯è‡ªå·±ç”Ÿæˆçš„ï¼Œä½¿ç”¨è®­ç»ƒ SAC è¿‡ç¨‹ä¸­ä¿å­˜ä¸‹æ¥çš„ checkpointï¼›æœ‰ä¸€äº›ä½¿ç”¨ expert æ•°æ®é›†ï¼Œå¦ä¸€äº›åˆ™ä½¿ç”¨ mixed æ•°æ®é›†ï¼Œè¯¦è§ Table 2ã€‚
*   æ‰€æ¯”è¾ƒçš„ baselineï¼šBatch PEARLã€Contextual BCQã€MBMLã€‚focal ç›´æ¥é‡‡ç”¨äº† MBML [ä»£ç ](https://github.com/Ji4chenLi/Multi-Task-Batch-RL)ä¸­çš„ Contextual BCQ å’Œ MBML çš„å®ç°ã€‚
*   å®éªŒç»“æœï¼šoutperform baselineã€‚å±•ç¤ºçš„æ˜¯ curveï¼Œå…¶ä¸­æ¨ªè½´æ˜¯ç”¨äº offline è®­ç»ƒçš„æ•°æ®é›†å¤§å°ï¼Œå³ transition çš„ä¸ªæ•°ï¼Œè€Œçºµè½´æ˜¯ average returnã€‚focal æ˜¯ fully-offline çš„ï¼Œæ²¡æœ‰ fine-tuneã€‚

å…¶ä»–ä¿¡æ¯ï¼š

*   intro çš„å‰ä¸¤æ®µï¼Œåœ¨è®² offline å’Œ meta-RL çš„åŠ¨æœºå’Œæ•…äº‹ï¼Œç¬¬ä¸‰æ®µå°±ç›´æ¥æå‡º focal äº†ã€‚
*   related work ä¸­æåˆ°äº†å¤§é‡ meta-RL æ–¹æ³•ï¼Œä¸è¿‡çœ‹æ—¶é—´å¯èƒ½éƒ½æ˜¯ 19 å¹´ä¹‹å‰çš„ï¼Œå¯èƒ½ä¼šæ¯”è¾ƒè€ã€‚focal å£°ç§°è‡ªå·±ä¸ PEARL æœ€ä¸ºç›¸å…³ã€‚
*   focal åªè€ƒè™‘ç¡®å®šæ€§çš„ MDPï¼Œè€Œä¸è€ƒè™‘çŠ¶æ€è½¬ç§»å…·æœ‰éšæœºæ€§çš„ MDPã€‚
    *   åœ¨ç¡®å®šæ€§ MDP ä¸‹ï¼Œç»™å®šä¸€ä¸ª (s,a) å’Œå¯¹åº”çš„ taskï¼Œå­˜åœ¨å”¯ä¸€çš„ (s', r)ã€‚Assumption 1 å‡è®¾ï¼Œå¦‚æœä¸¤ä¸ª task çš„ transition å’Œ reward éƒ½æ˜¯ä¸€æ ·çš„ï¼Œé‚£ä¹ˆå®ƒä»¬æ˜¯åŒä¸€ä¸ª taskã€‚ç”±æ­¤å¯ä»¥æ¨å‡ºï¼Œç»™å®šä¸€ä¸ª \\((s,a,s',r)\\)ï¼Œå¯ä»¥å”¯ä¸€ç¡®å®š taskã€‚
    *   Figure 4(b) å°† focal çš„ \\(q\_\\phi(z|c)\\) æ¢æˆäº† probabilistic context encoderï¼ˆè™½ç„¶ä¸æ˜¯å¾ˆæ˜ç™½æ€ä¹ˆåšåˆ°çš„ï¼‰ï¼Œå‘ç°æ€§èƒ½ä¸‹é™ã€‚
*   Finn et al. (2017) and Rakelly et al. (2019) è¿™ä¸¤ç¯‡æ–‡ç« æå‡ºäº† meta-RL çš„ benchmarkï¼Œç–‘ä¼¼é‡è¦ï¼Œéœ€è¦ check ä¸€ä¸‹ã€‚
*   focal åœ¨ä¼˜åŒ– actor å’Œ critic æ—¶ï¼Œä¼šæŠŠ z å†»ç»“ä½ï¼Œä¸ä¼šæ›´æ–° \\(q\_\\phi(z|c)\\)ï¼Œè®ºæ–‡åœ¨ 4.3 èŠ‚ä½¿ç”¨ disentangle è¿™ä¸ªè¯æ¥è¯´æ˜è¿™ä»¶äº‹ã€‚
    *   5.2.3 èŠ‚é€šè¿‡å®éªŒè¯´æ˜äº†è¿™ç§ disentangle çš„å¿…è¦æ€§ã€‚
    *   ï¼ˆåŒæ—¶ï¼Œè®ºæ–‡åœ¨é™„å½• C è¯´æ˜ï¼Œè‹¥ä»»åŠ¡åµŒå…¥ \\(z\_i, z\_j\\) è¿‡äºæ¥è¿‘ï¼Œè¿ç»­ç¥ç»ç½‘ç»œæ— æ³•åŒºåˆ†å…¶ä»·å€¼å‡½æ•°ã€‚å¦‚æœ Q å‡½æ•°çš„ bootstrapping è¯¯å·®åå‘ä¼ æ’­åˆ° \\(q\_\\phi(z|c)\\)ï¼Œä¼šè¿«ä½¿ç¼–ç å™¨ç”Ÿæˆç›¸è¿‘çš„çš„ z æ¥æœ€å°åŒ– TD è¯¯å·®ï¼Œç ´åä»»åŠ¡å¯åˆ†æ€§ã€‚ï¼ˆè®ºæ–‡å¥½åƒæ²¡æœ‰çœŸè¿™æ ·è¯´ï¼Œåº”è¯¥æ˜¯ kimi çš„å¹»è§‰ï¼‰
*   å…³äºä½¿ç”¨çš„ offline æ•°æ®é›†ï¼š
    *   focal è¯´å¯èƒ½ expert æ•°æ®é›†é‡Œï¼Œä¸€ä¸ª task é‡Œä¸€ä¸ª s åªä¼šå¯¹åº”ä¸€ä¸ª aï¼Œä¸åŒ task çš„ state-action åˆ†å¸ƒå‡ ä¹æ²¡æœ‰é‡å ï¼Œä¼šå¯¼è‡´ agent å­¦åˆ°ä¸€äº›åªè·Ÿ state å¯¹åº”ï¼Œè€Œä¸è·Ÿ (s,a,s',r) transition å¯¹åº”çš„ patternï¼›è€Œ mixed æ•°æ®é›†åˆ™å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚â€œè¿™æ­£æ˜¯ Li ç­‰äººï¼ˆ2019bï¼‰æ—¨åœ¨è§£å†³çš„é—®é¢˜ï¼Œç§°ä¸º MDP æ¨¡ç³Šæ€§ã€‚â€

### (UBER) Unsupervised Behavior Extraction via Random Intent Priors \[NeurIPS 2023\]

*   arxivï¼š[https://arxiv.org/abs/2310.18687](https://arxiv.org/abs/2310.18687)
*   pdfï¼š[https://arxiv.org/pdf/2310.18687](https://arxiv.org/pdf/2310.18687)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/2310.18687](https://ar5iv.labs.arxiv.org/html/2310.18687)
*   æ¥æºï¼šå¸ˆå…„çš„å·¥ä½œï¼ŒNeurIPS 2023ã€‚æ„Ÿè°¢å¸ˆå¼Ÿçš„è®²è§£ã€‚
*   å‚è€ƒåšå®¢ï¼šç›´æ¥çœ‹è¿™ä¸ªå¥½äº†ï¼Œ[CSDN | ã€è®ºæ–‡é˜…è¯»ç¬”è®°ã€‘UBERï¼šç”¨éšæœºæ„å›¾ä»æ— å¥–åŠ±æ•°æ®ä¸­æå–æœ‰ç”¨è¡Œä¸º](https://blog.csdn.net/iiiiii11/article/details/155364242)

ä¸»è¦å†…å®¹ï¼š

*   settingï¼šæˆ‘ä»¬æ‹¿åˆ°äº† single-task çš„æ²¡æœ‰ reward çš„ offline æ•°æ®é›†ï¼Œç°åœ¨æƒ³åŸºäºè¿™ä¸ªæ•°æ®é›†ï¼Œå­¦å‡ºæ¥å¯ä»¥åšç›¸å…³ task çš„ç­–ç•¥ã€‚
*   methodï¼šç›´æ¥ç»™è¿™ä¸ªæ•°æ®é›†æ ‡æ³¨ N ä¸ªéšæœº rewardï¼Œç„¶åè®­å‡ºæ¥ N ä¸ªç­–ç•¥ï¼Œæœ€åä½¿ç”¨ PEX æ–¹æ³•è¿›è¡Œ offline-to-onlineã€‚
*   ç†è®ºï¼ˆæ ¹æ®å°è±¡ å¯èƒ½æœ‰å¹»è§‰ï¼‰ï¼š
    *   Proposition 4.1 æŒ‡çš„æ˜¯ï¼Œç»™å®šä¸€ä¸ª policyï¼Œæ€»èƒ½æ„é€ å‡ºæ¥ä¸€ä¸ª rewardï¼Œä½¿å¾—è¿™ä¸ª policy æ˜¯è¿™ä¸ª reward ä¸‹çš„æœ€ä¼˜ policy ä¹‹ä¸€ã€‚
    *   Theorem 4.2 æŒ‡çš„æ˜¯ï¼Œåªè¦ç›®æ ‡è¡Œä¸ºåœ¨æ•°æ®é›†ä¸­æœ‰è¾ƒå¥½çš„è¦†ç›–ï¼Œæˆ‘ä»¬å°±èƒ½æœ‰æ•ˆåœ°å­¦ä¹ å®ƒã€‚ä½¿ç”¨å¤§å°ä¸º N çš„ offline datasetï¼Œè¿™æ ·å­¦å‡ºæ¥çš„æœ€å¥½æ€§èƒ½ä¸ optimal policy çš„å·®è·ï¼Œå¯ä»¥è¢« N bound ä½ã€‚ä½¿ç”¨äº† linear MDP å’Œ PEVI é‚£ä¸€å¥—ï¼Œæˆ‘ä¸æ‡‚è¿™äº›ç†è®ºã€‚
    *   Theorem 4.3 å¥½åƒæŒ‡çš„æ˜¯ï¼ŒUBER ä½¿ç”¨çš„æ„é€  random reward çš„æ–¹æ³•å¯ä»¥ç¦» true reward è¶³å¤Ÿè¿‘ï¼Œæ˜¯ä½¿ç”¨å²­å›å½’ï¼ˆridge regressionï¼‰æ¥è¯æ˜çš„ï¼Œå²­å›å½’ æˆ‘ä¹Ÿä¸æ‡‚ã€‚
*   å®éªŒï¼šåšäº† d4rl å’Œ metaworldã€‚è¿˜æ²¡ä»”ç»†çœ‹ã€‚æ¬è¿å‚è€ƒåšå®¢çš„å†…å®¹ï¼š

> ç»“æœ 1ï¼šéšæœºæ„å›¾ç¡®å®äº§ç”Ÿå¤šæ ·ä¸”é«˜è´¨é‡è¡Œä¸ºã€‚å®éªŒæ˜¾ç¤ºï¼ŒUBERæå–çš„è¡Œä¸ºç­–ç•¥ï¼š
> 
> *   æ€§èƒ½è¶…è¶ŠåŸå§‹æ•°æ®ï¼šç‰¹åˆ«æ˜¯åœ¨åŸå§‹æ•°æ®è´¨é‡ä¸é«˜æ—¶
> *   åˆ†å¸ƒæ›´åŠ å¤šæ ·ï¼šå›æŠ¥åˆ†å¸ƒçš„ç†µå€¼æ˜¾è‘—é«˜äºåŸå§‹æ•°æ®é›†å’Œè¡Œä¸ºå…‹éš†æ–¹æ³•
> 
> ç»“æœ 2ï¼šåœ¨çº¿å­¦ä¹ åŠ é€Ÿæ˜¾è‘—ã€‚åœ¨Mujocoè¿åŠ¨ä»»åŠ¡ä¸­ï¼ŒUBERç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼š
> 
> *   å­¦ä¹ é€Ÿåº¦æ›´å¿«ï¼šåœ¨ç›¸åŒç¯å¢ƒæ­¥æ•°ä¸‹è·å¾—æ›´é«˜å›æŠ¥
> *   æœ€ç»ˆæ€§èƒ½æ›´å¥½ï¼šåœ¨å¤šæ•°ä»»åŠ¡ä¸­è¾¾åˆ°æˆ–æ¥è¿‘ä¸“å®¶æ°´å¹³
> 
> ç»“æœ3ï¼šè·¨ä»»åŠ¡è¿ç§»èƒ½åŠ›ã€‚åœ¨ Meta-World çš„å¤šä»»åŠ¡å®éªŒä¸­ï¼ŒUBER å­¦åˆ°çš„è¡Œä¸ºç­–ç•¥èƒ½å¤ŸæˆåŠŸè¿ç§»åˆ°ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œè¯æ˜äº†å…¶è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚å¯èƒ½çš„åŸå› æ˜¯ï¼Œéšæœºå¥–åŠ±äº§ç”Ÿäº†é€šç”¨è¿åŠ¨åŸè¯­ï¼ˆå¦‚"æ¥è¿‘ç‰©ä½“"ã€â€œç²¾ç¡®æ§åˆ¶æœ«ç«¯æ‰§è¡Œå™¨â€ï¼‰ï¼Œè¿™äº›åŸè¯­åœ¨ä¸åŒä»»åŠ¡é—´å¯è¿ç§»ã€‚

### (IDAQ) Offline Meta Reinforcement Learning with In-Distribution Online Adaptation \[ICML 2023\]

*   arxivï¼š[https://arxiv.org/abs/2305.19529](https://arxiv.org/abs/2305.19529)
*   pdfï¼š[https://arxiv.org/pdf/2305.19529](https://arxiv.org/pdf/2305.19529)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/2305.19529](https://ar5iv.labs.arxiv.org/html/2305.19529)
*   æ¥æºï¼šå¸ˆå¼Ÿæ¨èçš„å·¥ä½œï¼ŒICML 2023ã€‚
*   å‚è€ƒåšå®¢ï¼š[ã€è®ºæ–‡é˜…è¯»ç¬”è®°ã€‘IDAQï¼šç¦»çº¿å…ƒå¼ºåŒ–å­¦ä¹ ä¸­çš„åˆ†å¸ƒå†…åœ¨çº¿é€‚åº”](https://blog.csdn.net/iiiiii11/article/details/155364351)
*   ï¼ˆå°±ç®—æœ‰åšå®¢ï¼Œä¹Ÿè¿˜æ˜¯çœ‹ä¸å¤ªæ‡‚ï¼Œä¸€æ˜¯ä¸å¤ªäº†è§£ multi-task çš„å…·ä½“ settingï¼ŒäºŒæ˜¯ä¸å¤ªèƒ½ get åˆ° offline ä»–ä»¬è®²çš„ distribution shift æ•…äº‹â€¦â€¦ æ‰¾æ—¶é—´å¥½å¥½å­¦ä¸€ä¸‹ï¼Œå¯èƒ½å…ˆçœ‹çœ‹ focal
*   baselineï¼šFOCALã€MACAWã€BOReLã€‚

### Offline Meta-Reinforcement Learning with Online Self-Supervision \[ICML 2022\]

*   arxivï¼š[https://arxiv.org/abs/2107.03974](https://arxiv.org/abs/2107.03974)
*   pdfï¼š[https://arxiv.org/pdf/2107.03974](https://arxiv.org/pdf/2107.03974)
*   htmlï¼š[https://ar5iv.labs.arxiv.org/html/2107.03974](https://ar5iv.labs.arxiv.org/html/2107.03974)
*   æ¥æºï¼šç–‘ä¼¼æ˜¯ offline meta-RL + offline-to-online çš„æ–‡ç« ï¼ŒICML 2022ã€‚æ„Ÿè§‰ä¸ç€æ€¥è¯»ã€‚

  

æ„Ÿè°¢å¸ˆå¼Ÿå’Œå‚è€ƒåšå®¢çš„è®²è§£ğŸµ