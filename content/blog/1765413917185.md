---
layout: post
title: '吴恩达深度学习课程四：计算机视觉  第一周：卷积基础知识（三）简单卷积网络'
date: "2025-12-11T00:45:17Z"
---
吴恩达深度学习课程四：计算机视觉 第一周：卷积基础知识（三）简单卷积网络
====================================

此分类用于记录吴恩达深度学习课程的学习笔记。  
课程相关信息链接如下：

1.  原课程视频链接：[\[双语字幕\]吴恩达深度学习deeplearning.ai](https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&from_spmid=playlist.playlist-detail.0.0&is_story_h5=false&mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1713085655&unique_k=DfBgvFW&up_id=8654113&vd_source=e035e9878d32f414b4354b839a4c31a4)
2.  github课程资料，含课件与笔记:[吴恩达深度学习教学资料](https://github.com/robbertliu/deeplearning.ai-andrewNG)
3.  课程配套练习（中英）与答案：[吴恩达深度学习课后习题与答案](https://blog.csdn.net/u013733326/article/details/79827273)

本篇为第四课的第一周内容，[1.6](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=113)到[1.8](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=115)的内容。

* * *

本周为第四课的第一周内容，这一课所有内容的中心只有一个：**计算机视觉**。应用在深度学习里，就是专门用来进行图学习的模型和技术，是在之前全连接基础上的“特化”，也是相关专业里的一个重要研究大类。  
**这一整节课都存在大量需要反复理解的内容和机器学习、数学基础。** 因此我会尽可能的补足基础，用比喻和实例来演示每个部分，从而帮助理解。  
本篇的内容关于简单卷积网络，是对之前知识的一次简单整合应用，篇幅较长，但理解上的难度并不大。

1\. 多维卷积：通道数
============

我们在[基础部分](https://www.cnblogs.com/Goblinscholar/p/19323640)里提到过，一张“亮度表”只能表示灰度图，而彩色图像都是是三维表格。  
到目前为止，我们一直在用灰度图来进行演示，也就是 \\(M \\times N\\) 的二维表格。但真实世界的图片不是二维的，而是三维的：

\\\[M \\times N \\times 3 \\\]

我们称之为 **RGB 三通道**。  
对于多通道，我们可以把它想象成一个“分层蛋糕”：  
上层是红色信息（R），  
中层是绿色信息（G），  
下层是蓝色信息（B） 。  
每一层都是一张 2D 图，但三层叠在一起，才构成完整的彩色图像，就像这样：  
![myplot2222222222222.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210190908142-922905378.png)

现在继续按我们之前的符号规范引入通道数：  
**我们用 \\(n\_c\\) 来表示通道数的多少，现在一幅图像就要表示为：**$$n \\times n \\times n\_c$$下面就展开一下多维卷积的过程。

1.1 卷积核如何应对多通道图像？
-----------------

现在，我们输入的图像“亮度表”从二维变成了多维，你认为卷积核要怎么修改才能适应这个变化？  
**答案很符合直觉：你变成几维，我也变成几维就好了。**  
对彩色图片进行卷积时，一个卷积核不再是 \\(f \\times f\\)，**它同样增加了通道数**：

\\\[f \\times f \\times n\_c \\\]

表示它对 R、G、B 三个通道各自有一块小卷积核。  
打个比方：**卷积核就像海绵，随着图像维度增加，卷积核也不是一层海绵，而是“叠了三层的海绵”，每层负责吸收一层颜色的信息，做对应层的“清洁处理”工作。**  
就像这样：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210190908033-180191755.png)  
我们继续，看看在这样的改变下如何进行卷积操作。

1.2 多维卷积如何进行？
-------------

先来看看结果：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210191243380-396968593.png)  
你发现，**三通道的输入图像经过三通道的卷积核，变成了一张单通道图**。  
这是一个比较反直觉的部分，为什么结果不是三通道而是变成了单通道？  
我们先介绍一下多维卷积的运算步骤，再从原理上回答这个问题。  
**卷积步骤如下：**

1.  对 R 通道卷积核的 R 部分做卷积。
2.  对 G 通道卷积核的 G 部分做卷积。
3.  对 B 通道卷积核的 B 部分做卷积。
4.  把**三份结果相加**，得到一个单独的数，这就是输出特征图的一个像素。

再按照课程内容演示一下：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210191243512-1061723617.png)  
现在，我们从原理上说说：**为什么非要把三层结果相加成一层，而不是让结果也是三通道？**

我们先回到卷积核的基本作用：提取局部特征。  
就像我们之前的[边缘检测部分](https://www.cnblogs.com/Goblinscholar/p/19323640)，我们用人工设计的一个固定卷积核来提取图像的竖直边缘。  
应用到这个例子中，**首先要明确的一点是：我们只是把一个样本用三个通道来表示，这并不是三个样本。**  
如果我们对三个通道进行竖直边缘检测，实际上也只是在检测一幅图像的竖直边缘。**当卷积核要在这一幅图像上判断“有没有竖直边缘”这种特征时，它必须综合这三个通道的共同表现，而不是分别给出三个答案。**  
所以结果自然是一张**单通道的特征图**，表示“这幅图像这个特征的强弱”。

再打个比方：  
你佩戴红色、绿色、蓝色的滤光镜，看同一个苹果，想知道“苹果轮廓在哪里”。  
这时，你不可能给出三个轮廓，而是把三次看到的轮廓信息综合，得到唯一一个真正的轮廓。

如果你还比较纠结这个问题，没关系，我们下面就来看看，**什么情况下，输出是多通道的**。

1.3 多特征提取
---------

很显然，如果我们想让计算机“认识”某些物体，比如猫，就不能像上面一样，对应一个样本，只检测它的竖直边缘，**对于这种低层特征，我们需要更多内容**，比如水平边缘，倾斜边缘，光亮程度，纹理变化等等·····  
只有这样，我们才让一步步拼出猫眼睛，猫耳朵，猫嘴巴，身体等**高层特征**，最终让机器学习到：什么是猫，对其他东西也是同理。  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210191252003-1809910772.png)  
**所以，在卷积中，对一个或者一批次样本，一次提取多个特征，才是更合理的做法。**  
如何实现这一点？  
同样很符合直觉：**我用这个卷积核提取竖直边缘，那我再来一个卷积核提取水平边缘，再来几个卷积核提取别的特征不就好了？**  
当然，要提前强调的是，在深度学习领域里，我**们不会明确的得知哪个卷积核对应哪个特征，这些是模型自己学习和更新的，但是，每一个卷积核，一定代表一个不同的角度。**

现在，重点来了，每一个卷积核对图像进行卷积操作就会生成一个特征图，那你觉得这些不同的特征图会如何组织呢？  
**答案就是通道数。**  
来进行演示：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210190921588-1637840404.png)  
现在，我们就得到了输出图像的通道规律：**输出图像的通道数等于该层卷积核数量**。  
下一部分，我们就来正式认识一下卷积网络。

2\. 卷积层的正向传播过程
==============

还记得在全连接网络中，数据经过一个全连接层的传播过程是什么样的吗？  
这是两个最重要的公式：

\\\[\\mathbf{Z^{\[L\]}} = \\mathbf{W^{\[L\]}} \\mathbf{A^{\[L-1\]}} + \\mathbf{b^{\[L\]}} \\\]

\\\[ \\mathbf{A^{\[L\]}} = g(\\mathbf{Z^{\[L\]}}) \\\]

如果你有些忘了它们是干什么的了，它们的总结在这里：[深度神经网络的关键概念](https://www.cnblogs.com/Goblinscholar/p/19164008)  
**而现在，对于一层中应用卷积操作而不是线性组合的层级，我们称为卷积层。**  
这里要强调一下，卷积层可以是隐藏层，全连接层也可以是隐藏层。前者是说明层级的**内容**，后者是说明层级的**位置**。  
现在就继续用上面的例子，来看看**卷积层是如何进行正向传播**的，先不摆公式，符号的引入让正式的卷积传播公式相当的乱，摆出来反而阻碍理解，我们先用全连接层的公式来类别一下。  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210190907677-2064004725.png)  
这一步的类比很容易理解，大致看来，我们就算把线性组合的内容换成了卷积操作。  
如果还用全连接网络的内容来类比，我们还可以说，一层中有几个神经元，就有几个卷积核，卷积核数量决定输出的通道数。  
但是后面我们就会了解到，**卷积层的表示不会再用全连接那样的“圆圈”来表示，它有一套自己的结构表示方法**，几乎不会再提到“神经元”这个词。

简单了解这点后，我们继续下一步：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210191243397-631665699.png)  
对于激活函数这里同样比较直观，就是对特征图中的**每一个像素逐个激活**。  
在全连接网络中，我们通过激活函数引入从样本到标签的非线性映射。  
而现在，激活函数同样能让网络能够**组合特征、产生“非线性特征”**，如果没有激活函数，我们永远不能从“边缘”组合出“形状”，因为组合是非线性行为。

这就是一个卷积层的正向传播过程了，下面我们就正式引入在网络中，对于卷积层的符号规范。

3\. 卷积层在网络中的符号规范
================

一下子列举出来，看起来会很多，但实际上很多都是之前使用的，我们也会在不断使用中越发熟悉。

3.1 符号规范
--------

符号

含义

说明

\\(W^{\[l\]}\\)

第 \\(l\\) 层的卷积核参数

形状为 \\((f, f, n\_c^{\[l-1\]}, n\_c^{\[l\]})\\)，即：核大小 × 输入通道数 × 本层卷积核个数

\\(b^{\[l\]}\\)

第 \\(l\\) 层的偏置

每个卷积核对应一个偏置；形状 \\((1, 1, 1, n\_c^{\[l\]})\\)

\\(A^{\[l-1\]}\\)

上一层的激活值（输入特征图）

形状为 \\((n\_H^{\[l-1\]}, n\_W^{\[l-1\]}, n\_c^{\[l-1\]})\\)

\\(Z^{\[l\]}\\)

卷积后的线性输出

还未进入激活函数

\\(A^{\[l\]}\\)

卷积层的激活输出

对 \\(Z^{\[l\]}\\) 的每个像素逐点激活（ReLU 等）

\\(f\\)

卷积核大小

若卷积核是 \\(3\\times 3\\)，则 \\(f=3\\)

\\(s\\)

步幅（stride）

每次卷积核滑动的像素数

\\(p\\)

Padding 大小

图像边缘填充的像素数

\\(n\_H^{\[l\]}\\)

第 \\(l\\) 层输出特征图的高度

由公式计算得到

\\(n\_W^{\[l\]}\\)

第 \\(l\\) 层输出特征图的宽度

由公式计算得到

\\(n\_c^{\[l\]}\\)

第 \\(l\\) 层输出通道数

**等于卷积核的数量**

了解了单个符号后，下面就看看应用这些符号在正向传播中的维度变化。

3.2 一次正向传播中的维度变化
----------------

首先，用符号表示就是这样：

阶段

大小

输入大小

\\(n\_H^{\[l-1\]} \\times n\_W^{\[l-1\]} \\times n\_C^{\[l-1\]}\\)

卷积后大小

\\(n\_H^{\[l\]} \\times n\_W^{\[l\]} \\times n\_C^{\[l\]}\\)

激活后大小

\\(n\_H^{\[l\]} \\times n\_W^{\[l\]} \\times n\_C^{\[l\]}\\)

输出大小

\\(n\_H^{\[l\]} \\times n\_W^{\[l\]} \\times n\_C^{\[l\]}\\)

再举个实例来来看看：  
设置参数：\\(f=3,\\ s=1,\\ p=1,\\ n\_C^{\[l\]}=8\\)，输入为 \\(32 \\times 32 \\times 3\\)  
传播过程如下：

阶段

大小（维度）

**输入（Input）**

\\(32 \\times 32 \\times 3\\)

**Padding 后**

\\((32 + 2\\cdot 1) \\times (32 + 2\\cdot 1) \\times 3 = 34 \\times 34 \\times 3\\)

**卷积核（Filters）**

每个卷积核：\\(3 \\times 3 \\times 3\\)；数量：8

**卷积输出 Z（未激活）**

使用公式：\\(n\_H^{\[l\]} = (34 - 3)/1 + 1 = 32\\)，最终：\\(32 \\times 32 \\times 8\\)

**激活 A（逐像素激活）**

与 Z 同维度：\\(32 \\times 32 \\times 8\\)

**输出**

\\(32 \\times 32 \\times 8\\)

我们继续，讨论一下之前提到过的网络参数问题。

4\. 卷积层的参数量
===========

还是在[第一部分里](https://www.cnblogs.com/Goblinscholar/p/19323640)，我们提到过，大图片输入全连接层会让网络的参数规模巨大，不仅硬件可能跟不上，更有可能出现[梯度现象](https://www.cnblogs.com/Goblinscholar/p/19190303)。

那卷积是如何缓解这个问题的呢？卷积层的参数量又是如何组织的呢？  
我们还是用上面的例子来进行演示：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210190907712-1175094467.png)

可以发现，对于相同的输入，卷积网络只需要维护 56 个参数，但是全连接网络却需要 218 个。  
但这不是重点，218 个参数好像也不是很多。  
**重点在于，卷积层的参数量不因输入图像尺寸变化而变化。** 继续往下看：  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210190922981-1180857508.png)  
**这就是卷积相对于全连接在处理这种大尺寸，多通道且有空间结构的数据时的核心优势所在。**

最后，看一个简单的卷积神经网络示例。

5\. 一个简单的卷积神经网络
===============

现在，经过一步步引入，我们来看一个简单的卷积神经网络。  
![image.png](https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251210191254223-1263704041.png)  
**之后，我们就会用这种“立方体”的形式来表示卷积网络，这也是文献中的常用形式。**

正向传播的过程不难理解，一步步计算就可以得到，但我想你可能有一个疑惑，那就是**为什么最后的特征图还是要展平输入全连接层？**  
我们展开一下：

5.1 卷积网络为什么要这么设计？
-----------------

我们通过卷积层和全连接层在这个任务中的作用来解答这个问题。  
先说结论：**卷积层负责“看图”，全连接层负责“做决定”。**

#### （1）卷积层逐步提取特征

我们知道，图像通过卷积层得到特征图，随着层层卷积，特征也从低级渐渐组合到高级，从边缘，纹理渐渐组成眼睛，耳朵。  
因此，我们可以说：**卷积层的作用是提取图像特征，为后续层提供更高层次的语义特征。**  
这是全连接层做不到的地方，因为它看不到空间结构。

#### （2）全连接层依据特征进行决策

当然，全连接层也有自己的作用。  
当卷积到最后，我们得到的特征图已经相当高级了，这个通道可能识别出了猫耳朵，那个通道可能识别出了猫眼睛。  
**但是在当前的任务要求中，我们不能把一幅图作为输出，即使经过激活函数，卷积层输出的仍是特征图而非“结果”。**  
谁能根据这些信息得出结果？就是全连接层。

#### （3）为什么现在又可以展平了？

我们最开始说展平会破坏空间结构，但是现在的展平，就不再有这么大的破坏性。  
为什么？  
原因很简单：**卷积层已经把图像的空间结构“消化”得差不多了。**  
早期卷积层确实非常依赖空间关系，你不能随便打乱像素位置，否则边缘、角点、纹理这种“几何意义”就会被破坏得一干二净。  
但到了网络后期，情况变了。在经历了多次卷积之后，特征图已经不再是“边缘弯曲”这种几何信息，而更像是“耳朵是否存在”“眼睛的形状是否明显”“身体轮廓形状”这样的**语义级信息**。  
也就是说，**空间结构逐渐被抽象成了“有没有”而不是“在哪儿”。** 它们的位置影响已经远不如它们的出现与否重要。  
现在，**全连接层就可以根据这些有什么，没有什么来做出最后的决定。**

最后，举一个比较形象的例子：**拼拼图**  
我们现在有一个非常难的拼图，非常的细碎，卷积层的作用就是把小拼块拼成大拼块，让难度降低。  
而全连接层就是以卷积层拼出的大拼块为依据，识别到“原来我在拼一只猫。”

6\. 总结
======

**概念**

**原理**

**比喻**

**多通道卷积核**

卷积核也需匹配输入的通道数，尺寸从 \\(f\\times f\\) 变为 \\(f\\times f\\times n\_c\\)。

海绵变成“叠三层的海绵”，每层分别处理 R/G/B 信息。

**多维卷积输出为单通道**

对 R/G/B 分别卷积后相加，得到一个数 → 输出一个特征图。是对同一图像的“整体特征”判断。

你戴三种颜色滤镜看同一苹果，最后你只能综合得到一个“轮廓”，不会有三个版本。

**特征图数量（输出通道数）**

一个卷积核提取一种特征；多个卷积核就得到多个特征图，形成输出的多个通道。

一组侦探同时观察同一画面：一个找边缘、一个找亮度、一个找纹理 → 每人报告一页结果。

**卷积层正向传播**

将全连接层的 \\(W A + b\\) 替换为卷积运算，再对每个像素激活。

原来是“数字相乘”，现在换成“贴着图像移动的小窗口扫描”。

**激活函数在卷积层中的作用**

对每一个像素位置逐点激活，使特征产生非线性组合。

没有激活就像用直尺拼图；加了激活就像用橡皮泥，可以弯、可以变形，能组合出复杂形状。

**卷积层参数量不随输入尺寸变大**

参数只依赖卷积核大小与数量，而不依赖输入宽高。

海绵的孔洞数固定，不管你擦大桌子还是小桌子，海绵“设计参数”都不增加。

**全连接层 vs 卷积层的分工**

卷积层负责提取特征；全连接层负责根据特征分类或决策。

卷积层是“眼睛”，全连接层是“裁判”。

**为什么最后要展平？**

因为卷积层已经把空间结构提炼得很浓缩、很抽象，这时展平不会破坏太多信息，方便 FC 层决策。

像把猫的五官特征总结成“说明书”，此时把说明书摊平给裁判看就够了。