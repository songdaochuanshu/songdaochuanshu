---
layout: post
title: 'Langchain 快速入门(一)'
date: "2026-01-23T00:46:56Z"
---
Langchain 快速入门(一)
=================

简介
==

langchain专门用于构建LLM大语言模型，其中提供了大量的prompt模板，和组件，通过chain(链)的方式将流程连接起来，操作简单，开发便捷。

环境配置
====

**安装langchain框架**

    pip install langchain langchain-community
    

其中langchain可以提供了各种大模型语言库选择，（这里只列举几个）例如：

    #chatgpt
    pip install langchain-openai
    #hugging face
    pip install langchain-huggingface
    #千问
    pip install langchain-qwq
    

1\. 让模型跑起来
==========

如何让你llm跑起来，这里用的是千问，来演示

### 案例

    import os
    from langchain_community.chat_models.tongyi import ChatTongyi
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    
    #这里是你的千问apikey
    os.environ["DASHSCOPE_API_KEY"] = "apikey"
    
    model = ChatTongyi(model="qwen-plus")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "你是一个精通{topic}的资深技术专家。"),
        ("user", "请用三句话解释一下什么是{concept}。")
    ])
    
    output_parser = StrOutputParser()
    
    chain = prompt | model | output_parser
    
    #文本输出
    response = chain.invoke({"topic": "Python", "concept": "列表"})
    print(response)
    
    #分割
    print("="*30)
    
    #流式输出
    for chunk in chain.stream({"topic": "人工智能", "concept": "神经网络"}):
        print(chunk, end="", flush=True)
    

### 代码解释

整个代码的流程如下：  
**创建模型->构建提示词->构建chain链->使用大模型**

##### 创建模型

这一步用不同的模型可能会不同  
这里利用langchain的千问库创建模型，可能会不同

    model = ChatTongyi(model="qwen-plus")
    
    #例如用chatgpt
    llm = init_chat_model("gpt-4o", model_provider="openai")
    

##### 构建提示词

这一步构建利用了langchain库提供提示词模板：  
其中用`{}`阔起来的在调用时可以动态用字典替换

    prompt = ChatPromptTemplate.from_messages([
        ("system", "你是一个精通{topic}的资深技术专家。"),
        ("user", "请用三句话解释一下什么是{concept}。")
    ])
    

各个角色功能如下：

**角色名称 (Role)**

**对应的类**

**作用说明**

**system**

`SystemMessage`

**系统提示词**。用于设定 AI 的“人格”、专业背景、行为准则或约束条件。它通常优先级最高，决定了后续对话的基调。

**user**

`HumanMessage`

**用户消息**。代表人类发送的内容。这是模型需要直接回答或处理的问题。

**ai**

`AIMessage`

**AI 消息**。代表模型之前的回复。在构建多轮对话（带记忆）时，需要把模型之前的回复传回去。

##### 构建chain链

这个是langchain的灵魂，这里简单说明，后面会发更详细的教学文章  
chain链的运行流程如下：  
**将输入填充prompt->将完整prompt喂给LLM->直接解析返回文本**

`StrOutputParser()`这个是langchain提供的文本解析器，用于将上面的结果解析为文本

    output_parser = StrOutputParser()
    chain = prompt | model | output_parser
    

##### 使用大模型

这里有两种方式：

1.  直接输出完整的文本

    response = chain.invoke({"topic": "Python", "concept": "列表"})
    print(response)
    

2.  流文本输出（打字机）

    for chunk in chain.stream({"topic": "人工智能", "concept": "神经网络"}):
        print(chunk, end="", flush=True)
    

**如果❤喜欢❤本系列教程，就点个关注吧，后续不定期更新~**