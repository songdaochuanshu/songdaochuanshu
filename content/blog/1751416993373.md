---
layout: post
title: '大模型基本概念介绍'
date: "2025-07-02T00:43:13Z"
---
大模型基本概念介绍
=========

目录

*   [什么是大模型](#什么是大模型)
*   [应用场景](#应用场景)
*   [分类](#分类)
    *   [按应用场景](#按应用场景)
    *   [按输入/输出内容](#按输入输出内容)
*   [历史](#历史)
*   [原理](#原理)
    *   [参数](#参数)
    *   [Transformer](#transformer)
    *   [训练](#训练)
*   [提示词工程](#提示词工程)
    *   [什么是prompt](#什么是prompt)
    *   [prompt设计法则](#prompt设计法则)
    *   [prompt设计说明](#prompt设计说明)
    *   [prompt设计示例](#prompt设计示例)

什么是大模型
------

大模型通常指LLM（Large Language Model ） 大语言模型，是一种基于深度学习构建的人工智能模型。具备强大的理解、分析、生成能力。训练一个大模型往往需要大量数据、大量存储、大量计算、大量硬件消耗。

应用场景
----

*   内容创作：写诗作文，制作音乐或者电影
*   具身机器人：跳舞、收拾房间、工厂里面工作
*   科研：蛋白质结构预测和分析，新材料研发

分类
--

### 按应用场景

*   通用大模型：不针对特定领域，具备广泛的知识储备和多种任务处理能力，可灵活应用于日常对话、文本创作、信息查询等多种场景。
*   领域专用大模型：聚焦于某一特定领域，在该领域内的知识深度和任务处理精度上更具优势。

### 按输入/输出内容

*   语言大模型：专注自然语言处理，如文字创作，问答助手
*   视觉大模型：智能机器人、智能驾驶
*   多模态大模型：即混合专家模型（MOE）是一种模型设计策略，他通过将多个模型直接结合在一起，以获得更好的预测性  
    能。在大模型中，MOE方案可以提高模型的容量和效率。当数据流经过MOE层时，每个输入标记动态的  
    被路由到一部分专家进行计算，这种方式允许更有效的计算，并且因为每个专家在特定任务上变得更专业  
    化，所有能够得到更好的效果。

历史
--

*   2022年11月，OpenAI发布GPT-3.5模型驱动的ChatGPT。
*   2024年3月，OpenAI发布GPT-4o，支持多模态，语音对话，用户多模态个性化场景。
*   2024年12月，OpenAI发布OpenAI -o1模型，此模型专注于复杂任务的推理；
*   2025年，Deepseek-R1发布并开源，成为首个可商用o1级别的推理模型，火爆全网。

原理
--

### 参数

大模型的参数就像是人的大脑神经元。神经元越多大脑处理能力越强，同理大模型的参数越多，推理能力越强，推理结果越准确。

模型的参数越大，拟合复杂情况的能力越强。模型参数不多扩大，量变将带来质变，“智能”开始出现。

### Transformer

Transformer 是用于训练大语言模型（LLM）的基础架构，通过自注意力机制（Self - Attention）、多头注意力机制（Multi - Head Attention）和前馈神经网络（Feed Forward Network）等组件，能够有效地处理文本序列，捕捉文本中的语义信息和长距离依赖关系，为 LLM 的训练提供了强大的基础，使其能够学习到丰富的语言知识和模式，从而具备强大的语言理解和生成能力

### 训练

**训练方式**

*   预训练（Pre-Train）：阅读大量基础学科的数据，这些数据往往无任何标注
*   SFT全参数微调：使用少量标注数据对大模型进行微调，会修改模型全量参数
*   SFT部分参数微调：使用比“SFT全参数微调”更少的标注数据对大模型进行微调，不需要修改模型参数。
*   RLHF基于人类反馈的强化学习：构建奖励模型，人类对大模型输出结果结果进行评分，将评分结果反馈给大模型，大模型不断自我优化
*   Prompt提示词工程：不需要标注数据，指导模型在推理过程中提取关键信息

**训练原理**

*   大语言模型：理解--->推理--->生成
*   视觉大模型：目标检测---> 语义分割--->图像分类
*   语音大模型：语音识别--->语音合成--->语义理解
*   多模态大模型：文生图--->文生视频--->跨模态理解
*   科学计算大模型：问题求解--->数值模拟--->系统建模

提示词工程
-----

### 什么是prompt

prompt作为预设的导航指令，是用户和大模型沟通的桥梁，编写质量上乘的prompt提示词，可以提高大模型输出内容的质量、范围、风格。

### prompt设计法则

*   明确性：提示词必须清晰、具体、避免出现歧义
*   相关性：提示词需要与任务或者对话上下文高度相关，应紧密围绕某一主题或者目标
*   引导性：引导大模型用特定方向或者风格输出
*   角色扮演：通过给大模型指定某一个角色（如：高速大模型你是大学老师），大模型会模仿该角色的语言风格，设身处地身临其境的思考
*   思维链技术：一步一步引导模型，拆分任务，分步推理，结构化引导让模型逐渐深入理解问题
*   少样本学习：通过提供少量示例来引导大模型理解任务需求并生成高质量回复

### prompt设计说明

一个好的prompt应该包含以下内容（一项或多项）：

*   任务描述：告诉大模型目标是什么
*   规则说明：任务要求，具体规则，注意事项，甚至可以指定大模型的角色
*   示例：提供样例供大模型参考
*   输入参数：告诉大模型应该如何处理输入参数

**设计语法**

*   可以用#号来突出和强调
*   可以用 - 号来结构化组织语言

### prompt设计示例

    
    
    # 任务描述
    
    你需要为用户设计一条个性化的旅游路线，结合用户提供的旅游地、游玩天数、预算、兴趣偏好等信息，规划包含交通、住宿、景点、美食的详细行程安排。
    
    
    # 规则说明：
    
    - 以专业旅游规划师的角色进行设计，确保行程合理且具有可行性。
    
    - 行程需涵盖每日具体安排，包括上午、下午、晚上的活动规划，精确到时间节点。
    
    - 交通方式推荐要考虑成本与效率，提供多种选择并注明价格区间。
    
    - 住宿推荐需区分不同预算档位（经济型、舒适型、豪华型），给出具体酒店名称及位置优势。
    
    - 景点介绍要包含特色亮点与门票价格，美食推荐需注明店铺名称、招牌菜品及人均消费。
    
    - 注意劳逸结合，每天安排的景点数量不宜超过3个，避免行程过于紧凑。
    
    
    # 示例：
    
    用户输入：旅游地为成都，游玩天数 3 天，预算中等，兴趣偏好为美食、历史文化。
    
    大模型输出：
    
    第一天
    
    上午（8:30-11:30）：乘坐地铁前往武侯祠，感受三国文化，门票 50 元。游览结束后，在附近的 “老房子・川菜博物馆” 品尝经典川菜，人均 80 元。
    
    下午（13:00-17:00）：步行前往锦里古街，欣赏古色古香的建筑，体验民俗文化，免费参观。在街边小店品尝三大炮、糖油果子等特色小吃。
    
    晚上（18:30-21:00）：打车前往宽窄巷子，观赏夜景，打卡特色网红餐厅 “饕林餐厅”，人均 90 元。住宿推荐：舒适型可选择 “成都博舍酒店”，位于太古里商圈，出行便捷，每晚约 1200 元；经济型可选择 “汉庭酒店（成都宽窄巷子店）”，性价比高，每晚约 300 元。
    
    第二天
    
    上午（9:00-12:00）：乘坐景区直通车前往杜甫草堂，感受诗圣故居的文化氛围，门票 50 元。附近午餐可选择 “陈麻婆豆腐（杜甫草堂店）”，品尝招牌麻婆豆腐，人均 70 元。
    
    下午（13:30-17:30）：打车前往金沙遗址博物馆，探索古蜀文明，门票 70 元。
    
    晚上（18:30-20:30）：在博物馆附近的 “钟水饺（蜀汉路店）” 享用晚餐，人均 40 元，之后返回酒店休息。
    
    第三天
    
    上午（9:00-12:00）：前往成都大熊猫繁育研究基地，看可爱的大熊猫，门票 55 元。午餐在基地内的餐厅解决，人均 30 元。
    
    下午（13:30-16:00）：返回市区，在春熙路逛街购物，感受成都繁华商圈氛围，免费。
    
    晚上（17:30-20:00）：在春熙路的 “小龙坎火锅（春熙店）” 享用地道成都火锅，人均 100 元，之后结束行程。
    
    # 输入参数
    
    旅游地（具体城市或地区）、游玩天数（具体数字）、预算（分为低、中、高或具体金额范围）、兴趣偏好（如自然风光、历史古迹、美食、购物等，可多项），根据这些输入参数，严格按照规则说明进行旅游路线设计 。
    
    
    

邮箱：cnaylor@163.com

技术交流QQ群：1158377441

欢迎关注我的微信公众号【TechnologyRamble】，后续博文将在公众号首发：

[![TechnologyRamble](https://images.cnblogs.com/cnblogs_com/Naylor/2399439/o_240521084523_2.png)](http://img.anlu58.com/logo/2.png)