---
layout: post
title: 'LLM ，MCP协议，A2A协议，RAG，智能体(AI Agent) 图解详细讲解'
date: "2025-08-24T00:45:43Z"
---
LLM ，MCP协议，A2A协议，RAG，智能体(AI Agent) 图解详细讲解
=========================================

LLM ，MCP协议，A2A协议，RAG，智能体(AI Agent) 图解详细讲解
=========================================

@

目录

*   [LLM ，MCP协议，A2A协议，RAG，智能体(AI Agent) 图解详细讲解](#llm-mcp协议a2a协议rag智能体ai-agent-图解详细讲解)
*   [MCP 概述](#mcp-概述)
    *   [如何理解 LLM 和 MCP](#如何理解-llm-和-mcp)
    *   [MCP 能做什么？](#mcp-能做什么)
        *   [对于程序员来说 MCP 能](#对于程序员来说-mcp-能)
        *   [对于大众用户来说 MCP 能](#对于大众用户来说-mcp-能)
    *   [MCP 的理解](#mcp-的理解)
    *   [程序员使用 MCP](#程序员使用-mcp)
        *   [使用前的准备工作：](#使用前的准备工作)
        *   [stdio 的本地环境安装](#stdio-的本地环境安装)
    *   [MCP 的工作原理](#mcp-的工作原理)
    *   [MCP 的工作流程](#mcp-的工作流程)
*   [A2A 协议](#a2a-协议)
*   [RAG 概述](#rag-概述)
    *   [为什么需要 RAG](#为什么需要-rag)
*   [智能体(AI Agent) 的概述](#智能体ai--agent-的概述)
    *   [智能体(AI Agent) 的核心五要素：](#智能体ai--agent-的核心五要素)
        *   [一. 核心要素 1：大模型(LLM)](#一-核心要素-1大模型llm)
        *   [二. 核心要素 2：记忆(Memory)](#二-核心要素-2记忆memory)
        *   [三. 核心要素 3：工具使用(Tool Use)](#三-核心要素-3工具使用tool-use)
        *   [四. 核心要素 4：规划决策(Planning)](#四-核心要素-4规划决策planning)
        *   [五. 核心要素 5：行动(Action)](#五-核心要素-5行动action)
*   [最后：](#最后)

MCP 概述
======

**两个互联网领域的重大挑战：**

*   第一：Agent 与 Tools(工具)的交互

Agent 需要调用外部工具和 API，访问数据库，执行代码等

MCP 协议解决

*   第二：Agent 与 Agent(其他智能体或用户)的交互

Agent 需要理解其他 Agent 的意图，协同完成任务，与用户进行自然的对话。

A2A 协议解决

如何理解 LLM 和 MCP
--------------

MCP 能做什么？
---------

### 对于程序员来说 MCP 能

*   举例 1：开发部署：

开发者通过自然语言指令“部署新版本到测试环境”，触发 MCP 链式调用 GitLab API （代码合并）、Jenkins API（构建镜像）、Slack API（通知团队）。

*   举例 2：SQL 查询

开发者通过自然语言输入，比如“查询某集团部门上个季度销售额”，就能查询出数 据库的数据，并结合大模型进行回答，不再需要编写 SQL，MCP 自动转换为精准 SQL 语句并执行。

举例 3：manus 智能体

Manus的每一次任务处理都至少需要调用网页搜索、网页访问、网页信息获取、本地文件创建、代码解释器等几十个外部工具。

这里就暴露了两个问题。

问题1：可供大模型调用的工具不足。

问题2：调用工作量很大。

借助 MCP，只要支持了该协议，就能轻松将各种数据源和工具连接到 LLM。

### 对于大众用户来说 MCP 能

举例 1：旅游规划

当我要去旅行时，旅行规划助手通过 MCP 同时调用天气 API（获取目的地气象）、交 通 API（查询航班动态）、地图 API（规划路线），AI 自动生成带实时数据的行程方案。

举例 2：联网搜索

我们在与 LLM 交互时，经常需要联网搜索最新信息以减少幻觉。然而，这里也存在问 题：

1、并非所有聊天机器人都支持联网功能

2、即使支持联网，也可能不包含你习惯使用的搜索引擎。

在没有 MCP 的情况下，用户只能等待开发者添加特定搜索引擎的支持。

有了 MCP 后，只需简单配置，就能将所需服务接入当前使用的聊天机器人。

举例 3：业绩查询

用户询问“查询上季度营业额”，MCP 自动组合调用 CRM 系统 API（获取客户数据） + 财务系统 API（调取报表）+ 邮件 API（发送总结报告）。

MCP 的理解
-------

**MCP（Model Context Protocol，模型上下文协议**） ，2024年11月底，由 Anthropic 推出的一种开放标准。旨在为大语言模型（LLM）提供统一的、 标准化方式与外部数据源和工具之间进行通信。

MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准。旨在为大语言模型（LLM）提供统一的、 标准化方式与外部数据源和工具之间进行通信。

*   **传统AI集成的问题：**这种为每个数据源构建独立连接的方式，可以被视为一个M\*N问题。
*   **问题**：架构碎片化，难以扩展，限制了AI获取必要上下文信息的能力
*   **MCP解决方案：**提供统一且可靠的方式来访问所需数据，克服了以往集成方法的局限性。

**MCP 作为一种标准协议，极大地简化了大语言模型与外部世界的交互方式，使开发者能够以统一的方式为 AI 应用添加各种能力。**

**MCP 的官方文档：**[https://modelcontextprotocol.io/introduction](https://modelcontextprotocol.io/introduction)

* * *

程序员使用 MCP
---------

MCP 的应用场景：

### 使用前的准备工作：

**MCP 的通信机制：**

根据 MCP 的规范，当前支持两种通信机制（传输方式）：

*   **stdio(标准输入输出)**：主要用在本地服务上，操作你本地的软件或者本地的文件。比如 Bleander 这种就只能用 Stdio 因为它没有在线服务。MCP 默认通信方式。
*   **SSE(Server-Sent-Events)：** 主要用在远程通信服务上，这个服务本身就有在线的 API，比如访问你的谷歌邮箱，天气情况等。简单的理解就是一种流式输出（服务器接受一点，就输出一点）

**MCP的通信机制：stdio方式：**

优点

• 这种方式适用于客户端和服务器在同一台机器上运行的场景，简单。

• stdio模式无需外部网络依赖，通信速度快，适合快速响应的本地应用。

• 可靠性高，且易于调试。

缺点

• Stdio 的配置比较复杂，我们需要做些准备工作，你需要提前安装需要的命令行工具。

• stdio模式为单进程通信，无法并行处理多个客户端请求，同时由于进程资源开销较大，不适合在本地运行大量服务。（限制了其在更复杂分布式场景中的使用）

* * *

**MCP的通信机制：SSE方式：**

场景

• SSE方式适用于客户端和服务器位于不同物理位置的场景。

• 适用于实时数据更新、消息推送、轻量级监控和实时日志流等场景

• 对于分布式或远程部署的场景，基于 HTTP 和 SSE 的传输方式则更为合适。

优点

• 配置方式非常简单，基本上就一个链接就行，直接复制他的链接填上就行

* * *

### stdio 的本地环境安装

stdio的本地环境有两种：

*   一种是Python 编写的服务，
*   一种用TypeScript 编写的服务。 分别对应着uvx 和 npx 两种指令。

\*\* stdio的本地环境安装：uvx\*\*

第1种：若已配置Python环境，可使用以下命令安装：

    pip install uv
    

> 安装了 uv ，就会按照对应 uvx

两种安装方式：

第2种：在Windows下可以通过PowerShell运行命令来安装 uv。

    
    powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex”
    

验证：重启终端并运行以下命令检查是否正常：

    uv --version
    uvx --help
    

stdio的本地环境安装：npx

Node.js下载的官网：[https://nodejs.org/zh-cn](https://nodejs.org/zh-cn)

配置环境变量，并测试

MCP 的工作原理
---------

**MCP 的 C / S 架构：**

5 个核心概念：

MCP 遵循客户端 —— 服务器架构(client-server) ，其中包含以下几个核心概念：

1.  MCP 主机（MCP Hosts）
2.  MCP 客户端（MCP Clients）
3.  MCP 服务器（MCP Servers）
4.  本地资源（Local Resources）
5.  远程资源（Remote Resources）

**MCP Host：**

作为运行 MCP 的主应用程序，例如 Claude Desktop、Cursor、Cline 或 AI 工具。 为用户提供与LLM交互的接口，同时集成 MCP Client 以连接 MCP Server。

**MCP Client：**

MCP client 充当 LLM 和 MCP server 之间的桥梁，嵌入在主机程序中，主要负责：

• 接收来自LLM的请求；

• 将请求转发到相应的 MCP server

• 将 MCP server 的结果返回给 LLM

有哪些常用的Clients

MCP 官网([https://modelcontextprotocol.io/clients)](https://modelcontextprotocol.io/clients)) 列出来一些支持 MCP 的 Clients。 分为两类：

• AI编程IDE：Cursor、Cline、Continue、Sourcegraph、Windsurf 等

• 聊天客户端：Cherry Studio、Claude、Librechat、Chatwise等

更多的Client参考这里：

MCP Clients：[https://www.pulsemcp.com/clients](https://www.pulsemcp.com/clients)

Awesome MCP Clients：[https://github.com/punkpeye/awesome-mcp-clients/](https://github.com/punkpeye/awesome-mcp-clients/)

**MCP Server：**

每个 MCP 服务器都提供了一组特定的工具，负责从本地数据或远程服务中检索信息。 是 MCP 架构中的关键组件。

与传统的远程 API 服务器不同，MCP 服务器既可以作为本地应用程序在用户设备上运 行，也可部署至远程服务器。

比如你让助手：

• “帮我查航班信息” → 它调用航班查询 API

• “算一下 37% 折扣后多少钱” → 它运行计算器函数

作用：让 LLM 不仅能“说”，还能“做”（执行代码、查询数据等）

**MCP Server 的本质：**本质是运行在电脑上的一个nodejs 或 python程序。可以理解为客户端用命令行调用了电脑上的 nodejs或 python程序。

• 使用 TypeScript 编写的 MCP server 可以通过 npx 命令来运行

• 使用 Python 编写的 MCP server 可以通过 uvx 命令来运行。

MCP 的工作流程
---------

API 主要有两个

• tools/list：列出 Server 支持的所有工具

• tools/call：Client 请求 Server 去执行某个工具， 并将结果返回

数据流向图：

A2A 协议
======

**A2A协议：开启Agent间 自 然 协 作**

**在 AI Agent 的世界里，主要解决两大互联领域的挑战：**

*   第一、Agent 与 Tools（工具）的交互

Agent 需要调用外部 API、访问数据库、执行代码等。

*   第二、Agent 与 Agent（其他智能体或用户）的交互

Agent 需要理解其他 Agent 的意图、协同完成任务、 与用户进行自然的对话。

**A2A 的发布：**

谷歌，25年4月10日发布开源的、应用层协议 A2A（Agent-to-Agent 协议），即 Agent-to-Agent。其设计目的是使智能体（Agent）间能够以一种自然的模态进行协 作，类似于人与人之间的互动。

Github 地址：[https://github.com/google/A2A](https://github.com/google/A2A)

**A2A 的设计意义：**

基于不同底层框架和供应 商平台创建的 AI Agent 之间 可以实现通信、发现彼此 的能力、协商任务并开展 合作，企业可以通过专业 的智能体团队处理复杂的 工作流程。这无疑是其最 为突出的贡献。

**举例：阿里云 & 火山云**

阿里云上创建的 AI Agent，通过A2A协议，可以与火山云上创建的 AI Agent 进行无缝的通信与协作。

**举例2：修理汽车：**

用户（或代表用户的智能体）对修理店智能体说：“给我看看左前轮的照片，似 乎漏液了，这种情况多久了？”

• A2A 协议使得人与智能体之间这种更自然、多轮次的对话式互动成为可能。

修理店智能体在诊断出问题后，可能需要向零件供应商智能体查询某个零件的库 存和价格。

• 这种智能体与智能体之间的协作同样需要 A2A 协议来支持。

**举例3：人才招聘：**

利用 A2A 协议，招聘流程可以如此高效：

在谷歌的 Agentspace 统一界面中，招聘经理可以向自己的智能体下达任务， 让其寻找与职位描述、工作地点和技能要求相匹配的候选人。

然后，该智能体立即与其他专业智能体(专门是用来寻找并招聘我们的人才的智能体)展开互动，寻找潜在候选人。 用户会收到推荐人选，之后可以指示自己的智能体安排进一步的面试，面试 环节结束后，还可以启动另一个智能体来协助进行背景调查。

**专业的事情，交给专业的处理这部分内容的智能体解决**。

RAG 概述
======

**什么是 RAG：**

RAG（Retrieval-Augmented Generation，检索增强生成）是一种结合**信息检索**（Retrieval）与**文本生成**（Generation）的技术，旨在提升大语言模型在回答专业

问题时的准确性和可靠性。

为什么需要 RAG
---------

**场景 1：**大型语言模型(LLM) 的训练依赖于网络上**海量公开的静态数据(注意仅仅只是公开的数据而已)**，而某些**特定领域**(如企业内部资料，专有技术文档等等)的数据通常不会作为公开的训练数据，导致模型在面对这些领域的查询时，可能因缺乏足够的信息而生产不准确甚至虚构的回复。

**解决方案：** 为了解决这一类问题，RAG 技术通过引入**向量数据库(Vector Database)**作为外部知识源，将模型缺失的知识以结构化的形式提供。

**场景 2：**

随着 LLM(大模型) 规模扩大，训练成本与周期相应增加。因此，包含**最新信息的数据难以融入模型训练**过程，无法及时反映最新的信息或动态变化。导致 LLM 在应对诸如 “请推荐当前热门影片”等时间敏感问题。

**解决方案：**提供联网搜索功能。

LLM 在考试的时候面对陌生的领域，答复能力有限，然后就准备放飞自我了，而此时 RAG 给了一些提示和思路，让 LLM 懂了开始往这个提示的方向做，最终考试的正确率从 60% 到了 90%。

智能体(AI Agent) 的概述
=================

**2025年，被视为智能体落地的元年！**

* * *

*   智能体在自主能力、决策能力、协作交互等方面展现出优势，弥补了大模型的不足，是未来大模型最主流的使用方式。

目前国内各大厂商推出的大模型智能体：

智能体的应用领域：

**什么是智能体：**

> OpenAI的元老翁丽莲于2023年6月在个人博客首次提出了现代AI Agent 架构。

**智能体（AI Agent）**是一种能够自主行动，感知环境，做出决策并与环境交互的计算机系统或实体，通常依赖大型语言模型作为其核心决策和处理单元，具备独立思考，调用工具去逐步完成给定目标的能力。

**智能体架构：**

* * *

智能体(AI Agent) 的核心五要素：
---------------------

### 一. 核心要素 1：大模型(LLM)

**大模型作为“大脑”：提供推理，规划和知识理解能力，是 AI Agent 的决策中枢。**

### 二. 核心要素 2：记忆(Memory)

记忆被分为：短期记忆，长期记忆。

*   **短期记忆：**存储**单次对话**周期的**上下文信息**，属于临时信息存储机制。受限于模型的上下文窗口长度。
*   **长期记忆：**可以**横跨多个任务或时间周期**，可存储并调用核心知识，非即时任务。

长期记忆，可以通过**大模型参数微调(固化知识)**，**知识图谱**(结构化语义网络)，或**向量数据库**(相似数据库)(相似性检索)方式实现。

### 三. 核心要素 3：工具使用(Tool Use)

**工具使用**：调用外部工具(如 API，数据库)扩展能力边界。

### 四. 核心要素 4：规划决策(Planning)

**规划决策**：通过任务分解，反思与自省框架实现复杂任务处理。例如：利用思维链(Chain of Thought) 将目标拆解为子任务，并通过反馈优化策略。

### 五. 核心要素 5：行动(Action)

**行动：** 实际执行决策的模块，涵盖软件接口操作(如自动订票)和物理交互(如机器人执行搬运)。比如：检索，推理，编程等。

最后：
===

> “在这个最后的篇章中，我要表达我对每一位读者的感激之情。你们的关注和回复是我创作的动力源泉，我从你们身上吸取了无尽的灵感与勇气。我会将你们的鼓励留在心底，继续在其他的领域奋斗。感谢你们，我们总会在某个时刻再次相遇。”