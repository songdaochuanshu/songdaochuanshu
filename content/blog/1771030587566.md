---
layout: post
title: 'Nano-vLLM-Ascend(æŒç»­æ›´æ–°ä¸­)'
date: "2026-02-14T00:56:27Z"
---
Nano-vLLM-Ascend(æŒç»­æ›´æ–°ä¸­)
=======================

[Nano-vLLM-Ascend](https://github.com/linzm1007/nano-vllm-ascend)
-----------------------------------------------------------------

é¡¹ç›®é“¾æ¥ï¼š[https://github.com/linzm1007/nano-vllm-ascend](https://github.com/linzm1007/nano-vllm-ascend)  
nano-vllmæ˜¯githubå¼€æºçš„ä¸€ä¸ªgpuæ¨ç†é¡¹ç›®ï¼ŒåŸºäºå¼€æºç‰ˆæœ¬å¼„çš„ä¸€ä¸ªascend npuç‰ˆæœ¬æ¨ç†å°demoï¼Œæ—¨åœ¨å¸®åŠ©åˆå­¦è€…äº†è§£æ¨ç†çš„æ•´ä½“æµç¨‹ï¼ŒåŒºåˆ«äºvllmï¼Œnano-vllmä½“é‡æ›´å°ï¼Œéº»é›€è™½å°äº”è„ä¿±å…¨ï¼Œæ›´æœ‰åŠ©äºåˆå­¦è€…å­¦ä¹ ï¼Œéå¸¸é€‚åˆç”¨äºç›¸å…³æ¦‚å¿µçš„ç†è§£ã€‚

æ¡†æ¶å±‚æµç¨‹å›¾
------

æ¨¡å‹å±‚æµç¨‹å›¾
------

ç‰¹æ€§
--

*   ğŸ“– **å¯è¯»ä»£ç åº“** - æ ¸å¿ƒçº¦2428è¡ŒPythonä»£ç çš„æ¸…æ™°å®ç°
*   âš¡ **ä¼˜åŒ–å¥—ä»¶** - å¼ é‡å¹¶è¡Œã€torchair Ascend IRå›¾ç¼–è¯‘å’Œå›¾ç¼“å­˜ã€èåˆç®—å­ã€å‰ç¼€ç¼“å­˜ç­‰

*   \[âœ…\] å¾…å®Œæˆï¼šç›®å‰åªæ”¯æŒå•ç®—å­, npuå›¾æ¨¡å¼å®ç°
*   \[âœ…\] æ”¯æŒCPUç¯å¢ƒè¿è¡Œï¼š[nano-vllm-cpu ä»£ç ä»“åº“](https://github.com/linzm1007/nano-vllm-cpu)
*   \[âœ…\] æ€§èƒ½ä¼˜åŒ–
*   \[â³\] æ”¯æŒæ¨¡å‹: Qwen3-0.6Bã€Qwen3-32Bã€Qwen2-0.5Bã€Qwen2.5-0.5Bã€Qwen2.5-0.5B-Instructã€Llama-3.2-1B-Instructã€Qwen3-30B-A3Bã€Qwen3-VL-2B-Instructã€MiniCPM4-0.5B
*   \[âœ…\] æ”¯æŒä¸€ä¸ªmoeæ¨¡å‹:Qwen3-30B-A3B(æš‚æ—¶ä¸æ”¯æŒå…¥å›¾)
*   \[ğŸ“…\] æ”¯æŒä¸€ä¸ªomniæ¨¡å‹
*   \[âœ…\] æ”¯æŒä¸€ä¸ªvlæ¨¡å‹:Qwen3-VL-2B-Instruct(æš‚æ—¶ä¸æ”¯æŒå…¥å›¾)
*   \[âœ…\] å®ç°page attention
*   \[ğŸ“…\] å®ç°ä¸€ä¸ªè‡ªå®šä¹‰ç®—å­
*   \[ğŸ“…\] æ”¯æŒåœ¨çº¿æ¨ç†

torchairæ¥å£å‚è€ƒ [https://www.hiascend.com/document/detail/zh/Pytorch/710/modthirdparty/torchairuseguide/torchair\_00008.html](https://www.hiascend.com/document/detail/zh/Pytorch/710/modthirdparty/torchairuseguide/torchair_00008.html)  
èåˆç®—å­æ¥å£å‚è€ƒ [https://www.hiascend.com/document/detail/zh/Pytorch/720/apiref/torchnpuCustomsapi/context/torch\_npu-npu\_fused\_infer\_attention\_score\_v2.md](https://www.hiascend.com/document/detail/zh/Pytorch/720/apiref/torchnpuCustomsapi/context/torch_npu-npu_fused_infer_attention_score_v2.md)  
attentionå®ç°å‚è€ƒ [https://gitee.com/omniai/omniinfer/blob/master/omni/layers/attention/backend/attention.py](https://gitee.com/omniai/omniinfer/blob/master/omni/layers/attention/backend/attention.py) forward\_vanillaå‡½æ•°

æ”¯æŒçš„æ¨¡å‹
-----

æ¶æ„

æ¨¡å‹

ç¤ºä¾‹ HF æ¨¡å‹

Qwen3ForCausalLM

Qwen3-0.6B,Qwen3-32B

Qwen2ForCausalLM

Qwen2-0.5B

LlamaForCausalLM

Llama-3.2-1B-Instruct

Qwen3MoeForCausalLM

Qwen3-30B-A3B

Qwen3VLForConditionalGeneration

Qwen2.5-VL-3B-Instruct

MiniCPMForCausalLM

MiniCPM4-0.5B

ä»£ç è¡Œæ•°
----

ğŸ“Š æ€»ä½“æ•°æ®

èŒƒå›´

æ–‡ä»¶æ•°

æ€»è¡Œæ•°

å æ¯”

nanovllm å…¨éƒ¨

20 ä¸ª

4,652 è¡Œ

100%

models ç›®å½•

5 ä¸ª

2,224 è¡Œ

47.8%

é™¤ models å¤–

15 ä¸ª

2,428 è¡Œ

52.2%

æ¨ç†ä¼˜åŒ–æŠ€æœ¯å¤§çº²
--------

ğŸ“š **å®Œæ•´æŠ€æœ¯æ–‡æ¡£**ï¼š[LLM æ¨ç†ä¼˜åŒ–æŠ€æœ¯å¤§çº²](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/inference_optimization_guide.md)

æœ¬æ–‡æ¡£æ•´ç†äº† LLM æ¨ç†é¢†åŸŸçš„ **20 å¤§ç±»å…³é”®æŠ€æœ¯**ï¼Œåˆ†ä¸º 7 ä¸ªå±‚æ¬¡ï¼š

### ğŸ”¥ æ ¸å¿ƒæŠ€æœ¯ï¼ˆåŸºç¡€å¿…å¤‡ï¼‰

*   **KV Cache ç®¡ç†**ï¼šPageAttentionã€Prefix Cachingã€KV Cache å‹ç¼©
*   **Attention ä¼˜åŒ–**ï¼šFlashAttentionã€GQA/MQAã€ç¨€ç–æ³¨æ„åŠ›
*   **æ‰¹å¤„ç†ç­–ç•¥**ï¼šContinuous Batchingã€Dynamic Batching

### ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼ˆè¿›é˜¶ï¼‰

*   **é‡åŒ–æŠ€æœ¯**ï¼šINT8/INT4/FP8ã€AWQã€GPTQã€GGUF
*   **æŠ•æœºé‡‡æ ·**ï¼šSpeculative Decodingã€Medusaã€Lookahead
*   **è§£ç ä¼˜åŒ–**ï¼šParallel Decodingã€Token Tree Verification

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

*   **è°ƒåº¦ç­–ç•¥**ï¼šFCFSã€SJFã€Priority-basedã€Preemption
*   **å†…å­˜ä¼˜åŒ–**ï¼šMemory Poolã€Swappingã€Offloading
*   **å¹¶è¡Œç­–ç•¥**ï¼šTensor/Pipeline/Expert/Sequence Parallelism

### ğŸ§  ç‰¹æ®Šåœºæ™¯

*   **é•¿ä¸Šä¸‹æ–‡**ï¼šRoPE Scalingã€StreamingLLMã€Ring Attention
*   **å¤šæ¨¡æ€**ï¼šVision-Languageã€Audio-Languageã€Unified Architecture
*   **MoE ä¼˜åŒ–**ï¼šExpert Routingã€Load Balancingã€All-to-All é€šä¿¡

### âš¡ åº•å±‚ä¼˜åŒ–

*   **å›¾ç¼–è¯‘**ï¼šTorchAirã€TensorRT-LLMã€Torch.compile
*   **ç®—å­èåˆ**ï¼šQKV Fusionã€Custom CUDA/Triton Kernels
*   **é€šä¿¡ä¼˜åŒ–**ï¼šNCCL/HCCLã€RDMAã€GPUDirect

### ğŸ“Š è¯„ä¼°è§‚æµ‹

*   **æ€§èƒ½åˆ†æ**ï¼šMemory/Compute Profilingã€Roofline Analysis
*   **å…³é”®æŒ‡æ ‡**ï¼šTTFTã€TPOTã€Throughputã€GPU Utilization

### ğŸ”® å‰æ²¿è¶‹åŠ¿

*   **æ¨¡å‹æ¶æ„**ï¼šMamba/RWKVã€Mixture of Depthsã€RetNet
*   **æœåŠ¡åŒ–**ï¼šDisaggregated Servingã€Elastic Scaling
*   **æ–°å…´æ–¹å‘**ï¼šæ¨ç†è’¸é¦ã€Early Exitã€Hardware-Aware NAS

* * *

Attention
---------

### PageAttention

**PageAttention** æ˜¯ vLLM çš„æ ¸å¿ƒåˆ›æ–°æŠ€æœ¯ï¼Œçµæ„Ÿæ¥è‡ªæ“ä½œç³»ç»Ÿçš„**è™šæ‹Ÿå†…å­˜åˆ†é¡µæœºåˆ¶**ï¼Œç”¨äºé«˜æ•ˆç®¡ç† LLM æ¨ç†ä¸­çš„ KV Cacheã€‚

#### æ ¸å¿ƒæ¦‚å¿µ

ä¼ ç»Ÿ KV Cache åˆ†é…æ–¹å¼ä¼šä¸ºæ¯ä¸ªåºåˆ—é¢„åˆ†é…æœ€å¤§å¯èƒ½é•¿åº¦çš„è¿ç»­å†…å­˜ï¼Œå¯¼è‡´ä¸¥é‡çš„å†…å­˜æµªè´¹å’Œç¢ç‰‡ã€‚PageAttention å€Ÿé‰´æ“ä½œç³»ç»Ÿåˆ†é¡µæ€æƒ³ï¼Œå°† KV Cache åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„ blockï¼ŒæŒ‰éœ€åŠ¨æ€åˆ†é…ã€‚

#### å…³é”®æŠ€æœ¯ç‚¹

æŠ€æœ¯

è¯´æ˜

**Block ç®¡ç†**

å°† KV Cache åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„ blockï¼ˆå¦‚ 16/32 tokensï¼‰ï¼Œæ¯ä¸ª block ç‹¬ç«‹åˆ†é…

**Block Table**

ç±»ä¼¼é¡µè¡¨çš„æ•°æ®ç»“æ„ï¼Œè®°å½•é€»è¾‘ token ä½ç½®åˆ°ç‰©ç† block çš„æ˜ å°„å…³ç³»

**éè¿ç»­å­˜å‚¨**

åŒä¸€åºåˆ—çš„ KV Cache å¯ä»¥åˆ†æ•£åœ¨å¤šä¸ªä¸è¿ç»­çš„ block ä¸­

**å†…å­˜å…±äº«**

å¹¶è¡Œè§£ç ï¼ˆå¦‚ beam searchï¼‰æ—¶å¯å…±äº« prompt çš„ KV cache

**Copy-on-Write**

å†™æ—¶å¤åˆ¶æœºåˆ¶ï¼Œä»…åœ¨éœ€è¦ä¿®æ”¹æ—¶æ‰å¤åˆ¶ block

#### å†…å­˜ä½¿ç”¨å¯¹æ¯”

    ä¼ ç»Ÿæ–¹å¼ï¼š
    - åºåˆ—é•¿åº¦ 1000ï¼Œæœ€å¤§æ”¯æŒ 4096
    - å†…å­˜å ç”¨ï¼š4096 * hidden_size
    - æµªè´¹ç‡ï¼šçº¦ 75%
    
    PageAttentionï¼š
    - åºåˆ—é•¿åº¦ 1000ï¼Œblock_size=16
    - éœ€è¦ blockï¼š1000/16 = 63 ä¸ª
    - å®é™…åˆ†é…ï¼š63 * 16 = 1008 tokens
    - æµªè´¹ç‡ï¼šä»… 0.8%
    

#### ä»£ç å®ç°

    # nanovllm/layers/attention.py
    # Block Table æ˜ å°„
    block_table = context.block_tables  # æ˜ å°„è¡¨
    
    # Slot Mapping - å°† token æ˜ å°„åˆ° block ä¸­çš„å…·ä½“ä½ç½®
    # slot_mapping æ ¼å¼ï¼š[block_idx, offset_in_block]
    context.slot_mapping
    
    # åˆ†é¡µå­˜å‚¨ KV Cache
    torch_npu._npu_reshape_and_cache(
        k, v,
        k_cache.view(num_blocks, block_size, num_kv_heads, head_dim),
        v_cache.view(num_blocks, block_size, num_kv_heads, head_dim),
        slot_mapping.int()
    )
    

#### ä¼˜åŠ¿

1.  **å†…å­˜æ•ˆç‡**ï¼šæŒ‰éœ€åˆ†é…ï¼Œæ— å†…éƒ¨ç¢ç‰‡
2.  **åŠ¨æ€æ‰©å±•**ï¼šåºåˆ—å¢é•¿æ—¶åªéœ€åˆ†é…æ–° block
3.  **å†…å­˜å…±äº«**ï¼šå¤šä¸ªåºåˆ—å¯å…±äº«ç›¸åŒçš„ prompt KV Cache
4.  **é«˜ååé‡**ï¼šæ”¯æŒæ›´å¤§çš„ batch size

#### è¯¦ç»†å¯¹æ¯”

ğŸ“„ **è¯¦ç»†æŠ€æœ¯æ–‡æ¡£**ï¼š[HuggingFace Transformers æ—©æœŸå®ç°ä¸ PageAttention å¯¹æ¯”](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/pageattention_comparison.md)

åŒ…å«ï¼š

*   æ—©æœŸ Transformers ä»£ç å®ç°åˆ†æ
*   å†…å­˜æµªè´¹çš„é‡åŒ–å¯¹æ¯”ï¼ˆ75% vs 6.25%ï¼‰
*   ä¸åŒ batch size å’Œåºåˆ—é•¿åº¦çš„è¯¦ç»†å¯¹æ¯”è¡¨
*   vLLM è®ºæ–‡æ•°æ®æ¥æºè¯´æ˜
*   å®é™…ä»£ç ç¤ºä¾‹å’Œåœºæ™¯åˆ†æ

* * *

### FlashAttention

**FlashAttention** æ˜¯æ–¯å¦ç¦å¤§å­¦æå‡ºçš„ **IO æ„ŸçŸ¥** æ³¨æ„åŠ›ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡åˆ†å—è®¡ç®—å’Œå‡å°‘ HBMï¼ˆé«˜å¸¦å®½å†…å­˜ï¼‰è®¿é—®æ¥æå‡æ€§èƒ½ã€‚

#### æ ¸å¿ƒé—®é¢˜

æ ‡å‡† Attention å®ç°éœ€è¦å­˜å‚¨ä¸­é—´ç»“æœï¼ˆæ³¨æ„åŠ›çŸ©é˜µï¼‰åˆ° HBMï¼Œå¯¼è‡´ï¼š

*   **å†…å­˜ç“¶é¢ˆ**ï¼šHBM å¸¦å®½è¿œä½äºè®¡ç®—é€Ÿåº¦
*   **O(NÂ²) å†…å­˜**ï¼šåºåˆ—é•¿åº¦çš„å¹³æ–¹çº§å†…å­˜å¢é•¿
*   **å¤šæ¬¡æ•°æ®æ¬è¿**ï¼šQã€Kã€V éœ€è¦å¤šæ¬¡è¯»å†™ HBM

#### æ ¸å¿ƒåˆ›æ–°

æŠ€æœ¯

åŸç†

æ•ˆæœ

**Tilingï¼ˆåˆ†å—ï¼‰**

å°† Qã€Kã€V åˆ†å—åŠ è½½åˆ°é«˜é€Ÿ SRAM

å‡å°‘ HBM è®¿é—®æ¬¡æ•°

**Online Softmax**

æµå¼è®¡ç®— softmaxï¼Œæ— éœ€å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ

å†…å­˜é™è‡³ O(N)

**Recomputation**

åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ä¸­é—´å€¼

ç‰ºç‰²è®¡ç®—æ¢å†…å­˜

**Kernel Fusion**

å¤šä¸ªæ“ä½œèåˆä¸ºå•ä¸ª CUDA kernel

å‡å°‘ kernel å¯åŠ¨å¼€é”€

#### å†…å­˜å±‚æ¬¡ç»“æ„å¯¹æ¯”

    GPU å†…å­˜å±‚æ¬¡ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  HBM (High Bandwidth Memory)        â”‚  â† 1.5 TB/sï¼Œå®¹é‡å¤§ä½†é€Ÿåº¦æ…¢
    â”‚  - å®¹é‡ï¼š40-80 GB                   â”‚  â† æ ‡å‡† Attention åœ¨æ­¤é¢‘ç¹è¯»å†™
    â”‚  - å»¶è¿Ÿï¼šé«˜                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  SRAM (Static RAM / Shared Memory)  â”‚  â† 19 TB/sï¼Œå®¹é‡å°ä½†é€Ÿåº¦å¿«
    â”‚  - å®¹é‡ï¼š~100 KB per SM             â”‚  â† FlashAttention ä¸»è¦åœ¨æ­¤è®¡ç®—
    â”‚  - å»¶è¿Ÿï¼šæä½                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    

#### è®¡ç®—æµç¨‹å¯¹æ¯”

**æ ‡å‡† Attentionï¼š**

    1. ä» HBM åŠ è½½ Q, K, V
    2. è®¡ç®— S = QK^T â†’ å†™å…¥ HBM
    3. è®¡ç®— P = softmax(S) â†’ å†™å…¥ HBM  
    4. è®¡ç®— O = PV â†’ å†™å…¥ HBM
    âŒ å¤šæ¬¡ HBM è¯»å†™ï¼Œå†…å­˜å ç”¨ O(NÂ²)
    

**FlashAttentionï¼š**

    1. åˆ†å—åŠ è½½ Qáµ¢, Kâ±¼, Vâ±¼ åˆ° SRAM
    2. åœ¨ SRAM ä¸­è®¡ç®— softmax
    3. ç´¯åŠ ç»“æœåˆ°è¾“å‡º
    4. ä¸¢å¼ƒä¸­é—´ç»“æœï¼Œé‡å¤ç›´åˆ°å®Œæˆ
    âœ… ä»…éœ€ O(N) å†…å­˜ï¼Œå¤§å¹…å‡å°‘ HBM è®¿é—®
    

#### ä»£ç å®ç°

    # nanovllm/layers/attention_ori.py
    from flash_attn import (
        flash_attn_varlen_func,      # Prefill é˜¶æ®µ
        flash_attn_with_kvcache      # Decode é˜¶æ®µ
    )
    
    # Prefill - å¤„ç†å˜é•¿åºåˆ—ï¼Œæ”¯æŒ PageAttention
    flash_attn_varlen_func(
        q, k, v,
        max_seqlen_q=max_seqlen_q,
        cu_seqlens_q=cu_seqlens_q,    # ç´¯è®¡é•¿åº¦æ”¯æŒå˜é•¿
        causal=True,                  # å› æœæ©ç 
        block_table=block_table       # PageAttention block table
    )
    
    # Decode - å• token æ¨ç†ï¼Œå¤ç”¨åˆ†é¡µ KV Cache
    flash_attn_with_kvcache(
        q.unsqueeze(1),               # [batch, 1, num_heads, head_dim]
        k_cache, v_cache,             # åˆ†é¡µ KV ç¼“å­˜
        cache_seqlens=context_lens,   # å®é™…åºåˆ—é•¿åº¦
        block_table=block_table       # block table æ˜ å°„
    )
    

#### æ€§èƒ½æ”¶ç›Š

*   **å†…å­˜æ•ˆç‡**ï¼šä» O(NÂ²) é™è‡³ O(N)
*   **è®¡ç®—é€Ÿåº¦**ï¼šA100 ä¸Šå¯è¾¾ **2-4 å€**åŠ é€Ÿ
*   **åºåˆ—é•¿åº¦**ï¼šæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼ˆå¦‚ 100K+ tokensï¼‰

* * *

### ä¸‰ç§ Attention å®ç°å¯¹æ¯”

æœ¬é¡¹ç›®åŒ…å«ä¸‰ç§ Attention å®ç°ï¼Œé€‚ç”¨äºä¸åŒåœºæ™¯ï¼š

ç‰¹æ€§

attention\_ori.py

attention.py

attention\_torch\_native.py

**åº•å±‚å®ç°**

Flash Attention åº“

NPU åŸç”Ÿç®—å­

PyTorch åŸç”Ÿ

**é€‚ç”¨å¹³å°**

CUDA GPU

åä¸ºæ˜‡è…¾ NPU

é€šç”¨ï¼ˆCPU/GPUï¼‰

**æ€§èƒ½**

â­â­â­â­â­

â­â­â­â­â­

â­â­

**å¯è¯»æ€§**

â­â­

â­â­

â­â­â­â­â­

**ç”¨é€”**

ç”Ÿäº§ç¯å¢ƒï¼ˆGPUï¼‰

ç”Ÿäº§ç¯å¢ƒï¼ˆNPUï¼‰

å­¦ä¹ è°ƒè¯•

**é€‰æ‹©å»ºè®®ï¼š**

*   **ç”Ÿäº§ç¯å¢ƒï¼ˆGPUï¼‰**ï¼šä½¿ç”¨ `attention_ori.py`ï¼ˆFlash Attentionï¼‰
*   **ç”Ÿäº§ç¯å¢ƒï¼ˆæ˜‡è…¾ NPUï¼‰**ï¼šä½¿ç”¨ `attention.py`ï¼ˆNPU ç®—å­ï¼‰
*   **å­¦ä¹ /è°ƒè¯•**ï¼šä½¿ç”¨ `attention_torch_native.py`ï¼ˆæ˜“äºç†è§£ï¼‰

æ–‡æ¡£ç´¢å¼•
----

æœ¬ç›®å½•åŒ…å«é¡¹ç›®å„æ¨¡å—çš„è¯¦ç»†æµç¨‹å›¾æ–‡æ¡£ï¼Œä½¿ç”¨ Mermaid è¯­æ³•ç»˜åˆ¶ã€‚

### Engine æ¨¡å—æµç¨‹å›¾

æ–‡æ¡£

æè¿°

[docs/engine/sequence\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/sequence_flowchart.md)

Sequence åºåˆ—çŠ¶æ€ç®¡ç†æµç¨‹

[docs/engine/block\_manager\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/block_manager_flowchart.md)

BlockManager KVç¼“å­˜å—ç®¡ç†æµç¨‹

[docs/engine/scheduler\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/scheduler_flowchart.md)

Scheduler è°ƒåº¦å™¨æµç¨‹

[docs/engine/model\_runner\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/model_runner_flowchart.md)

ModelRunner æ¨¡å‹è¿è¡Œå™¨æµç¨‹

[docs/engine/llm\_engine\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/llm_engine_flowchart.md)

LLMEngine ä¸»å¼•æ“æµç¨‹

### Layers æ¨¡å—æµç¨‹å›¾

æ–‡æ¡£

æè¿°

[docs/layers/linear\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/linear_flowchart.md)

çº¿æ€§å±‚ä¸å¼ é‡å¹¶è¡Œç­–ç•¥

[docs/layers/attention\_torch\_native\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/attention_torch_native_flowchart.md)

PyTorchåŸç”ŸAttentionå®ç°

[docs/layers/attention\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/attention_flowchart.md)

NPUä¸“ç”¨Attentionå®ç°

[docs/layers/attention\_ori\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/attention_ori_flowchart.md)

Flash Attentionä¼˜åŒ–å®ç°

[docs/layers/sampler\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/sampler_flowchart.md)

é‡‡æ ·å™¨ï¼ˆæ¸©åº¦é‡‡æ ·ï¼‰

[docs/layers/rotary\_embedding\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/rotary_embedding_flowchart.md)

RoPEä½ç½®ç¼–ç 

[docs/layers/layernorm\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/layernorm_flowchart.md)

RMSNormå½’ä¸€åŒ–å±‚

[docs/layers/embed\_head\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/embed_head_flowchart.md)

è¯åµŒå…¥ä¸LM Head

[docs/layers/activation\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/activation_flowchart.md)

SwiGLUæ¿€æ´»å‡½æ•°

### Models æ¨¡å—æµç¨‹å›¾

æ–‡æ¡£

æè¿°

[docs/models/llama\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/llama_flowchart.md)

Llamaæ¨¡å‹æ¶æ„

[docs/models/qwen3\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/qwen3_flowchart.md)

Qwen3æ¨¡å‹æ¶æ„

[docs/models/qwen3\_vl\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/qwen3_vl_flowchart.md)

Qwen3-VLå¤šæ¨¡æ€æ¨¡å‹æ¶æ„

[docs/models/qwen3\_moe\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/qwen3_moe_flowchart.md)

Qwen3-MoEç¨€ç–ä¸“å®¶æ¨¡å‹æ¶æ„

[docs/models/mini\_cpm4\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/mini_cpm4_flowchart.md)

MiniCPM4æ¨¡å‹æ¶æ„

[docs/models/models\_map\_flowchart.md](https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/models_map_flowchart.md)

æ¨¡å‹æ³¨å†Œæ˜ å°„å…³ç³»

* * *

benchæ•°æ®
-------

ä»…ä¾›å‚è€ƒï¼Œç¡¬è½¯æ¡ä»¶ä¸åŒï¼Œè·‘å‡ºçš„æ•°æ®ä¹Ÿä¼šæœ‰å·®å¼‚

#### ä¸åŒæ¨¡å‹å¯¹æ¯”

model

Output Tokens

Time (s)

Throughput (tokens/s)

TP

Qwen3-0.6B

143,770

36.82

3904.20

1

Qwen2-0.5B

143,770

20.71

6940.84

1

Qwen2.5-0.5B-Instruct

143,770

19.82

7252.67

1

Llama-3.2-1B-Instruct

143,770

25.45

5648.50

1

Qwen3-32B

143,770

206.69

695.59

2

Qwen3-32B

143,770

119.86

1199.50

4

#### å…¶ä»–æ¡†æ¶å¯¹æ¯”(2025-12-30)

vLLM Nano-vLLM æ•°æ®æ¥æº [https://github.com/GeeeekExplorer/nano-vllm](https://github.com/GeeeekExplorer/nano-vllm)

Inference Engine

Output Tokens

Time (s)

Throughput (tokens/s)

vLLM

133,966

98.37

1361.84

Nano-vLLM

133,966

93.41

1434.13

Nano-vLLM-Ascend python torchåŸç”Ÿå®ç°

4805

257.49

18.66

Nano-vLLM-Ascend èåˆç®—å­+å›¾ç¼–è¯‘bs=256

133,966

33.88

3954.20

#### å›¾æ¨¡å¼ä¸åŒbså¯¹æ¯”(2025-12-30)

Batch Size

Output Tokens

Time (s)

Throughput (tokens/s)

bs=16

133,966

107.23

1249.37

bs=32

133,966

75.89

1765.35

bs=48

133,966

64.84

2066.22

bs=64

133,966

54.06

2478.31

bs=128

133,966

43.08

3109.56

bs=256

133,966

33.88

3954.20

#### å•ç®—å­Paddingå’ŒNon-paddingå¯¹æ¯”(2025-12-30)

bs=256

Prepare Strategy

Output Tokens

Time (s)

Throughput (tokens/s)

Padding

133,966

158.46

845.41

Non-padding

133,966

152.14

880.55

ç¯å¢ƒæ­å»ºï¼ˆå‚è€ƒvllm-ascendï¼‰
-------------------

[https://docs.vllm.ai/projects/vllm-ascend-cn/zh-cn/latest/quick\_start.html](https://docs.vllm.ai/projects/vllm-ascend-cn/zh-cn/latest/quick_start.html)

ubuntu

    # Update DEVICE according to your device (/dev/davinci[0-7])
    export DEVICE=/dev/davinci0
    # Update the vllm-ascend image
    # Atlas A2:
    # export IMAGE=quay.io/ascend/vllm-ascend:v0.14.0rc1
    # Atlas A3:
    # export IMAGE=quay.io/ascend/vllm-ascend:v0.14.0rc1-a3
    export IMAGE=quay.io/ascend/vllm-ascend:v0.14.0rc1
    docker run --rm \
    --name vllm-ascend \
    --shm-size=1g \
    --device $DEVICE \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \
    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -v /root/.cache:/root/.cache \
    -p 8000:8000 \
    -it $IMAGE bash
    # Install curl
    apt-get update -y && apt-get install -y curl
    

å®‰è£…ä¾èµ–
----

    pip install .
    

æ¨¡å‹ä¸‹è½½
----

    huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
      --local-dir ~/huggingface/Qwen3-0.6B/ \
      --local-dir-use-symlinks False
    

å¿«é€Ÿå¼€å§‹
----

è¯·å‚è§ example.py äº†è§£ç”¨æ³•ã€‚è¯¥ API ä¸ vLLM çš„æ¥å£åŸºæœ¬ä¸€è‡´ï¼Œä»…åœ¨ LLM.generate æ–¹æ³•ä¸Šå­˜åœ¨ä¸€äº›ç»†å¾®å·®å¼‚ï¼š

    from nanovllm import LLM, SamplingParams
    llm = LLM("/YOUR/MODEL/PATH", enforce_eager=True, tensor_parallel_size=1)
    sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
    prompts = ["Hello, Nano-vLLM."]
    outputs = llm.generate(prompts, sampling_params)
    outputs[0]["text"]
    

exampleè¿è¡Œç»“æœ
-----------

benchç¯å¢ƒ
-------

ä»…ä¾›å‚è€ƒ  
ascend-dmi -c #æŸ¥çœ‹

*   ç¡¬ä»¶ç¯å¢ƒâ€‹ï¼š
    *   1.æ˜¾å¡:A3 910C
    *   2.é©±åŠ¨ç‰ˆæœ¬:24.1.rc3.10
    *   3.å›ºä»¶ç‰ˆæœ¬:7.5.0.109.220
*   â€‹è½¯ä»¶ç¯å¢ƒâ€‹ï¼š
    *   1.CANNåŒ… 8.3.RC1
    *   2.PTAç‰ˆæœ¬ï¼štorch-npu 2.5.1.post2+gitd7a85f8ï¼Œtorch 2.5.1

Benchmark
---------

See `bench.py` for benchmark.

**Test Configuration:**

*   Model: Qwen3-0.6B
*   Total Requests: 256 sequences
*   Input Length: Randomly sampled between 100â€“1024 tokens
*   Output Length: Randomly sampled between 100â€“1024 tokens

**Performance Results:**  
Nano-vLLM-Ascend å®åœ¨å¤ªæ…¢äº†åªè·‘äº†10æ¡seq

Inference Engine

Output Tokens

Time (s)

Throughput (tokens/s)

vLLM

133,966

98.37

1361.84

Nano-vLLM

133,966

93.41

1434.13

Nano-vLLM-Ascend

4805

257.49

18.66

qwen3-0.6B layers
-----------------

    ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (qkv_proj): QKVParallelLinear()
          (o_proj): RowParallelLinear()
          (rotary_emb): RotaryEmbedding()
          (attn): Attention()
          (q_norm): RMSNorm()
          (k_norm): RMSNorm()
        )
        (mlp): Qwen3MLP(
          (gate_up_proj): MergedColumnParallelLinear()
          (down_proj): RowParallelLinear()
          (act_fn): SiluAndMul()
        )
        (input_layernorm): RMSNorm()
        (post_attention_layernorm): RMSNorm()
      )
    )
    
    

è“å¤©å’Œç™½äº‘æ˜¯æ ‡é…ã€‚