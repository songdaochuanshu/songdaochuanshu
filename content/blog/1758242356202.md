---
layout: post
title: 'ç”¨FastAPIå’ŒStreamlitå®ç°ä¸€ä¸ªChatBot'
date: "2025-09-19T00:39:16Z"
---
ç”¨FastAPIå’ŒStreamlitå®ç°ä¸€ä¸ªChatBot
=============================

ç”¨FastAPI+Streamlitå®ç°ä¸€ä¸ªæµå¼å“åº”çš„ChatBot

å‰è¨€
--

æœ¬æ–‡ä½¿ç”¨FastAPI+Streamlitå®ç°ä¸€ä¸ªæµå¼å“åº”ç±»ChatGPTçš„LLMåº”ç”¨ï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªdemoï¼Œåç»­ä¼šåŸºäºæ­¤å®ç°ä¸€ä¸ªå®Œæ•´çš„MCP Client + MCP Serverçš„MCPåº”ç”¨ã€‚

Streamlitæ˜¯ä¸“ä¸ºæœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦é¡¹ç›®æ‰“é€ çš„å¼€æºPythonåº“ï¼Œå®ƒå…è®¸å¼€å‘è€…å¿«é€Ÿåˆ›å»ºç¾è§‚çš„äº¤äº’å¼Webåº”ç”¨ï¼Œè€Œæ— éœ€å‰ç«¯å¼€å‘ç»éªŒã€‚é€šè¿‡ç®€å•çš„Pythonè„šæœ¬ï¼Œå°±èƒ½æ„å»ºå‡ºåŠŸèƒ½ä¸°å¯Œçš„æ•°æ®åº”ç”¨ç•Œé¢ã€‚è€Œä¸”å®˜æ–¹æ–‡æ¡£å°±æœ‰ChatBotçš„ç¤ºä¾‹ï¼Œç›´æ¥æ‹¿è¿‡æ¥ç¨å¾®ä¿®æ”¹ä¸‹å°±èƒ½ä½¿ç”¨äº†ï¼Œä¸Šæ‰‹èµ·æ¥éå¸¸ç®€å•ã€‚

ä¹‹æ‰€ä»¥ä¸ç›´æ¥åœ¨Streamlitå®ç°MCP Clientï¼Œæ˜¯å› ä¸ºMCP SDKçš„æ–¹æ³•å‡ ä¹éƒ½æ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œè€ŒStreamlitä»…æ”¯æŒåŒæ­¥æ–¹æ³•ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸­é—´å±‚æ¥é›†æˆMCP Clientï¼Œè€Œè¿™ä¸ªä¸­é—´å±‚æˆ‘ç”¨çš„å°±æ˜¯FastAPIã€‚é€šè¿‡FastAPIå®ç°æµå¼å“åº”ï¼Œç„¶ååœ¨Streamlitå†è¿›è¡Œæµå¼å¤„ç†ã€‚

ç”¨Streamlitå®ç°ä¸€ä¸ªç®€å•çš„ChatBot
------------------------

æœ¬æ–‡å¤§éƒ¨åˆ†åŸºäºStreamitçš„chatbotç¤ºä¾‹ï¼Œæ²¡åšä»€ä¹ˆæ”¹åŠ¨ï¼Œåªæ˜¯å…ˆå°è¯•ä¸‹å¦‚ä½•ä½¿ç”¨streamlitã€‚å…¶ä¸­`pkg.cfg`æ¨¡å—æä¾›ä¸€äº›é…ç½®ä¿¡æ¯ï¼Œåé¢ä¹Ÿéƒ½ä¼šç”¨åˆ°ï¼Œé€»è¾‘æ¯”è¾ƒç®€å•ï¼Œæ‰€ä»¥è¿™é‡Œå°±ä¸è´´ä¸Šäº†ï¼Œæœ€åçš„è¡¥å……éƒ¨åˆ†å†è´´ä¸Šã€‚

    import streamlit as st
    from langchain_openai import ChatOpenAI
    
    from pkg.config import cfg
    
    
    with st.sidebar:
        st.button("Clear Chat", on_click=lambda: st.session_state.pop("messages", None), width="stretch")
    
    st.title("MCP Chatbot")
    st.caption("ğŸš€ A Streamlit chatbot powered by Qwen")
    
    llm = ChatOpenAI(
        base_url=cfg.llm_base_url,
        model=cfg.llm_model,
        api_key=cfg.llm_api_key,
        temperature=0.3,
    )
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    
    # Display chat messages from history on app rerun
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).markdown(msg["content"])
    
    # React to user input
    if prompt := st.chat_input(placeholder="What's up?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        # st.chat_message("user").write(prompt)
        with st.chat_message("user"):
            st.markdown(prompt)
    
        def steam_llm():
            for chunk in llm.stream(input=st.session_state.messages):
                yield chunk.content
    
        with st.chat_message("assistant"):
            msg = st.write_stream(steam_llm())
        st.session_state.messages.append({"role": "assistant", "content": msg})
    

åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŸºç¡€çš„èŠå¤©æœºå™¨äººåº”ç”¨ï¼š

1.  ä½¿ç”¨Streamlitçš„ä¾§è¾¹æ æ·»åŠ äº†æ¸…é™¤èŠå¤©è®°å½•çš„æŒ‰é’®
2.  åˆå§‹åŒ–äº†Qwenå¤§è¯­è¨€æ¨¡å‹
3.  å®ç°äº†èŠå¤©å†å²è®°å½•çš„å­˜å‚¨å’Œæ˜¾ç¤º
4.  é€šè¿‡`llm.stream`æ–¹æ³•å®ç°äº†æµå¼å“åº”ï¼Œåœ¨ç”¨æˆ·ç•Œé¢ä¸Šé€å­—æ˜¾ç¤ºAIå›å¤

FastAPIå®ç°æµå¼å“åº”å’Œå®¢æˆ·ç«¯æµå¼å¤„ç†
---------------------

FastAPIçš„`StreamingResponse`å®ç°äº†æµå¼å“åº”ï¼Œrequestsã€httpxã€aiohttpç­‰httpå®¢æˆ·ç«¯æ¨¡å—éƒ½æ”¯æŒæµå¼å¤„ç†ï¼Œè¿™é‡Œä»¥httpxä¸ºä¾‹

### æœåŠ¡ç«¯

    from http import HTTPStatus
    from typing import Sequence
    
    import uvicorn
    from fastapi import FastAPI
    from fastapi.responses import (JSONResponse, PlainTextResponse,
                                   StreamingResponse)
    from langchain_core.messages import AIMessage, HumanMessage
    from langchain_openai import ChatOpenAI
    from pydantic import BaseModel
    
    from pkg.config import cfg
    from pkg.log import logger
    
    app = FastAPI()
    
    class Message(BaseModel):
        role: str
        content: str
    
    class UserAsk(BaseModel):
        thread_id: str
        messages: Sequence[Message]
    
    llm = ChatOpenAI(
        model=cfg.llm_model,
        api_key=cfg.llm_api_key,
        base_url=cfg.llm_base_url,
        temperature=0.3,
    )
    
    async def generate_response(messages: Sequence[Message]):
        """ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äºå®æ—¶ç”Ÿæˆæ–‡æœ¬"""
        msgs = []
        for m in messages:
            if m.role in ("human", "user"):
                msgs.append(HumanMessage(content=m.content))
            elif m.role in ("ai", "assistant"):
                msgs.append(AIMessage(content=m.content))
            else:
                print(f"Unknown role: {m.role}")
        async for chunk in llm.astream(msgs):
            # Ensure only string is yielded
            if hasattr(chunk, "content"):
                yield str(chunk.content)
            else:
                yield str(chunk)
            
    @app.get("/health")
    async def health():
        return PlainTextResponse(content="ok", status_code=HTTPStatus.OK)
    
    @app.post("/stream")
    async def post_ask_stream(user_ask: UserAsk):
        logger.info(f"user_ask: {user_ask}")
        if not user_ask.messages:
            return JSONResponse(content={"error": "query is empty"}, status_code=HTTPStatus.BAD_REQUEST)
        generator = generate_response(user_ask.messages)
        return StreamingResponse(generator, media_type="text/event-stream")
    
    
    if __name__ == "__main__":
        uvicorn.run(app, host="127.0.0.1", port=8000)
    

åœ¨æœåŠ¡ç«¯ä»£ç ä¸­ï¼š

1.  æˆ‘ä»¬å®šä¹‰äº†ä¸¤ä¸ªæ•°æ®æ¨¡å‹ï¼š`Message`ï¼Œç”¨äºå¤„ç†èŠå¤©æ¶ˆæ¯çš„æ ¼å¼
2.  ä½¿ç”¨`ChatOpenAI`åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
3.  `generate_response`å‡½æ•°æ˜¯ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨ï¼Œå°†èŠå¤©å†å²è½¬æ¢ä¸ºLangChainæ¶ˆæ¯æ ¼å¼å¹¶æµå¼ç”Ÿæˆå›å¤
4.  `/stream`ç«¯ç‚¹æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›æµå¼å“åº”
5.  `/health`ç«¯ç‚¹ç”¨äºå¥åº·æ£€æŸ¥

### å®¢æˆ·ç«¯æµå¼å¤„ç†ç¤ºä¾‹

    import asyncio
    
    import httpx
    
    
    def test_sync_stream():
        """åŒæ­¥æ–¹å¼æµ‹è¯•æµå¼å“åº”"""
        print("=== åŒæ­¥æ–¹å¼æµ‹è¯•æµå¼å“åº” ===")
        with httpx.stream("POST", "http://127.0.0.1:8000/stream", 
                         json={
                             "thread_id": "test_thread_1",
                             "messages":[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}],
                         }) as response:
            print("å“åº”çŠ¶æ€ç :", response.status_code)
            for chunk in response.iter_text():
                if chunk:
                    print(chunk, end='', flush=True)
        print("\n" + "="*50 + "\n")
    
    async def test_async_stream():
        """å¼‚æ­¥æ–¹å¼æµ‹è¯•æµå¼å“åº”"""
        print("=== å¼‚æ­¥æ–¹å¼æµ‹è¯•æµå¼å“åº” ===")
        async with httpx.AsyncClient() as client:
            async with client.stream("POST", "http://127.0.0.1:8000/stream",
                                    json={
                                        "thread_id": "test_thread_2",
                                        "messages": [{"role": "user", "content": "å†™ä¸€é¦–å…³äºå¤å¤©é›¨å¤©çš„ç°ä»£è¯—"}]
                                    }) as response:
                print("å“åº”çŠ¶æ€ç :", response.status_code)
                async for chunk in response.aiter_text():
                    if chunk:
                        print(chunk, end='', flush=True)
        print("\n" + "="*50 + "\n")
    
    def test_health_endpoint():
        """æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
        print("=== æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹ ===")
        response = httpx.get("http://127.0.0.1:8000/health")
        print("å¥åº·æ£€æŸ¥å“åº”:", response.text)
        print("çŠ¶æ€ç :", response.status_code)
        print("="*50 + "\n")
    
    if __name__ == "__main__":
        test_health_endpoint()
        
        test_sync_stream()
        
        asyncio.run(test_async_stream())
    

å®¢æˆ·ç«¯ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨httpxå¤„ç†æµå¼å“åº”ï¼š

1.  `test_sync_stream`å‡½æ•°æ¼”ç¤ºäº†åŒæ­¥æ–¹å¼å¤„ç†æµå¼å“åº”
2.  `test_async_stream`å‡½æ•°æ¼”ç¤ºäº†å¼‚æ­¥æ–¹å¼å¤„ç†æµå¼å“åº”
3.  ä¸¤ç§æ–¹å¼éƒ½ä½¿ç”¨äº†httpxçš„æµå¼APIé€å—å¤„ç†å“åº”å†…å®¹

Streamlité›†æˆ
-----------

    import httpx
    import streamlit as st
    
    def stream_llm(messages: list = []):
        with httpx.stream("POST", "http://127.0.0.1:8000/stream", json={"thread_id": "test_thread_1", "messages": messages}) as resp:
            for chunk in resp.iter_text():
                if chunk:
                    yield chunk
    
    
    with st.sidebar:
        st.button("Clear Chat", on_click=lambda: st.session_state.pop("messages", None), width="stretch")
    
    st.title("MCP Chatbot")
    st.caption("ğŸš€ A Streamlit chatbot powered by Qwen")
    
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    
    # Display chat messages from history on app rerun
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).markdown(msg["content"])
    
    # React to user input
    if prompt := st.chat_input(placeholder="What's up?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
    
        with st.chat_message("assistant"):
            msg = st.write_stream(stream_llm(st.session_state.messages))
        st.session_state.messages.append({"role": "assistant", "content": msg})
    

é›†æˆä»£ç ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°`stream_llm`æ¥é€šè¿‡httpxè¿æ¥FastAPIåç«¯ï¼Œå¹¶å°†æµå¼å“åº”ä¼ é€’ç»™Streamlitå‰ç«¯æ˜¾ç¤ºã€‚è¿™æ ·å°±å®ç°äº†ä»å‰ç«¯åˆ°åç«¯çš„å®Œæ•´æµå¼å¤„ç†é“¾è·¯ã€‚

è¡¥å……
--

*   `pkg/config.py`

    import json
    from pathlib import Path
    
    class Config:
        def __init__(self):
            p = Path(__file__).parent.parent / "conf" / "config.json"
            if not p.exists():
                raise FileNotFoundError(f"Config file not found: {p}")
            self.data = self.read_json(str(p))
    
        def read_json(self, filepath: str) -> dict:
            with open(filepath, "r") as f:
                return json.load(f)
            
        @property
        def llm_model(self) -> str:
            return self.data["llm"]["model"]
        
        @property
        def llm_api_key(self):
            return self.data["llm"]["api_key"]
        
        @property
        def llm_base_url(self) -> str:
            return self.data["llm"]["base_url"]
        
        @property
        def server_host(self) -> str:
            return self.data["server"]["host"]
        
        @property
        def server_port(self) -> int:
            return self.data["server"]["port"]
        
    cfg = Config()
    

*   `pkg/log.py`

    import logging
    import sys
    
    def set_formatter():
        """è®¾ç½®formatter"""
        fmt = "%(asctime)s | %(name)s | %(levelname)s | %(filename)s:%(lineno)d | %(funcName)s | %(message)s"
        datefmt = "%Y-%m-%d %H:%M:%S"
        return logging.Formatter(fmt, datefmt=datefmt)
    
    
    def set_stream_handler():
        return logging.StreamHandler(sys.stdout)
    
    def set_file_handler():
        return logging.FileHandler("app.log", mode="a", encoding="utf-8")
    
    
    def get_logger(name: str = "mylogger", level=logging.DEBUG):
        logger = logging.getLogger(name)
    
        formatter = set_formatter()
        # handler = set_stream_handler()
        handler = set_file_handler()
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    
        logger.setLevel(level)
    
        return logger
    
    
    logger = get_logger()
    

æœ¬æ–‡æ¥è‡ªåšå®¢å›­ï¼Œä½œè€…ï¼š[èŠ±é…’é”„ä½œç”°](https://www.cnblogs.com/XY-Heruo/)ï¼Œè½¬è½½è¯·æ³¨æ˜åŸæ–‡é“¾æ¥ï¼š[https://www.cnblogs.com/XY-Heruo/p/19099957](https://www.cnblogs.com/XY-Heruo/p/19099957)