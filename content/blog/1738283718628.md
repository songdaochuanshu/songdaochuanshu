---
layout: post
title: '近期最值得关注的AI技术报告与Agent综述！'
date: "2025-01-31T00:35:18Z"
---
近期最值得关注的AI技术报告与Agent综述！
=======================

Deepseek这么火，来学学底层技术吧！

写在前面
====

如题，近期优秀的大模型层出不穷。作为技术人，需要阅读高质量的AI技术报告或论文，并且掌握未来应用趋势。本文将推荐一些高质量的AI技术报告，以及Agent智能体综述。

大模型技术报告
=======

DeepSeek-V3 Technical Report

作者：DeepSeek

时间：2024.12.27

内容提要：主要介绍了DeepSeek-V3模型，这是一个拥有6710亿参数的专家混合（MoE）语言模型，其中每个token激活370亿参数。通过算法、框架和硬件的协同设计，该模型克服了跨节点MoE训练中的通信瓶颈，实现了近完全的计算-通信重叠，显著提高了训练效率并降低了训练成本。在仅花费266.4万H800 GPU小时的情况下，DeepSeek-V3完成了14.8万亿token的预训练，成为目前最强的开源基础模型。此外，该模型还引入了从DeepSeek-R1系列模型中提取推理能力的创新方法，并在知识、代码、数学和推理等多个基准测试中表现出色，性能与领先的闭源模型相当。

链接：[arxiv.org/pdf/2412.19437](https://arxiv.org/pdf/2412.19437)

DeepSeek\_R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

作者：DeepSeek

时间：2025.01.23

内容提要：R1是近期火爆全网的深度求索模型。文中介绍了DeepSeek-AI团队通过强化学习（RL）开发的第一代推理模型DeepSeek-R1-Zero和DeepSeek-R1，其中DeepSeek-R1-Zero通过纯RL训练展示了强大的推理能力但存在可读性问题，而DeepSeek-R1通过引入冷启动数据和多阶段训练进一步提升了推理性能，达到了与OpenAI-o1-1217相当的水平；文章还展示了通过蒸馏技术将推理能力迁移到小模型上的成功实践，显著提升了小模型的推理表现，并开源了多个模型供研究社区使用，同时探讨了蒸馏与RL的优劣，指出未来研究方向包括提升通用能力、解决语言混合问题及优化软件工程任务性能。

链接：[github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek\_R1.pdf](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)

DeepSeek MoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models

作者：DeepSeek

时间：2024.01.11

内容提要：详细介绍了DeepSeek-MoE模型的设计，提出细粒度专家分割（Fine-grained Expert Segmentation）和共享专家隔离（Shared Expert Isolation）技术，解决传统MoE模型专家冗余和泛化性不足的问题。 仅用13B激活参数量达到与Llama2 70B相当的性能，训练成本降低80%。

链接：[arxiv.org/pdf/2401.06066](https://arxiv.org/pdf/2401.06066)

Kimi k1.5

作者：Moonshot

时间：2025.01.22

内容提要：Kimi一如既往认为长文本是核心。其中，Kimi k1.5 是一个通过强化学习（RL）训练的多模态大型语言模型（LLM）。Kimi k1.5通过扩展上下文窗口和改进的策略优化方法，在多个基准测试中达到了最先进的推理性能，与OpenAI的o1模型相当。此外，文章还提出了long2short方法，通过长链推理（CoT）技术提升短链推理模型的性能，取得了显著的性能提升。这些方法不仅提高了模型的推理能力，还增强了其在多模态任务中的表现。

链接：[github.com/MoonshotAI/Kimi-k1.5/blob/main/Kimi\_k1.5.pdf](https://github.com/MoonshotAI/Kimi-k1.5/blob/main/Kimi_k1.5.pdf)

Extending Context Window of Large Language Models via Semantic Compression

作者：Department of Mathematical Sciences, Tsinghua University, Theory Lab, 2012 Labs, Huawei Technologies

时间：2023.12.15

内容提要：这篇文章提出了一种新颖的语义压缩方法，用于扩展大型语言模型（LLMs）的上下文窗口，使其能够处理比原始模型长6-8倍的文本，而无需对预训练模型进行微调或增加计算成本。该方法通过利用信息论中的源编码概念，使用预训练模型在将输入传递给LLMs之前减少长输入的语义冗余。实验结果表明，这种方法在包括问答、总结、少样本学习和信息检索等多种任务中有效地扩展了LLMs的上下文窗口，并在保持文本生成流畅性的同时减少了计算开销。

链接：[arxiv.org/pdf/2312.09571](https://arxiv.org/pdf/2312.09571)

Agent综述
=======

Agent AI: Surveying the Horizons of Multimodal Interaction

作者：斯坦福大学李飞飞团队

时间：2024.01.25

内容提要：这篇80页的综述系统性地总结了多模态AI智能体的发展，探讨了其在具身交互、跨现实任务中的应用，以及如何结合大语言模型（LLM）和视觉语言模型（VLM）构建更复杂的智能体系统。论文还提出了“无限代理”概念，支持跨物理和虚拟环境的多模态生成与编辑。

链接：[arxiv.org/pdf/2401.03568](https://arxiv.org/pdf/2401.03568)

Google Whiterpaper Agents2

作者：Google

时间：2024.09

内容提要：Google 出品的 Agents白皮书。详细介绍了AI代理的核心架构，包括模型层（Model Layer）、工具层（Tool Layer） 和 编排层（Orchestration Layer），并探讨了其与传统语言模型的区别、学习能力、实际应用以及未来发展，旨在推动AI代理在各领域的广泛应用。

链接：[drive.google.com/file/d/1oEjiRCTbd54aSdB\_eEe3UShxLBWK9xkt/view](https://drive.google.com/file/d/1oEjiRCTbd54aSdB_eEe3UShxLBWK9xkt/view)

参考实现：[github.com/alibaba/spring-ai-alibaba/](https://github.com/alibaba/spring-ai-alibaba/)

『注:本文来自博客园“小溪的博客”，若非声明均为原创内容，请勿用于商业用途，转载请注明出处http://www.cnblogs.com/xiaoxi666/』