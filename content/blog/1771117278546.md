---
layout: post
title: 'OneTrans：在工业级推荐系统中以单一 Transformer 实现特征交互与序列建模的统一框架'
date: "2026-02-15T01:01:18Z"
---
OneTrans：在工业级推荐系统中以单一 Transformer 实现特征交互与序列建模的统一框架
--------------------------------------------------

OneTrans：在工业级推荐系统中以单一 Transformer 实现特征交互与序列建模的统一框架
==================================================

摘要
--

在推荐系统中，扩展特征交互模块（例如 Wukong、RankMixer）或用户行为序列模块（例如 LONGER）已经取得了显著成果。然而，这两类工作通常沿着彼此独立的路径推进，这不仅阻碍了双向信息交互，也限制了统一优化与统一扩展能力。

本文提出 OneTrans，一种统一的 Transformer 主干网络，可同时完成用户行为序列建模与特征交互。OneTrans 采用统一的 tokenizer，将序列属性与非序列属性共同转换为单一的 token 序列。堆叠的 OneTrans 模块对相似的序列 token 共享参数，而对非序列 token 分配特定参数。通过因果注意力机制与跨请求的 KV 缓存机制，OneTrans 支持中间表示的预计算与缓存，在训练与推理阶段均显著降低计算成本。

在工业级规模数据集上的实验结果表明，OneTrans 随参数规模扩展具有良好的可扩展性，稳定优于多个强基线模型，并在真实在线 A/B 实验中实现了用户级 GMV 提升 5.68%。

1 引言
----

推荐系统在各类信息服务中发挥着基础性作用，例如电商、流媒体和社交网络。工业级推荐系统通常采用级联式排序架构。首先，在召回阶段从十亿规模的候选库中选出数百个候选项；随后，在排序阶段对每个候选进行打分并返回 Top-k 结果。深度学习推荐模型（DLRM）已广泛应用于工业推荐系统的排序阶段。

本文聚焦排序阶段，遵循 DLRM 风格的排序范式。当前主流方法通常围绕两个相互独立的模块进行迭代优化：（a）序列建模模块，将用户多行为序列编码为与候选相关的表示，通常采用局部注意力或 Transformer 编码器；（b）特征交互模块，通过分解方法、显式交叉网络或基于特征组的注意力机制，学习非序列特征（如用户画像、物品画像和上下文特征）之间的高阶交叉关系。如图 1(a) 所示，这类方法通常先将用户行为编码为压缩的序列表示，然后与非序列特征拼接，再通过特征交互模块学习高阶交互。本文将这一设计称为“先编码后交互”（encode-then-interaction）流程。

大语言模型（LLM）的成功表明，扩大模型规模（如参数规模和训练数据规模）可以带来可预测的性能提升。这一现象也启发了推荐系统中的相关研究。在特征交互方面，Wukong 通过堆叠因子分解机模块并结合线性压缩来建模高阶特征交互，并建立了扩展规律；RankMixer 通过硬件友好的 token 混合机制以及 token 特定的前馈网络（FFN）实现了良好的规模扩展能力。在序列建模方面，LONGER 采用因果 Transformer 建模长用户行为历史，并证明随着深度和宽度的增加，性能呈单调提升趋势。

尽管上述方法在实践中有效，但将序列建模与特征交互作为独立模块会带来两个主要局限。第一，“先编码后交互”的流程限制了双向信息流，使静态或上下文特征难以充分影响序列表示的建模。第二，模块分离导致执行过程割裂并增加系统延迟；相比之下，统一的 Transformer 主干结构可以复用 LLM 的成熟优化技术，如 KV 缓存、内存高效注意力机制和混合精度训练，从而实现更高效的规模扩展。

为此，本文提出 OneTrans，一种创新性的架构范式，通过统一的 Transformer 主干网络同时完成用户行为序列建模与特征交互。如图 1(b) 所示，OneTrans 在统一主干内部实现了双向信息交互。其核心在于一个统一的 tokenizer，将序列特征（多样化行为序列）和非序列特征（用户、物品及上下文静态特征）共同转换为单一的 token 序列，然后送入由多层堆叠的 OneTrans 模块组成的金字塔结构中。该结构是针对工业推荐系统定制的 Transformer 变体。

考虑到推荐系统中 token 来源多样、语义异构（不同于 LLM 中单一文本 token），每个 OneTrans 模块采用类似 HiFormer 的混合参数化策略。具体而言，所有来自序列特征的 token 共享同一组 Q/K/V 和 FFN 参数，而每个非序列 token 则分配独立的特定参数，以保留其独特语义。

不同于传统的“先编码后交互”框架，OneTrans 通过统一的因果 Transformer 主干消除了序列与非序列特征之间的结构性隔离，使推荐系统的扩展方式与 LLM 实践保持一致：通过调整主干网络的深度和宽度即可扩展整体模型规模，同时可无缝继承成熟的 LLM 优化技术，如 FlashAttention 和混合精度训练。尤其是跨候选与跨请求的 KV 缓存机制，可将包含 C 个候选的会话时间复杂度从 𝑂(C) 降至 𝑂(1)，从而使大规模 OneTrans 部署成为可能。

### 主要贡献

本文的贡献可概括为四点：

1.  **统一框架**：提出 OneTrans 单一 Transformer 主干排序模型，配备统一 tokenizer，将序列与非序列特征编码为统一 token 序列，并通过统一 Transformer 模块同时完成序列建模与特征交互。
    
2.  **面向推荐系统的定制设计**：为弥合 LLM 与推荐任务之间的差距，提出混合参数化机制，对多样化的非序列 token 分配特定参数，同时对所有序列 token 共享参数。
    
3.  **高效训练与推理**：通过金字塔策略逐层裁剪序列 token，并引入跨请求 KV 缓存复用用户侧计算结果。同时采用 FlashAttention、混合精度训练和半精度推理等优化手段，降低内存与计算开销。
    
4.  **规模扩展与在线部署验证**：OneTrans 随模型规模增加呈现近似对数线性性能增长趋势，在真实生产数据上验证了规模规律；在线部署后，在保持工业级延迟的前提下，显著提升关键业务指标。
    

2 相关工作
------

早期推荐系统模型，如 DIN 及其面向会话的变体 DSIN，采用局部注意力机制学习与候选物品条件相关的用户历史行为摘要，但通常将行为压缩为针对每个候选的固定长度向量，从而限制了长程依赖关系的建模能力。自注意力方法，如 SASRec、BERT4Rec 和 BST，通过允许序列中每个位置关注完整历史，消除了这一瓶颈，并通过双向掩码机制提升了样本利用效率。

近年来，随着推荐系统中规模规律（scaling law）研究的推进，LONGER 将序列建模扩展至工业级规模，通过高效注意力机制和面向服务的架构设计，支持超长用户行为序列建模。然而，在主流工业流程中，这些序列编码器通常仍与特征交互模块相互独立，导致特征融合发生在后期阶段，而非与静态上下文特征进行联合优化。

在特征交互方向，早期推荐系统依赖人工构造的交叉特征或自动乘性交互层。经典模型如 Wide&Deep、FM/DeepFM 和 DCN/DCNv2 提供了高效的低阶或有界阶交互能力。然而，近期的规模研究发现，一旦交叉层堆叠到一定程度，继续增加层数往往无法带来性能提升，模型效果趋于平台期。

为突破预设交叉形式的刚性限制，基于注意力的模型能够自动学习高阶交互关系。AutoInt 学习任意阶关系，HiFormer 通过组特定投影更好地刻画异构且非对称的特征交互。随着规模化方法逐渐应用于特征交互模块，大规模系统如 Wukong 通过堆叠 FM 风格交互模块并结合线性压缩，实现了可预测的性能增长；RankMixer 则在严格延迟约束下，通过并行 token 混合和稀疏 MoE 实现了良好的规模扩展能力。然而，这类交互模块通常仍遵循“交互后置”的范式，将交互限制在独立阶段，阻碍了与用户序列建模的统一优化。

总体而言，推荐系统的发展长期沿着两条相对独立的路径推进：序列建模与特征交互。InterFormer 通过基于摘要的双向交叉架构尝试弥合两者差距，实现信号互通，但仍保留为两个独立模块，其交叉结构增加了架构复杂度和执行割裂性。在缺乏统一主干进行联合建模与整体优化的情况下，实现系统级规模扩展仍面临挑战。

近期生成式推荐（Generative Recommenders）将推荐任务建模为序列转导问题，并提出如 HSTU 等高效长上下文主干结构。这一路线与依赖丰富非序列特征的 DLRM 系统形成互补。

3 方法
----

在详细介绍方法之前，首先说明任务设定。

在级联式工业推荐系统中，每当召回阶段为用户 𝑢 返回一个候选集合（通常为数百个候选物品）后，排序模型会为每个候选物品 𝑖 预测一个得分：

\\\[\\hat{y}\_{u,i} = f(i, NS, S; \\Theta) \\\]

其中，NS 表示来自用户、候选物品及上下文的非序列特征集合；S 表示用户的历史行为序列集合；Θ 为可训练参数。常见的预测任务包括点击率（CTR）与点击后转化率（CVR）：

\\\[CTR\_{u,i} = P(click=1 \\mid NS, S; \\Theta) \\\]

\\\[CVR\_{u,i} = P(conv=1 \\mid click=1, NS, S; \\Theta) \\\]

### 3.1 OneTrans 框架概述

如图 2(a) 所示，OneTrans 采用统一 tokenizer，将序列特征 S 映射为 S-tokens，将非序列特征 NS 映射为 NS-tokens。随后，一个金字塔式堆叠的 Transformer 在单一计算图中联合处理这一统一 token 序列。初始 token 序列定义为：

\\\[X^{(0)} = \[S\\text{-tokens}; NS\\text{-tokens}\] \\in \\mathbb{R}^{(L\_S + L\_{NS}) \\times d} \\\]

该序列由 (L\_S) 个 S-tokens 与 (L\_{NS}) 个 NS-tokens 拼接而成，所有 token 的维度均为 (d)。需要注意的是，在 S-tokens 中插入了可学习的 \[SEP\] token，用于区分不同类型的用户行为序列边界。

如图 2(b) 所示，每个 OneTrans Block 通过如下方式逐层更新 token 表示：

\\\[Z^{(n)} = \\text{MixedMHA}(\\text{Norm}(X^{(n-1)})) + X^{(n-1)} \\\]

\\\[X^{(n)} = \\text{MixedFFN}(\\text{Norm}(Z^{(n)})) + Z^{(n)} \\\]

其中，MixedMHA（混合多头注意力）与 MixedFFN（混合前馈网络）采用混合参数化策略（见图 2(c)）：所有序列 token 共享同一组 Q/K/V 与 FFN 权重，而每个非序列 token 在注意力层与前馈层中均分配独立的专属参数。

模型采用统一的因果掩码（causal mask），施加自回归约束，使每个位置只能关注其之前的 token。具体而言，NS-tokens 允许访问全部 S-tokens 的历史信息，从而实现充分的跨 token 交互。

通过堆叠多个此类 Block，并对 S-tokens 施加金字塔式尾部裁剪（tail truncation），模型逐层将高阶信息压缩并蒸馏至 NS-tokens 中。最终的 token 表示输入至任务特定的预测头完成输出。

### 统一建模带来的结构优势

通过将序列与非序列特征统一为单一 token 序列，并使用因果 Transformer 进行建模，OneTrans 摒弃了传统“先编码后交互”流程。这种统一设计在单一 Transformer 堆叠中自然实现了：

1.  单序列内部的交互建模
2.  多序列之间的交互建模
3.  用户、物品与上下文等多源特征之间的交互
4.  序列特征与非序列特征之间的交互

统一建模形式使得模型可以无缝继承成熟的大模型工程优化技术，包括 KV 缓存与内存高效注意力机制，从而显著降低推理延迟。该统一框架在单一可扩展架构下处理多序列与跨域推荐任务，具备良好的扩展潜力。

### 3.2 特征与 Token 化

为构建初始 token 序列 (X^{(0)})，OneTrans 首先通过特征预处理流程，将所有原始特征映射为 embedding 向量。这些 embedding 随后被划分为两部分：（i）多行为序列子集；（ii）表示用户、物品或上下文的非序列特征子集。随后分别对两类特征应用不同的 tokenizer。

#### 3.2.1 非序列特征的 Token 化

非序列特征 NS 包括数值特征（如价格、CTR）和类别特征（如用户 ID、物品类目）。所有特征先进行分桶或 one-hot 编码，再映射为 embedding。

由于工业系统通常包含数百个特征，且重要性差异显著，因此控制非序列 token 数量 (L\_{NS}) 有两种方式：

**（1）分组式 Tokenizer（Group-wise Tokenizer）**  
与 RankMixer 对齐，将特征手动划分为语义组 ({g\_1, ..., g\_{L\_{NS}}})。每个组内特征拼接后通过独立的 MLP 投影：

\\\[NS\\text{-tokens} = MLP\_1(concat(g\_1)), ..., MLP\_{L\_{NS}}(concat(g\_{L\_{NS}})) \\\]

**（2）自动切分 Tokenizer（Auto-Split Tokenizer）**  
将所有特征拼接后，通过一个统一 MLP 投影，再进行切分：

\\\[NS\\text{-tokens} = split(MLP(concat(NS)), L\_{NS}) \\\]

Auto-Split 方式通过一次稠密投影减少 kernel 启动开销，相较分组式方法更高效。论文将在实验中对两种方式进行对比。

最终，非序列 token 化得到 (L\_{NS}) 个非序列 token，每个维度为 (d)。

#### 3.2.2 序列特征的 Token 化

OneTrans 支持多行为序列输入：

\\\[S = {S\_1, ..., S\_n}, \\quad S\_i = {e\_{i1}, ..., e\_{iL\_i}} \\\]

每个序列 (S\_i) 包含 (L\_i) 个事件 embedding。每个事件 embedding 由物品 ID 及其附加信息（如类目、价格等）拼接构成。

由于不同序列的原始维度可能不同，对每个序列 (S\_i) 使用共享投影网络 (MLP\_i)，将所有事件映射至统一维度 (d)：

\\\[\\tilde{S}\*i = MLP\_i(e\*{i1}), ..., MLP\_i(e\_{iL\_i}) \\in \\mathbb{R}^{L\_i \\times d} \\\]

对齐后的序列 (\\tilde{S}\_i) 可通过两种规则合并为统一 token 序列：

1.  **时间感知（Timestamp-aware）**：按时间顺序交错所有事件，并添加序列类型标识。
2.  **时间无关（Timestamp-agnostic）**：按行为影响力排序拼接，例如“购买 → 加购 → 点击”，在序列之间插入可学习的 \[SEP\] token。高意图行为排在前面。

消融实验表明，当时间戳可用时，时间感知方式优于按行为影响排序的方式。

形式化表示为：

\\\[S\\text{-tokens} = Merge(\\tilde{S}\_1, ..., \\tilde{S}\_n) \\in \\mathbb{R}^{L\_S \\times d} \\\]

其中

\\\[L\_S = \\sum\_{i=1}^{n} L\_i + L\_{SEP} \\\]

### 3.3 OneTrans Block

如图 2(b) 所示，每个 OneTrans Block 是一个 pre-norm 因果 Transformer，输入为规范化后的 token 序列：前 (L\_S) 个为序列 token，后 (L\_{NS}) 个为非序列 token。

受异构特征组研究的启发，OneTrans 对标准 Transformer 进行了轻量级改造，引入混合参数机制：同质的 S-tokens 共享参数；而来源与语义异构的 NS-tokens 使用各自的专属参数。

由于推荐系统的 token 序列混合了统计分布差异较大的 S-tokens 与 NS-tokens，若采用 post-norm 结构容易导致注意力塌缩和训练不稳定。因此，OneTrans 采用 RMSNorm 作为 pre-norm，对所有 token 进行归一化，从而对齐尺度并稳定优化过程。

#### 3.3.1 混合（共享/专属）因果注意力

OneTrans 使用标准多头注意力（MHA）和因果掩码，唯一的变化在于 Q/K/V 的参数化方式。

设第 (i) 个 token 为 (x\_i \\in \\mathbb{R}^d)，则：

\\\[q\_i, k\_i, v\_i = W\_i^Q x\_i,\\quad W\_i^K x\_i,\\quad W\_i^V x\_i \\\]

其中参数矩阵采用混合参数化：

\\\[W\_i^\\Psi = \\begin{cases} W\_S^\\Psi, & i \\le L\_S \\quad （S\\text{-tokens 共享参数）} \\ W\_{NS,i}^\\Psi, & i > L\_S \\quad （NS\\text{-tokens 专属参数）} \\end{cases} \\\]

因果掩码的设计为 NS-tokens 排在 S-tokens 之后，从而形成如下结构：

1.  **S 侧行为**  
    每个 S-token 仅关注其之前的 S-token。

*   若采用时间感知顺序，每个行为基于历史建模；
*   若采用按意图排序方式，因果掩码使高意图行为影响后续低意图行为。

2.  **NS 侧行为**  
    每个 NS-token 可访问完整 S 历史，相当于对行为证据进行目标注意力聚合；同时可访问前序 NS-token，增强非序列 token 之间的交互。
    
3.  **支持金字塔结构**  
    因果结构使信息逐步向后集中，为逐层裁剪 token 的金字塔机制提供自然支持。
    

#### 3.3.2 混合（共享/专属）前馈网络

FFN 采用相同的混合参数策略：

\\\[MixedFFN(x\_i) = W\_{2,i} , \\phi(W\_{1,i} x\_i) \\\]

其中 (W\_{1,i}) 与 (W\_{2,i}) 遵循上述混合参数规则：  
S-tokens 共享参数，NS-tokens 使用专属参数。

#### 小结

与标准因果 Transformer 相比，OneTrans 仅在参数化层面进行修改：

*   NS-tokens 使用专属 QKV 与 FFN；
*   S-tokens 共享一套参数；
*   单一因果掩码统一连接整个序列。

这种设计使 NS-tokens 能够聚合完整行为历史，同时保持 Transformer 风格的高效计算结构。

3.4 金字塔堆叠（Pyramid Stack）
------------------------

如第 3.3 节所述，因果掩码会将信息逐步集中到序列的后部位置。利用这一“信息向尾部聚集”的结构特性，OneTrans 采用金字塔式调度策略：在每一层 OneTrans Block 中，仅最近的一部分 S-tokens 参与 Query 计算，而 Key 与 Value 仍在完整序列上计算；随着网络深度增加，参与 Query 的 token 数量逐层缩减。

设输入 token 序列为

\\\[X = {x\_i}\_{i=1}^{L} \\\]

定义尾部索引集合

\\\[Q = {L - L' + 1, ..., L}, \\quad L' \\le L \\\]

在该层中，仅对 (i \\in Q) 的 token 计算 Query：

\\\[q\_i = W\_i^Q x\_i, \\quad i \\in Q \\\]

而 Key 与 Value 仍在完整索引集合 ({1, ..., L}) 上计算。注意力计算完成后，仅保留 (i \\in Q) 的输出表示，从而将 token 长度缩减为 (L')，形成跨层逐步收缩的金字塔结构。

该设计带来两个核心优势：

1.  **渐进式信息蒸馏**  
    长行为序列的信息被逐层压缩至尾部 token，模型容量集中于最具信息量的近期行为，并逐步将高阶信息汇聚至 NS-tokens。
    
2.  **计算效率提升**  
    注意力复杂度由 (O(L^2 d)) 降为 (O(L L' d))，FFN 复杂度与 (L') 线性相关。随着 Query 集合缩小，FLOPs 与激活内存显著降低。
    

3.5 训练与部署优化
-----------

### 3.5.1 跨请求 KV 缓存（Cross-Request KV Caching）

在工业推荐系统中，同一请求下的多个候选样本在训练与推理阶段通常连续处理：它们的 S-tokens 完全相同，而 NS-tokens 随候选物品变化。基于这一结构，OneTrans 引入 KV Caching 机制，形成统一的两阶段计算范式。

**阶段 I（S 侧，每请求一次）**  
对全部 S-tokens 进行因果注意力计算，并缓存其 Key/Value 及注意力输出。该阶段在每个请求中仅执行一次。

**阶段 II（NS 侧，每候选一次）**  
对每个候选物品计算其 NS-tokens，并与缓存的 S 侧 Key/Value 进行跨注意力计算，随后进入 token-specific FFN 层。  
对于候选相关的序列特征（如 SIM），由于无法复用 S 侧缓存，需先通过池化方式预聚合为 NS-tokens。

KV 缓存机制将 S 侧计算在候选间进行摊销，使单候选计算量大幅下降，避免重复计算，从而显著提升吞吐率。

此外，由于用户行为序列是追加式增长的，可将 KV 缓存扩展至跨请求复用：  
每次新请求复用前一缓存，仅对新增行为计算增量 Key/Value。  
因此单次请求的序列计算复杂度由 (O(L)) 降至 (O(\\Delta L))，其中 (\\Delta L) 为自上次请求以来新增行为数量。

### 3.5.2 统一 LLM 优化策略

为进一步提升效率，OneTrans 引入成熟的大模型工程优化方法：

1.  **FlashAttention-2**  
    通过分块计算与 kernel 融合，减少注意力计算中的 I/O 开销与二次激活存储需求，从而在训练与推理阶段降低内存占用并提升吞吐率。
    
2.  **混合精度训练（BF16/FP16）与激活重计算**  
    采用低精度计算以降低显存占用，并结合激活重计算策略，在反向传播阶段重新计算部分前向激活值。  
    该策略以少量额外计算换取显著内存节省，使得更大 batch 与更深模型成为可能，而无需修改模型结构。
    

总体而言，金字塔结构负责控制序列长度与计算规模，KV 缓存负责跨候选与跨请求的计算复用，而 LLM 优化技术则进一步提升底层算子效率。三者结合，使 OneTrans 在工业规模下具备可扩展性与可部署性。

4 实验
----

通过离线评估与在线实验，本文围绕以下研究问题展开分析：

**RQ1：统一堆叠结构 vs. “先编码后交互”范式**  
在计算资源相当的条件下，单一 Transformer 主干是否能够稳定优于传统的 encode–then–interaction 架构？

**RQ2：关键设计因素分析**  
通过消融实验评估不同设计选择对性能与效率的影响，包括：

*   输入层设计（如 tokenizer 方式、序列融合策略）；
*   OneTrans Block 结构设计（如参数共享策略、注意力形式、金字塔堆叠机制）。

**RQ3：系统效率评估**  
在相同 OneTrans 计算图下，金字塔堆叠、跨请求 KV 缓存、FlashAttention-2，以及混合精度训练与激活重计算，是否能够有效降低 FLOPs、显存占用与推理延迟？

**RQ4：规模规律（Scaling Law）**  
随着模型长度（token 序列长度）、宽度（(d\_{model})）、深度（层数）的扩展，损失或性能是否呈现预期的近似对数线性增长趋势？

**RQ5：在线 A/B 测试效果**  
在满足生产环境延迟约束的前提下，OneTrans 在线部署是否能够在关键业务指标（如人均订单数、用户级 GMV）上实现统计显著的提升？

该实验设计系统性地从架构有效性、设计合理性、系统效率、规模扩展能力以及真实业务效果五个维度进行验证。

### 4.1 实验设置

#### 4.1.1 数据集

在离线评估中，我们在一个大规模工业排序场景下评估 OneTrans，使用严格隐私合规条件下的生产日志数据（所有个人可识别信息均已匿名化并哈希处理）。

数据按时间顺序划分，所有特征均在曝光时刻进行快照，以防止时间信息泄露并保证线上线下一致性。标签（如点击与下单）在与生产环境对齐的固定时间窗口内聚合。数据集统计信息见表 1。

#### 4.1.2 任务与指标

我们评估两个二分类排序任务（见公式 (2)）：CTR 与 CVR。性能指标包括：

*   **AUC**
*   **UAUC（基于曝光加权的用户级 AUC）**

##### Next-batch 评估方式

数据按时间顺序处理。对于每个 mini-batch：

1.  在评估模式下记录预测结果；
2.  随后在同一 batch 上进行训练。

AUC 与 UAUC 按天计算，然后对各天结果进行宏平均。

##### 效率指标

*   **Params**：模型参数量（不包含稀疏 embedding）
*   **TFLOPs**：训练计算量（batch size = 2048 时的 TFLOPs）

#### 4.1.3 基线模型

我们构建了工业标准模型组合作为对比基线，使用相同特征并匹配计算预算。

在 encode–then–interaction 范式下：

*   从生产中常用的 DCNv2 + DIN 作为基础模型；
*   逐步增强特征交互模块：  
    DCNv2 → Wukong → HiFormer → RankMixer。

在固定 RankMixer 的前提下，进一步替换序列建模模块：

*   StackDIN → Transformer → LONGER。

#### 4.1.4 超参数设置

我们报告两种模型规模：

*   **OneTrans-S**
    
    *   6 层 OneTrans Block
    *   宽度 (d = 256)
    *   4 个注意力头
    *   约 1 亿参数
*   **OneTrans-L**
    
    *   8 层
    *   宽度 (d = 384)

#### 输入与金字塔设置

*   序列特征采用时间感知融合方式；
    
*   非序列特征使用 Auto-Split tokenizer；
    
*   使用启发式金字塔调度：
    
    *   OneTrans-S：序列 Query token 从 1190 线性缩减至 12
    *   OneTrans-L：从 1500 缩减至 16

具体实现为：跨层线性减少序列 Query token 数量，并在每层将 token 数量四舍五入至 32 的倍数，最顶层 token 数量与非序列 token 数量一致。

#### 优化与基础设施

采用双优化器策略（不使用 weight decay）：

*   稀疏 embedding：Adagrad（β₁=0.1，β₂=1.0）
*   稠密参数：RMSProp（lr=0.005，alpha=0.99999，momentum=0）

训练稳定性策略包括：

*   Pre-Norm 结构
*   全局梯度范数裁剪

训练阶段：

*   每 GPU batch size = 2048
*   稠密层梯度裁剪阈值 = 90
*   稀疏层梯度裁剪阈值 = 120
*   使用 16 张 H100 GPU，数据并行 All-Reduce

在线推理阶段：

*   每 GPU batch size = 100
*   在吞吐率与延迟之间进行权衡优化

该实验设置确保了模型规模、训练稳定性与工业部署约束之间的平衡，使离线评估与线上效果具有高度一致性。

### 4.2 RQ1：性能评估

我们以 DCNv2 + DIN 作为对比基准模型，该模型为本场景中规模扩展前的生产基线（见表 2）。

在 encode–then–interaction 范式下，分别对两个组件进行独立扩展均能带来收益：

*   升级特征交互模块（DCNv2 → Wukong → HiFormer → RankMixer）；
*   升级序列建模模块（StackDIN → Transformer → LONGER）；

上述改进在 CTR AUC/UAUC 与 CVR AUC 上均表现出稳定提升。

在我们的系统中，AUC 或 UAUC 提升超过 +0.1% 即被视为具有实际意义；超过 +0.3% 通常对应在线 A/B 测试中的统计显著提升。由于用户级样本数量较少且波动较大，CVR UAUC 指标需谨慎解读。

在统一架构下，OneTrans-S 相比基线取得：

*   CTR：+1.13% / +1.77%（AUC / UAUC）
*   CVR：+0.90% / +1.66%（AUC / UAUC）

在相近参数规模与训练算力条件下（2.64T vs. 2.51T），OneTrans-S 也显著优于 RankMixer + Transformer，验证了统一建模的优势。

进一步扩展模型规模，OneTrans-L 取得最佳整体表现：

*   CTR：+1.53% / +2.79%
*   CVR：+1.14% / +3.23%

随着模型容量增加，性能呈现稳定、可预测的增长趋势。

总结而言，在单一 Transformer 主干中统一序列建模与特征交互，相比独立扩展其中任一模块，能够带来更可靠且更具计算效率的性能提升。这一结果直接回答了 RQ1，并为统一架构提供了实证支持。

### 4.3 RQ2：设计选择的消融研究

我们对 OneTrans 的关键设计进行系统性消融实验，以量化各组件的贡献。完整结果见表 3。实验变体包括：

#### 输入层变体

1.  将 Auto-Split Tokenizer 替换为 Group-wise Tokenizer（表 3 第 1 行）；
2.  使用时间无关的序列融合方式替代时间感知融合（第 2 行）；
3.  在时间感知融合中移除可学习的 \[SEP\] token（第 3 行）。

#### OneTrans Block 结构变体

4.  所有 token 共享同一组 Q/K/V 与 FFN 参数，而不对 NS-tokens 使用专属参数（第 4 行）；
5.  使用全注意力（full attention）替代因果注意力（第 5 行）；
6.  取消金字塔堆叠，在所有层保留完整 token 序列（第 6 行）。

#### 消融结论

1.  **Auto-Split Tokenizer 优于人工分组方式**  
    相比将非序列特征人工划分为语义组，自动构建 NS-token 的方式效果更优。这表明由模型自动学习特征组织结构优于人为先验分组。
    
2.  **时间感知融合优于按行为意图排序**  
    在存在时间戳信息时，按时间顺序融合优于按行为影响力排序，说明时间信息比主观设定的行为强弱排序更具表达力。
    
3.  **\[SEP\] token 有助于序列区分**  
    在时间无关融合下，可学习的 \[SEP\] token 有助于模型区分不同序列，提高表示能力。
    
4.  **NS-token 专属参数优于共享参数**  
    为非序列 token 分配专属 Q/K/V 与 FFN 参数优于完全共享投影，有助于增强不同特征来源之间的表达区分能力。
    
5.  **因果注意力与全注意力效果接近，但因果注意力更具工程优势**  
    性能上二者相近，但全注意力会破坏 KV 缓存等标准优化手段，因此不具备系统层面的可扩展性。
    
6.  **金字塔设计不会损失效果，却显著节省计算**  
    在所有层保留完整 token 序列并未带来性能提升。OneTrans 能有效将信息压缩至尾部 token，因此金字塔结构可以安全裁剪 Query token，从而降低计算量。
    

此外，在固定 TFLOPs 预算下，金字塔设计支持接近 1.75 倍更长的输入序列，相比全长度设计更能有效利用序列扩展带来的收益。

总体而言，消融结果表明 OneTrans 的关键设计并非经验性堆叠，而是具有明确结构必要性：自动 token 构建、时间感知融合、NS 专属参数以及金字塔裁剪共同构成了性能与效率兼顾的核心机制。

4.4 RQ3：系统效率
------------

为量化第 3.5 节提出的系统优化策略，我们在一个未做优化的 OneTransS 基线模型上逐项进行消融，并在表 4 中报告训练与推理阶段的效率指标。

实验结果表明：

1.  **金字塔堆叠（Pyramid Stack）**  
    通过裁剪序列 Query token，显著降低训练阶段的运行时间与显存占用，同时减少线上服务阶段的 p99 延迟与内存消耗。
    
2.  **跨请求 KV 缓存（Cross-request KV Caching）**  
    消除重复的序列计算，在训练与推理阶段均稳定提升运行效率与内存利用率。
    
3.  **FlashAttention**  
    在训练阶段带来显著加速效果，而在推理阶段的改进相对有限但仍有收益。
    
4.  **混合精度与重计算（Mixed Precision + Re-computation）**  
    对线上服务收益最大，显著降低 p99 延迟与推理显存开销，同时提升训练效率。
    

### 综合结论

上述结果验证了将大模型（LLM）系统优化技术迁移到大规模推荐场景的有效性。在此基础上，模型进一步扩展至 OneTransL，并在表 5 中展示其线上效率可与规模远小得多的 DCNv2+DIN 基线模型相当。

这说明统一的 Transformer 主干结构具有重要工程优势：它允许直接复用成熟的大模型优化技术，从而在模型规模显著提升的同时，维持可控的在线计算开销。

总体来看，本节证明了 OneTrans 在扩大模型容量的同时，并未牺牲系统效率，而是通过架构统一化实现了性能与工程可扩展性的兼顾。

4.5 RQ4：Scaling-Law 验证
----------------------

我们从三个维度系统研究 OneTrans 的扩展规律（scaling laws）：

1.  **长度（Length）**：输入 token 序列长度
2.  **深度（Depth）**：堆叠 Transformer block 的层数
3.  **宽度（Width）**：隐藏层维度大小

### 单轴扩展结果

如图 3(a) 所示：

*   **增加序列长度带来最大收益**，因为更长的行为序列提供了更丰富的用户行为证据。
    
*   在深度与宽度之间存在明显权衡：
    
    *   增加深度通常比单纯增加宽度带来更大的性能提升，因为更深的网络可以学习更高阶交互与更丰富的抽象表示。
    *   但更深的模型增加串行计算路径，影响延迟。
    *   扩宽维度则更易并行化，在硬件利用率上更友好。

因此，在实际部署时，深度与宽度的选择应综合考虑性能收益与目标硬件预算下的系统效率。

### 联合扩展与对比分析

我们进一步联合增加 OneTrans 的深度与宽度，同时对比扩展 RankMixer+Transformer 基线（主要在 RankMixer 侧扩展至 1B 参数规模），并绘制 ΔUAUC 相对于训练 FLOPs（对数尺度）的关系曲线。

图 3(b) 表明：

*   OneTrans 与 RankMixer 均呈现清晰的对数线性趋势（log-linear scaling）。
*   但 **OneTrans 的斜率更陡峭**，即在相同计算增量下带来更大的性能提升。

其原因可能在于：

*   RankMixer 的扩展缺乏统一主干结构；
*   基于 MoE 的扩展主要体现在 FFN 隐藏层维度的“宽度扩展”；
*   而 OneTrans 通过统一 Transformer 主干实现结构层面的整体扩展，提升了参数与计算利用效率。

### 结论与部署边界

这些结果表明：

*   OneTrans 在参数效率与计算效率方面更具优势；
*   在工业部署场景中，其性能–计算权衡更具吸引力。

目前，OneTransL 已可在严格的线上 p99 延迟约束下稳定部署。但进一步超出当前规模的扩展仍受到在线效率瓶颈限制。未来工作将聚焦于系统与模型的联合优化，以突破这一扩展上限。

从本节可以提炼出一个核心结论：  
**在推荐系统场景中，统一 Transformer 主干不仅带来结构表达优势，也带来更优的 scaling-law 斜率，即单位算力对应更高的性能增益。这一点是其相较于传统“组件拼接式扩展”方法的根本优势。**

4.6 RQ5：在线 A/B 实验
-----------------

我们在两个大规模工业场景中评估 OneTrans 的业务影响：

1.  **Feeds**（首页信息流）
2.  **Mall**（包含 Feeds 及其他子场景的整体业务环境）

流量在用户/账号层级进行哈希切分与随机分配。控制组与实验组均基于过去 1.5 年的生产数据进行训练与部署，以确保对比公平。

### 实验设置

*   **控制组（Control）**：  
    RankMixer + Transformer（约 1 亿神经网络参数），不使用序列 KV 缓存。
    
*   **实验组（Treatment）**：  
    OneTransL，并采用第 3.5 节所述的服务端优化策略。
    

评估指标包括：

*   用户级 click/u、order/u、gmv/u（相对 RankMixer+Transformer 的 Δ%）；
*   双侧 95% 置信区间（基于用户分层 bootstrap）；
*   端到端延迟（p99 每曝光延迟变化百分比，数值越低越好）。

### 实验结果

如表 6 所示，OneTransL 在两个场景均取得稳定提升。

#### Feeds 场景

*   click/u：+7.737%
*   order/u：+4.3510%
*   gmv/u：+5.6848%
*   p99 延迟：−3.91%

#### Mall 场景

*   click/u：+5.143%
*   order/u：+2.5772%
*   gmv/u：+3.6696%
*   p99 延迟：−3.26%

结果表明，相较于强基线 RankMixer+Transformer，统一建模框架在提升核心业务指标的同时，反而降低了线上服务延迟。

### 泛化能力分析

进一步观察到：

*   用户 Active Days 提升 +0.7478%；
*   冷启动商品 order/u 提升 +13.59%。

这说明 OneTrans 在冷启动与泛化能力方面具有显著优势，能够更有效地建模稀疏或低频行为模式。

### 综合结论

本节验证了统一 Transformer 主干不仅在离线指标与系统效率上具有优势，在真实大规模线上环境中亦能实现：

1.  显著的核心业务指标提升；
2.  服务延迟下降；
3.  冷启动泛化能力增强。

从工程与业务双重维度看，OneTrans 实现了性能、效率与泛化能力的协同提升，证明统一建模设计在工业推荐系统中的实际可行性与商业价值。

5 结论
----

本文提出 OneTrans，一种用于个性化排序的统一 Transformer 主干结构，用以替代传统的“编码–交互”（encode–then–interaction）范式。

该方法通过统一的 Tokenizer 将序列特征与非序列特征转换为单一 token 序列，并通过统一的 Transformer Block 在同一框架内同时完成序列建模与特征交互。其中：

*   对同质的序列 token 共享参数；
*   对异质的非序列 token 使用专属参数，以增强表达能力。

为保证统一架构在大规模场景下的效率：

*   采用金字塔式调度策略，逐层裁剪序列 token；
*   使用跨请求 KV 缓存复用用户侧计算；
*   同时引入大模型风格的系统优化技术，如 FlashAttention 与混合精度计算。

在大规模实验中，OneTrans 随模型宽度与深度扩展呈现近似对数线性（log-linear）的性能增长趋势，并在真实线上环境中取得统计显著的业务指标提升，同时保持生产级延迟要求。

总体而言，该统一设计为推荐系统的规模扩展提供了一条可行路径，使其能够直接复用近年来推动大语言模型发展的系统优化技术，实现模型能力与工程效率的协同提升。

posted on 2026-02-14 11:31  [GlenTt](https://www.cnblogs.com/GlenTt)  阅读(59)  评论(0)    [收藏](javascript:void\(0\))  [举报](javascript:void\(0\))