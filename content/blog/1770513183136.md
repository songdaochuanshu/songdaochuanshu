---
layout: post
title: '决策树极简入门'
date: "2026-02-08T01:13:03Z"
---
决策树极简入门
=======

我们在学习机器学习算法时，往往会被各种枯燥的数学公式所劝退。

今天，我将尝试用结合实际生活的方式，来介绍一个非常经典，而且可能是最“懂你心意”的算法——决策树 (Decision Tree)。。

别被这个术语吓到了，其实你每天点外卖的时候都在用它。

想象一下，下午三点，你站在奶茶店门口（或者打开了外卖App），面对眼花缭乱的菜单，你的大脑为了保护你的体重，立刻启动了一个“决策树”程序：

1.  这杯奶茶含糖吗？ -> 如果是全糖 -> 不喝，会胖死 ❌。
2.  \-> 如果是无糖 -> 再看看。
3.  加没加小料？ -> 没加？ -> 没灵魂，不喝 ❌。
4.  \-> 加了波霸/珍珠？ -> 完美！买它！ ✅

![画板](https://img2024.cnblogs.com/blog/83005/202602/83005-20260207080350892-40837281.jpg)

看，这就是一棵决策树！把你脑海里纠结的过程画下来，它就是一个倒立的树状流程图。

但在机器学习里，我们不是自己画图，而是让计算机通过学习历史订单数据，自己总结出这套“点单秘籍”。

它是怎么做到的？别急，我们要用几杯奶茶来教会你。🧋

🌲 第一部分：解剖决策树
=============

在深入之前，咱们先对齐一下 **“行话”**。虽然它叫树，但在计算机科学里，这棵树通常是倒着长的（根在上面，叶子在下面）。

*   **根节点** (`Root Node`): 树的最顶端。也就是最关键的那个问题（比如：甜度是多少？）。
*   **决策节点** (`Decision Node`): 中间的那些站点，负责根据某个特征（比如小料、冷热）把数据分流。
*   **叶节点** (`Leaf Node`): 树的末端。到了这里，不再问问题了，直接给出最终判决（比如：喝！ 或者 快逃！）。

🧠 第二部分：树是怎么“长”出来的？
===================

这才是最迷人的地方。如果你给模型一堆奶茶数据，它怎么知道先看“甜度”还是先看“价格”？

这就涉及到两个超级重要的概念：**熵** (`Entropy`) 和 **信息增益** (`Information Gain`)。

1\. 什么是熵 (Entropy)？
-------------------

物理学里说**熵**代表**混乱程度**。在决策树里，**熵**代表数据的 **“不纯度”**（你也可以理解为\*\* “纠结程度” \*\*）。

*   **场景 A**: 你面前有10杯奶茶，全是无糖波霸奶茶。这数据太纯了，熵 = 0。你闭着眼拿一杯都是你想喝的，完全不用纠结。
*   **场景 B:** 你面前有10杯奶茶，5杯是你最爱的无糖，5杯是甜到齁的全糖，混在一起。这太混乱了，熵 = 1（最高）。你完全猜不到下一杯是不是“雷”。

机器学习的目标就是：通过问问题（分裂），让数据的熵越来越小，直到变成 0（完全纯净）。

2\. 信息增益 (Information Gain)
---------------------------

这就是我们的 **“筛选标准”**。

*   **信息增益** = **分裂前的熵** - **分裂后的熵**

简单说：如果我按“甜度”分，能让这堆数据变得多“干净”？ 哪个问题能帮我排除掉最多的干扰项，我们就选哪个问题当老大（根节点）！

📊 第三部分：手动算一算 (奶茶案例)
====================

假设我们收集了你过去买的50次奶茶记录，你的口味偏好非常明显：只喝无糖。  
数据分布如下：

*   **全糖**: `25`杯 -> 结果全是 不喝 (❌)
*   **无糖**: `25`杯 -> 结果是 喝 (✅)

我们要决定：先按“甜度”分，还是先按“加没加冰”分？

方案一：按“甜度”切一刀 🔪
---------------

*   左边（**全糖堆**）: 25杯全是❌。完美！这堆数据的熵直接变成0了！（不用再问别的了，直接扔掉）。
*   右边（**无糖堆**）: 25杯全是✅。完美！熵也是0！

方案二：按“加冰”切一刀 🧊
---------------

假设全糖和无糖里都有加冰和去冰的情况。

*   左边（**加冰堆**）: 混杂着全糖(❌)和无糖(✅)。还是很乱，熵很高。
*   右边（**去冰堆**）: 同样混杂。

很明显，按**甜度分**的信息增益最大，因为它能帮我们瞬间把“绝对不喝”的那部分挑出来。

所以，机器会选择 **甜度** 作为根节点！

💻 第四部分：Python 代码实战
===================

光说不练假把式。作为工程师，我们要用代码说话。我们会使用 Python 的 `scikit-learn` 库。

假设我们整理好了数据 `milktea.csv`：

Sugar (甜度)

Topping (小料)

Decision (喝吗?)

Full (全糖)

Pearls (珍珠)

0 (No)

Zero (无糖)

None (无)

0 (No - 太寡淡)

Zero (无糖)

Pearls (珍珠)

1 (Yes)

...

...

...

1\. 预处理与训练
----------

机器看不懂中文或单词，我们要把它翻译成数字。

    import pandas as pd
    from sklearn import tree
    import matplotlib.pyplot as plt
    
    # 1. 模拟一点奶茶数据
    # 假设我们的逻辑是：只有"无糖(Zero)"且"加珍珠(Pearls)"才喝
    data = pd.DataFrame({
        'Sugar':   ['Full', 'Zero', 'Full', 'Zero', 'Half', 'Zero'],
        'Topping': ['Pearls', 'None', 'None', 'Pearls', 'Pearls', 'Pudding'],
        'Drink':   [0, 0, 0, 1, 0, 1]  # 1=喝, 0=不喝 (假设只要是无糖且有小料就喝)
    })
    
    # 2. 数据预处理：把文字变成数字 (Mapping)
    # 甜度: Full=0, Zero=1, Half=2
    # 小料: Pearls=0, None=1, Pudding=2
    data['Sugar_Code'] = data['Sugar'].map({'Full': 0, 'Zero': 1, 'Half': 2})
    data['Topping_Code'] = data['Topping'].map({'Pearls': 0, 'None': 1, 'Pudding': 2})
    
    features = ['Sugar_Code', 'Topping_Code']
    X = data[features]
    Y = data['Drink']
    
    # 3. 训练模型
    # criterion='entropy' 表示我们使用“熵”来作为分裂标准
    clf = tree.DecisionTreeClassifier(criterion='entropy')
    clf = clf.fit(X, Y)
    
    print("🤖 奶茶鉴定模型训练完毕！")
    

2\. 可视化这棵树
----------

让我们看看机器脑子里想的图长什么样。

    # 4. 画出决策树
    plt.figure(figsize=(10,6))
    tree.plot_tree(clf, 
                   feature_names=['Sugar', 'Topping'],  
                   class_names=['Pass', 'Drink'], # Pass=不喝, Drink=喝
                   filled=True, # 颜色越深代表机器越确信
                   rounded=True)
    plt.show()
    

![](https://img2024.cnblogs.com/blog/83005/202602/83005-20260207080350764-430638444.png)

3\. 预测新数据
---------

这时候，老板推出了一款新品：无糖 + 珍珠。你要不要尝尝？

*   无糖 = 1
*   珍珠 = 0

    # 预测 [Sugar=1, Topping=0]
    new_tea = [[1, 0]]
    prediction = clf.predict(new_tea)
    
    if prediction[0] == 1:
        print("决策结果：买它！🧋😋")
    else:
        print("决策结果：哒咩！❌")
    
    ## 运行结果：
    '''
    决策结果：买它！🧋😋
    '''
    

🚀 总结
=====

今天我们通过一杯奶茶学习了：

1.  **决策树**就是一套帮你做选择的“流程图”。
2.  机器利用**熵**（乱不乱）和**信息增益**（变干净了吗）来寻找最佳的筛选条件。
3.  用 `scikit-learn` 几行代码就能搞定。

机器学习其实离生活很近。希望这棵“树”能帮你不仅选对模型，还能选对最适合你的那杯下午茶！