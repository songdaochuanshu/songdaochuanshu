---
layout: post
title: 'LLM应用剖析: 手机智能助理Phone Agent'
date: "2025-12-12T00:44:44Z"
---
LLM应用剖析: 手机智能助理Phone Agent
==========================

![LLM应用剖析: 手机智能助理Phone Agent](https://img2024.cnblogs.com/blog/602535/202512/602535-20251211173019595-1483104226.png) 本文介绍了智谱开源的AutoGLM构建的Phone Agent，包含具体的流程、原理以及详细的部署步骤以及一点思考。

1\. **背景**
==========

*   智谱开源了基于AutoGLM构建的Phone Agent，短短两天star就到了6.7K，只需要输入"帮我在美团上搜索下附件的火锅店"，Phone Agent即可自动操作手机，打开美团，输入火锅，点击搜索等操作一气呵成。和上周爆火的字节AI手机，有异曲同工之妙。
*   抱着浓厚的兴趣，第一时间研读并上手实战了一番，遂记录下部署过程，源码分析，以及调试运行。

2. **功能介绍**
===========

*   Phone Agent能够以多模态方式理解手机屏幕内容，并通过自动化操作实现用户的请求任务。
*   项目通过ADB(Android Debug Bridge)来控制设备，以视觉语言模型进行屏幕感知，再结合智能规划能力生成并执行操作流程。
*   用户输入一句话，"打开B站，搜一下 one little finger儿歌，并收藏一下"，Phone Agent即可自动解析意图、理解当前界面、规划下一步动作并完成整个流程。
*   系统内置敏感操作确认机制，并支持在登录或验证码场景下进行人工接管。
*   支持远程ADB调试能力，可通过Wifi或者网络连接设备，实现灵活的远程控制与开发。

3\. 部署教程
========

*   本人部署环境如下：
*   手机华为P70
*   采用ADB中的USB连接方式
*   windows 10操作系统
*   采用vllm进行部署，显卡为A40 48G，模型为Autoglm-Phone-9B(模型加载需20G显存，执行任务时占用42G显存)

3.1 Python环境
------------

*   Python版本: 3.12.11，uv作为包管理

3.2 ADB安装
---------

*   下载并安装官方[ADB安装包](https://developer.android.com/tools/releases/platform-tools?hl=zh-cn)，选择并下载platform-tools-latest-windows.zip
*   ADB加入到环境变量PATH中，例如D:\\ws\\学习\\Phone Agent\\platform-tools

3.3 设备启动开发者模式和USB调试
-------------------

*   启用开发者模式: 华为P70位置为"设置->关于手机->HarmononyOS版本"，连续点击7次左右，直到弹出"开发者模式已启用"
*   启用USB调试: 开发者模式开启后，在"设置->系统和更新->开发人员选项->USB调试"，勾选启用。
*   设备检测: 使用USB连接手机，连接方式选"传输文件"，弹出"是否允许USB调试"，点击确定。然后在CMD窗口执行"adb devices"查看设备列表，展示如下内容，即说明安装成功。

3.4 安装ADB Keyboard
------------------

*   下载输入法安装包[ADBKeyboard.apk](https://github.com/senzhk/ADBKeyBoard/blob/master/ADBKeyboard.apk)，并在CMD中执行"adb install ADBKeyboard.apk"。
*   安装完成后，需要在"设置->系统和更新->语言和输入法->ADB Keyboard"勾选启用。

3.5 模型部署
--------

*   安装依赖: pip install -r requirements.txt
*   下载模型: 在Model Scope中采用git clone下载模型[AutoGLM-Phone-9B](https://modelscope.cn/models/ZhipuAI/AutoGLM-Phone-9B)
*   模型启动

    nohup python3 -m vllm.entrypoints.openai.api_server \
     --served-model-name autoglm-phone-9b \
     --allowed-local-media-path ~/media   \
     --mm-encoder-tp-mode data \
     --mm_processor_cache_type shm \
     --mm_processor_kwargs "{\"max_pixels\":5000000}" \
     --max-model-len 25480  \
     --chat-template-content-format string \
     --limit-mm-per-prompt "{\"image\":10}" \
     --model /path/AutoGLM-Phone-9B \
     --host 172.16.0.32 \
     --port 9000 > logs/vllm_9000.log 2>&1 &
    

*   测试模型部署:

    python scripts/check_deployment_cn.py --base-url http://172.16.0.32:9000/v1 --model autoglm-phone-9b
    
    #返回结果，即模型部署成功
    <think>用户想要比较这个洗发水在京东和淘宝上的价格，然后选择最便宜的平台下单。当前在小红书app上，显示的是一个关于LUMMI MOOD洗发水的帖子。
    
    我需要：
    1. 先启动京东app，搜索这个洗发水
    2. 查看京东的价格
    3. 再启动淘宝app，搜索这个洗发水
    4. 查看淘宝的价格
    5. 比较价格后，选择最便宜的京东或淘宝下单
    
    首先，我需要从当前的小红书界面退出，然后启动京东app。</think>
    <answer>do(action="Launch", app="京东")
    

4\. 整体流程
========

*   首先用户输入任务: 美团搜索附近火锅
*   第一次执行任务时，创建system\_message，并且截取当前手机屏幕图片，将用户问题及截屏图作为输入，调用模型，模型输出思考过程及执行动作"启动美团"，然后采用adb进行启动美团app
*   然后将美团首页截图，并调用模型，模型分析搜索框位置，并执行文本输入
*   文本输入完成后，再截图，模型识别"搜索"按钮并进行点击，完成任务。
*   完整的过程记录如下：

    Task: 美团搜索附近火锅
    ==================================================
    💭 1. 思考过程:
    --------------------------------------------------
    用户想要在美团搜索附近的火锅。我需要：
    1. 启动美团应用
    2. 在美团中搜索"附近火锅"
    
    首先，我需要启动美团应用。根据允许的应用列表，美团是可用的应用之一。
    
    当前屏幕显示的是系统桌面，我可以看到各种应用图标。我应该使用Launch功能直接启动美团应用。
    --------------------------------------------------
    🎯 执行动作:
    {
      "action": "Launch",
      "app": "美团",
      "_metadata": "do"
    }
    ==================================================
    
    ==================================================
    💭 2. 思考过程:
    --------------------------------------------------
    好的，美团应用已经打开了。我可以看到美团的主页面，顶部有一个搜索框，显示"京都念慈庵"。我需要点击这个搜索框， 然后输入"附近火锅"进行搜索。
    
    搜索框位于屏幕顶部，大约在坐标 (389, 114) 附近。
    --------------------------------------------------
    🎯 执行动作:
    {
      "action": "Tap",
      "element": [
        389,
        114
      ],
      "_metadata": "do"
    }
    ==================================================
    
    ==================================================
    💭 3. 思考过程:
    --------------------------------------------------
    好的，搜索框已经被激活了，我可以看到键盘出现了，搜索框中显示"附近好吃的火锅"。现在我需要输入"附近火锅"来搜索 。根据指南，Type操作会自动清除现有文本，所以我直接输入即可。
    --------------------------------------------------
    🎯 执行动作:
    {
      "action": "Type",
      "text": "附近火锅",
      "_metadata": "do"
    }
    ==================================================
    
    ==================================================
    💭 4. 思考过程:
    --------------------------------------------------
    很好！我已经输入了"附近火锅"，现在可以看到搜索框中显示"附近火锅"，并且下方出现了相关的搜索建议，包括：       
    - 附近好吃的火锅店
    - 附近火锅
    - 附近火锅店推荐
    - 附近自助火锅
    以及一些具体的火锅店信息，如杨家火锅（滨湖店）、丽清大别山鹅火锅等。
    
    现在我需要点击"搜索"按钮来执行搜索。搜索按钮在右上角，黄色按钮，坐标大约在(867, 129)。
    --------------------------------------------------
    🎯 执行动作:
    {
      "action": "Tap",
      "element": [
        867,
        129
      ],
      "_metadata": "do"
    }
    ==================================================
    
    ==================================================
    💭 5. 思考过程:
    --------------------------------------------------
    太好了！搜索结果已经显示出来了。我可以看到附近火锅的搜索结果，包括：
    
    1. 许府牛火锅(合肥悦方店) - 评分4.3，963条评论，¥50/人，距离1.9km
    2. 2014海鲜烧烤·涮羊肉·羊蝎子... - 评分4.9，572条评论，¥78/人，距离2.4km
    3. 浪里寻·臻牛养生牛肉火... - 评分5.0，4.9万条评论，¥63/人，距离3.4km
    
    页面显示了多个筛选选项：全部、外卖、团购、地点、笔记等。当前显示的是"全部"选项卡。
    
    任务已经完成，我成功在美团搜索了附近的火锅店，并显示了搜索结果。
    --------------------------------------------------
    🎯 执行动作:
    {
      "_metadata": "finish",
      "message": "任务完成！我已经成功在美团搜索了附近的火锅店。搜索结果显示了多家附近的火锅店，包括：\n\n1. **许府牛火锅(合肥悦方店)** - 评分4.3分，963条评论，人均¥50，距离1.9km\n2. **2014海鲜烧烤·涮羊肉·羊蝎子** - 评分4.9分，572条评论，人均¥78，距离2.4km  \n3. **浪里寻·臻牛养生牛肉火** - 评分5.0分，4.9万条评论，人均¥63，距离3.4km\n\n页面还提供了多种筛选选项（全部、外卖、团购、地点等），你可以根据需要选择合适的筛选条件来查看更多火锅店 信息。"
    }
    ==================================================
    
    

5\. 总结与思考
=========

*   (1) Phone Agent总体的思想是：基于用户的输入，以及每一步手机的截屏，让模型判断当前所处任务的环节，以及下一步的执行动作。
*   (2) 所有的执行环节均由adb操作相应指令，包括启动app、单击、双击、滑动、后退、文本输入等。
*   (3) 作为学习参考项目，整体的交互流程还是相对冗余的，简单的4步操作，模型每次都需要分析判断，因此耗时很长。
*   (4) 如果有新app需要高效地自动执行，该如何保障模型的识别准确率？

6\. 参考
======

*   (1) 项目地址: [https://github.com/zai-org/Open-AutoGLM](https://github.com/zai-org/Open-AutoGLM)