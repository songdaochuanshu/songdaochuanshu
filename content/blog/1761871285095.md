---
layout: post
title: 'Skill Discovery | RGSD：基于高质量参考轨迹，预训练 skill space'
date: "2025-10-31T00:41:25Z"
---
Skill Discovery | RGSD：基于高质量参考轨迹，预训练 skill space
================================================

① 用对比学习把参考轨迹的 embedding 尽可能拉远，② 使用 DIAYN reward 同时做模仿学习和 skill discovery。

  

*   论文标题：Reference Guided Skill Discovery。
*   ICLR 2026 的新文章。
*   arxiv：[https://arxiv.org/abs/2510.06203](https://arxiv.org/abs/2510.06203)
*   pdf：[https://arxiv.org/pdf/2510.06203](https://arxiv.org/pdf/2510.06203)
*   html：[https://arxiv.org/html/2510.06203](https://arxiv.org/html/2510.06203)
*   open review：[https://openreview.net/forum?id=IaGf8Eh5Uo](https://openreview.net/forum?id=IaGf8Eh5Uo)

* * *

目录

*   [1 解决的 gap 和 motivation](#1-解决的-gap-和-motivation)
*   [2 具体 method](#2-具体-method)
    *   [2.1 DIAYN 简述](#21-diayn-简述)
    *   [2.2 RGSD 阶段一：预训练，构建有语义的 latent space](#22-rgsd-阶段一预训练构建有语义的-latent-space)
    *   [2.3 RGSD 阶段二：并行 imitation learning 与 skill discovery](#23-rgsd-阶段二并行-imitation-learning-与-skill-discovery)
    *   [2.4 这篇文章的 trick](#24-这篇文章的-trick)
*   [3 实验](#3-实验)
*   [4 为什么 RGSD 不能与 metra 相结合（根据论文原文）](#4-为什么-rgsd-不能与-metra-相结合根据论文原文)
*   [5 相关思考](#5-相关思考)

* * *

1 解决的 gap 和 motivation
----------------------

首先，RGSD（reference guided skill discovery）这篇文章做的是技能发现（skill discovery），即，希望 agent 在没有人工设定奖励的情况下，自己学出一组多样且有意义的技能，以便后续用于各种任务（如走到某处、躲避障碍）。

然而，现有的 skill discovery 方法在高自由度的系统中（如 69 维动作、359 维状态的 SMPL 人形机器人），容易学出杂乱无章的无意义行为，比如抖腿 抖手，而非站立 跑步这种行为。高自由度系统中，探索空间太大，而真正有意义的技能只占一小部分。

因此，一个自然的想法是：我们可否利用一些参考（reference），即预先给定的 expert 轨迹，来引导 agent 学更有意义的行为呢？RGSD 在试图做这件事，它的故事是，希望利用参考轨迹，预先构建一个 focus on 有意义 skill 的 skill latent space，然后在这个 latent space 里做 skill discovery。

原文是这样说的：

> 为了克服高自由度技能发现中的维度灾难，我们需要预先构建一个语义上有意义的技能潜在空间，并将探索限制在该空间内。

在另一个角度，RGSD 是一个介于 skill discovery 和 imitation learning 之间的方法：

*   传统无监督方法（如 DIAYN、METRA）：通过最大化 skill 与 state 的互信息来鼓励多样性，但在高自由度系统中容易学出杂乱无章的动作，如四肢乱晃。
*   模仿学习方法（如 ASE、CALM）：能较好地复现 reference motion，但缺乏发现新技能的能力，学到的技能范围窄。

一句话总结：RGSD 通过先用 reference 轨迹构建有语义的 skill latent space，再在该 space 中并行进行 imitation learning 与 skill discovery，有效解决了高自由度系统中技能“无意义”的问题，既能高保真模仿，又能自动发现相关新技能，且在下游任务中表现优异。

2 具体 method
-----------

### 2.1 DIAYN 简述

DIAYN 的核心思想是：不同的技能应该导致不同的状态分布。

目标：最大化技能变量 \\(Z\\) 和状态 \\(S\\) 之间的互信息 \\(I(S; Z)\\)。  
互信息分解：\\(I(S; Z) = H(Z) - H(Z|S)\\)。

*   \\(H(Z)\\)：技能分布本身的熵，鼓励技能多样性（通过固定一个均匀分布的先验 \\(p(z)\\) 来最大化）。
*   \\(-H(Z|S)\\)：给定一个状态，技能的不确定性应该很小，即，从一个状态应该能很容易地推断出是哪个技能产生了它。

实现方式：

1.  引入一个判别器（编码器）\\(q\_\\phi(z|s)\\)，它负责根据状态 \\(s\\) 来预测技能 \\(z\\)。
2.  策略 \\(\\pi\_\\theta(a|s, z)\\) 的奖励函数被设计为：鼓励访问那些能让判别器轻松识别出技能 \\(z\\) 的状态。

DIAYN 奖励公式：\\(r(s, z) = \\log q\_\\phi(z|s) - \\log p(z)\\)

*   \\(\\log q\_\\phi(z|s)\\)：鼓励策略访问能让技能 \\(z\\) 被准确识别的状态。
*   \\(-\\log p(z)\\)：作为一个先验项，如果某个技能 \\(z\\) 很少被采样（\\(p(z)\\) 小），则奖励更高，从而鼓励探索所有技能。

### 2.2 RGSD 阶段一：预训练，构建有语义的 latent space

目标：将 reference motion \\(\\mathcal{M}\\)（即 trajectory）的 embedding 嵌入到一个单位超球面中，使得同一 motion 的所有状态嵌入方向一致，不同 motion 的嵌入方向分离。

方法：使用对比学习（InfoNCE Loss）训练编码器 \\(q\_\\phi(z|s)\\)。

1.  编码器建模：我们将 \\(q\_\\phi(z|s)\\) 建模为一个 von Mises–Fisher (vMF) distribution（好像可以理解为球面上的高斯分布）：\\(q\_\\phi(z|s) \\propto \\exp(\\kappa \\mu\_\\phi(s)^\\top z)\\)，其中 \\(\\mu\_\\phi(s)\\) 是网络输出的均值方向（已归一化），\\(\\kappa\\) 是集中度参数。
2.  对比学习：
    *   从数据集 \\(\\mathcal{M}\\) 中采样一个 motion \\(m\\)。
    *   从 \\(m\\) 中采样两个状态作为 anchor \\(s^a\\) 和正样本 \\(s^+\\)，从其他动作中采样状态作为负样本 \\(s^-\\)。
    *   计算它们的嵌入：\\(z^a = \\mu\_\\phi(s^a), z^+ = \\mu\_\\phi(s^+), z^- = \\mu\_\\phi(s^-)\\)。
    *   优化 InfoNCE loss（关于为什么可以写成 infoNCE loss，附录有相关数学）：
        
        \\\[\\mathcal{L}\_{\\text{InfoNCE}} = -\\log \\frac{\\exp(\\text{sim}(z^a, z^+)/T)}{\\exp(\\text{sim}(z^a, z^+)/T) + \\sum\_j \\exp(\\text{sim}(z^a, z\_j^-)/T)} \\\]
        
        其中 \\(\\text{sim}(u, v) = u^\\top v\\)（余弦相似度），\\(T = 1/\\kappa\\) 是温度系数。

结果：预训练后，同一动作的所有状态 \\(s \\in m\\) 都有相同的嵌入方向 \\(\\mu\_\\phi(s) = z\_m\\)。

### 2.3 RGSD 阶段二：并行 imitation learning 与 skill discovery

在阶段一，我们只训练了 encoder \\(\\mu\_\\phi\\)，而没有训练策略。此阶段，策略 \\(\\pi\_\\theta(a|s, z)\\) 开始与环境交互并学习。

模仿和发现共享同一个 policy network，并且，共享同一个奖励函数形式（即 DIAYN 的奖励），但技能 \\(z\\) 的采样方式不同。

1.  技能 \\(z\\) 的采样：
    
    *   以概率 \\(p\\)（模仿）：采样一个参考动作 \\(m\\)，计算其平均嵌入 \\(z\_m = \\frac{1}{l} \\sum\_{s \\in m} \\mu\_\\phi(s)\\)。让策略执行技能 \\(z = z\_m\\)。
    *   以概率 \\(1-p\\)（发现）：从标准正态分布采样并归一化，\\(z = k / \\|k\\|, k \\sim \\mathcal{N}(0, I)\\)。
2.  计算 reward：
    
    *   我们将预训练的编码器 \\(q\_\\phi\\) 冻结，记为 \\(q\_\\phi\\)。然后，初始化一个可训练的发现编码器 \\(q'\_\\phi\\)，其参数从 \\(q\_\\phi\\) 复制而来。
    *   reward 公式：\\(r(s,z) = -\\log p(z) + \\log q\_\\phi(z | s) = C + \\kappa\\mu\_\\phi(s)^\\top z\\)
    *   对于模仿任务，奖励是当前状态 \\(s\\) 与目标技能 \\(z\_m\\) 的相似度（通过冻结的、完美的编码器 \\(q\_\\phi\\) 计算）。
    *   对于发现任务，奖励是标准的 DIAYN 奖励，但编码器 \\(q'\_\\phi\\) 是可训练的。
3.  编码器更新：
    
    *   模仿：通过最大化 \\(\\log q\_\\phi(z | s)\\) 来学策略。
    *   发现：为了防止 \\(q'\_\\phi\\) 在发现过程中破坏已学到的潜在空间，我们添加一个 KL 散度的 loss 项：\\(\\mathcal{L}\_{\\text{KL}} = \\alpha \\cdot \\text{KL}(q'\_\\phi(\\cdot|s) \\| q\_\\phi(\\cdot|s))\\)。
4.  策略更新：
    
    *   使用 PPO 作为 RL 算法，最大化上述奖励 \\(r(s, z)\\) 以及策略的熵，来更新策略 \\(\\pi\_\\theta\\)。

### 2.4 这篇文章的 trick

*   To exploit this local concavity in practice, we apply early termination: whenever the agent deviates from the reference motion beyond a specified threshold measured by cartesian error, the episode is terminated. 为了在实践中利用这种局部凹陷，我们应用了早期终止：每当智能体偏离参考运动超过笛卡尔误差测量的指定阈值时，该情节就会终止。
*   we adopt reference state initialization (RSI), which samples initial states directly from the reference motions. RSI prevents the emergent of disjoint skill sets by ensuring that imitation and discovery operate over overlapping state distributions. 我们采用参考状态初始化（RSI），直接从参考运动中对初始状态进行采样。RSI 通过确保模仿和发现在重叠的状态分布上运行来防止不相交技能集的出现。
*   为了在发现过程中保护学习到的潜在空间，我们从冻结的 \\(q\_\\phi\\) 初始化一个单独的编码器 \\(q'\_\\phi\\)，并加一个最小化这两个 q 之间的 KL 散度的 loss。
*   我们并行训练发现和模仿，with a ratio parameter p，以概率 p 进行 imitation learning，1-p 进行 skill discovery，以便共享策略和价值函数能够将高保真行为的知识从模仿转移到发现中。这两个过程共享相同的奖励函数和潜在空间形式，因此这些共享组件可以稳定地优化。
*   为确保训练稳定，所有方法都采用了提前终止条件：每当机器人摔倒时，该回合即终止。（LGSD 也是这样的，使用这种方法把 metra 卡下去了）
*   在做实验比较的时候，对于 CALM，由于它也包含运动编码器，因此选择能够代表每个运动的正确潜在变量是直接的。对于没有编码器的方法，我们均匀地采样 500 个潜在向量，选择其中使笛卡尔误差最小的一个，并使用这个向量重新评估以确保公平性。我们发现 500 个样本是足够的，因为进一步增加数量并没有带来明显的改进。（感觉这样做是好的、公平的；值得学习，实验里比较公平的细节或许应该写出来）

3 实验
----

实验 setting：

*   环境：Isaac Gym 中的 SMPL 人形机器人（69 维动作，359 维状态）。
*   数据集：ACCAD 运动数据库中的 20 个参考动作（走路、跑步、侧步 sidestepping、后退、出拳等）。
*   评估指标：
    *   模仿保真度：Cartesian 误差（位置误差）、FID 分数（运动自然度）。这两个 metrics 的介绍可参考[博客](https://blog.csdn.net/iiiiii11/article/details/153429985)。
    *   技能多样性：能否发现与 reference 动作的新变种（如往不同方向走的 sidestepping）。
    *   下游任务性能：如“sidestepping 到达目标”任务的成功率。

实验结果：

*   模仿效果：RGSD 在多数任务上 Cartesian 误差最低（如跑步误差 7.7cm），表明能高保真复现参考动作。
*   技能发现：能生成语义相关的新技能（如不同方向的侧步、多角度出拳），且 FID 分数稳定，说明新技能既多样又自然。
*   下游任务：在“侧步到达目标”任务中，RGSD 成功率与 CALM 相当，但运动保真度更高（FID 34.3 vs. CALM 的 46.7）。

4 为什么 RGSD 不能与 metra 相结合（根据论文原文）
--------------------------------

这部分对应论文 5.4 节和附录 F。

核心原因：METRA 的奖励机制与“重复性动作”存在根本性冲突，而这类动作是 RGSD 技能库的重要组成部分。

具体来说，问题体现在以下三个层面：

1.  奖励计算失效：
    
    *   METRA 的奖励是 \\((\\phi(s\_{T}) - \\phi(s\_{0}))^{\\top} z\\)，它鼓励 agent 在 latent space 中沿着技能方向 \\(z\\) 产生位移。
    *   然而，对于**重复性动作**（如行走），一个周期结束后，智能体在**局部坐标系**下的姿态 \\(s\_T\\) 与起始姿态 \\(s\_0\\) 几乎完全相同。
    *   因此，\\(\\phi(s\_{T}) \\approx \\phi(s\_{0})\\)，导致奖励 \\(\\approx 0\\)。这意味着，**执行一个完美的周期行为反而无法获得任何奖励**，这与奖励最大化的目标相悖。
2.  状态增强的副作用：
    
    *   **方案一：添加全局坐标。** 在 agent 越跑越远的情况下，这可以区分 \\(s\_0\\) 和 \\(s\_T\\)，但带来了新问题：
        *   全局坐标是**无界**的，RGSD 声称 METRA 会轻易利用这一点：智能体只需学会向不同方向移动，就能最大化奖励，而无需学习有意义的身体动作（如摆臂、迈腿）。
        *   这导致 latent space 被全局坐标主导，技能发现失败。
    *   **方案二：添加时间变量**，即当前的 timestep 值。这同样能区分状态，但同样带来新问题：
        *   时间变量会迫使 latent space 形成一个以时间为刻度的“等高线”结构。
        *   在探索时，RGSD 声称，agent 从一个时间步跳到下一个时间步，可能在潜在空间中产生**巨大的、不连续的跳跃**（例如，跨过“等高线”组成的山，从一侧跳到另一侧），这违反了 METRA 要求相邻状态潜在距离小于 1 的约束。
        *   最终，导致训练变得极不稳定。
3.  与 RGSD 设计哲学的冲突：
    
    *   RGSD 的核心是**预先构建一个稳定、语义清晰的（超球面）潜在空间**。
    *   而 METRA 为了最大化技能差异，其学习过程会动态地、剧烈地改变潜在空间的结构（如附录 F 图 7 所示）。
    *   这两种 latent space 处理方式是相互矛盾的。因此，将 METRA 的探索机制强加于 RGSD 预结构化好的空间上，会破坏后者的语义基础。

5 相关思考
------

*   （abstract 里的 manifold 这个词，第一次见，比较新奇）
*   思考，encoder 设置成 \\(z = \\mu(s)\\) 会不会没那么好用，导致跑步时的各种姿势都被映射到同一个 z 上，并且，各个 motion 可能会包含一些公共 state；会不会 \\(z=\\mu(s,s')\\) 更好一些；
*   fig 4 用俯视图的轨迹来说明 skill 的多样性，这确实符合 skill discovery 工作的可视化的惯例。然而，它表现出多样行为（如不同角度的转弯）的 skill，所对应的 skill 都是同一个 \\(z\_m\\)。因此，这好像跟狭义上的 skill discovery 的多样性不太 match，即，它并不是不同 skill z 能生成不同的行为，不过在某种程度上，它确实学到了不同的行为。
*   如果在同一个 z 下，动作也是 diverse 的，这意味着什么？意味着（比如说）不同角度的转弯，所对应的动作，都被映射到了同一个 embedding 下。思考，embedding 有这样的性质，要不因为 RGSD 的预训练，训出来就会这样（毕竟有一个 KL 散度的 loss 约束 embedding 不能变太多），要不因为 RGSD 的 policy 不小心做出了不同角度的转弯，\\(phi'\\) 为了让 agent 的 reward 最大化，就也允许不同角度的转弯了。
*   如果 RGSD 是真的，那么 RGSD 用 metra 不 work，或许是因为 metra 更注重结果（metra 这类方法，每一步的 reward 都是 \\(\\phi(s\_T)-\\phi(s\_0)\\) 裂项出来的），所以没法学到原地打转 过程性的技能。
*   合作者说这篇文章的理论都是对的，只不过假设一个 motion m 里的所有 state 映射到同一个 embedding \\(phi\\)，这个假设有点奇怪。
*   （公式 6 7 求导得到的那个常数 C 项，合作者有些怀疑正确性）