---
layout: post
title: '一镜到底，通过Llama大模型架构图看透transformers原理'
date: "2025-01-22T00:35:33Z"
---
一镜到底，通过Llama大模型架构图看透transformers原理
==================================

想了解大模型 Llama 的工作原理？Llama Nuts and Bolts 项目不依赖外部库，通过 Go 语言从零构建 Llama 3.1 8B-Instruct 模型，为学习者提供了一个教育性深度探索，让您动手实践理解大型语言模型。

一镜到底，通过Llama大模型架构图看透transformers原理
==================================

Llama Nuts and Bolts是Github上使用Go语言从零重写Llama3.1 8B-Instruct模型推理过程（80亿参数规模）的实战类开源项目，其作者是来自土耳其的Adil Alper DALKIRAN。

如果你对于 LLM（大语言模型）和 Transformers 的工作原理感兴趣，并对相关概念略知一二，但仍想深入理解，那么这个项目非常适合你！

这个项目最大的特色是：

*   使用Go语言从零开发，不依赖任何机器学习库和数学计算库，走出 Python 生态的舒适区
    
*   配备完整的大模型推理的流程图，透视大模型如何运作的细节
    
*   完备的文档和代码说明，能够亲身体验机器学习的基础知识、Transformers 模型、注意力机制、旋转位置嵌入（RoPE）以及背后的数学原理
    

Llama Nuts and Bolts项目代码和文档地址：

*   [https://github.com/adalkiran/llama-nuts-and-bolts](https://github.com/adalkiran/llama-nuts-and-bolts)
    
*   [https://adalkiran.github.io/llama-nuts-and-bolts/](https://adalkiran.github.io/llama-nuts-and-bolts/)
    

![Llama Nuts and Bolts](https://mz-blog-res.oss-cn-beijing.aliyuncs.com/img/b002/llama-nuts-and-bolts-screen-record.gif)

需要注意的是，该项目仅为教育目的开发，未经过生产环境或商业使用测试。其目标是创建一个实验性项目，能够在完全不依赖 Python 生态系统的情况下对 Llama 3.1 8B-Instruct 模型进行推理。

这个项目使用Go语言，不使用任何现有的机器学习或数学计算库，从零实现一个控制台应用程序，通过使用预训练的 Llama 3.1 8B-Instruct 模型参数生成文本输出。

开发这个项目的过程使作者深入研究了 transformers 模型的内部结构，并发现了之前没有意识到的细节，包括作者已经了解的理论知识，还有需要重新学习的新内容，并从中获得了新见解。

Llama Nuts and Bolts 的第一个版本于 2024 年 3 月 12 日发布，适配 Llama 2 模型，而其最新的版本支持Llama 3.1 8B-Instruct模型。

话不多说，先上图。

![Llama 3.1 8B-Instruct大模型推理的完整流程图](https://mz-blog-res.oss-cn-beijing.aliyuncs.com/img/b002/DIAG01-complete-model.drawio.svg)

Llama transformers 架构的特点
------------------------

与经典transformers架构相比，Llama 的transformers架构具有几个显著特征：

*   仅解码器架构 Decoder-Only Architecture：Llama 纯文本模型只有解码器decorder，没有encoder，这意味着它仅专注于根据输入上下文生成序列，无需编码器，因此它主要依赖自注意力机制来捕捉输入序列中的依赖关系。Llama 是一个仅解码器模型，这意味着它仅专注于根据输入上下文生成序列。这与像 BERT 或 T5 这样的编码器-解码器模型形成对比，后者同时利用编码器来理解输入和解码器来生成输出。
*   自注意力机制 Self-Attention Mechanism：Llama 纯文本模型不包括交叉注意力层。Llama 自注意力层用于解码器encoder内处理输入序列，而交叉注意力层在编码器-解码器模型中更为常见，其中编码器处理一个输入（例如，源语言），解码器则基于该处理信息生成输出。Llama 使用自注意力以捕捉输入文本中的依赖关系，而无需交叉注意力层。这使其能够生成连贯且上下文相关的文本。