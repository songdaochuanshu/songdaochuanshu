---
layout: post
title: '从0到1，无代码微调并部署本地大语言模型LLM'
date: "2026-02-15T01:01:18Z"
---
从0到1，无代码微调并部署本地大语言模型LLM
=======================

前言
==

**LLM模型微调** 能让大模型掌握特定行业的深度知识，能够实现AI虚拟主播，AI医生，AI程序员，AI网络安全工程师等特定领域的延展。更重要的是，当有本地部署的硬件条件限制时，能够让微调后小的大语言模型等效百亿级的大语言模型

**测试环境：windows11，RTX4070显卡**  
**下面将手把手带你跑通无代码模型微调的全过程**

环境安装
====

### 必要的工具：

*   git： [https://git-scm.cn/](https://git-scm.cn/) （方便拉取资源）
*   python： [https://www.python.org/](https://www.python.org/) （微调和运行必要环境）

### 流程：

1.  创建文件夹，并拉取 llama-factory项目

    mkdir D:/LLM-Tuning
    cd D:/LLM-Tuning
    git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
    cd LLaMA-Factory
    

2.  安装LLaMA-Factory需要的环境

    pip install -e ".[torch,metrics]"
    pip install modelscope
    

3.  验证环境

    python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
    

正常输出如下：

> 错误：正常来说安装完后验证环境会显示显卡型号，但是我在安装时，会出现报错，原因是它安装了错误的cuda版本，需要重新安装`torch`  
> 解决方法如下：
> 
>     pip uninstall torch torchvision torchaudio
>     
>     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
>     
> 
> 如果其他版本请参考官网： [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)

微调
==

这里用于演示，只对模型做一个自我认知的微调

### 准备数据集

**拉取数据集**

    git clone https://www.modelscope.cn/datasets/DanKe123abc/yuki_identity_sft.git
    

**修改数据集**  
下载完后，目录结构如下：

我们需要关注的是`yuki_identity_sft.jsonl`文件，用编辑器将下列文字全局替换：

    Yuki => 陈千语
    DanKe => 管理员
    

效果图如下：

### 准备本地模型

这里使用的是`qwen2.5_1.5B`用于演示  
**下载模型**

    from modelscope import snapshot_download
    
    download_dir = "D:\\Models\\Qwen2.5-1.5B-Instruct"
    
    model_dir = snapshot_download(
        'qwen/Qwen2.5-1.5B-Instruct', 
        cache_dir=download_dir, 
        revision='master'
    )
    
    print(f"下载完成！模型路径为: {model_dir}")
    

### 微调

**配置数据集信息**

1.  打开`D:\LLM-Tuning\LLaMA-Factory\data`文件，将刚刚修改好的数据集`yuki_identity_sft.jsonl`文件拖入文件夹中
2.  打开`dataset_info.json`文件，添加新配置：

    "MytestData": {
        "file_name":"yuki_identity_sft.jsonl",
        "columns": {
          "messages": "conversations"
        },
        "tags": {
          "role_tag": "role",
          "content_tag": "content",
          "user_tag": "user",
          "assistant_tag": "assistant"
        },
        "formatting": "sharegpt"
      },
    

**打开LLamaFactory微调面板**

    python -m llamafactory.cli webui
    

设置参数如图，其他的默认就行：

设置完后直接点击开始，模型就开始训练了，训练完后会出现下面提示：

验证模型
====

### 加载训练完后的lora模型

### 训练前后的大模型对比

**训练前**

**训练后**

**观察图片可以发现，微调后qwen2.5认为自己是陈千语，自己由管理员开发的**

大模型部署
=====

**下面不是新手向**  
如果只是希望学习微调的在这里已经结束了，下面是本系列教程的后续，如何用`langchain`部署本地的LLM微调大语言模型

### 环境配置

安装需要的环境

    pip install peft langchain langchain-huggingface
    

### 下面是样例代码

代码流程如下：  
**加载基座模型->加载 LoRA 权重->正在合并权重->构建Langchain通道->调用模型**

    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
    from peft import PeftModel
    from langchain_huggingface import HuggingFacePipeline
    from langchain_core.prompts import PromptTemplate
    
    BASE_MODEL_PATH = r'D:\Models\Qwen2.5-1.5B-Instruct\qwen\Qwen2___5-1___5B-Instruct'
    LORA_PATH = r'D:\D_MyProject\LLM-Tuning\LLaMA-Factory\saves\Qwen2.5-1.5B\lora\train_2026-02-13-23-16-50\checkpoint-260'
    
    print("1. 正在加载基座模型...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto",  
        trust_remote_code=True
    )
    
    print("2. 正在加载 LoRA 权重 ...")
    model = PeftModel.from_pretrained(base_model, LORA_PATH)
    
    print("3. 正在合并权重 ...")
    model = model.merge_and_unload()
    
    print("4. 构建 LangChain 管道...")
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=200,    
        do_sample=True,        
        temperature=0.7,      
        repetition_penalty=1.1 
    )
    
    llm = HuggingFacePipeline(pipeline=pipe)
    
    print("\n=== 陈千语上线 ===\n")
    
    respone = llm.invoke('你好，你是谁？')
    print(f"{respone}")
    

### 演示效果

**至此，我们成功的实现了大模型LLM从微调到部署，把之前的langchain串起来...**

**如果❤喜欢❤本系列教程，就点个关注吧，后续不定期更新~**

本文来自博客园，作者：[ClownLMe](https://www.cnblogs.com/ClownLMe/)，转载请注明原文链接：[https://www.cnblogs.com/ClownLMe/p/19615980](https://www.cnblogs.com/ClownLMe/p/19615980)