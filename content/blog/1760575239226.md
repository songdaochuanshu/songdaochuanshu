---
layout: post
title: '从SGD到AdamW：深度学习优化器演进全解析与实践指南'
date: "2025-10-16T00:40:39Z"
---
从SGD到AdamW：深度学习优化器演进全解析与实践指南
----------------------------

从SGD到AdamW：深度学习优化器演进全解析与实践指南
============================

**摘要**
------

在深度学习中，优化器（Optimizer）是连接模型与数据的桥梁，它负责根据损失函数的梯度来更新模型的参数，以期找到一组能最小化损失的“最优解”。优化器的选择和调优，直接关系到模型的训练速度、收敛性以及最终的泛化能力。本文将遵循“**问题提出 → 方案演进**”的逻辑脉络，系统性地梳理从经典的梯度下降法到现代深度学习模型中默认的AdamW优化器的发展历程。

我们将深入探讨五类核心优化器：

1.  **梯度下降家族 (BGD, SGD, MBGD)**：奠定基础，解决效率与噪声的权衡。
2.  **动量方法 (Momentum, NAG)**：引入惯性，克服“峡谷”地形，加速收敛。
3.  **自适应学习率方法 (Adagrad)**：为每个参数定制学习率，应对稀疏数据。
4.  **改进的自适应方法 (RMSProp)**：引入遗忘机制，解决Adagrad学习率耗尽问题。
5.  **集大成者 (Adam, AdamW)**：融合动量与自适应思想，并修正权重衰减，成为当前主流。

文章将为每一类优化器剖析其**核心动机、数学原理、适用场景、优缺点**，并清晰地揭示其**相对于前代方法的关键改进**，最后提供一份面向实践的选型指南。

* * *

**一、 核心问题：如何在复杂“损失地貌”中高效寻路？**
-----------------------------

深度学习的目标是最小化一个高维、非凸的损失函数 \\(L(\\theta)\\)，其参数 \\(\\theta\\) 的维度可达成千上万甚至万亿。我们可以将这个损失函数想象成一个极其复杂、崎岖不平的“地貌景观”（Loss Landscape）。我们的任务，就是从一个随机的初始位置（参数初始化），一步步走到这个地貌的最低点（全局最小值）。

优化器就是我们的“寻路策略”。最朴素的想法是，在当前位置环顾四周，找到最陡峭的下坡方向，然后朝着这个方向走一小步。这个“最陡峭的下坡方向”就是**负梯度** \\((-\\nabla\_\\theta L(\\theta))\\)。所有优化器都围绕着两个核心问题来构建策略：

1.  **方向（Direction）**：朝哪个方向走？是严格沿着当前点的负梯度，还是考虑历史方向的“惯性”？
2.  **步长（Step Size / Learning Rate）**：朝选定的方向走多远？是所有参数都走相同的距离，还是为不同参数定制不同的步长？

不同优化器正是对这两个问题给出了不同的答案，从而在复杂的损失地貌中展现出各异的寻路行为。

* * *

**二、 梯度下降法：朴素而坚实的第一步**
----------------------

### **1\. 出现背景**

梯度下降法是最直观的数值优化方法。深度学习的目标函数通常是定义在整个训练集上的经验风险：

\\\[L(\\theta)=\\frac{1}{n}\\sum\_{i=1}^n \\ell(f\_\\theta(x\_i), y\_i) \\\]

其中 \\(n\\) 是样本总数。要计算真实梯度 \\(\\nabla\_\\theta L(\\theta)\\)，需要遍历所有 \\(n\\) 个样本。在动辄百万、千万甚至亿级样本的数据集上，计算一次真实梯度的成本是无法接受的。此外，神经网络的非凸性意味着，即使方向完全准确，也可能陷入局部最优或鞍点。因此，我们需要更高效、且具备一定“探索性”的梯度近似方法。

### **2\. 公式推导与含义**

设当前参数为 \\(\\theta\_t\\)，学习率为 \\(\\eta > 0\\)，单个样本 \\((x\_i, y\_i)\\) 产生的梯度为 \\(g\_i=\\nabla\_\\theta \\ell(f\_{\\theta\_t}(x\_i), y\_i)\\)。

*   **批量梯度下降 (Batch Gradient Descent, BGD)**
    
    \\\[g\_t = \\nabla\_\\theta L(\\theta\_t) = \\frac{1}{n}\\sum\_{i=1}^n g\_i \\\]
    
    \\\[\\theta\_{t+1} = \\theta\_t - \\eta \\cdot g\_t \\\]
    
    **含义**：每一步更新都使用**全部**训练数据计算梯度。这个方向准确地指向了当前点经验风险下降最快的方向。
    
*   **随机梯度下降 (Stochastic Gradient Descent, SGD)**  
    随机抽取一个样本 \\(i\_t \\in \\{1, ..., n\\}\\)：
    
    \\\[\\theta\_{t+1} = \\theta\_t - \\eta \\cdot g\_{i\_t} \\\]
    
    **含义**：用**单个**随机样本的梯度作为整体梯度的无偏估计。\\(E\[g\_{i\_t}\] = g\_t\\)，但其方差巨大。每一步更新成本极低，但轨迹充满噪声，非常“摇晃”。
    
*   **小批量梯度下降 (Mini-Batch Gradient Descent, MBGD)**  
    随机抽取一个大小为 \\(B\\) 的小批量 \\(\\mathcal{B}\_t \\subset \\{1, ..., n\\}\\)：
    
    \\\[g\_t = \\frac{1}{B}\\sum\_{i\\in\\mathcal{B}\_t} g\_i \\\]
    
    \\\[\\theta\_{t+1} = \\theta\_t - \\eta \\cdot g\_t \\\]
    
    **含义**：在BGD和SGD之间取得平衡。通过对一个小批量内的梯度进行平均，既降低了更新的计算成本，又显著减小了梯度的方差，使得收敛过程更稳定。这是现代深度学习训练的**事实标准**。通常我们所说的“SGD”实际上指的就是MBGD。
    

### **3\. 适用场景与优缺点**

方法

优点

缺点

适用场景

**BGD**

更新方向稳定，理论性质好

计算成本极高；非凸场景易陷入鞍点

仅适用于小型数据集或凸优化问题

**SGD**

更新成本极低；噪声有助于跳出局部最优和鞍点

梯度方差大，收敛过程震荡剧烈，需要小心调整学习率

理论分析，或作为其他更复杂优化器的基础

**MBGD**

计算高效（可并行化），梯度方差可控

仍可能在病态曲率的“峡谷”地貌中震荡

**深度学习的默认选择**

### **4\. 与前一代方法的改进点**

*   **SGD 相对 BGD**：用随机近似替代全量计算，将单步更新的复杂度从 \\(O(n)\\) 降至 \\(O(1)\\)，并引入了有益的噪声。
*   **MBGD 相对 SGD**：用小批量平均来估计梯度，有效降低了梯度估计的方差，使得收敛更稳定，同时充分利用了现代计算硬件（如GPU）的并行计算能力，提升了训练吞吐量。
*   **但都由于梯度不稳定，都容易陷入局部极小值处**

* * *

**三、 动量方法：引入惯性，冲过“狭窄峡谷”**
-------------------------

### **1\. 出现背景**

MBGD虽然有效，但在某些常见的损失地貌中会遇到麻烦。想象一个狭长、陡峭的峡谷：在垂直于峡谷走向的方向上，梯度很大，导致参数在谷壁两侧来回“锯齿状”震荡；而在沿着峡谷走向（通往最优解）的方向上，梯度很小，导致前进缓慢。我们希望在震荡方向上减速，在平坦方向上加速。动量（Momentum）方法就是为了解决这一问题。

其物理类比是：一个从山上滚下的小球，它不仅受到当前地心引力（梯度）的影响，还保留了之前的速度（惯性）。这股惯性使得它在平坦的道路上持续加速，而在来回震荡的斜坡上，相反方向的力会相互抵消，从而抑制震荡。

### **2\. 公式推导与含义**

*   **经典动量 (Heavy-ball Momentum)**  
    引入一个“速度”或“一阶动量”向量 \\(v\_t\\)，它是历史梯度的指数移动平均（Exponentially Moving Average, EMA）。
    
    \\\[v\_t = \\beta v\_{t-1} + (1-\\beta) g\_t \\\]
    
    \\\[\\theta\_{t+1} = \\theta\_t - \\eta v\_t \\\]
    
    *   \\(\\beta\\) 通常取 0.9 左右。  
        **含义**：\\(v\_t\\) 累积了过去梯度。如果历史梯度方向一致，\\(v\_t\\) 会越来越大，实现加速；如果历史梯度方向反复变化，\\(v\_t\\) 中的正负项会相互抵消，实现减速。\\(\\beta\\) 扮演了“摩擦系数”的角色，决定了历史梯度的衰减速度。
*   **Nesterov 加速梯度 (Nesterov Accelerated Gradient, NAG)**  
    NAG 对经典动量做了一个聪明的修改：它不计算当前点的梯度，而是先“预估”一下按照当前动量会走到哪里，然后在那个“未来”的点计算梯度，再用这个梯度来修正最终的步进方向。
    
    1.  **预估未来位置** (Lookahead position): \\(\\theta\_{lookahead} = \\theta\_t - \\eta \\beta v\_{t-1}\\)
    2.  **在未来位置计算梯度**: \\(g\_{lookahead} = \\nabla\_\\theta L(\\theta\_{lookahead})\\)
    3.  **更新速度与位置**:
        
        \\\[v\_t = \\beta v\_{t-1} + g\_{lookahead} \\\]
        
        \\\[\\theta\_{t+1} = \\theta\_t - \\eta v\_t \\\]
        
    
    **含义**：NAG 具有“前瞻性”。如果动量即将带领我们冲过头，NAG 能在未来点上感知到这一点（梯度会指向反方向），从而提前减速，缓解超调（overshooting）问题，收敛更快更稳。
    

### **3\. 适用场景与优缺点**

*   **优点**：
    *   显著加速在病态曲率（ill-conditioned）问题上的收敛。
    *   有效抑制SGD的震荡，允许使用稍大的学习率。
    *   对于凸问题，有更强的理论收敛保证。
*   **缺点**：
    *   引入了新的超参数 \\(\\beta\\)，需要调节。
    *   对于变化剧烈的损失地貌，过大的动量可能导致冲出最优区域。

### **4\. 相对上一代的改进点**

*   **动量 相对 MBGD**：核心改进在于**方向**的计算。它不再只依赖当前梯度，而是通过对历史梯度进行指数平滑，构造了一个更稳定、更有效的前进方向，从而在宏观上加速了收敛。

* * *

**四、 Adagrad：为每个参数定制学习率**
-------------------------

### **1\. 出现背景**

动量方法统一地调整了所有参数的更新方向，但未解决另一个问题：**所有参数共享同一个学习率 \\(\\eta\\) 是否合理？**

在很多场景，尤其是您所熟悉的推荐系统和自然语言处理中，输入特征是高度稀疏的。例如，一个用户的ID或一个商品的ID，作为Embedding层的输入，在整个训练数据中可能只出现几次。对于这些**稀疏特征**对应的参数，我们希望在它们每次出现时都给予较大的更新，以“快速学习”。而对于那些**频繁出现**的特征（如某些常用词），我们则希望更新更保守，因为它们已经得到了充分的训练。Adagrad (Adaptive Gradient Algorithm) 应运而生，旨在为每个参数自动地、自适应地调整学习率。

### **2\. 公式推导与含义**

Adagrad 为每个参数维护一个累积的平方梯度值 \\(s\_t\\)。

\\\[s\_t = s\_{t-1} + g\_t \\odot g\_t \\quad (\\text{其中 } \\odot \\text{ 表示逐元素相乘}) \\\]

\\\[\\theta\_{t+1} = \\theta\_t - \\frac{\\eta}{\\sqrt{s\_t} + \\epsilon} \\odot g\_t \\\]

**含义**：

*   \\(s\_{t,j}\\)（\\(s\_t\\) 的第 \\(j\\) 个分量）累加了参数 \\(\\theta\_j\\) 自训练开始以来所有梯度的平方。
*   参数 \\(\\theta\_j\\) 的**有效学习率**变成了 \\(\\frac{\\eta}{\\sqrt{s\_{t,j}} + \\epsilon}\\)。
*   如果一个参数的梯度一直很大（更新频繁），\\(s\_{t,j}\\) 会很大，其有效学习率就会变小。
*   如果一个参数的梯度一直很小或很稀疏（更新很少），\\(s\_{t,j}\\) 会很小，其有效学习率就会较大。
*   \\(\\epsilon\\) 是一个极小的常数（如 \\(10^{-8}\\)），用于防止分母为零。  
    这相当于对优化问题进行了**对角预条件化**（Diagonal Preconditioning），有效地为不同参数设置了不同的学习率。

### **3\. 适用场景与优缺点**

*   **优点**：
    *   对稀疏特征场景极为有效，无需手动调整学习率。
    *   在训练前期，当梯度较大时，能自动降低学习率，表现稳健。
*   **缺点**：
    *   **致命缺陷**：分母中的 \\(s\_t\\) 是单调递增的。随着训练的进行，所有参数的有效学习率最终都会趋近于零，导致训练提前“死亡”，无法继续学习。

### **4\. 相对上一代的改进点**

*   **Adagrad 相对 动量**：这是一个范式的转变。优化的焦点从**平滑更新方向**转向了**自适应调整每个参数的步长**。它开创了自适应学习率方法的先河，解决了统一学习率在异构参数空间中的局限性。

* * *

**五、 RMSProp：解决Adagrad学习率急剧下降并最终耗尽的问题**
---------------------------------------

### **1\. 出现背景**

RMSProp (Root Mean Square Propagation) 的提出，正是为了解决Adagrad学习率急剧下降并最终耗尽的问题。其核心思想非常直观：我们真的需要平等地记住所有历史梯度吗？或许，我们更应该关注近期的梯度信息。因此，RMSProp 将 Adagrad 的梯度平方累加，改为一种**指数移动平均**。

### **2\. 公式推导与含义**

\\\[s\_t = \\rho s\_{t-1} + (1-\\rho) g\_t \\odot g\_t \\\]

\\\[\\theta\_{t+1} = \\theta\_t - \\frac{\\eta}{\\sqrt{s\_t} + \\epsilon} \\odot g\_t \\\]

**含义**：

*   与Adagrad唯一的区别在于 \\(s\_t\\) 的计算方式。它不再是简单的累加，而是对历史平方梯度的加权平均。
*   \\(\\rho\\) 是一个衰减率（通常设为0.9或0.99），它控制了历史信息的遗忘速度。
*   这种机制使得 \\(s\_t\\) 不再单调递增。如果近期梯度变小，\\(s\_t\\) 也会随之下降，从而让有效学习率能够回升。这使得RMSProp能更好地适应非平稳目标（即损失函数的形状在训练过程中会发生变化）。

### **3\. 适用场景与优缺点**

*   **优点**：
    *   继承了Adagrad对参数的自适应学习率能力。
    *   通过遗忘机制解决了学习率耗尽问题，更适合长期训练。
    *   在非平稳或有噪声的优化问题上表现出色。
*   **缺点**：
    *   仍然需要手动设置学习率 \\(\\eta\\) 和衰减率 \\(\\rho\\)。
    *   虽然在实践中极为成功，但其最初是作为Geoff Hinton课程讲义中的一个技巧提出的，缺乏严格的论文发表和理论分析。

### **4\. 相对上一代的改进点**

*   **RMSProp 相对 Adagrad**：关键改进在于将对历史平方梯度的**无限记忆累加**，改为了**带遗忘的指数移动平均**。这一改变彻底解决了学习率单调递减至零的问题，极大地增强了算法的鲁棒性和适用范围。

* * *

**六、 Adam 与 AdamW：动量与自适应的结合**
-----------------------------

### **1\. 出现背景**

至此，我们有两条并行的改进路线：

1.  **动量**：平滑更新方向（估计梯度的一阶矩）。
2.  **Adagrad/RMSProp**：自适应参数步长（估计梯度的二阶矩）。

一个自然而然的问题是：我们能否将两者结合，同时享受惯性带来的方向稳定性和自适应学习率带来的步长调整？Adam (Adaptive Moment Estimation) 正是这一思想的结晶。它不仅结合了两者的优点，还引入了**偏差修正**，使其在训练初期表现更加稳健。

而后，研究者发现Adam在与L2正则化（权重衰减）结合使用时存在一个微妙但重要的问题，AdamW应运而生，通过**解耦权重衰减**提供了更有效的正则化方案，并迅速成为Transformer等大模型的标配。

### **2\. 公式推导与含义**

*   **Adam**  
    Adam同时维护梯度的一阶矩（动量）\\(m\_t\\) 和二阶矩（未开方的RMSProp项）\\(v\_t\\) 的指数移动平均：
    
    1.  **更新一阶矩和二阶矩估计**:
        
        \\\[m\_t = \\beta\_1 m\_{t-1} + (1-\\beta\_1) g\_t \\quad (\\text{动量项}) \\\]
        
        \\\[v\_t = \\beta\_2 v\_{t-1} + (1-\\beta\_2) g\_t \\odot g\_t \\quad (\\text{自适应项}) \\\]
        
    2.  **偏差修正 (Bias Correction)**:  
        由于 \\(m\_0\\) 和 \\(v\_0\\) 初始化为0，在训练初期，\\(m\_t\\) 和 \\(v\_t\\) 会偏向于0。Adam通过以下方式进行修正：
        
        \\\[\\hat{m}\_t = \\frac{m\_t}{1 - \\beta\_1^t} \\\]
        
        \\\[\\hat{v}\_t = \\frac{v\_t}{1 - \\beta\_2^t} \\\]
        
    3.  **参数更新**:
        
        \\\[\\theta\_{t+1} = \\theta\_t - \\frac{\\eta}{\\sqrt{\\hat{v}\_t} + \\epsilon} \\odot \\hat{m}\_t \\\]
        
    
    **含义**：Adam的更新规则可以直观理解为：**用带有动量的梯度 \\(\\hat{m}\_t\\) 作为更新方向，同时用类似RMSProp的 \\(\\sqrt{\\hat{v}\_t}\\) 来对每个参数的学习率进行自适应缩放**。偏差修正确保了在训练初期，矩估计也是无偏的。
    
*   **AdamW (Adam with Decoupled Weight Decay)**  
    在标准的L2正则化中，损失函数会增加一项 \\(\\frac{\\lambda}{2}||\\theta||^2\\)，其梯度为 \\(\\lambda\\theta\\)。在Adam中，这个正则项梯度会进入 \\(m\_t\\) 和 \\(v\_t\\) 的计算，使得权重衰减的效果与梯度的量级耦合在一起，导致对大学习率和较大权重的参数，实际的权重衰减效果会减弱。  
    AdamW 将权重衰减从梯度计算中解耦出来，直接在参数更新步骤中应用：
    
    1.  **执行标准的Adam更新 (不含正则项的梯度)**：
        
        \\\[\\theta'\_{t+1} = \\theta\_t - \\eta \\cdot \\text{AdamUpdate}(\\hat{m}\_t, \\hat{v}\_t) \\\]
        
    2.  **独立进行权重衰减**:
        
        \\\[\\theta\_{t+1} = \\theta'\_{t+1} - \\eta \\lambda \\theta\_t \\\]
        
    
    **含义**：权重衰减不再影响梯度的移动平均估计。它变成了一个独立的、对所有参数（通常除了Normalization层和Bias）施加的、与其梯度大小无关的“缩放”步骤，更符合其作为正则化手段的初衷。
    

### **3\. 适用场景与优缺点**

*   **优点**：
    *   结合了动量和自适应学习率的优点，收敛速度快，对超参数选择相对不敏感。
    *   AdamW的解耦权重衰减，在大型模型（如Transformers）上被证明能带来更好的泛化性能。
    *   已成为绝大多数现代深度学习任务的**默认首选优化器**。
*   **缺点**：
    *   有研究指出，在某些CV任务上，精调的SGD+Momentum最终的泛化性能可能略优于Adam。
    *   内存占用比SGD稍大（需要存储一阶和二阶矩）。

### **4\. 相对上一代的改进点**

*   **Adam 相对 RMSProp/动量**：首次将一阶矩（动量）和二阶矩（自适应学习率）的估计无缝结合，并引入偏差修正，形成了一个全面且强大的优化框架。
*   **AdamW 相对 Adam**：修正了权重衰减在自适应优化器中的实现方式，通过解耦使其正则化效果更稳定、更符合理论预期，尤其在大模型和需要强正则化的场景下表现更优。

* * *

**七、 总结与实践选型指南**
----------------

### **1\. 实践选型建议**

*   **追求最佳泛化性能的CV任务**：
    
    *   **首选 `SGD + Momentum`**。虽然调参更具挑战性，但配合精心设计的学习率衰减策略（如Cosine Annealing）和数据增强，往往能找到泛化能力更强的（更“平坦”的）最小值点。
*   **NLP、Transformer、大语言模型（LLM）**：
    
    *   **无悬念首选 `AdamW`**。这类模型对权重衰减的正确实现非常敏感，AdamW已成为社区和各大框架的绝对标准。通常配合 `Warmup + Cosine` 的学习率调度器。
*   **推荐系统（RecSys）**：
    
    *   这是一个混合场景。对于处理ID类稀疏特征的巨大 **Embedding 层**，**Adagrad**（或其变种如 FTRL）因其对稀疏特征的优异处理能力，仍然是工业界非常强的基线。
    *   对于处理稠密特征的 **DNN/Tower 部分**，**`AdamW`** 是最佳选择。
    *   因此，混合使用优化器（Hybrid Optimizer）是常见策略。
*   **强化学习（RL）、GANs、训练早期不稳定的任务**：
    
    *   **`Adam` 或 `RMSProp`** 都是很好的选择。它们的自适应特性有助于在动态和噪声环境中稳定训练过程。

### **2\. 核心参数参考**

*   **SGD+Momentum**: `momentum` (β) 通常设为 `0.9`。
*   **Adam/AdamW**: `betas` 通常设为 `(0.9, 0.999)`。对于训练非常长或梯度变化快的任务，可尝试减小 \\(\\beta\_2\\) (如 `0.98` 或 `0.95`) 以提高对近期梯度的响应速度。`eps` (ε) 通常为 `1e-8`。`weight_decay` (\\(\\lambda\\)) 是一个关键的正则化超参，需根据模型和任务进行调整（典型值范围 `1e-4` 到 `1e-1`）。

理解这些优化器的内在动机和演化路径，将使您在面对具体问题时，能够超越“默认参数”，做出更明智的决策，从而更高效地训练出性能优越的模型。

posted on 2025-10-15 16:45  [GRITJW](https://www.cnblogs.com/GlenTt)  阅读(61)  评论(0)    [收藏](javascript:void\(0\))  [举报](javascript:void\(0\))