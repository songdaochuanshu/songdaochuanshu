---
layout: post
title: '吴恩达深度学习课程二： 改善深层神经网络 第一周：深度学习的实践（三）'
date: "2025-10-31T00:41:25Z"
---
吴恩达深度学习课程二： 改善深层神经网络 第一周：深度学习的实践（三）
===================================

此分类用于记录吴恩达深度学习课程的学习笔记。  
课程相关信息链接如下：

1.  原课程视频链接：[\[双语字幕\]吴恩达深度学习deeplearning.ai](https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&from_spmid=playlist.playlist-detail.0.0&is_story_h5=false&mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1713085655&unique_k=DfBgvFW&up_id=8654113&vd_source=e035e9878d32f414b4354b839a4c31a4)
2.  github课程资料，含课件与笔记:[吴恩达深度学习教学资料](https://github.com/robbertliu/deeplearning.ai-andrewNG)
3.  课程配套练习（中英）与答案：[吴恩达深度学习课后习题与答案](https://blog.csdn.net/u013733326/article/details/79827273)

本篇为第二课第一周的内容，[1.6](https://www.bilibili.com/video/BV1FT4y1E74V/?p=52&spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=e035e9878d32f414b4354b839a4c31a4)和[1.7](https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&vd_source=e035e9878d32f414b4354b839a4c31a4&p=53)的内容。

* * *

本周为第二课的第一周内容，就像课题名称一样，本周更偏向于深度学习实践中出现的问题和概念，在有了第一课的机器学习和数学基础后，可以说，在理解上对本周的内容不会存在什么难度。

当然，我也会对一些新出现的概念补充一些基础内容来帮助理解，在有之前基础的情况下，按部就班即可对本周内容有较好的掌握。  
本篇继续上篇的内容，介绍dropout 正则化。

1\. dropout 正则化
===============

1.1 原理介绍
--------

Dropout（随机失活）是一种**在训练过程中随机“丢弃”部分神经元**的正则化方法。  
它的核心思想是：在每次训练迭代时，**随机让一部分神经元暂时不参与前向传播和反向传播**，从而防止网络过度依赖某些特定节点。

通俗地讲，就是在每次迭代时，会随机出现“**修路**”情况来关闭一些神经元，避免模型“**太喜欢某条常走的弯路**”，尝试多条路径，从而增强泛化能力，就像这样：  
![Pasted image 20251030100158](https://img2024.cnblogs.com/blog/3708248/202510/3708248-20251030112135517-574560709.png)

在数学上，设一个隐藏层的输出为 \\(A^{\[l\]}\\)，那么 Dropout 的过程可表示为：

\\\[D^{\[l\]} = \\text{rand}(A^{\[l\]}.shape) < keep\_{prob} \\\]

\\\[A^{\[l\]} = A^{\[l\]} \* D^{\[l\]} \\\]

\\\[A^{\[l\]} = \\frac{A^{\[l\]}}{keep\_{prob}} \\\]

其中：

*   \\(D^{\[l\]}\\) 是一个与 \\(A^{\[l\]}\\) 形状相同的随机掩码矩阵；
*   \\(keep\_{prob}\\) 是保留神经元的概率；
*   除以 \\(keep\_{prob}\\) 是为了保持整体激活期望一致（防止数值偏移）。

别慌，这几个公式堆在一起看起来确实挺吓人，我们同样**展开一些需要理解的内容**：

#### （1）什么叫随机掩码矩阵？

“随机掩码矩阵”其实就是一张**决定谁能“上场”的随机名单表**。  
在每一轮训练中，`rand(A.shape)` 会生成一个介于 0~1 的随机矩阵，如果某个位置的随机数小于 \\(keep\_{prob}\\) ，那对应的神经元就被“保留”，否则就被“屏蔽”。  
用一个实例来说明：假设我们有一个隐藏层输出：

\\\[A^{\[l\]} = \\begin{bmatrix} 0.2 & 0.6 & 0.4 & 0.9 \\end{bmatrix} \\\]

现在，我们设置保留概率 \\(keep\_{prob} = 0.5\\)，然后随机生成：

\\\[D^{\[l\]} = \\begin{bmatrix} 1 & 0 & 1 & 0 \\end{bmatrix} \\\]

于是更新后的输出就是：

\\\[A^{\[l\]} = A^{\[l\]} \* D^{\[l\]} = \[0.2, 0, 0.4, 0\] \\\]

这就表示——**在这一轮训练中，第 2 和第 4 个神经元被“临时关闭”**。  
它们既不会参与当前的前向传播，也不会计算梯度更新。  
换句话说，每一轮上场的神经元阵容都不同， 有时候 1、3 上，有时候 2、4 上，像在打轮换赛。

#### （2）保持整体激活期望一致是什么意思？

由于每次训练时有一部分神经元被“关掉”，如果不做任何处理，剩下神经元的**输出总量就会变小**。  
这会导致模型的数值分布发生**偏移**，训练和测试阶段的行为不一致。

这句话是什么意思？**什么叫偏移？怎么就不一致了？** 我们来详细解释一下：

在训练阶段，我们启用 Dropout——每一轮随机关闭一部分神经元；  
而在测试阶段，我们**不再丢弃神经元**，希望所有连接都参与计算。

因此，如果不做“除以 \\(keep\_{prob}\\)”的调整，**训练**时网络看到的**激活值较小**，而**测试**时所有神经元都激活，**信号强度会突然变大**。

打个比方，这就相当于：  
模型在训练时习惯了“音量 50%”，但一到测试就被拉成“音量 100%”， 结果预测结果可能大幅波动，这就是所谓的**分布偏移（distribution shift）**  
即同样的输入数据，在训练和测试时，网络的激活分布不一样，表现出不同的“行为模式”。

而调整的目的就是：**让训练时的信号强度和测试时一致**，这样模型在上场时才不会突然音量上升而不适应。

我们继续用上面的例子：  
原来的激活平均值为：

\\\[\\text{mean}(A^{\[l\]}) = \\frac{0.2 + 0.6 + 0.4 + 0.9}{4} = 0.525 \\\]

Dropout 之后（关掉一半神经元）：

\\\[A^{\[l\]} = \[0.2, 0, 0.4, 0\] \\Rightarrow \\text{mean} = 0.15 \\\]

平均值直接变小很多，这会让网络误以为“信号整体变弱”，从而影响学习。  
所以我们把输出除以保留概率：

\\\[A^{\[l\]} = \\frac{A^{\[l\]}}{keep\_{prob}} = \[0.4, 0, 0.8, 0\] \\\]

这时平均值恢复到：

\\\[\\text{mean} = 0.3 \\\]

虽然不完全相等，但数量级一致，**期望保持平衡**。

**换成人话就是： 虽然有一半神经元请假了，但留下来的要多干一倍活，这样团队输出不变。（难绷）**

#### （3）为什么这么“随机”的机制能起作用？

随机丢弃神经元，会让网络在每次训练中都看到一个**不同的子网络**。  
于是整个训练过程，就像在同时训练**一大群共享参数的小网络**。

最终，当我们在测试时把所有神经元都打开，网络的行为就相当于这些小网络预测结果的**集成平均**。  
因此，Dropout 能显著提升模型的稳健性，减少过拟合。  
**就像一个团队经过无数次不同组合的演练， 最终每个人都能独当一面，不再依赖特定搭档。**

总之，dropout正则化就是每次训练都会让网络“瘦身”，但每次瘦的部分不同。  
这样，网络学到的不是一条固定通路，而是**多条冗余且稳健的特征路径**。

1.2 “人话版总结”
-----------

可以把神经网络想成一张复杂的城市路网，每条“路”就是一条神经元之间的连接。  
在没有正则化时，模型总喜欢走几条特别顺畅的“老路”，久而久之就太依赖这些路线了。  
一旦测试阶段路况稍有不同（数据分布变化），模型就会懵，因为它**从来没学会走别的路**。  
而 Dropout 做的事，就是在每次训练时——  
**随机封几条路去维修，让模型被迫换条路走。**  
久而久之，模型就能适应多种交通方案，学会多条“通往目的地”的路径。  
等到测试阶段，所有道路都重新开放，模型就像整个城市的交通系统都训练有素：  
不管哪条路通，都能通向正确的结果。  
这也就是 Dropout 提升模型**泛化能力**的根本原因。

类型

内容

形象比喻

**优点**

1\. 有效防止过拟合，让模型不过度依赖特定神经元。2. 提高模型的鲁棒性（稳健性），相当于训练了多个“子网络”的集成效果。3. 在一定程度上还能起到特征选择作用，让网络更“均衡”地使用不同特征。

修路让模型学会多条路线，不怕某条主路堵车。

**缺点**

1\. 训练时间变长（因为每次激活模式不同，收敛更慢）。2. 不适合用于推理阶段，测试时必须关闭 Dropout。3. 如果 \\(keep\_{prob}\\) 过低，会导致模型学习信号太弱，出现欠拟合。

修太多路，车都走不动了；修太少路，又起不到练兵效果。

2\. 应用正则化和调节学习率的关系？
===================

在[上一篇](https://www.cnblogs.com/Goblinscholar/p/19174206)的结尾，我们提出了这样一个问题：应用正则化和直接调节学习率有什么不同呢？  
详细点说：  
**既然正则化最终是要影响参数的大小，那我是不是调一调学习率也能达到类似的效果？**  
要回答这个问题，我们先分别看看二者到底在干什么。

2.1 学习率：决定“走多快”
---------------

学习率 \\(\\alpha\\) 是梯度下降中最直观的参数。  
它控制着模型在参数空间中更新的**步伐大小**：

\\\[W := W - \\alpha , dW \\\]

*   学习率太大：模型可能“迈太大步”，直接越过最优点，甚至震荡发散。
*   学习率太小：模型每次只挪一点点，训练速度慢到令人发疯。  
    **通俗地讲：学习率决定你“往山谷底走的步子多大”**。  
    太大了会一脚踩空；太小了磨到天荒地老。

2.2 正则化：决定“走哪条路”
----------------

正则化并不是控制你“走得快不快”，而是**在梯度更新时施加一种施加一种特定的影响力**。  
以 L2 正则化为例，参数更新公式是：

\\\[W := W - \\alpha \\left(dW + \\frac{\\lambda}{m} W \\right) \\\]

可以看出：

*   \\(\\frac{\\lambda}{m}W\\) 这一项，会在每次更新时把权重往 0 拉一点；
*   它的目的不是减慢步伐，而是**修正方向**——让参数不至于“长歪”。

Dropout也是同理， 就像在训练道路上随机设置一些坑洞，让车手学会绕过，而不是死死踩同一条路，它并不减慢你的油门（学习率依然决定速度），而是防止模型走到“捷径陷阱”，学得太偏。

**通俗地说：学习率是油门，正则化是方向盘。**  
学习率太大，车容易冲出路，没有正则化，车会偏离中心线。  
这里我用GPT画了一张图，或许能帮助记忆：  
![Pasted image 20251030111929](https://img2024.cnblogs.com/blog/3708248/202510/3708248-20251030112209529-462739389.png)

2.3 总结
------

对比项

学习率调节

正则化（L2 / Dropout）

**目的**

控制参数更新的速度

控制参数更新的方向与幅度，使其不过大，或防止过拟合

**影响阶段**

优化器（梯度下降）

损失函数（多加一项惩罚）或训练策略（Dropout）

**公式体现**

\\(W := W - \\alpha dW\\)

\\(W := W - \\alpha (dW + \\frac{\\lambda}{m}W)\\) 或随机丢掉部分神经元

**形象比喻**

决定“走多快”

决定“往哪走”，Dropout 让车手学会绕开陷阱

**错误调节后果**

步子太大，震荡不收敛；太小，训练缓慢

惩罚太强，模型太简单（欠拟合）；太弱，模型太复杂（过拟合）；Dropout 太大，模型收敛慢

**交互影响**

学习率越大，惩罚效果越显著；两者需协调

一般配合调节，防止权重过大或收敛太慢

**总之，学习率与正则化是互补的，而不是替代关系。**  
一个决定“快慢”，一个决定“方向和稳健性”；  
只有搭配得当，模型才能**又快又稳地收敛到合理的最优点**。

对正则化的介绍暂时就到此为止，下一篇会简要介绍一些其他帮助缓解过拟合的方法，说实话，有些方法甚至会给我们一种“耍小聪明”的感觉。