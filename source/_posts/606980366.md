---
layout: post
title: "Hadoop（五）C#操作Hive"
date: "2022-05-04T06:25:13.470Z"
---
Hadoop（五）C#操作Hive
=================

Hive
----

Hive将HiveQL（类sql语言）_**转为MapReduce**_，完成数据的查询与分析，减少了编写MapReduce的复杂度。它有以下优点：

*   学习成本低：熟悉sql就能使用
*   良好的数据分析：底层基于MapReduce实现

同样存在一些缺点：

*   HiveDL表达能力有限
*   效率不高
*   Hive调优比较困难

Hive架构
------

1.  用户通过Hive的用户接口（User Interfaces）与hive交互，常见的用户接口有CLI，JDBC/ODBC，WEB UI等
2.  Hive将元数据存在Meta Store中，元数据包括数据库、表、列、类型、数据所在目录等
3.  HiveQL Process Engine实现HiveQL的语法分析、优化生成对应的查询计划，存于HDFS中。
4.  由Execution Engine实现HiveQL Process Engine与MapReduce的结合。最终实现对HDFS中数据的处理。

![](https://img2022.cnblogs.com/blog/1033233/202205/1033233-20220503162433885-817395860.png)

Hive工作流程
--------

![](https://img2022.cnblogs.com/blog/1033233/202205/1033233-20220503163317302-250859426.png)

1.  Execute Query：Hive接口，如命令行或Web UI发送查询驱动程序（任何数据库驱动程序，如JDBC，ODBC等）来执行。
2.  Get Plan： 在驱动程序帮助下查询编译器，分析查询检查语法和查询计划或查询的要求。
3.  Get MetaData：编译器发送元数据请求到Metastore（任何数据库）。
4.  Send MetaData：Metastore发送元数据，以编译器的响应。
5.  Send Plan：编译器检查要求，并重新发送计划给驱动程序。到此为止，查询解析和编译完成。
6.  Excute Plan：驱动程序发送的执行计划到执行引擎。
7.  Excute Job：执行作业的过程是一个MapReduce工作。执行引擎发送作业给JobTracker，在名称节点并把它分配作业到TaskTracke。
    1.  MetaData Ops：在执行时，执行引擎可以通过Metastore执行元数据操作。
8.  Fetch Results：执行引擎接收来自数据节点的结果。
9.  Send Results：执行引擎发送这些结果值给驱动程序。
10.  Send Results：驱动程序将结果发送给Hive接口。

Hive安装
------

1.下载Hive3.1.2

[https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.2/](https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.2/ "https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.2/")

2.解压

tar -zxvf apache-hive-3.1.2\-bin.tar.gz

3.配置环境变量

vi /etc/profile

export HIVE\_HOME\=/usr/local/hive312/apache-hive-3.1.2\-bin
export HIVE\_CONF\_DIR\=/usr/local/hive312/apache-hive-3.1.2\-bin/conf

#生效
source /etc/profile

4.将mysql作为metastore，下载mysql-connetctor

[https://mvnrepository.com/artifact/mysql/mysql-connector-java/5.1.46](https://mvnrepository.com/artifact/mysql/mysql-connector-java/5.1.46 "https://mvnrepository.com/artifact/mysql/mysql-connector-java/5.1.46")

将其放入$HIVE\_HOME/lib文件夹中

5.配置conf/hive-env.sh

cd conf  
mv hive-env.sh.template hive-env.sh

6.将以下内容加入hive-env.sh

export JAVA\_HOME=/usr/local/java18/jdk1.8.0\_331
export HADOOP\_HOME\=/usr/local/hadoop323/hadoop-3.2.3
export HIVE\_HOME\=/usr/local/hive312/apache-hive-3.1.2\-bin
export HIVE\_CONF\_DIR\=/usr/local/hive312/apache-hive-3.1.2\-bin/conf

7.添加conf/hive-site.xml文件

<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration\>
  <property\>
    <name\>javax.jdo.option.ConnectionURL</name\>
    <value\>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value\>
    <description\>JDBC connect string for a JDBC metastore</description\>
  </property\>
  <property\>
    <name\>javax.jdo.option.ConnectionDriverName</name\>
    <value\>com.mysql.jdbc.Driver</value\>
    <description\>Driver class name for a JDBC metastore</description\>
  </property\>
  <property\>
    <name\>javax.jdo.option.ConnectionUserName</name\>
    <value\>root</value\>
    <description\>username to use against metastore database</description\>
  </property\>
  <property\>
    <name\>javax.jdo.option.ConnectionPassword</name\>
    <value\>admin</value\>
    <description\>password to use against metastore database</description\>
  </property\>
</configuration\>

8.替换guava文件

因为hadoop/share/hadoop/common/lib目录下的guava和/apache-hive-3.1.2-bin/lib目录下的guava版本不同。需要将版本将hadoop高版本的guava拷贝到hive的目录下，删除hive低的版本。

cp /usr/local/hadoop323/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0\-jre.jar /usr/local/hive312/apache-hive-3.1.2\-bin/lib/

rm -rf /usr/local/hive312/apache-hive-3.1.2\-bin/lib/guava-19.0.jar

9.初始化metastore

./bin/schematool -dbType mysql -initSchema

10.使用Hive

bin/hive

这种方式默认启动了cli，相当于以下命令

bin/hive --service cli

11.查看表

hive> show tables;

12.启动Hive Thrift Server

bin/hive --service hiveserver2 &

通过jps验证RunJar是否启动

![](https://img2022.cnblogs.com/blog/1033233/202205/1033233-20220503171435676-91139134.png)

 也可以查看10000端口是否处于监听状态

\[root@localhost apache-hive-3.1.2\-bin\]# netstat -anop |grep 10000
tcp6       0      0 :::10000                :::\*                    LISTEN      12207/java           off (0.00/0/0)

Hive操作
------

Hive是一种数据库，可以定义数据库与表来结构化数据。

[https://cwiki.apache.org/confluence/display/Hive//Home#Home-UserDocumentation](https://cwiki.apache.org/confluence/display/Hive//Home#Home-UserDocumentation "https://cwiki.apache.org/confluence/display/Hive//Home#Home-UserDocumentation")

### 数据库操作

Hive中默认存在一个default的数据库，默认的操作都应用在这个库上，可以通过bin/hive  这个cli命令查看。

hive> show databases;
OK
default
Time taken: 0.075 seconds, Fetched: 1 row(s)

#### 新建数据库

#新建数据库
CREATE DATABASE|SCHEMA \[IF NOT EXISTS\] <database name>

新建一个chesterdb

create database chesterdb;

#### 删除数据库

DROP DATABASE StatementDROP (DATABASE|SCHEMA) \[IF EXISTS\] database\_name  \[RESTRICT|CASCADE\];

CASCADE代表在删除数据库之前要把其中的表删除。

删除chesterdb

hive> drop database chesterdb;

### 表操作

#### 新建表

CREATE \[TEMPORARY\] \[EXTERNAL\] TABLE \[IF NOT EXISTS\] \[db\_name.\] table\_name

\[(col\_name data\_type \[COMMENT col\_comment\], ...)\]
\[COMMENT table\_comment\]
\[ROW FORMAT row\_format\]
\[STORED AS file\_format\]

在defaultdb中新建一个表

 CREATE TABLE IF NOT EXISTS employee ( eid int, name String, salary String, destination String)
 COMMENT 'Employee details'
 ROW FORMAT DELIMITED
 FIELDS TERMINATED BY '\\t'
 LINES TERMINATED BY '\\n'
 STORED AS TEXTFILE;

#### 插入本地文件数据

新建一个e.txt文件

\[root@localhost ~\]# cat e.txt
1201    Gopal   45000   Technical manager
1202    Manisha 45000   Proof reader

将其load进入employee表

LOAD DATA LOCAL INPATH '/root/e.txt' OVERWRITE INTO TABLE employee;

查看employee中是否存在数据

select \* from employee;

#### 插入HDFS文件数据

load hdfs中的文件到hive，新建一个e2.txt

\[root@localhost ~\]# cat e2.txt
1203    Gopal1  45000   Technical manager
1204    Manisha1        45000   Proof reader

将其put到hdfs中的chesterdata文件夹

\[root@localhost ~\]# hdfs dfs -mkdir /chesterdata
\[root@localhost ~\]# hdfs dfs -put e2.txt /chesterdata

然后在hive的cli中将hdfs中的/chesterdata/e2.txt load到employee表，需要注意的是，load之后hdfs中的文件就会被删除

load data inpath '/chesterdata/e2.txt'  OVERWRITE INTO TABLE employee;

查看employee中是否存在数据

select \* from employee;

#### 删除表

DROP TABLE \[IF EXISTS\] table\_name;

### 分区

#### 新建分区

Hive可以通过分区实现数据的隔离，这样可实现数据的快速查询。

 CREATE TABLE IF NOT EXISTS employee2 ( eid int, name String, salary String)
 COMMENT 'Employee details'
 PARTITIONED BY (destination String)
 ROW FORMAT DELIMITED
 FIELDS TERMINATED BY '\\t'
 LINES TERMINATED BY '\\n'
 STORED AS TEXTFILE;

我们将e2.txt load进此表

hdfs dfs -put e2.txt /chesterdata

load data inpath '/chesterdata/e2.txt'  OVERWRITE INTO TABLE employee2;

#### 重命名分区

ALTER TABLE employee2 PARTITION (destination='Technical manager') RENAME TO PARTITION (destination='T2');

#### 删除分区

ALTER TABLE employee2 DROP  PARTITION (destination='T2');

### 查询

hive的查询支持我们常见的where、order by、group by、join等语句

select Name,count(1) from employee2 where destination='Technical manager' group by Name;

### 视图

hive也支持视图，创建视图语法如下

CREATE VIEW emp\_v2 AS
select Name,count(1) from employee2 where destination='Technical manager' group by Name;

查看视图

select \* from emp\_v2;

C#如何连接Hive
----------

可以通过odbc来连接。

1.首先需要配置hadoop，从任何主机登录的root用户可以作为任意组的用户

<property>
    <name>hadoop.proxyuser.root.hosts</name>
    <value>\*</value>
</property>
<property>
    <name>hadoop.proxyuser.root.groups</name>
    <value>\*</value>
</property>

重启hadoop

sbin/stop-dfs.sh
sbin/stop-yarn.sh

sbin/start-dfs.sh
sbin/start-yarn.sh

2.启动hiveserver2 thrift server，其默认端口为10000

bin/hive --service hiveserver2 &

可通过10002端口验证是否thrift server启动

![](https://img2022.cnblogs.com/blog/1033233/202205/1033233-20220503224249198-1321833684.png)

3.下载odbc,并安装（同样有linux版本）

[http://package.mapr.com/tools/MapR-ODBC/MapR\_Hive/MapRHive\_odbc\_2.1.1.0013/Windows/](http://package.mapr.com/tools/MapR-ODBC/MapR_Hive/MapRHive_odbc_2.1.1.0013/Windows/ "http://package.mapr.com/tools/MapR-ODBC/MapR_Hive/MapRHive_odbc_2.1.1.0013/Windows/")

4.打开odbc，添加dsn

![](https://img2022.cnblogs.com/blog/1033233/202205/1033233-20220503223352629-488378673.png)

 5.新建console，并添加hive访问类

using System.Data;
using System.Data.Odbc;

public class HiveOdbcClient
{

    public static HiveOdbcClient Current
    {
        get { return new HiveOdbcClient(); }
    }
    public void ExcuteNoQuery(string dns, string sql)
    {
        OdbcConnection conn \= null;

        try
        {
            conn \= new OdbcConnection(dns);
            conn.Open();
            OdbcCommand cmd \= new OdbcCommand(sql, conn);
            cmd.ExecuteNonQuery();
        }
        catch (Exception ex)
        {
            throw ex;
        }
        finally
        {
            if (null != conn)
            {
                conn.Close();
            }
        }
    }


    public DataSet Query(string dns, string sql, string tblName = "tbl")
    {
        DataSet set = new DataSet();
        OdbcConnection conn \= null;

        try
        {
            conn \= new OdbcConnection(dns);
            conn.Open();
            OdbcCommand cmd \= conn.CreateCommand();
            cmd.CommandText \= sql;
            OdbcDataAdapter adapter \= new OdbcDataAdapter(cmd);
            adapter.Fill(set, tblName);
        }
        catch (Exception ex)
        {
            throw ex;
        }
        finally
        {
            if (null != conn)
            {
                conn.Close();
            }
        }

        return set;
    }
}

通过C#直接新加hive表

string dns = "DSN=test;UID=root;PWD=";

string sql = "show tables";

string sql2 = "create table Employee(ID string,Code string,Name string)";
HiveOdbcClient.Current.ExcuteNoQuery(dns, sql2);

Console.WriteLine(HiveOdbcClient.Current.Query(dns, sql));

6.通过bin/hive进入交互式命令，查看employee新建成功

hive> show tables;
OK
employee
Time taken: 0.62 seconds, Fetched: 1 row(s)