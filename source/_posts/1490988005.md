---
layout: post
title: "å¤„ç†æ–‡æœ¬æ•°æ®ï¼ˆä¸Šï¼‰:è¯è¢‹"
date: "2022-06-03T08:22:16.400Z"
---
å¤„ç†æ–‡æœ¬æ•°æ®ï¼ˆä¸Šï¼‰:è¯è¢‹
============

æˆ‘ä»¬è®¨è®ºè¿‡è¡¨ç¤ºæ•°æ®å±æ€§çš„ä¸¤ç§ç±»å‹çš„ç‰¹å¾ï¼šè¿ç»­ç‰¹å¾ä¸åˆ†ç±»ç‰¹å¾ï¼Œå‰è€…ç”¨äºæè¿°æ•°é‡ï¼Œåè€…æ˜¯å›ºå®šåˆ—è¡¨ä¸­çš„å…ƒç´ ã€‚  
ç¬¬ä¸‰ç§ç±»å‹çš„ç‰¹å¾ï¼šæ–‡æœ¬

*   æ–‡æœ¬æ•°æ®é€šå¸¸è¢«è¡¨ç¤ºä¸ºç”±å­—ç¬¦ç»„æˆçš„å­—ç¬¦ä¸²ã€‚

1ã€ç”¨å­—ç¬¦ä¸²è¡¨ç¤ºçš„æ•°æ®ç±»å‹
-------------

æ–‡æœ¬é€šå¸¸åªæ˜¯æ•°æ®é›†ä¸­çš„å­—ç¬¦ä¸²ï¼Œä½†å¹¶éæ‰€æœ‰çš„å­—ç¬¦ä¸²ç‰¹å¾éƒ½åº”è¯¥è¢«å½“ä½œæ–‡æœ¬æ¥å¤„ç†ã€‚

å­—ç¬¦ä¸²ç‰¹å¾æœ‰æ—¶å¯ä»¥è¡¨ç¤ºåˆ†ç±»å˜é‡ã€‚åœ¨æŸ¥çœ‹æ•°æ®ä¹‹å‰ï¼Œæˆ‘ä»¬æ— æ³•çŸ¥é“å¦‚ä½•å¤„ç†ä¸€ä¸ªå­—ç¬¦ä¸²ç‰¹å¾ã€‚

â­å››ç§ç±»å‹çš„å­—ç¬¦ä¸²æ•°æ®ï¼š

*   1ã€åˆ†ç±»æ•°æ®
    
    *   åˆ†ç±»æ•°æ®ï¼ˆcategorical dataï¼‰æ˜¯æ¥è‡ªå›ºå®šåˆ—è¡¨çš„æ•°æ®ã€‚
*   2ã€å¯ä»¥åœ¨è¯­ä¹‰ä¸Šæ˜ å°„ä¸ºç±»åˆ«çš„è‡ªç”±å­—ç¬¦ä¸²
    
    *   ä½ å‘ç”¨æˆ·æä¾›çš„ä¸æ˜¯ä¸€ä¸ªä¸‹æ‹‰èœå•ï¼Œè€Œæ˜¯ä¸€ä¸ªæ–‡æœ¬æ¡†ï¼Œè®©ä»–ä»¬å¡«å†™è‡ªå·±æœ€å–œæ¬¢çš„é¢œè‰²ã€‚
    *   è®¸å¤šäººçš„å›ç­”å¯èƒ½æ˜¯åƒ â€œé»‘è‰²â€ æˆ– â€œè“è‰²â€ ä¹‹ç±»çš„é¢œè‰²åç§°ã€‚å…¶ä»–äººå¯èƒ½ä¼šå‡ºç°ç¬”è¯¯ï¼Œä½¿ç”¨ä¸åŒçš„å•è¯æ‹¼å†™ï¼ˆæ¯”å¦‚ â€œgrayâ€ å’Œ â€œgreyâ€ ï¼‰ï¼Œæˆ–ä½¿ç”¨æ›´åŠ å½¢è±¡çš„å…·ä½“åç§° ï¼ˆæ¯”å¦‚ â€œåˆå¤œè“è‰²â€ï¼‰ã€‚
    *   å¯èƒ½æœ€å¥½å°†è¿™ç§æ•°æ®ç¼–ç ä¸ºåˆ†ç±»å˜é‡ï¼Œä½ å¯ä»¥åˆ©ç”¨æœ€å¸¸è§çš„æ¡ç›®æ¥é€‰æ‹©ç±»åˆ«ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰ç±»åˆ«ï¼Œä½¿ç”¨æˆ·å›ç­”å¯¹åº”ç”¨æœ‰æ„ä¹‰ã€‚
*   3ã€ç»“æ„åŒ–å­—ç¬¦ä¸²æ•°æ®
    
    *   æ‰‹åŠ¨è¾“å…¥å€¼ä¸ä¸å›ºå®šçš„ç±»åˆ«å¯¹åº”ï¼Œä½†ä»æœ‰ä¸€äº›å†…åœ¨çš„ç»“æ„ï¼ˆstructureï¼‰ï¼Œæ¯”å¦‚åœ°å€ã€äººåæˆ–åœ°åã€æ—¥æœŸã€ç”µè¯å·ç æˆ–å…¶ä»–æ ‡è¯†ç¬¦ã€‚
*   4ã€æ–‡æœ¬æ•°æ®
    
    *   ä¾‹å­åŒ…æ‹¬æ¨æ–‡ã€èŠå¤©è®°å½•å’Œé…’åº—è¯„è®ºï¼Œè¿˜åŒ…æ‹¬èå£«æ¯”äºšæ–‡é›†ã€ç»´åŸºç™¾ç§‘çš„å†…å®¹æˆ–å¤è…¾å ¡è®¡åˆ’æ”¶é›†çš„ 50 000 æœ¬ç”µå­ä¹¦ã€‚æ‰€æœ‰è¿™äº›é›†åˆåŒ…å«çš„ä¿¡æ¯å¤§å¤šæ˜¯ç”±å•è¯ç»„æˆçš„å¥å­ã€‚

2ã€ç¤ºä¾‹åº”ç”¨ï¼šç”µå½±è¯„è®ºçš„æƒ…æ„Ÿåˆ†æ
----------------

ä½œä¸ºæœ¬ç« çš„ä¸€ä¸ªè¿è¡Œç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç”±æ–¯å¦ç¦ç ”ç©¶å‘˜ Andrew Maas æ”¶é›†çš„ IMDb ï¼ˆInternet Movie Databaseï¼Œäº’è”ç½‘ç”µå½±æ•°æ®åº“ï¼‰ç½‘ç«™çš„ç”µå½±è¯„è®ºæ•°æ®é›†ã€‚

æ•°æ®é›†é“¾æ¥ï¼š[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)

è¿™ä¸ªæ•°æ®é›†åŒ…å«è¯„è®ºæ–‡æœ¬ï¼Œè¿˜æœ‰ä¸€ä¸ªæ ‡ç­¾ï¼Œç”¨äºè¡¨ç¤ºè¯¥è¯„è®ºæ˜¯ â€œæ­£é¢çš„â€ï¼ˆpositiveï¼‰è¿˜æ˜¯ â€œè´Ÿé¢çš„â€ ï¼ˆnegativeï¼‰ã€‚

IMDb ç½‘ç«™æœ¬èº«åŒ…å«ä» 1 åˆ° 10 çš„æ‰“åˆ†ã€‚ä¸ºäº†ç®€åŒ–å»ºæ¨¡ï¼Œè¿™äº›è¯„è®ºæ‰“åˆ†è¢«å½’çº³ä¸ºä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†ï¼Œè¯„åˆ†å¤§äºç­‰äº 7 çš„è¯„è®ºè¢«æ ‡è®°ä¸º â€œæ­£é¢çš„â€ï¼Œè¯„åˆ†å°äºç­‰äº 4 çš„è¯„è®ºè¢«æ ‡è®°ä¸º â€œè´Ÿé¢çš„â€ï¼Œä¸­æ€§è¯„è®ºæ²¡æœ‰åŒ…å«åœ¨æ•°æ®é›†ä¸­ã€‚

å°†æ•°æ®è§£å‹ä¹‹åï¼Œæ•°æ®é›†åŒ…æ‹¬ä¸¤ä¸ªç‹¬ç«‹æ–‡ä»¶å¤¹ä¸­çš„æ–‡æœ¬æ–‡ä»¶ï¼Œä¸€ä¸ªæ˜¯è®­ç»ƒæ•°æ®ï¼Œä¸€ä¸ªæ˜¯æµ‹è¯•æ•°æ®ã€‚æ¯ä¸ªæ–‡ä»¶å¤¹åˆéƒ½æœ‰ä¸¤ä¸ªå­æ–‡ä»¶å¤¹ï¼Œä¸€ä¸ªå«ä½œ posï¼Œä¸€ä¸ªå«ä½œ negã€‚

pos æ–‡ä»¶å¤¹åŒ…å«æ‰€æœ‰æ­£é¢çš„è¯„è®ºï¼Œæ¯æ¡è¯„è®ºéƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„æ–‡æœ¬æ–‡ä»¶ï¼Œneg æ–‡ä»¶å¤¹ä¸ä¹‹ç±»ä¼¼ã€‚scikit-learn ä¸­æœ‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°å¯ä»¥åŠ è½½ç”¨è¿™ç§æ–‡ä»¶å¤¹ç»“æ„ä¿å­˜çš„æ–‡ä»¶ï¼Œå…¶ä¸­æ¯ä¸ªå­æ–‡ä»¶å¤¹å¯¹åº”äºä¸€ä¸ªæ ‡ç­¾ï¼Œè¿™ä¸ªå‡½æ•°å«ä½œ load\_filesã€‚æˆ‘ä»¬é¦–å…ˆå°† load\_files å‡½æ•°åº”ç”¨äºè®­ç»ƒæ•°æ®ï¼š

      from sklearn.datasets import load_files
      from sklearn.model_selection import train_test_split
    
    
      reviews_train = load_files("../../datasets/aclImdb/train/")
      # load_files è¿”å›ä¸€ä¸ª Bunch å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«è®­ç»ƒæ–‡æœ¬å’Œè®­ç»ƒæ ‡ç­¾
    
      #åŠ è½½æ•°æ®
      text_train,y_train = reviews_train.data,reviews_train.target
    
      #æŸ¥çœ‹æ•°æ®
      print("type of text_train: {}".format(type(text_train)))
      print("length of text_train: {}".format(len(text_train)))
      print("text_train[6]:\n{}".format(text_train[6]))
    
      '''
      ```
      type of text_train: <class 'list'>
      length of text_train: 25000
      text_train[6]:
      b"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending."
      ```
      '''
    

ğŸ“£  
ä½ å¯ä»¥çœ‹åˆ°ï¼Œtext\_train æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º 25 000 çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯åŒ…å«ä¸€æ¡è¯„è®ºçš„å­—ç¬¦ä¸²ã€‚æˆ‘ä»¬æ‰“å°å‡ºç´¢å¼•ç¼–å·ä¸º 1 çš„è¯„è®ºã€‚ä½ è¿˜å¯ä»¥çœ‹åˆ°ï¼Œè¯„è®ºä¸­åŒ…å«ä¸€äº› HTML æ¢è¡Œç¬¦ã€‚è™½ç„¶è¿™äº›ç¬¦å·ä¸å¤ªå¯èƒ½å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹äº§ç”Ÿå¾ˆå¤§å½±å“ï¼Œä½†æœ€å¥½åœ¨ç»§ç»­ä¸‹ä¸€æ­¥ä¹‹å‰æ¸…æ´—æ•°æ®å¹¶åˆ é™¤è¿™ç§æ ¼å¼ï¼š

      import numpy as np
    
      text_train = [doc.replace(b"<br />", b" ") for doc in text_train]
      #æ”¶é›†æ•°æ®é›†æ—¶ä¿æŒæ­£ç±»å’Œåç±»çš„å¹³è¡¡ï¼Œè¿™æ ·æ‰€æœ‰æ­£é¢å­—ç¬¦ä¸²å’Œè´Ÿé¢å­—ç¬¦ä¸²çš„æ•°é‡ç›¸ç­‰ï¼š
      print("Samples per class (training): {}".format(np.bincount(y_train)))
    
      #æˆ‘ä»¬ç”¨åŒæ ·çš„æ–¹å¼åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼š
      reviews_test = load_files("../../datasets/aclImdb/test/")
      text_test, y_test = reviews_test.data, reviews_test.target
    
      print("Number of documents in test data: {}".format(len(text_test)))
      print("Samples per class (test): {}".format(np.bincount(y_test)))
    
      text_test = [doc.replace(b"<br />", b" ") for doc in text_test]
    
      '''
      ```
      Samples per class (training): [12500 12500]
      Number of documents in test data: 25000
      Samples per class (test): [12500 12500]
      ```
      '''
    

æˆ‘ä»¬è¦è§£å†³çš„ä»»åŠ¡å¦‚ä¸‹ï¼šç»™å®šä¸€æ¡è¯„è®ºï¼Œæˆ‘ä»¬å¸Œæœ›æ ¹æ®è¯¥è¯„è®ºçš„æ–‡æœ¬å†…å®¹å¯¹å…¶åˆ†é…ä¸€ä¸ª â€œæ­£é¢çš„â€ æˆ– â€œè´Ÿé¢çš„â€ æ ‡ç­¾ã€‚  
è¿™æ˜¯ä¸€é¡¹æ ‡å‡†çš„äºŒåˆ†ç±»ä»»åŠ¡ã€‚  
ä½†æ˜¯ï¼Œæ–‡æœ¬æ•°æ®å¹¶ä¸æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥å¤„ç†çš„æ ¼å¼ã€‚  
æˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬çš„å­—ç¬¦ä¸²è¡¨ç¤ºè½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºï¼Œä»è€Œå¯ä»¥å¯¹å…¶åº”ç”¨æœºå™¨å­¦ä¹ ç®—æ³•ã€‚

3ã€å°†æ–‡æœ¬æ•°æ®è¡¨ç¤ºä¸ºè¯è¢‹
------------

ç”¨äºæœºå™¨å­¦ä¹ çš„æ–‡æœ¬è¡¨ç¤ºæœ‰ä¸€ç§æœ€ç®€å•çš„æ–¹æ³•ï¼Œä¹Ÿæ˜¯æœ€æœ‰æ•ˆä¸”æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œå°±æ˜¯ä½¿ç”¨è¯è¢‹ï¼ˆbag-of-wordsï¼‰è¡¨ç¤ºã€‚

*   ä½¿ç”¨è¿™ç§è¡¨ç¤ºæ–¹å¼æ—¶ï¼Œæˆ‘ä»¬èˆå¼ƒäº†è¾“å…¥æ–‡æœ¬ä¸­çš„å¤§éƒ¨åˆ†ç»“æ„ï¼Œå¦‚ç« èŠ‚ã€æ®µè½ã€å¥å­å’Œæ ¼å¼ï¼Œåªè®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯åœ¨æ¯ä¸ªæ–‡æœ¬ä¸­çš„å‡ºç°é¢‘æ¬¡ã€‚
*   èˆå¼ƒç»“æ„å¹¶ä»…è®¡ç®—å•è¯å‡ºç°æ¬¡æ•°ï¼Œè¿™ä¼šè®©è„‘æµ·ä¸­å‡ºç°å°†æ–‡æœ¬è¡¨ç¤ºä¸º â€œè¢‹â€ çš„ç”»é¢

å¯¹äºæ–‡æ¡£è¯­æ–™åº“ï¼Œè®¡ç®—è¯è¢‹è¡¨ç¤ºåŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ã€‚

*   ï¼ˆ1ï¼‰åˆ†è¯ï¼ˆtokenizationï¼‰ã€‚
    *   å°†æ¯ä¸ªæ–‡æ¡£åˆ’åˆ†ä¸ºå‡ºç°åœ¨å…¶ä¸­çš„å•è¯ï¼Œæ¯”å¦‚æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ’åˆ†ã€‚
*   ï¼ˆ2ï¼‰æ„å»ºè¯è¡¨ï¼ˆvocabulary buildingï¼‰ã€‚
    *   æ”¶é›†ä¸€ä¸ªè¯è¡¨ï¼Œé‡Œé¢åŒ…å«å‡ºç°åœ¨ä»»æ„æ–‡æ¡£ä¸­çš„æ‰€æœ‰è¯ï¼Œ å¹¶å¯¹å®ƒä»¬è¿›è¡Œç¼–å·ï¼ˆæ¯”å¦‚æŒ‰å­—æ¯é¡ºåºæ’åºï¼‰ã€‚
*   ï¼ˆ3ï¼‰ç¼–ç ï¼ˆencodingï¼‰ã€‚
    *   å¯¹äºæ¯ä¸ªæ–‡æ¡£ï¼Œè®¡ç®—è¯è¡¨ä¸­æ¯ä¸ªå•è¯åœ¨è¯¥æ–‡æ¡£ä¸­çš„å‡ºç°é¢‘æ¬¡ã€‚

### 3.1ã€å°†è¯è¢‹åº”ç”¨äºç©å…·æ•°æ®é›†

è¯è¢‹è¡¨ç¤ºæ˜¯åœ¨ CountVectorizer ä¸­å®ç°çš„ï¼Œå®ƒæ˜¯ä¸€ä¸ªå˜æ¢å™¨ï¼ˆtransformerï¼‰ã€‚

æˆ‘ä»¬é¦–å…ˆå°†å®ƒåº”ç”¨äºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ ·æœ¬çš„ç©å…·æ•°æ®é›†ï¼Œæ¥çœ‹ä¸€ä¸‹å®ƒçš„å·¥ä½œåŸç†ï¼š

      bards_words =["The fool doth think he is wise,",
                    "but the wise man knows himself to be a fool"]
    
      #å¯¼å…¥CountVectorizerå¹¶å°†å…¶å®ä¾‹åŒ–
      from sklearn.feature_extraction.text import CountVectorizer
    
      vect = CountVectorizer()
      vect.fit(bards_words)
    
      #æ‹Ÿåˆ CountVectorizer åŒ…æ‹¬è®­ç»ƒæ•°æ®çš„åˆ†è¯ä¸è¯è¡¨çš„æ„å»ºï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ vocabulary_ å±æ€§æ¥è®¿é—®è¯è¡¨ï¼š
    
      print("Vocabulary size: {}".format(len(vect.vocabulary_)))
      print("Vocabulary content:\n {}".format(vect.vocabulary_))
    
      '''
      ```
      Vocabulary size: 13
      Vocabulary content:
       {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}
      ```
      '''
    

  

      #æˆ‘ä»¬å¯ä»¥è°ƒç”¨ transform æ–¹æ³•æ¥åˆ›å»ºè®­ç»ƒæ•°æ®çš„è¯è¢‹è¡¨ç¤ºï¼š
    
      bag_of_words = vect.transform(bards_words)
      print("bag_of_words:{}".format(repr(bag_of_words)))
    
      '''
      ```
      bag_of_words:<2x13 sparse matrix of type '<class 'numpy.int64'>'
      	with 16 stored elements in Compressed Sparse Row format>
      ```
      '''
    

ğŸ“£

è¯è¢‹è¡¨ç¤ºä¿å­˜åœ¨ä¸€ä¸ª SciPy ç¨€ç–çŸ©é˜µä¸­ï¼Œè¿™ç§æ•°æ®æ ¼å¼åªä¿å­˜éé›¶å…ƒç´ ã€‚

è¿™ä¸ªçŸ©é˜µçš„å½¢çŠ¶ä¸º 2Ã—13ï¼Œæ¯è¡Œå¯¹åº”äºä¸¤ä¸ªæ•°æ®ç‚¹ä¹‹ä¸€ï¼Œæ¯ä¸ªç‰¹å¾å¯¹åº”äºè¯è¡¨ä¸­çš„ä¸€ä¸ªå•è¯ã€‚  
è¿™é‡Œä½¿ç”¨ç¨€ç–çŸ©é˜µï¼Œæ˜¯å› ä¸ºå¤§å¤šæ•°æ–‡æ¡£éƒ½åªåŒ…å«è¯è¡¨ä¸­çš„ä¸€å°éƒ¨åˆ†å•è¯ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç‰¹å¾æ•°ç»„ä¸­çš„å¤§éƒ¨åˆ†å…ƒç´ éƒ½ä¸º 0ã€‚  
æƒ³æƒ³çœ‹ï¼Œä¸æ‰€æœ‰è‹±è¯­å•è¯ï¼ˆè¿™æ˜¯è¯è¡¨çš„å»ºæ¨¡å¯¹è±¡ï¼‰ç›¸æ¯”ï¼Œä¸€ç¯‡ç”µå½±è¯„è®ºä¸­å¯èƒ½å‡ºç°å¤šå°‘ä¸ªä¸åŒçš„å•è¯ã€‚  
ä¿å­˜æ‰€æœ‰ 0 çš„ä»£ä»·å¾ˆé«˜ï¼Œä¹Ÿæµªè´¹å†…å­˜ã€‚

### 3.2ã€å°†è¯è¢‹åº”ç”¨äºç”µå½±è¯„è®º

ä¸Šä¸€èŠ‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†è¯è¢‹å¤„ç†è¿‡ç¨‹ï¼Œä¸‹é¢æˆ‘ä»¬å°†å…¶åº”ç”¨äºç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æçš„ä»»åŠ¡ã€‚  
å‰é¢æˆ‘ä»¬å°† IMDb è¯„è®ºçš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®åŠ è½½ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆtext\_train å’Œ text\_ testï¼‰ï¼Œç°åœ¨æˆ‘ä»¬å°†å¤„ç†å®ƒä»¬

      vect = CountVectorizer().fit(text_train)
      X_train = vect.transform(text_train)
      print("X_train:\n{}".format(repr(X_train)))
    
      '''
      ```
      X_train:
      <25000x74849 sparse matrix of type '<class 'numpy.int64'>'
      	with 3431196 stored elements in Compressed Sparse Row format>
      ```
      '''
    

X\_train æ˜¯è®­ç»ƒæ•°æ®çš„è¯è¢‹è¡¨ç¤ºï¼Œå…¶å½¢çŠ¶ä¸º 25 000Ã—74 849ï¼Œè¿™è¡¨ç¤ºè¯è¡¨ä¸­åŒ…å« 74 849 ä¸ª å…ƒç´ ã€‚æ•°æ®åŒæ ·è¢«ä¿å­˜ä¸º SciPy ç¨€ç–çŸ©é˜µã€‚æˆ‘ä»¬æ¥æ›´è¯¦ç»†åœ°çœ‹ä¸€ä¸‹è¿™ä¸ªè¯è¡¨ã€‚è®¿é—®è¯è¡¨çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å‘é‡å™¨ï¼ˆvectorizerï¼‰çš„ get\_feature\_name æ–¹æ³•ï¼Œå®ƒå°†è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”äºä¸€ä¸ªç‰¹å¾ï¼š

      feature_names = vect.get_feature_names()
    
      #ç‰¹å¾æ•°
      print("Number of features: {}".format(len(feature_names)))
      #å‰20ä¸ªç‰¹å¾
      print("First 20 features:\n{}".format(feature_names[:20]))
      #ä¸­é—´çš„ç‰¹å¾
      print("Features 20010 to 20030:\n{}".format(feature_names[20010:20030]))
      #é—´éš”2000æ‰“å°ä¸€ä¸ªç‰¹å¾
      print("Every 2000th feature:\n{}".format(feature_names[::2000]))
    
    
      '''
      ```
      Number of features: 74849
      First 20 features:
      ['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']
      Features 20010 to 20030:
      ['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']
      Every 2000th feature:
      ['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bÃªte', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']
      ```
      '''
    

è¯è¡¨çš„å‰ 10 ä¸ªå…ƒç´ éƒ½æ˜¯æ•°å­—ã€‚  
æ‰€æœ‰è¿™äº›æ•°å­—éƒ½å‡ºç°åœ¨è¯„è®ºä¸­çš„æŸå¤„ï¼Œå› æ­¤è¢«æå–ä¸ºå•è¯ã€‚  
å¤§éƒ¨åˆ†æ•°å­—éƒ½æ²¡æœ‰ä¸€ç›®äº†ç„¶çš„è¯­ä¹‰ï¼Œé™¤äº† â€œ007â€ï¼Œåœ¨ ç”µå½±çš„ç‰¹å®šè¯­å¢ƒä¸­å®ƒå¯èƒ½æŒ‡çš„æ˜¯è©¹å§†æ–¯ â€¢ é‚¦å¾·ï¼ˆJames Bondï¼‰è¿™ä¸ªè§’è‰²ã€‚  
ä»æ— æ„ä¹‰çš„ â€œå•è¯â€ ä¸­æŒ‘å‡ºæœ‰æ„ä¹‰çš„æœ‰æ—¶å¾ˆå›°éš¾ã€‚  
è¿›ä¸€æ­¥è§‚å¯Ÿè¿™ä¸ªè¯è¡¨ï¼Œæˆ‘ä»¬å‘ç°è®¸å¤šä»¥ â€œdraâ€ å¼€å¤´çš„è‹±è¯­å•è¯ã€‚

*   å¯¹äº â€œdraughtâ€ã€â€œdrawbackâ€ å’Œ â€œdrawerâ€ï¼Œå…¶å•æ•°å’Œå¤æ•°å½¢å¼éƒ½åŒ…å«åœ¨è¯è¡¨ä¸­ï¼Œå¹¶ä¸”ä½œä¸ºä¸åŒçš„å•è¯ã€‚è¿™äº›å•è¯å…·æœ‰å¯†åˆ‡ç›¸å…³çš„è¯­ä¹‰ï¼Œå°†å®ƒä»¬ä½œä¸ºä¸åŒçš„å•è¯è¿›è¡Œè®¡æ•°ï¼ˆå¯¹åº”äºä¸åŒçš„ç‰¹å¾ï¼‰å¯èƒ½ä¸å¤ªåˆé€‚ã€‚

åœ¨å°è¯•æ”¹è¿›ç‰¹å¾æå–ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆé€šè¿‡å®é™…æ„å»ºä¸€ä¸ªåˆ†ç±»å™¨æ¥å¾—åˆ°æ€§èƒ½çš„é‡åŒ–åº¦é‡ã€‚

*   æˆ‘ä»¬å°†è®­ç»ƒæ ‡ç­¾ä¿å­˜åœ¨ y\_train ä¸­ï¼Œè®­ç»ƒæ•°æ®çš„è¯è¢‹è¡¨ç¤ºä¿å­˜åœ¨ X\_train ä¸­ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªæ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ã€‚
    
*   å¯¹äºè¿™æ ·çš„é«˜ç»´ç¨€ç–æ•°æ®ï¼Œç±»ä¼¼ LogisticRegression çš„çº¿æ€§æ¨¡å‹é€šå¸¸æ•ˆæœæœ€å¥½ã€‚
    
        from sklearn.model_selection import GridSearchCV
        
        #ç½‘æ ¼æœç´¢
        param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}
        grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)
        grid.fit(X_train, y_train)
        
        print("Best cross-validation score: {:.2f}".format(grid.best_score_))
        print("Best parameters: ", grid.best_params_)
        
        #æµ‹è¯•é›†ä¸ŠæŸ¥çœ‹æ³›åŒ–æ€§èƒ½
        X_test = vect.transform(text_test)
        print("Test score: {:.2f}".format(grid.score(X_test, y_test)))
        
        '''
        ```
        Best cross-validation score: 0.89
        Best parameters:  {'C': 0.1}
        Test score: 0.88
        ```
        '''