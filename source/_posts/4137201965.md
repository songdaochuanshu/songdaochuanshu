---
layout: post
title: "阿里妈妈牟娜：定向广告新一代点击率预估主模型——深度兴趣演化网络"
date: "2022-05-22T03:09:35.454Z"
---
阿里妈妈牟娜：定向广告新一代点击率预估主模型——深度兴趣演化网络
================================

**分享嘉宾**：牟娜 阿里巴巴 高级算法工程师

**编辑整理**：孙锴

**内容来源**：DataFun AI Talk《定向广告新一代点击率预估主模型——深度兴趣演化网络》

**出品社区**：DataFun

* * *

**导读：** 本次带给大家分享是阿里妈妈在2018年做的模型上的创新——深度兴趣演化网络（Deep Interest Evolution Network），分享将从以下几个方面展开——

1.  提出该模型的背景及原因
    
2.  该模型的结构详解
    
3.  该模型的最终效果
    

\--

01 背景
=====

#1、业务形态
-------

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213253027-661864954.png)

在介绍该模型创新背景之前，先来看一下我们的业务形态：当我们打开淘宝的时候，首先呈现的是一个banner形式的广告；在首页猜你喜欢场景下，或者购物链路的其他场景下，会出现一些单品的广告：在推荐的商品浏览列表，即信息流场景下，会在列表中穿插广告投放，且投放位置固定，这些广告将和正常推荐浏览的商品一起呈现出来。

_tips：如果广告的形态特别明显，会破坏用户的体验，比如浏览的顺畅感。所以，推荐用户感兴趣的东西，使得用户感觉不到广告的存在，是十分重要的。_

在一般的广告建模里，通常根据广告信息、用户信息、上下文信息，去判断用户是否会点击这个广告。区别于搜索广告这种用户带有明显意图的主动的query查询行为，在展示广告业务场景下，用户并没有明确的意图。此时，应当如何建模，用户会有什么样的兴趣，了解并解决这些问题，对我们工作非常重要。

### 2、简单模型

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213254090-1459457667.png)

从模型的视角来看，早期的模型形态为：简单模型+复杂的人工设计特征。很多公司在初期都是这样的形式：LR模型+非常复杂的特征工程。

而随着计算机的性能的提升，大家能够利用的数据和计算资源也越来越多的时候，我们便尝试把挖掘潜在特征的工作交给模型来做，这就是深度学习出场的过程。

在LR时代，我们团队做了一些尝试，其中一个是引入了MLR模型，即：把LR模型分成多片，每片建模一部分数据，此方式相当于引入一部分非线性能力。在这个过程中，我们发现，与只用LR相比，MLR模型引入的这部分非线性，对我们的最终效果产生了明显的提升。

在2016年的时候，我们团队开始尝试引入深度学习来解决ctr提升的问题。

### 3、神经网络

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213256538-1260029349.png)

第一代神经网络ctr模型如图所示，第一层是非常简单的原生特征，包括：用户特征，候选广告特征，上下文特征。这些特征在经过lookup的方式做embedding之后,被concat一起，送入多层的dnn网络，最后做一个softmax。这是一个最基础的ctr神经网络模型。

在这种最简单的dnn模型基础之上，衍生出了非常多的其他的模型，比如DeepFm，做一些特征之间的交叉；pnn也是；然后是deep&wide模型，其中的deep部分可以通过多层MLP学习数据中的非线性规律，同时设计了wide部分以复用传统浅层模型时代保留下来的丰富的人工设计特征。

_tips：模型演进的路线：增强泛化能力、保留记忆能力、挖掘组合关系。_

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213300317-68237044.png)

然而上述通用的设计，还不足以应对我们的业务场景，因为淘宝的用户个性化程度非常高，千人千面，每个人看的东西都不一样，每个人的兴趣点也不一样，行为非常丰富，所以一些简单的神经网络模型，单靠增加人工设计的特征或者简单的代数式先验设计，在我们的场景下太过于低效了，还不足以把用户的兴趣挖掘的特别透彻。

如图所示，根据用户的历史行为，我们看到用户的兴趣点是非常宽泛并且杂乱的。此时，在通用的Embedding&MLP范式下设计出的模型，是无法针对用户丰富多样的兴趣，做出特别操作的，仅仅是把所有行为的Embedding sum在一起作为用户的历史行为表达。而这一操作存在大量信息损失。

_tips：这里需要强调的是，用户行为的多样性，反映了用户兴趣的多样性，即每个人感兴趣的物品、种类是很多的，尤其是在淘宝这样综合性的购物网站。_

此外，兴趣本身也会随着时间逐渐演化，前面提到的模型对于这种包含演化信息的数据，就更加无能为力了。

_tips：大家可以想象一下，自己在网络上购物，比如买衣服的时候，一年前喜欢的风格和现在喜欢的风格可能是不一样的，是存在一个逐渐演变的过程的，如果用前面提出的模型，会把这种逐渐演变的信息丢失掉。_

面对这些问题，我们提出了对模型的改造。

### 4、深度兴趣网络

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213302952-539965324.png)

针对用户的兴趣信息的挖掘，我们迈出的第一步对模型的改造是DIN（Deep Interest Network），这是我们在2017年展开的主要工作。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213312266-702365983.png)

虽然说用户的兴趣是多种多样的，但是我们回过头看一下我们的ctr预估要解决的是什么问题。我们是在给定一个候选广告和用户的情况下，去预测点击的结果。当候选广告给定的时候，我们可以用候选广告去反向激活历史行为中的商品，把跟广告相关的商品拉出来，计算用户的兴趣可能是什么。我们利用了候选广告集，最终通过反向激活挖掘出历史行为中与候选广告相关的兴趣。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213314360-1358733108.png)

具体的模型设计如图：通过候选广告，用反向激活的方式，与用户历史行为相关联，按照候选广告与历史点击商品的相关性的高低，来赋予历史行为不同的权重。通过这种方式获取到和当前广告相关的历史行为表达向量以及对应的相关权重，做weighted sum pooling之后，就得到了跟候选广告相关的用户兴趣向量表达。这种方式比直接对所有历史行为做sum pooling增加了兴趣表达的灵活性，同时，随着候选广告的不同，该方式也会得到不同的兴趣表达。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213316471-626188614.png)

如图是一个基于din所得到不同历史行为中包含的广告的权重的例子。候选广告是羽绒服，我们看到历史行为集中衣服相关的广告权重较高，而杯子之类的广告相关性很低。这个例子我们也可以看到din的优点。但是，从刚才的过程中，我们也发现到din还是有一些不足， 这个模型忽略了兴趣随着时间之间演化这样一个重要的性质。

同样一件羽绒服，你会发现以前喜欢的款式和现在喜欢的款式会发生一些变化。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213317579-496057104.png)

那么在2018年，我们的工作重点就是针对这样一个兴趣随时间演化的特点来进行建模以及模型的改造。

\--

02 深度兴趣演化网络
===========

由此引出DIEN（Deep Interest Evolution Netowork）。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213322392-1677170009.png)

首先，用户的兴趣随时间演化这样一个特点，做过深度学习的同学们会容易想到序列建模，即把历史行为按时间序列铺开做序列建模。这样一个直观的想法，我们当然也做过尝试，但是效果并不理想。

如图是一个用户的真实足迹，用户在看窗帘，突然买了别的产品；用户在看旅游产品，突然买了猫咪用品。

_tips：选购旅游产品的时间线一般拉的比较长，因此选购期间难免会看一些日常的其他商品。_

这样一个行为序列是一个杂乱无章的过程，这样的序列与自然语言处理遇到的有序序列是完全不同的，在这样的场景下，序列被打断是一个常规行为。因此单纯的序列建模在这种场景下会失败就不难理解了。用户的兴趣是隐藏在杂乱无章的行为序列背后的，针对这样的情况，我们提出了新的解决方案。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213323387-1538683852.png)

我们已知：用户的兴趣隐藏在行为之后，虽然行为杂乱无章，透过行为，我们发现，其实兴趣的表达要比行为的表达更为稳定的。

当我们提取了兴趣表达之后，还需要对兴趣随时间演化的趋势进行建模。

因此我们将这些问题归纳、抽象、并最终设计了两个模块：兴趣提取模块、兴趣演化模块。

### 1、兴趣提取模块

**关于兴趣提取模块：**

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213326644-18372922.png)

假设用户浏览了一条裤子，那么裤子id是一个特征，该特征对于推荐系统来说是一个较为随机的特征，然而这个id类特征代表的物品的背后，比如用户是喜欢这个裤子的颜色、样式、功能等某些特点，这些特点是我们希望兴趣提取模块可以获取到的，也即，找到与这些随机特征相关联的泛化特征，并进行建模。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213330183-3608109.png)

如图我们可以看到，在embedding层之后，我们对用户特征，上下文特征，广告特征的处理方式并没有改变。而行为序列特征做embedding之后，我们增加了兴趣提取模块。

由于我们的目标是挖掘商品背后的兴趣表达，用户某一时刻的兴趣，不仅与当前的行为相关，也与历史各个时刻的行为相关，因此，我们决定使用GRU模型来对历史行为序列建模，并提取兴趣特征。

_tips：我们用GRU代替LSTM是因为在效果相差无几的前提下，前者比后者要节省更多的参数。在神经网络模型整体结构非常复杂的大前提下，我们会尽量将每一个模块简单化、轻量化。_

通过GRU提取出隐层状态的表达，我们认为这是对用户兴趣的抽象。除了使用GRU之外，我们还引入了辅助loss的功能，用来辅助提取兴趣表达。

_tips：引入辅助loss的原因在于：原始的GRU所提取的隐层状态的表达，受到最后时刻的兴趣的影响程度更高一些，而历史时刻的兴趣随着时间越来越远，会被模型慢慢遗忘。辅助loss将所有历史时刻的loss叠加，学习时可以学到更多历史兴趣特征。_

并且，由于辅助loss的数据的来源是全网的点击信息，而不仅仅是广告样本的点击，这样会增加很多额外的信息，会更好的刻画用户在全网的兴趣。

**辅助loss的作用有三点：**

*   辅助loss利用的label反馈是点击序列pattern而不仅仅是ctr信号；
*   能有效解决长序列梯度传播问题，因为在现实场景中，用户兴趣序列有可能非常长，若直接用GRU，没有辅助loss，则会面临长序列梯度消失问题；
*   通过点击pattern的学习，出来hidden state能学的更好，Embedding通过反向传播也能学到更多语义表达，使得学习更加有效。

辅助loss的构建方式：我们将历史行为序列中的有点击行为的样本label标记为1，有曝光无点击行为的样本label标记为0并进行负采样，组合后送入GRU模型，并构建辅助的loss信号，与最终的loss相加后进行学习。

### 2、兴趣演化模块

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213333346-285837784.png)

在兴趣演化模块，回想我们的业务场景，有两点值得注意：

*   用户行为的随机跳转较多，无规律可言；
*   具体到某个兴趣，存在随时间演化的趋势。

那么我们有没有办法使我们的模型可以有区别对待这些历史行为，然后只关注与候选广告相关性较强演化。这就是兴趣演化模块引入的背景。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213336039-77879719.png)

现在的状态如下：

*   由于候选广告已经给定，而我们也只关心跟候选广告相关的兴趣点；
*   在历史行为序列经过GRU之后，用户的兴趣表达已经提取出来了。

此刻，我们开始针对性的对演化过程进行建模。

首先是加入attention机制，根据与候选广告的相关性，对历史序列里的商品进行加权；以此得到attention score。

在attention机制之后，再加入一层改进的GRU，称之为AUGRU。

_tips：引入该模型的原因在于，在AUGRU里面，我们使用attention score来控制update门的权重，这样既保留了原始的更新方向，又能根据与候选广告的相关程度来控制隐层状态的更新力度。_

举个极端的例子：假如该时刻的行为与候选广告相关度为1，我们希望这个行为能更新用户兴趣的隐状态即h(t)=f(h(t-1), i\_t)，而当行为与候选广告不相关的时候我们要保留当前状态，即：h(t)=h(t-1)。

假如我们不采用这种改进的GRU方式，而直接把attention score乘在每个兴趣向量上作为下一层普通GRU的输入的话， 这种做法会直接影响了输入的scale，而不是准确的控制什么时候该更新，更新的程度和方向是怎么样的，因此可能存在信息的损失。

还是举个极端的例子：假如某行为与候选广告不相关，那么隐状态的更新是h(t)=f(h(t-1), 0), 0向量并不会不更新，而是会将hidden state更新到一个新的地方去，这并不是我们期望的。

通过attention机制，我们得以从繁杂的商品中选取相关的兴趣，并通过AUGRU模型，最终更精准的得到广告的相关兴趣。

\--

03 效果
=====

### 1、离线效果

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213338047-2103618132.png)

最后，我们给大家介绍一下实际的效果。这个模型不仅在生产任务上取得了良好的效果，在学术界也取得了不错的成果。

如图，上半部分是我们在公开数据集（来自于亚马逊商城）上所做的实验。我们对电子产品、书籍两个类目的数据用不同的模型做了实验。可以看出来，DIEN模型的效果是最好的。

图中下半部分是生产任务数据集的实验，我们采用了和公开数据集相同的模型，图中也可以看出，DIEN在列出的模型中，表现也是最优的，而且相对base\_model，AUC提升了1.9个百分点。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213339608-1619478931.png)

在公开数据集上，我们将不同步骤进行了拆分，可以看到每个模块的提升效果。

### 2、线上效果

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521213341838-74614939.png)

我们通过A/B test 观察了一个月的数据，平均带来了17%的ecpm提升，带来了巨大的商业价值。

* * *

今天的分享就到这里，谢谢大家。

阅读更多技术原创文章，请关注**微信公众号“DataFunTalk”**。  
本文首发于微信公众号“DataFunTalk”。