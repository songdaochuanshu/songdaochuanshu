---
layout: post
title: "论文解读（MERIT）《Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning》"
date: "2022-04-26T18:23:00.623Z"
---
论文解读（MERIT）《Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning》
=======================================================================================================

论文信息
====

> 论文标题：Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning  
> 论文作者：Ming Jin, Yizhen Zheng, Yuan-Fang Li, Chen Gong, Chuan Zhou, Shirui Pan  
> 论文来源：2021, IJCAI  
> 论文地址：[download](https://arxiv.org/pdf/2105.05682.pdf)   
> 论文代码：[download](https://github.com/GRAND-Lab/MERIT)

1 Introduction
==============

　　创新：融合交叉视图对比和交叉网络对比。

2 Method
========

　　算法图示如下：

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220426215322694-1860275898.png)

　　模型组成部分：

*   *   Graph augmentations
    *   Cross-network contrastive learning
    *   Cross-view contrastive learning

2.1 Graph Augmentations
-----------------------

*   **Graph Diffusion (GD)**

　　　　$S=\\sum\\limits \_{k=0}^{\\infty} \\theta\_{k} T^{k} \\in \\mathbb{R}^{N \\times N}\\quad\\quad\\quad(1)$

　　这里采用 PPR kernel：

　　　　$S=\\alpha\\left(I-(1-\\alpha) D^{-1 / 2} A D^{-1 / 2}\\right)^{-1}\\quad\\quad\\quad(2)$

*   **Edge Modification (EM)**

　　给定修改比例 $P$ ，先随机删除 $P/2$ 的边，再随机添加$P/2$ 的边。（添加和删除服从均匀分布）

*   **Subsampling (SS)**

　　在邻接矩阵中随机选择一个节点索引作为分割点，然后使用它对原始图进行裁剪，创建一个固定大小的子图作为增广图视图。

*   **Node Feature Masking (NFM)**

　　给定特征矩阵 $X$ 和增强比 $P$，我们在 $X$ 中随机选择节点特征维数的 $P$ 部分，然后用 $0$ 掩码它们。

　　在本文中，将 SS、EM 和 NFM 应用于第一个视图，并将 SS+GD+NFM 应用于第二个视图。

2.2 Cross-Network Contrastive Learning
--------------------------------------

　　MERIT 引入了一个孪生网络架构，它由两个相同的编码器(即 $g\_{\\theta}$, $p\_{\\theta}$, $g\_{\\zeta}$ 和 $p\_{\\zeta}$)组成，在 online encoder 上有一个额外的预测器$q\_{\\theta}$，如 Figure 1 所示。

　　这种对比性的学习过程如 Figure 2(a) 所示：

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220426225145421-1496847307.png)

　　其中：

*   *   $H^{1}=q\_{\\theta}\\left(Z^{1}\\right)$　　
    *   $Z^{1}=p\_{\\theta}\\left(g\_{\\theta}\\left(\\tilde{X}\_{1}, \\tilde{A}\_{1}\\right)\\right)$　　
    *   $Z^{2}=p\_{\\theta}\\left(g\_{\\theta}\\left(\\tilde{X}\_{2}, \\tilde{A}\_{2}\\right)\\right)$　　
    *   $\\hat{Z}^{1}=p\_{\\zeta}\\left(g\_{\\zeta}\\left(\\tilde{X}\_{1}, \\tilde{A}\_{1}\\right)\\right)$　　
    *   $\\hat{Z}^{2}=p\_{\\zeta}\\left(g\_{\\zeta}\\left(\\tilde{X}\_{2}, \\tilde{A}\_{2}\\right)\\right)$　　

　　参数更新策略（动量更新机制）：

　　　　$\\zeta^{t}=m \\cdot \\zeta^{t-1}+(1-m) \\cdot \\theta^{t}\\quad\\quad\\quad(3)$

　　其中，$m$、$\\zeta$、$\\theta$ 分别为动量参数、target network 参数和 online network 参数。

　　损失函数如下：

　　　　$\\mathcal{L}\_{c n}=\\frac{1}{2 N} \\sum\\limits \_{i=1}^{N}\\left(\\mathcal{L}\_{c n}^{1}\\left(v\_{i}\\right)+\\mathcal{L}\_{c n}^{2}\\left(v\_{i}\\right)\\right)\\quad\\quad\\quad(6)$

　　其中：

　　　　$\\mathcal{L}\_{c n}^{1}\\left(v\_{i}\\right)=-\\log {\\large \\frac{\\exp \\left(\\operatorname{sim}\\left(h\_{v\_{i}}^{1}, \\hat{z}\_{v\_{i}}^{2}\\right)\\right)}{\\sum\_{j=1}^{N} \\exp \\left(\\operatorname{sim}\\left(h\_{v\_{i}}^{1}, \\hat{z}\_{v\_{j}}^{2}\\right)\\right)}}\\quad\\quad\\quad(4) $

　　　　$\\mathcal{L}\_{c n}^{2}\\left(v\_{i}\\right)=-\\log {\\large \\frac{\\exp \\left(\\operatorname{sim}\\left(h\_{v\_{i}}^{2}, \\hat{z}\_{v\_{i}}^{1}\\right)\\right)}{\\sum\_{j=1}^{N} \\exp \\left(\\operatorname{sim}\\left(h\_{v\_{i}}^{2}, \\hat{z}\_{v\_{j}}^{1}\\right)\\right)}}\\quad\\quad\\quad(5) $

2.3 Cross-View Contrastive Learning
-----------------------------------

　　损失函数：

　　　　$\\mathcal{L}\_{c v}^{k}\\left(v\_{i}\\right)=\\mathcal{L}\_{\\text {intra }}^{k}\\left(v\_{i}\\right)+\\mathcal{L}\_{\\text {inter }}^{k}\\left(v\_{i}\\right), \\quad k \\in\\{1,2\\}\\quad\\quad\\quad(10)$

　　其中：

　　　　$\\mathcal{L}\_{c v}=\\frac{1}{2 N} \\sum\\limits \_{i=1}^{N}\\left(\\mathcal{L}\_{c v}^{1}\\left(v\_{i}\\right)+\\mathcal{L}\_{c v}^{2}\\left(v\_{i}\\right)\\right)\\quad\\quad\\quad(9)$

　　　　$\\mathcal{L}\_{\\text {inter }}^{1}\\left(v\_{i}\\right)=-\\log {\\large \\frac{\\exp \\left(\\operatorname{sim}\\left(h\_{v\_{i}}^{1}, h\_{v\_{i}}^{2}\\right)\\right)}{\\sum\_{j=1}^{N} \\exp \\left(\\operatorname{sim}\\left(h\_{v\_{i}}^{1}, h\_{v\_{j}}^{2}\\right)\\right)}}\\quad\\quad\\quad(7) $

　　　　$\\begin{aligned}\\mathcal{L}\_{i n t r a}^{1}\\left(v\_{i}\\right) &=-\\log \\frac{\\exp \\left(\\operatorname{sim}\\left(h\_{v\_{i}}^{1}, h\_{v\_{i}}^{2}\\right)\\right)}{\\exp \\left(\\operatorname{sim}\\left(h\_{v\_{i}}^{1}, h\_{v\_{i}}^{2}\\right)\\right)+\\Phi} \\\\\\Phi &=\\sum\\limits\_{j=1}^{N} \\mathbb{1}\_{i \\neq j} \\exp \\left(\\operatorname{sim}\\left(h\_{v\_{i}}^{1}, h\_{v\_{j}}^{1}\\right)\\right)\\end{aligned}\\quad\\quad\\quad(8)$

2.4 Model Training
------------------

　　　　$\\mathcal{L}=\\beta \\mathcal{L}\_{c v}+(1-\\beta) \\mathcal{L}\_{c n}\\quad\\quad\\quad(11)$

3 Experiment
============

**数据集**

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220426231254308-2102834638.png)

**基线实验**

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220426231337818-1184745279.png)

因上求缘，果上努力~~~~ 作者：[Learner-](https://www.cnblogs.com/BlairGrowing/)，转载请注明原文链接：[https://www.cnblogs.com/BlairGrowing/p/16196841.html](https://www.cnblogs.com/BlairGrowing/p/16196841.html)