---
layout: post
title: "论文解读（MCGC）《Multi-view Contrastive Graph Clustering》"
date: "2022-04-11T04:23:57.607Z"
---
论文解读（MCGC）《Multi-view Contrastive Graph Clustering》
===================================================

论文信息
====

> 论文标题：Multi-view Contrastive Graph Clustering  
> 论文作者：Erlin Pan、Zhao Kang  
> 论文来源：2021, NeurIPS  
> 论文地址：[download](https://proceedings.nips.cc/paper/2021/file/10c66082c124f8afe3df4886f5e516e0-Paper.pdf)  
> 论文代码：download

1 介绍
====

 　　本文贡献：

*   *   使用Graph Filter 过滤了高阶噪声数据；　　
    *   提出 Graph Contrastive Regularizer 改善了视图的质量；　　

2 方法
====

2.1 定义
------

　　将多视图图数据定义为 $G=\\left\\{\\mathcal{V}, E\_{1}, \\ldots, E\_{V}, X^{1}, \\ldots, X^{V}\\right\\}$，其中 $\\mathcal{V}$ 表示 $N$ 个节点的集合，$e\_{i j} \\in E\_{v}$ 表示第 $v$ 个视图中节点 $i$ 与节点 $j $ 的关系，$X^{v}=\\left\\{x\_{1}^{v}, \\ldots, x\_{N}^{v}\\right\\}^{\\top}$ 为特征矩阵。邻接矩阵 $\\left\\{\\widetilde{A}^{v}\\right\\}\_{v=1}^{V}$ 描述了初始图的结构。$\\left\\{D^{v}\\right\\}\_{v=1}^{V}$ 表示不同视图中的度矩阵。归一化邻接矩阵 $A^{v}=\\left(D^{v}\\right)^{-\\frac{1}{2}}\\left(\\widetilde{A}^{v}+I\\right)\\left(D^{v}\\right)^{-\\frac{1}{2}}$ 和相应的图拉普拉斯算子 $L^{v}=I-A^{v}$。

2.2 Graph Filtering
-------------------

　　$N$ 个节点的特征矩阵 $ X \\in \\mathbb{R}^{N \\times d}$ 可以被视为 $ d$ 个 $N$ 维图信号。根据底层图，一个自然信号在附近的节点上应该是平滑的。平滑信号 $H$ 可以通过解决以下优化问题来实现：

　　　　$\\underset{\\text{H}}{\\text{min}}\\; \\|H-X\\|\_{F}^{2}+s \\operatorname{Tr}\\left(\\mathrm{H}^{\\top} \\mathrm{LH}\\right)\\quad\\quad\\quad(1) $

　　其中，$s>0$是一个平衡参数，$L $ 是与 $X$ 相关的拉普拉斯矩阵，可以通过对 $\\text{Eq.1}$ 求导得到 $\\text{H}$：

　　　　$H=(I+s L)^{-1} X\\quad\\quad\\quad(2)$

　　为了避免求矩阵转置，我们用它的一阶泰勒级数展开式来近似 $\\text{H}$，即 $H=(I−sL)X$。一般来说，第 $m$ 阶图滤波可以写成

　　　　$H=(I-s L)^{m} X\\quad\\quad\\quad(3)$

　　其中 $m$ 是一个非负整数。图滤波可以在保留图的几何特征的同时，过滤出不良的高频噪声。

> 推导过程
> 
> 　　$\\|H-X\\|\_{F}^{2}+\\operatorname{sTR}\\left(H^{\\top} L H\\right)$
> 
> 　　$\\Leftrightarrow  $ 
> 
> 　　$\\begin{aligned}&(H-X)^{\\top}(H-X)+S\\left(H^{\\top} L H\\right) \\\\=&\\left(H^{\\top}-X^{\\top}\\right)(H-X)+S H^{\\top} L H \\\\=& H^{\\top} H-H^{\\top} X-X^{\\top} H+X^{\\top} X+S H^{\\top} L H\\end{aligned}$
> 
> 　　此外
> 
>       ${\\large \\begin{array}{l} &\\frac{\\partial\\left(H^{\\top} H-H^{\\top} X-X^{\\top} H+X^{\\top} X+s H^{\\top} L H\\right)}{\\partial H} \\\\&= \\frac{\\partial H^{\\top} H}{\\partial H}-\\frac{\\partial H^{\\top} X}{\\partial H}-\\frac{\\partial X^{\\top} H}{\\partial H}+\\frac{\\partial X^{\\top} X}{\\partial H}+\\frac{\\partial s H^{\\top} L H}{\\partial H} \\\\&= 2 H-X-X+s\\left(L H+L^{\\top} H\\right) \\\\&= 2 H-2 X+s L H  +s L^{\\top} H \\\\&=\\left(2+S L+S L^{\\top}\\right) H-2 X\\end{array}} $
> 
> 　　$\\begin{array}{l}2(I+S L) H=2 X \\\\H=(I+S L)^{-1} X\\end{array}$
> 
> 回忆：
> 
> 　　$\\|A\\|\_{F}=\\sqrt{\\sum\\limits\_{i}^{n} \\sum\\limits \_{j}^{n} a\_{i j}^{2}}$
> 
> 　　泰勒展开 $(I-A)^{-1}=I+A+A^{2}+A^{3}+\\cdots(\\rho(A)<1)$

2.3 Graph Learning
------------------

　　为从平滑的表示 $H$ 中学习到一个优化的图 $S$，这里考虑使用自表达模型（ self-expression）【每个数据点都可以用其他数据样本的线性组合来表示】去表示：

　　　　$\\underset{S}{\\text{min}}\\left\\|H^{\\top}-H^{\\top} S\\right\\|\_{F}^{2}+\\alpha\\|S\\|\_{F}^{2}\\quad\\quad\\quad(4)$

　　其中，$S \\in \\mathbb{R}^{N \\times N}$ 为图矩阵，$\\alpha>0$ 为权衡参数。

　　第一项是重构损失，第二项是作为一个正则化项，以避免平凡解。许多其他的正则化器也可以被应用，如核范数，稀疏$\\ell\_{1}$范数。

　　为了处理多视图数据，我们可以为每个视图计算一个平滑表示的 $H^{v}$，并扩展 $\\text{Eq.4 }$ 通过引入一个加权因子来区分不同观点的贡献。

　　　　$\\underset{S, \\lambda^{v}}{\\text{min}} \\sum\\limits \_{v=1}^{V} \\lambda^{v}\\left(\\left\\|H^{v \\top}-H^{v \\top} S\\right\\|\_{F}^{2}+\\alpha\\|S\\|\_{F}^{2}\\right)+\\sum\\limits\_{v=1}^{V}\\left(\\lambda^{v}\\right)^{\\gamma}\\quad\\quad\\quad(5)$

　　其中，$\\lambda^{v} $ 为第 $v$ 个视图的权值，$\\gamma$ 为平滑参数。$\\text{Eq.5 }$ 学习了一个由所有视图共享的 Consensus Graph $S$。为了学习更有鉴别性的 $S$，我们在本文中引入了一种新的正则化器。

2.4 Graph Contrastive Regularizer
---------------------------------

　　本文选择将每个节点及其 $k$ 个近邻(KNN)视为正对。然后，我们通过在图矩阵 $S$ 上应用对比正则化器，而不是使用节点特征，从而在图级上进行对比学习。它可以表示为

　　　　$\\mathcal{J}=\\sum\\limits\_{i=1}^{N} \\sum\\limits \_{j \\in \\mathbb{N}\_{i}^{v}}-{\\large \\log \\frac{\\exp \\left(S\_{i j}\\right)}{\\sum\\limits\_{p \\neq i}^{N} \\exp \\left(S\_{i p}\\right)}} \\quad\\quad\\quad(6)$

　　其中，$\\mathbb{N}\_{i}^{v}$ 表示第 $v$ 个视图中节点 $i$ 的 $k$ 个最近邻。

　　$\\text{Eq.6 }$是将邻居拉近，并将非邻居分开，以提高图的质量。最终，我们提出的多视点对比图聚类(MCGC)模型可以表述为：

　　　　$\\underset{S, \\lambda^{v}}{\\text{min}} \\sum\\limits\_{v=1}^{V} \\lambda^{v}\\left(\\left\\|H^{v \\top}-H^{v \\top} S\\right\\|\_{F}^{2}+\\alpha \\sum\\limits\_{i=1}^{N} \\sum\\limits\_{j \\in \\mathbb{N}\_{i}^{v}}-{\\large \\log \\frac{\\exp \\left(S\_{i j}\\right)}{\\sum\\limits\_{p \\neq i}^{N} \\exp \\left(S\_{i p}\\right)}} \\right)+\\sum\\limits\_{v=1}^{V}\\left(\\lambda^{v}\\right)^{\\gamma}\\quad\\quad\\quad(7)$

　　与现有的多视图聚类方法不同，MCGC从多视图属性和多个结构图中探索整体信息。此外，它从平滑信号而不是原始数据构建一个 consensus graph。

2.5 Optimization
----------------

　　在等式中有两组 $\\text{Eq.7 }$，很难直接解决它们。为了优化它们，我们采用了一种交替优化策略，即每次更新一个变量并固定所有其他变量。

　　**固定 $\\lambda^{v}$ , 优化 $S$：**

　　因为 $\\lambda^{v}$  是固定的，所以我们的目标函数可以表示为：

　　　　$\\underset{S}{\\text{min}}  \\sum\\limits\_{v=1}^{V} \\lambda^{v}\\left(\\left\\|H^{v \\top}-H^{v \\top} S\\right\\|\_{F}^{2}+\\alpha \\sum\\limits\_{i=1}^{N} \\sum\\limits\_{j \\in \\mathbb{N}\_{i}^{v}}-\\log {\\large \\frac{\\exp \\left(S\_{i j}\\right)}{\\sum\\limits\_{p \\neq i}^{N} \\exp \\left(S\_{i p}\\right)}} \\right)\\quad\\quad\\quad(8)$

　　$S$ 可以用梯度下降法简单地求解，它在 $t$ 时代的导数可以记为

　　　　$\\nabla\_{1}^{(\\mathrm{t})}+\\alpha \\nabla\_{2}^{(t)}\\quad\\quad\\quad(9)$

　　第一个项是：

　　　　$\\nabla\_{1}^{(\\mathrm{t})}=2 \\sum\\limits \_{v=1}^{V} \\lambda^{v}\\left(-\\left\[H^{v} H^{v \\top}\\right\]\_{i j}+\\left\[H^{v} H^{v \\top} S^{(t-1)}\\right\]\_{i j}\\right)\\quad\\quad\\quad(10)$

　　定义：$K^{(\\mathrm{t}-1)}=\\sum\_{p \\neq i}^{N} \\exp \\left(S\_{i p}^{(t-1)}\\right)$ ，$n$为所有邻居的数目，因此第二项为：

　　　　$\\nabla\_{2}^{(t)}=\\left\\{\\begin{array}{l}\\sum\\limits\_{v=1}^{V} \\lambda^{v}\\left(-1+{\\large \\frac{n \\exp \\left(S\_{i j}^{(t-1)}\\right)}{K^{(t-1)}}} \\right), \\text { if } j \\text { in } \\mathbb{N}\_{i}^{v} \\\\\\sum\\limits\_{v=1}^{V} \\lambda^{v}\\left({\\large \\frac{n \\exp \\left(S\_{i j}^{(t-1)}\\right)}{K^{(t-1)}}} \\right), \\text { otherwise }\\end{array}\\right.\\quad\\quad\\quad(11)$

　　然后采用 Adam 优化策略来更新 $S$。为了提高收敛速度，我们用 $S^{\*}$ 初始化 $S$，其中 $S^{\*}$ 是 $\\text{Eq.5 }$ 的解。

　　**固定 $S$ , 优化 $\\lambda^{v} $：**

　　对于每个视图 $v$，我们定义了 $M^{v}=\\left\\|H^{v \\top}-H^{v \\top} S\\right\\|\_{F}^{2}+\\alpha \\mathcal{J}$。然后，将损失函数简化为

　　　　$\\underset{\\lambda^{v}}{\\text{min}} \\sum\\limits \_{v=1}^{V} \\lambda^{v} M^{v}+\\sum\\limits\_{v}^{V}\\left(\\lambda^{v}\\right)^{\\gamma}\\quad\\quad\\quad(12)$

　　通过将它的导数设为零，我们得到

　　　　$\\lambda^{v}=\\left(\\frac{-M^{v}}{\\gamma}\\right)^{\\frac{1}{\\gamma-1}}\\quad\\quad\\quad(13)$

　　我们交替优化 $S$ 和 $\\lambda^{v}$ 直到收敛。完整的过程在 Algorithm 1 中概述。

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220410233130615-288449342.png)

　　最后一步使用K-Means进行聚类。

3 Experiments
=============

3.1 Datasets and Metrics
------------------------

　　**数据集**

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220410233350526-905476574.png)

　　**评价指标**

*   *   Accuracy (ACC)　　
    *   normalized Mutual Information (NMI)　　
    *   Adjusted Rand Index (ARI)　　
    *   F1 score　　

3.2 Results
-----------

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220411094700811-299398426.png)

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220411101238284-921393524.png)

　　**结果分析**

*   *   与单视图的 GAE 方法相比，MCGC 在ACM、DBLP、IMDB上的ACC改善效果分别提高了 9%、4%、19%以上，虽然使用深度神经网络，但它不能探索视图的互补性；
    *   与 PMNE 相比，ACC、NMI、ARI、F1 平均提高了 16%、20%、20%、12% ；
    *   对 LINE、RMSC、SwMC的改善更为显著。这可以归因于在MCGC中对特征信息和结构信息的探索；
    *   尽管O2MA、O2MAC和MAGCN都捕获了属性和结构信息，但MCGC的性能仍然大大优于它们。具体来说，MCGC 在 ACC、NMI 和 F1 上的 O2MAC 性能平均分别提高了近 6%、9%、11%。关于 MAGCN，所有指标的改进都超过了20%。与基于学习的对比方法相比，我们的改进也令人印象深刻；
    *   特别是，与 COMPLETER 相比，在 Amazon 数据集上的改进超过了 30%，这说明 MCGC 受益于图结构信息。MCGC 还将 MVGRL 的性能提高了20%。通过比较 MCGC 和 MCGC\* 的结果，我们可以看到选择邻居的策略确实对性能有影响；

4 Ablation Study
================

4.1 The Effect of Contrastive Loss
----------------------------------

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220411102848403-1628842602.png)

　　验证 Contrastive regularizer 的有效性：

*   *   在所有数据集上，没有对比损失导致性能急剧下降。MCGC 在 DBLP、ACM、IMDB、Amazon 数据集上的 ACC 性能分别提高了16%、8%、5% 和 12% ；

4.2 The Effect of Multi-View Learning
-------------------------------------

　　为了演示多视图学习的效果，本文评估了以下单视图模型的性能

　　　　$\\underset{S}{\\text{min}}\\left\\|H^{\\top}-H^{\\top} S\\right\\|\_{F}^{2}+\\alpha \\sum\\limits \_{i=1}^{N} \\sum\\limits\_{j \\in \\mathbb{N}\_{i}}-\\log \\frac{\\exp \\left(S\_{i j}\\right)}{\\sum\_{p \\neq i}^{N} \\exp \\left(S\_{i p}\\right)}\\quad\\quad\\quad(14)$

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220411104739224-1515433075.png)

　　**结果分析：**

*   *   可以观察到，当合并所有视图时，总是能达到最佳的性能。此外，不同视图的性能有很大差异。这就证明了在 $\\text{Eq.7}$ 中使用 ${\\lambda}^{v}$ 的必要性。因此，探索多视角信息的互补性是有益的；

4.3 The Effect of Graph Filtering
---------------------------------

　　为了理解 graph filtering 的贡献，本文进行了另一组实验。如果没有 graph filtering ，我们的目标函数就变成了

　　　　$\\underset{S, \\lambda^{\\nu}}{\\text{min}} \\sum\\limits\_{v=1}^{V} \\lambda^{v}\\left(\\left\\|X^{v \\top}-X^{v \\top} S\\right\\|\_{F}^{2}+\\alpha \\sum\\limits\_{i=1}^{N} \\sum\\limits\_{j \\in \\mathbb{N}\_{i}^{v}}-\\log {\\large \\frac{\\exp \\left(S\_{i j}\\right)}{\\sum\\limits\_{p \\neq i}^{N} \\exp \\left(S\_{i p}\\right)}} \\right)+\\sum\\limits\_{v=1}^{V}\\left(\\lambda^{v}\\right)^{\\gamma}\\quad\\quad\\quad(15)$

　　![](https://img2022.cnblogs.com/blog/1664108/202204/1664108-20220411105711253-2077916919.png)

　　**结果分析：**

　　将这个模型表示为 MCGC-。MCGC、ACC对ACM、DBLP、IMDB的ACC分别下降了 0.8%、1.3% 和 0.8%。这表明图滤波对我们的模型有积极的影响。对于其他指标，MCGC在大多数情况下也优于 MCGC-。

5 Conclusion
============

　　在本文中，我们提出了一种新的方法(MCGC)，不仅利用属性内容，而且利用图的结构信息。特别地，引入 Graph Filtering 来滤除噪声分量，并采用对比正则化器来进一步提高学习图的质量。

相关论文
====

2018—IJCAI——Scalable Multiplex Network Embedding

2020—PMLR——A simple framework for contrastive learning of visual representations

2020—IJCAI——Multi-view attribute graph convolution networks for clustering

2019—IJCAI ——Multi-view spectral clustering network  

2021—AAAI——Contrastive clustering

因上求缘，果上努力~~~~ 作者：[Learner-](https://www.cnblogs.com/BlairGrowing/)，转载请注明原文链接：[https://www.cnblogs.com/BlairGrowing/p/16127245.html](https://www.cnblogs.com/BlairGrowing/p/16127245.html)