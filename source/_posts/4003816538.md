---
layout: post
title: "论文解读（ARVGA）《Learning Graph Embedding with Adversarial Training Methods》"
date: "2022-06-07T07:18:22.680Z"
---
论文解读（ARVGA）《Learning Graph Embedding with Adversarial Training Methods》
=======================================================================

论文信息
====

> 论文标题：Learning Graph Embedding with Adversarial Training Methods  
> 论文作者：Shirui Pan, Ruiqi Hu, Sai-fu Fung, Guodong Long, Jing Jiang, Chengqi Zhang  
> 论文来源：2020, ICLR  
> 论文地址：[download](https://arxiv.org/abs/1901.01250)   
> 论文代码：download

1 Introduction
==============

　　众多图嵌入方法关注于保存图结构或最小化重构损失，忽略了隐表示的嵌入分布形式，因此本文提出对抗正则化框架（adversarially regularized framework）。

2 Method
========

　　ARGA 框架如下：

　　![](https://img2022.cnblogs.com/blog/1664108/202206/1664108-20220606171924398-417712475.png)

　　组成部分：

*   *   Graph convolutional autoencoder
    *   Adversarial regularization

2.1 Graph Convolutional Autoencoder
-----------------------------------

　　一个频谱卷积函数 $f\\left(\\mathbf{Z}^{(l)}, \\mathbf{A} \\mid \\mathbf{W}^{(l)}\\right)$ :

　　　　$\\mathbf{Z}^{(l+1)}=f\\left(\\mathbf{Z}^{(l)}, \\mathbf{A} \\mid \\mathbf{W}^{(l)}\\right)  \\quad\\quad\\quad(1)$

　　采用GCN :

　　　　$f\\left(\\mathbf{Z}^{(l)}, \\mathbf{A} \\mid \\mathbf{W}^{(l)}\\right)=\\phi\\left(\\widetilde{\\mathbf{D}}^{-\\frac{1}{2}} \\widetilde{\\mathbf{A}} \\widetilde{\\mathbf{D}}^{-\\frac{1}{2}} \\mathbf{Z}^{(l)} \\mathbf{W}^{(l)}\\right) \\quad\\quad\\quad(2)$

　　图编码器

　　　　$\\mathbf{Z}^{(1)}=f\_{\\text {Relu }}\\left(\\mathbf{X}, \\mathbf{A} \\mid \\mathbf{W}^{(0)}\\right)  \\quad\\quad\\quad(3)$  
　　　　$\\mathbf{Z}^{(2)}=f\_{\\text {linear }}\\left(\\mathbf{Z}^{(1)}, \\mathbf{A} \\mid \\mathbf{W}^{(1)}\\right) \\quad\\quad\\quad(4)$

　　我们的图卷积编码器 $\\mathcal{G}(\\mathbf{Z}, \\mathbf{A})=   q(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A}) $ 将图结构和节点内容编码为一个表示的 $\\mathbf{Z}=q(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A})=\\mathbf{Z}^{(2)}$。

　　　　$q(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A})=\\prod\\limits\_{i=1}^{n} q\\left(\\mathbf{z}\_{\\mathbf{i}} \\mid \\mathbf{X}, \\mathbf{A}\\right) \\quad\\quad\\quad(5)$

　　　　$q\\left(\\mathbf{z}\_{\\mathbf{i}} \\mid \\mathbf{X}, \\mathbf{A}\\right)=\\mathcal{N}\\left(\\mathbf{z}\_{i} \\mid \\boldsymbol{\\mu}\_{i}, \\operatorname{diag}\\left(\\boldsymbol{\\sigma}^{2}\\right)\\right)\\quad\\quad\\quad(6)$

　　这里，$\\boldsymbol{\\mu}=\\mathbf{Z}^{(2)}$ 是均值向量 $\\boldsymbol{z}\_{i}$ 的矩阵；同样，$\\log \\sigma=f\_{\\text {linear }}\\left(\\mathbf{Z}^{(1)}, \\mathbf{A} \\mid \\mathbf{W}^{\\prime(1)}\\right) $ 在 $\\text{Eq.3}$ 的第一层与 $\\boldsymbol{\\mu}$ 共享权值 $\\mathbf{W}^{(0)}$。

**Decoder model**

　　我们的解码器模型用于重建图形数据。我们可以重建图结构 $\\mathbf{A}$，内容信息 $\\mathbf{X}$，或者两者都可以重建，本文注重重建图结构 $\\mathbf{A}$。

　　Decoder 是 $p(\\hat{\\mathbf{A}} \\mid \\mathbf{Z})$。

　　我们训练了一个基于图嵌入的链接预测层：

　　　　$p(\\hat{\\mathbf{A}} \\mid \\mathbf{Z})=\\prod\_{i=1}^{n} \\prod\_{j=1}^{n} p\\left(\\hat{\\mathbf{A}}\_{i j} \\mid \\mathbf{z}\_{i}, \\mathbf{z}\_{j}\\right)\\quad\\quad\\quad(7)$

　　　　$p\\left(\\hat{\\mathbf{A}}\_{i j}=1 \\mid \\mathbf{z}\_{i}, \\mathbf{z}\_{j}\\right)=\\operatorname{sigmoid}\\left(\\mathbf{z}\_{i}^{\\top}, \\mathbf{z}\_{j}\\right)\\quad\\quad\\quad(8)$

　　这里的预测 $\\hat{\\mathbf{A}}$ 应该接近于地面真相 $\\mathbf{A}$。

**Graph Autoencoder Model**

　　嵌入 $Z$ 和重构图 $\\hat{\\mathbf{A}}$ 可以表示如下：

　　　　$\\hat{\\mathbf{A}}=\\operatorname{sigmoid}\\left(\\mathbf{Z} \\mathbf{Z}^{\\top}\\right), \\text { here } \\mathbf{Z}=q(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A})\\quad\\quad\\quad(9)$

**Optimization**

　　对于图编码器，我们通过以下方法来最小化图数据的重构误差：

　　　　$\\mathcal{L}\_{0}=\\mathbb{E}\_{q(\\mathbf{Z} \\mid(\\mathbf{X}, \\mathbf{A}))}\[\\log p(\\mathbf{A} \\mid \\mathbf{Z})\]\\quad\\quad\\quad(10)$

　　对于变分图编码器，我们对变分下界进行了优化如下：

　　　　$\\mathcal{L}\_{1}=\\mathbb{E}\_{q(\\mathbf{Z} \\mid(\\mathbf{X}, \\mathbf{A}))}\[\\log p(\\mathbf{A} \\mid \\mathbf{Z})\]-\\mathbf{K L}\[q(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A}) \\| p(\\mathbf{Z})\]\\quad\\quad\\quad(11)$

　　其中，$\\mathbf{K L}\[q(\\bullet) \\| p(\\bullet)\]$ 是 $q(\\bullet)$ 和 $p(\\bullet)$ 之间的 KL 散度。$p(\\bullet)$ 是一个先验分布，它在实践中可以是一个均匀分布，也可以是一个高斯分布 ：$p(\\mathbf{Z})= \\prod\\limits\_{i} p\\left(\\mathbf{z}\_{i}\\right)=\\prod\\limits\_{i} \\mathcal{N}\\left(\\mathbf{z}\_{i} \\mid 0, \\mathbf{I}\\right)$。

****2.2**  Adversarial Model $\\mathcal{D}(\\mathbf{Z}) $**
-----------------------------------------------------------

　　我们的模型的基本思想是强制潜在表示 $\\mathbf{Z}$ 来匹配一个先验分布，这是通过一个对抗性的训练模型来实现的。对抗性模型是建立在一个标准的多层感知器(MLP)上，其中输出层只有一维的 $sigmoid$ 函数。对抗模型作为一个鉴别器来区分潜在代码是来自先前的 $p\_{z}$（positive）还是图编码器 $\\mathcal{G}(\\mathbf{X}, \\mathbf{A})$（negative）。通过最小化训练二值分类器的交叉熵代价，最终在训练过程中对嵌入方法进行正则化和改进。该成本的计算方法如下：

　　　　$-\\frac{1}{2} \\mathbb{E}\_{\\mathbf{z} \\sim p\_{z}} \\log \\mathcal{D}(\\mathbf{Z})-\\frac{1}{2} \\mathbb{E}\_{\\mathbf{X}} \\log (1-\\mathcal{D}(\\mathcal{G}(\\mathbf{X}, \\mathbf{A}))) \\quad\\quad\\quad(12)$

　　在我们的论文中，我们检查了对所有模型和任务，设置 $p\_{z}$ 为高斯分布和均匀分布。

**Adversarial Graph Autoencoder Model**

　　用鉴别器 $\\mathcal{D}(\\mathbf{Z})$ 训练编码器模型的方程可以写如下：

　　　　$\\underset{\\mathcal{G}}{\\text{min }}  \\underset{\\mathcal{D}}{\\text{max }} \\mathbb{E}\_{\\mathbf{z} \\sim p\_{z}}\[\\log \\mathcal{D}(\\mathbf{Z})\]+\\mathbb{E}\_{\\mathbf{x} \\sim p(\\mathbf{x})}\[\\log (1-\\mathcal{D}(\\mathcal{G}(\\mathbf{X}, \\mathbf{A})))\]\\quad\\quad\\quad(13)$

　　其中 $\\mathcal{G}(\\mathbf{X}, \\mathbf{A})$ 和 $\\mathcal{D}(\\mathbf{Z})$ 表示上述说明的发生器和鉴别器。

2.3 Algorithm Explanation
-------------------------

　　算法如下：

　　![](https://img2022.cnblogs.com/blog/1664108/202206/1664108-20220607085432093-647785161.png)

2.4 Decoder Variations
----------------------

　　在 ARGA 和 ARVGA 模型中，解码器仅仅是作为嵌入 $z$ 的点积执行的链路预测层。实际上，解码器也可以是图卷积层，也可以是链路预测层和图卷积解码器层的组合。

**GCN Decoder for Graph Structure Reconstruction (ARGA GD)**

　　我们对编码器进行了修改，增加了两个图的卷积层来重建图的结构。

　　这种方法的变体被命名为 ARGAGD。Fig 2 展示了 ARGAGD 的体系结构。

　　![](https://img2022.cnblogs.com/blog/1664108/202206/1664108-20220607091327146-1259054249.png)

　　在这种方法中，解码器的输入将从编码器中嵌入，并且图卷积解码器构造如下：

　　　　$\\mathbf{Z}\_{D}=f\_{\\text {linear }}\\left(\\mathbf{Z}, \\mathbf{A} \\mid \\mathbf{W}\_{D}^{(1)}\\right)\\quad\\quad\\quad(14)$

　　　　$\\mathbf{O}=f\_{\\text {linear }}\\left(\\mathbf{Z}\_{D}, \\mathbf{A} \\mid \\mathbf{W}\_{D}^{(2)}\\right)\\quad\\quad\\quad(15)$

　　其中，$\\mathbf{Z}$ 是从图编码器学习到的嵌入，而 $\\mathbf{Z}\_{D}$ 和 $\\mathbf{O}$ 是从图解码器的第一层和第二层的输出。$\\mathbf{O}$ 的水平维数等于节点数。然后，我们计算出重建误差如下：

　　　　$\\mathcal{L}\_{A R G A\_{-} G D}=\\mathbb{E}\_{q(\\mathbf{O} \\mid(\\mathbf{X}, \\mathbf{A}))}\[\\log p(\\mathbf{A} \\mid \\mathbf{O})\]\\quad\\quad\\quad(16)$

**GCN Decoder for both Graph Structure and Content Information Reconstruction (ARGA AX)**

　　我们进一步修改了我们的图的卷积解码器，以重建图的结构 $\\mathbf{A}$ 和内容信息 $\\mathbf{X}$。该体系结构如 Fig.3 所示。

　　![](https://img2022.cnblogs.com/blog/1664108/202206/1664108-20220607091653253-1970586374.png)

　　我们用与每个节点相关的特征数固定第二图卷积层的维数，因此第二层的输出 $\\mathbf{O} \\in \\mathbb{R}^{n \\times f} \\ni \\mathbf{X}$。在这种情况下，重构损失由两个误差组成。首先，图结构的重构误差可以最小化如下：

　　　　$\\mathcal{L}\_{A}=\\mathbb{E}\_{q(\\mathbf{O} \\mid(\\mathbf{X}, \\mathbf{A}))}\[\\log p(\\mathbf{A} \\mid \\mathbf{O})\] \\quad\\quad\\quad(17)$

　　然后用类似的公式可以最小化节点内容的重构误差：

　　　　$\\mathcal{L}\_{X}=\\mathbb{E}\_{q(\\mathbf{O} \\mid(\\mathbf{X}, \\mathbf{A}))}\[\\log p(\\mathbf{X} \\mid \\mathbf{O})\] \\quad\\quad\\quad(18)$

　　最终的重构误差是图的结构和节点内容的重构误差之和：

　　　　$\\mathcal{L}\_{0}=\\mathcal{L}\_{A}+\\mathcal{L}\_{X}\\quad\\quad\\quad(19)$

3 Experiments
=============

**数据集**

　　![](https://img2022.cnblogs.com/blog/1664108/202206/1664108-20220607092053499-287631378.png)

**节点聚类**

　　![](https://img2022.cnblogs.com/blog/1664108/202206/1664108-20220607092411021-1842479039.png)

4 Conclusion
============

　　本文提出了一种新的对抗性图嵌入框架。我们认为现有的图嵌入算法都是非正则化方法，忽略了潜在表示的数据分布，在真实图数据中嵌入不足。我们提出了一种对抗性训练方案来正则化潜在码，并强制使潜在码匹配先验分布。对抗性模块与一个图卷积自动编码器共同学习，以产生一个鲁棒表示。我们还利用了ARGA的一些有趣的变化，如ARGADG和ARGAAX，来讨论图卷积解码器对重构图结构和节点内容的影响。实验结果表明，我们的算法ARGA和ARVGA在链路预测和节点聚类任务中优于基线算法。

　　反向正则化图自动编码器(ARGA)有几个方向。我们将研究如何使用ARGA模型生成一些真实的图\[64\]，这可能有助于发现生物领域的新药。我们还将研究如何将标签信息合并到ARGA中来学习鲁棒图嵌入。

因上求缘，果上努力~~~~ 作者：[Learner-](https://www.cnblogs.com/BlairGrowing/)，转载请注明原文链接：[https://www.cnblogs.com/BlairGrowing/p/16348958.html](https://www.cnblogs.com/BlairGrowing/p/16348958.html)