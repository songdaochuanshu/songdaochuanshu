---
layout: post
title: "论文翻译：2022_2022_TEA-PSE 2.0：Sub-Band Network For Real-Time Personalized Speech Enhancement"
date: "2023-02-02T04:19:48.857Z"
---
论文翻译：2022\_2022\_TEA-PSE 2.0：Sub-Band Network For Real-Time Personalized Speech Enhancement
===========================================================================================

> 论文地址：TEA-PSE 2.0：用于实时个性化语音增强的子带网络
> 
> 论文代码：[  
> ](https://github.com/echocatzh/MTFAA-Net)
> 
> 引用：

摘要
==

　　个性化语音增强(Personalized speech enhancement，**PSE**)利用额外的线索，如说话人embeddings来去除背景噪声和干扰语音，并从目标说话人提取语音。此前，Tencent - Ethereal - Audio - Lab个性化语音增强(TEA-PSE)系统在ICASSP 2022深度噪声抑制(DNS2022)挑战赛中排名第一。在本文中，我们将TEA-PSE扩展到它的子带版本TEA-PSE 2.0，以降低计算复杂度并进一步提高性能。具体来说，我们采用有限脉冲响应滤波器组和频谱分割来降低计算复杂度。我们在系统中引入了时频卷积模块(TFCM)，用小的卷积核来增加感受野。此外，我们探索了几种训练策略来优化两级网络，并研究PSE任务中的各种损失函数。在语音增强性能和计算复杂度方面，TEA-PSE 2.0明显优于TEA-PSE。在DNS 2022盲测试集上的实验结果表明，与之前的TEA-PSE相比，TEA-PSE 2.0提高了0.102 OVRL个性化DNS MOS，仅进行了21.9%的乘法-累积操作。

**索引术语**：个性化语音增强，子带，实时，深度学习

1  引言
=====

　　实时通信(RTC)在我们的日常生活中变得不可或缺。然而，语音质量受到背景噪声、混响、背景说话人的语音等影响。有效的语音增强在RTC系统中起着重要的作用。传统的语音增强主要是去除背景噪声和混响。它不能过滤掉干扰的说话人。为此，我们提出了个性化语音增强(PSE)\[1 4\]，根据目标说话人录入的语音片段，从所有其他说话人和背景噪声中提取目标说话人的声音。

　　最新的ICASSP 2022 DNS挑战赛\[5\]旨在推广全频段实时个性化语音增强任务。TEA-PSE\[6\]通过专门设计的两阶段框架在ICASSP 2022 DNS个性化语音增强评估集上获得优异的性能。但它具有27.84 G每秒的乘法累积运算(mac)的高计算复杂度，并直接对全频段信号执行，实时因子(RTF)为0.96。此外，**TEA-PSE中使用的编码器-解码器结构不能有效地捕获长程相关性，因为根据\[7\]，卷积的感受野受到限制**。

　　为了降低计算复杂度，

*   **第一种方法是特征压缩**。例如，RNNoise\[8\]和Personalized PercepNet\[9\]分别使用Bark-scale和等效矩形带宽(ERB) scale压缩全频带输入特征。这种特征压缩方法不可避免地会丢失关键的频段信息，导致性能不佳。
*   **第二种方法是频谱分割**，这在最近的语音增强(SE)研究中很常见。Lv et al.\[10\]和Li et al.\[11\]在短时傅里叶变换(STFT)后进行频谱分裂，将堆叠的子带作为批处理，而不是直接对全带特征进行建模。与这些批处理方法不同，DMF-Net\[12\]和SF-Net\[13\]采用级联结构的频谱分裂。在处理较高频带时，预处理过的较低频带会给出外部知识引导。
*   **第三种方法是基于有限脉冲响应(FIR)的子带分析与合成**，该方法可以有效降低经典数字信号处理\[14\]的带宽。多频带WaveRNN\[15\]和多频带MelGAN\[16\]在文本到语音(TTS)任务的子带处理中获得了较高的MOS效果。这种用于音乐源分离(MSS)任务的子带处理\[17\]明显优于全带处理。

　　另一方面，最近的多阶段方法在直观的假设下，将原来复杂的语音增强问题分解为多个更简单的子问题，并在每个阶段逐步得到更好的解的前提下，表现出了优异的性能。尽管有专门设计的模型体系结构，我们也注意到这些方法中采用的优化策略是非常不同的。具体来说，SDD-Net\[18\]和TEA-PSE\[6\]在训练当前模块时**冻结前一阶段的模块**。不同的是，CTS-Net\[19\]以**较低的学习率对前面的模块进行微调**。Wang et al.\[20\]在分阶段训练的基础上，采用端到端的训练方法，**用一个损失函数同时优化不同的模块**。多阶段训练方法的最佳训练策略有待进一步的比较研究。

　　在本文中，我们提出了TEA-PSE 2.0，以进一步提高感知语音质量，同时显著抑制噪声和干扰，降低计算复杂度。我们的贡献是三重的。首先，我们利用设计的FIR滤波器和直接频谱分割扩展了原始TEA-PSE模型的子带处理。子带输入比全带输入具有更好的语音增强性能，并显著加快了模型推理速度。其次，利用时频卷积模块\[21\]对模型进行升级，利用小的卷积核来增加模型的感受野；实验结果表明，改进后的两级网络具有显著的性能提升。最后，我们比较了多阶段方法的顺序优化和联合优化。本文还研究了PSE的单域和多域损失函数。结果表明，序列优化训练策略和多域损失函数的优越性。最后，在ICASSP 2022 DNS挑战盲测试集上，提出的TEA-PSE 2.0优于之前的TEA-PSE，具有0.102 OVRL个性化DNSMOS (PDNSMOS)\[22\]评分提高。令人印象深刻的是，它的计算复杂度只有TEA-PSE的21.9%

2  问题公式化
========

　　假设$y$是目标说话人、干扰说话人和背景噪声的混合物，由一个麦克风在时域捕获。

$$公式1：y(n)=s(n)\*h\_s(n)+z(n)\*h\_z(n)+v(n)$$

其中$n$表示波形采样点索引，$s$表示目标说话人的信号，$z$表示干扰说话人的信号，$h\_s$和$h\_z$表示说话人和麦克风之间的房间脉冲响应(RIR)， $v$表示附加噪声。我们用$e$表示目标说话人的注册语音。在频域中，式(1)可表示为

$$公式2：Y(t, f)=S(t, f) \\cdot H\_s(t, f)+Z(t, f) \\cdot H\_z(t, f)+V(t, f)$$

其中$t$和$f$分别是帧索引和频率索引。我们在论文中没有明确地考虑到去混响。在实验过程中，背景噪声、干扰语音和混响可能同时存在。

3  TEA-PSE 2.0 
===============

3.1  子带分解
---------

![](https://img2023.cnblogs.com/blog/1433301/202212/1433301-20221228102828933-1806627010.png)

图1所示 TEA-PSE 2.0总体流程图

　　在图1中，我们展示了TEA-PSE 2.0的总体流程图。我们设计了两个流程来比较不同子带处理策略的建模能力。第一个流程是红线，它是基于FIR滤波器组在信号级进行子带分析和合成。第二个流程是绿线，这是频谱分割与合并。其中$K$为采样间隔，$F\_M \\in R^{M\*M}$为STFT核，$F\_M^{-1}$为iSTFT核，M为窗长。

　　**FIR滤波器组分析与合成(FAS)**。我们采用稳定高效的滤波器组 伪正交镜滤波器组\[14\](pseudo quadrature mirror filter bank (PQMF))进行子带分解和信号重构。分析和合成都包含K个FIR滤波器组，其中K表示子带数。

*   *   **子带编码器**有三个过程，包括FIR分析、下采样和STFT。我们用$y\_k$表示分析和下采样的输出，其中$k \\in \[1, K\]$是子带索引。$y\_k$的采样率为$\\frac{1}{K}$，$Y\_k \\in C^{T\*F'}$表示$y\_k$对应的频域。然后沿通道维度堆叠$Y\_k$，形成特征$Y\_{fas} \\in C^{T\*F'\*K}$，作为PSE模块的输入。
    *   **子带译码器**有三个过程，分别是iSTFT、上采样和FIR合成。我们沿着通道维度对PSE模块的输出$\\hat{S}\_{fas} \\in C^{T\*F'\*K}$进行切片，作为每个子带输出$\\hat{S}\_k \\in C^{T\*F'}$的预测。经过iSTFT和上采样后，$\\hat{S}\_k$变为$\\hat{s}\_k$，且$\\hat{s}\_k$的采样率与$y$相同。最后，我们将$\\hat{s}\_k$通过一组合成滤波器组重构源信号$\\hat{s}$。

　　**频谱分割与合并(SSM)**。我们对每个子带$Y\_k \\in C^{T\*\\frac{F}{K}}$沿频率轴对全频段频谱$Y \\in C^{T\*F}$进行拆分，然后沿通道轴进行堆叠，形成输入特征$Y\_{ssm} \\in C^{T\*\\frac{F}{K}\*K}$作为PSE模块的输入。在信号重构过程中，对网络输出信号$\\hat{S}\_{ssm} \\in C^{T\*\\frac{F}{K}\*K}$进行重构，恢复全频段频谱$\\hat{S} \\in C^{T\*F}$。

3.2 两阶段PSE网络
------------

![](https://img2023.cnblogs.com/blog/1433301/202212/1433301-20221228104337432-456701943.png)

图2所示。(a) PSE模块的详细信息。(b) MAG-Net 的网络详情

　　PSE模块的详细信息如图2(a)所示。我们保留了与TEA-PSE\[6\]相同的两阶段框架，分别包含了MAG-Net和COM-Net来处理 幅度(magnitude)和复数(complex)特征。图2(b)显示了PSE模块的MAG-Net的细节。我们用$E$表示由说话人编码器网络提取的说话人embedding。对于MAG-Net，我们使用观测信号$Y$的幅值作为输入，目标幅值作为训练目标。MAG-Net 对噪声成分和干扰语音有较强的抑制作用。然后，将$\\hat{S}\_1$与噪声相位$e^{j\\varphi (Y)}$耦合，得到实虚(RI)谱$(\\hat{S}\_r^1,\\hat{S}\_i^1)$作为COM-Net的输入。我们还将观测到的频谱$(Y\_r, Y\_i)$作为COM-Net的输入，进一步去除残留噪声和干扰语音成分。在COM-Net的输入$(\\hat{S}\_r^1,\\hat{S}\_i^1)$和输出之间进行残差连接，形成最终的输出$(\\hat{S}\_r^2,\\hat{S}\_i^2)$。COM-Net具有与MAGNet相似的网络拓扑结构，而其双解码器架构被设计用于单独估计RI频谱。

![](https://img2023.cnblogs.com/blog/1433301/202212/1433301-20221228120151568-1489838595.png)

图3。(a) GTCM的详细资料。(b)编码器FD层的详细信息。(c)解码器中FU层的详细信息

　　**编码器和解码器**。编码器由三个频率下采样(FD)层组成。图3(b)显示了FD层的详细情况。其次是Gated Conv (GConv)\[23,24\]，累积层范数(cLN) \[25\]， PReLU和时频卷积模块(TFCM)\[21\]。我们使用TFCM用小的卷积核来增加感受野，解决了原来的卷积编解码器结构\[7\]中感受野有限的问题。译码器有三层频率上采样(FU)层，其结构如图3(c)所示。它采用镜面结构作为FD层，用转置门控Conv (TrGConv)代替GConv。

　　**堆叠门控时序卷积模块**。我们的堆叠门控时序卷积模块(S-GTCM)重复了一堆GTCM层，如图3(a)所示，它保持了与TEA-PSE\[6\]相同的架构。GTCM包含两个点卷积(PConv)和一个扩张卷积(DConv)。在相邻卷积之间，对PReLU和cLN进行插值。在输入和输出之间应用残差连接，用于训练更深层次的网络。在每个S-GTCM层中，第一个GTCM层接受混合语音的学习表示$X \\in R^{T\*F''}$以及说话人embedding  的$E \\in R^D$，而其他GTCM层只接受混合语音特征作为输入。首先沿时间维重复说话人embedding，形成$E' \\in R^{T\*D}$，然后传递PConv以保持与学习表示相同的维数。然后将学习到的表示与嵌入在特征维度中的说话者相乘。

3.3 损失函数
--------

　　我们首先应用尺度不变信噪比(SISNR)\[25\]损失

$$公式3：\\mathcal{L}\_{\\mathrm{si}-\\mathrm{snr}}=20 \\log \_{10} \\frac{\\left\\|\\left(\\hat{s}^T s / s^T s\\right) \\cdot s\\right\\|}{\\left\\|\\left(\\hat{s}^T s / s^T s\\right) \\cdot s-\\hat{s}\\right\\|},$$

其中$\\hat{s}$表示估计的说话人。

　　然后使用幂律压缩相位感知( power-law compressed phase-aware，PLCPA)损耗。PLCPA有利于ASR的准确性和感知质量\[26,27\]。它主要由以下两部分组成：幅度损失$L\_{mag}$和相位损失$L\_{pha}$。

$$公式4：\\mathcal{L}\_{\\mathrm{mag}}=\\frac{1}{T} \\sum\_t^T \\sum\_f^F \\|\\left. S(t, f)\\right|^p-\\left.|\\hat{S}(t, f)|^p\\right|^2$$

$$公式5：\\mathcal{L}\_{\\text {pha }}=\\frac{1}{T} \\sum\_t^T \\sum\_f^F \\|\\left. S(t, f)\\right|^p e^{j \\varphi(S(t, f))}-\\left.|\\hat{S}(t, f)|^p e^{j \\varphi(\\hat{S}(t, f))}\\right|^2$$

　　我们利用非对称损耗来惩罚目标说话人的估计频谱，这有利于缓解过度抑制\[28\]。

$$公式6：\\mathcal{L}\_{\\text {asym }}=\\frac{1}{T} \\sum\_t^T \\sum\_f^F\\left|\\operatorname{ReLU}\\left(|S(t, f)|^p-|\\hat{S}(t, f)|^p\\right)\\right|^2$$

　　两级网络使用以下损失进行训练。我们最初只用$L1$训练MAG-Net。

$$公式7：L\_1=L\_{si-snr}+L\_{mag}+L\_{asym}$$

在此基础上，将预先训练好的magg - net参数冻结，只对COM-Net进行优化

$$公式8：L\_2=L\_{si-snr}+L\_{mag}+L\_{pha}+L\_{asym}$$

$\\hat{S}$和$S$分别为估计光谱和clean 光谱。谱压缩因子$p$设置为 0.5。算子$\\varphi $计算一个复数的相位。

4  实验
=====

4.1 数据集
-------

　　我们使用ICASSP 2022 DNS-challenge全频段数据集\[5\]进行实验。大约750小时的纯净语音和181小时的噪音剪辑组成了个性化DNS (PDNS)训练集。总共有3230个说话人。噪声数据来自DEMAND\[31\]、Freesound和AudioSet\[32\]数据库。

　　我们使用了700小时的纯净语音数据和150小时的噪声数据，它们都来自PDNS数据集，以生成训练集。选取50小时的纯净语音数据和15小时的噪声数据生成开发集。我们基于RT60 $\\in$ \[0.1, 0.6\]s的image method \[33\]生成100,000个房间脉冲响应(RIRs)。房间的大小可以用$W\*d\*h$表示，其中w $\\in$ \[3, 8\]m, d $\\in$ \[3, 8\]m, h $\\in$ \[3, 4\]m。麦克风分散在空间中，$h\_{mic}$范围为\[0,1.2\]m。语音声源可以在房间的任何地方找到，$h\_{speech}$范围为\[0.6,1.8\]m。声源与麦克风之间的距离范围为\[0.3,6.0\]m。在训练集和开发集中分别有80,000和10,000个RIR。

　　评价数据可分为两部分。仿真集旨在评估模型在不可见说话人上的性能。我们使用KING-ASR-215数据集作为源语音，PDNS噪声集中剩余的16小时数据作为源噪声，剩余的10,000个RIR创建了201个说话人的2010个noise-clean配对的模拟集。在每对noise-clean对中加入一个SIR范围为\[5,20\]dB的随机干扰说话人和一个信噪比范围为\[5,20\]dB的随机噪声。第二部分是DNS2022盲测试集，由859个真实测试片段组成，其中121个片段存在干扰语音，其余738个片段不存在干扰语音。应该注意的是，在训练、开发和模拟评估集之间的源数据没有重叠。

4.2 训练步骤
--------

　　训练数据是动态生成的，每batch分割为**10秒**的块，信噪比和SIR范围分别为\[5,20\]dB和\[5,20\]dB。混合物的刻度调整为\[35,15\] dBFS。此外，在训练过程中，将源语音数据的50%与RIR进行卷积，以模拟混响场景。此外，20%的训练数据只包含一个干扰说话人，30%包含一个干扰说话人和一种类型的噪声，30%只包含一种类型的噪声，剩下的20%包含两种类型的噪声。

　　我们对观测到的信号采用汉宁窗，帧长为20 ms，帧移为10 ms。STFT包含1024个观测信号输入点，采样率为48 kHz，导致513-dim特征。Adam优化器\[34\]用于优化每个神经模型，使用$1e-3$的初始学习速率。如果验证损失连续两个周期没有下降，则学习率减半。每个阶段训练60个epoch。我们应用最大l2范数5进行**梯度裁剪**。

　　编码器有3个FD层，解码器有3个FU层。GConv和TrGConv在时间轴和频率轴上的核大小和步长分别设为(1, 7)和(1, 4)。在4个子带和8个子带的实验中，沿频率轴的步幅分别为3和2。对于所有GConv和TrGConv，通道数量保持在80。一个TFCM包含6个卷积层，膨胀率为$\\{2^0, 2^1, 2^2, 2^3, 2^4, 2^5\\}$。每个TFCM中深度DConv的核大小为(3,3)，每个TFCM中所有Conv的通道数为80。GTCM中所有子模块的通道数设置为80，最后一个PConv除外。一个S-GTCM包含4个对应的GTCM层，对于DConv，内核大小为5，膨胀率分别为{1,2,5,9}。我们堆叠了4个S-GTCM块，在连续帧之间建立长期关系，并结合说话人嵌入。

　　为了从目标说话人的注册语音中提取说话人embedding，我们使用了预先训练好的开源ECAPA-TDNN\[35\]网络。

4.3 评价指标
--------

　　使用了几个客观指标，包括宽带感知评估语音质量(PESQ)\[36\]用于语音质量，短时客观可理解性(STOI)\[37\]及其扩展版本ESTOI\[38\]用于可理解性，SISNR\[39\]用于语音失真。我们采用基于ITU-T P.835\[40\]的非侵入式主观评价指标PDNSMOS P.835\[22\]来评价主观语音性能。PDNSMOS P.835是专门为PSE任务设计的，更多细节可以在\[22\]中找到。对于所有的度量，高值表示更好的性能。

5  结果与分析
========

　　我们设计了几组实验来验证性能的不同方面，包括a)全带输入(F1-F6)， b)基于SSM的子带处理(S1-S3)， c)基于FAS的子带处理(A1-A3)， d)训练策略(M1- M4)， e)损失函数(L1-L3)。MAG-Net (F5)用L1优化，COM-Net (F6)用L2优化。我们使用用于盲测试集的开源预训练模型评估NSNet2 \[30\] (F7)和DeepFilterNet2 \[29\] (F8)的结果。粗体结果表示每列中的最佳结果，最佳结果画了下划线。

5.1 全带消融
--------

　　从表1中可以看出，两级MAG&COM-Net (F3)在所有指标上都优于其他方法。比较F3和F2 (TEA-PSE)， MAG&COM-Net的性能优于TEA-PSE, MACs只有TEA-PSE的60%。对比F3和F4(不含TFCM的MAG&COM-Net)， TFCM模块可以有效地扩展感受野，学习编码器和解码器的时间相关性。在表2中，TFCM带来了0.116 SIG、0.081 BAK和0.136 OVRL性能提升。将F5 (MAG-Net)和F6 (COM-Net)与F3进行比较，我们使用两级网络来进一步抑制噪声成分和不需要的干扰说话人声音，这对于PSE任务是有效的。对于表2中的DNS盲测试集，MAG&COM-Net优于其他基线系统。

表1 以PESQ, STOI (%)， ESTOI(%)和SISNR (dB)来表示仿真集上的性能

![](https://img2023.cnblogs.com/blog/1433301/202212/1433301-20221228120254723-1493737067.png)

5.2 子带消融
--------

　　基于FAS和SSM的子带处理优于全带特征，同时大大降低了计算复杂度。这是因为子带输入使模型能够将不同的能力分配给不同的子带\[17\]，而全带输入的模型往往会由于高低频能量\[41\]的差异而出现高频损失。对于基于SSM的子带处理，S2 (SSM, K=4)和S3 (SSM, K=8)的性能相似，且优于S1 (SSM, K=2)。这是因为，随着步幅的减小，模型的频率建模能力有可能提高。对于基于FAS的子带处理，随着子带数的增加，信号重构误差会变大。所以在表1中，A3 (FAS, K=8)的效果不如A1 (FAS, K=2)和A2 (FAS, K=4)。总的来说，4子带输入在子带输入实验中表现最好。

5.3 训练策略消融
----------

　　A2是我们在训练策略和损失函数的消融研究中后续实验的backbone。我们探索了4种不同的两阶段网络训练策略，包括M1): 预先训练MAG-Net，然后在训练COM-Net时冻结它，M2)：预先训练MAG-Net，然后在训练COM-Net时以较小的学习率对其进行微调，M3)：同时使用L1 + L2训练MAG-Net和COM-Net, M4)：只使用L2训练整个网络。一般来说，M1(冻结)提供了最好的整体性能，如表2所示，M2 (finetune)也产生了出色的性能。对于M4(仅)，仅优化L2会对性能产生负面影响，这表明在所有模块上设置优化限制的重要性。对于M3(joint)，joint训练策略在表1中总体表现最差。根据上面的实验结果，我们得出结论，序列训练，即逐个优化网络模块，是PSE任务中两阶段模型的首选策略，尽管它将花费更多的训练时间。

表2 DNS2022盲测试机上的性能。PDNSMOS P.835指标包括语音质量(SIG)、背景噪声质量(BAK)和总体质量(OVRL)

![](https://img2023.cnblogs.com/blog/1433301/202212/1433301-20221228120349285-1017662193.png)

5.4 损失函数消融
----------

　　时域损失函数常用于语音分离任务，而频域损失函数常用于语音增强任务。为了在PSE任务中找到最优的损失策略，我们对不同域的损失函数进行了实验，包括L1)：将时域损失函数$L\_{si-snr}$与频域损失函数$L\_{mag}$, $L\_{pha}$, $L\_{asym}$结合，L2)：时域损失函数，L3)：频域损失函数。在表2中，L1 (tfloss)总体表现最好。这说明了从时域和频域对模型进行优化的价值。在表1中，L3(floss)的PESQ、STOI和ESTOI得分最高。

6 结论
====

　　本文提出了一种新的子带两级个性化语音增强网络TEA-PSE 2.0，它是TEA-PSE的升级版。得益于基于fas的子带处理和TFCM模块，TEA-PSE 2.0(表2中的A2)在计算复杂度上仅为TEA-PSE的21.9% MACs，在ICASSP 2022 PDNS盲测试集中实现了最先进的PDNSMOS性能。并证明了按顺序优化MAG-Net和COM-Net是最好的训练策略。此外，利用时域和频域相结合的损失函数对模型进行优化是有价值的。在未来，我们将特别考虑PSE中的语音去混音，以进一步提高语音质量。

7 致谢
====

　　作者感谢西北工业大学的S. Lv和J. Sun在子带处理方面的有益讨论，以及网易公司的Y. Fu在$MTFA-Net$实现方面的有益讨论。

8  参考文献
=======

\[1\] Q. Wang, H. Muckenhirn, K. Wilson, P. Sridhar, Z. Wu, J. R. Hershey, R. A. Saurous, R. Weiss, Y. Jia, and I. L. Moreno, VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking, in Interspeech, 2019, pp. 2728 2732.

\[2\] K. Zmol ıkov a, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani, L. Burget, and J. Cernock\`y, Speakerbeam: Speaker aware neural network for target speaker extraction in speech mixtures, IEEE J. Sel. Topics Signal Process. , vol. 13, no. 4, pp. 800 814, 2019.

\[3\] C. Xu, W. Rao, E. S. Chng, and H. Li, Spex: Multi-scale time domain speaker extraction network, IEEE/ACM Trans. Audio, Speech, Language Process. , vol. 28, pp. 1370 1384, 2020.

\[4\] M. Ge, C. Xu, L. Wang, E. S. Chng, J. Dang, and H. Li, SpEx+: A Complete Time Domain Speaker Extraction Network, in Interspeech, 2020, pp. 1406 1410.

\[5\] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych, S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, et al., ICASSP 2022 deep noise suppression challenge, in ICASSP. IEEE, 2022, pp. 9271 9275.

\[6\] Y. Ju, W. Rao, X. Yan, Y. Fu, S. Lv, L. Cheng, Y. Wang, L. Xie, and S. Shang, TEA-PSE: Tencent-etherealaudio-lab Personalized Speech Enhancement System for ICASSP 2022 DNS CHALLENGE, in ICASSP. IEEE, 2022, pp. 9291 9295.

\[7\] S. Zhao, B. Ma, K. N. Watcharasupat, and W. Gan, FRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement, in ICASSP. IEEE, 2022, pp. 9281 9285.

\[8\] J. Valin, A hybrid dsp/deep learning approach to realtime full-band speech enhancement, in MMSP. IEEE, 2018, pp. 1 5.

\[9\] R. Giri, S. Venkataramani, J. Valin, U. Isik, and A. Krishnaswamy, Personalized PercepNet: Real-Time, Low-Complexity Target Voice Separation and Enhancement, in Interspeech, 2021, pp. 1124 1128.

\[10\] S. Lv, Y. Hu, S. Zhang, and L. Xie, DCCRN+: Channel-Wise Subband DCCRN with SNR Estimation for Speech Enhancement, in Interspeech, 2021, pp. 2816 2820.

\[11\] J. Li, D. Luo, Y. Liu, Y. Zhu, Z. Li, G. Cui, W. Tang, and W. Chen, Densely connected multi-stage model with channel wise subband feature for real-time speech enhancement, in ICASSP. IEEE, 2021, pp. 6638 6642.

\[12\] G. Yu, Y. Guan, W. Meng, C. Zheng, and H. Wang, DMF-Net: A decoupling-style multi-band fusion model for real-time full-band speech enhancement, arXiv preprint arXiv:2203.00472, 2022.

\[13\] G. Yu, A. Li, W. Liu, C. Zheng, Y. Wang, and H. Wang, Optimizing shoulder to shoulder: A coordinated subband fusion model for real-time full-band speech enhancement, arXiv preprint arXiv:2203.16033, 2022.

\[14\] T. Q. Nguyen, Near-perfect-reconstruction pseudo-qmf banks, IEEE Trans. Signal Process. , vol. 42, no. 1, pp. 65 76, 1994.

\[15\] C. Yu, H. Lu, N. Hu, M. Yu, C. Weng, K. Xu, P. Liu, D. Tuo, S. Kang, G. Lei, D. Su, and D. Yu, DurIAN: Duration Informed Attention Network for Speech Synthesis, in Interspeech, 2020, pp. 2027 2031.

\[16\] G. Yang, S. Yang, K. Liu, P. Fang, W. Chen, and L. Xie, Multi-band MelGAN: Faster waveform generation for high-quality text-to-speech, in SLT. IEEE, 2021, pp. 492 498.

\[17\] H. Liu, L. Xie, J. Wu, and G. Yang, Channel-Wise Subband Input for Better Voice and Accompaniment Separation on High Resolution Music, in Interspeech, 2020, pp. 1241 1245.

\[18\] A. Li, W. Liu, X. Luo, G. Yu, C. Zheng, and X. Li, A Simultaneous Denoising and Dereverberation Framework with Target Decoupling, in Interspeech, 2021, pp. 2801 2805.

\[19\] A. Li, W. Liu, C. Zheng, C. Fan, and X. Li, Two heads are better than one: A two-stage complex spectral mapping approach for monaural speech enhancement, IEEE/ACM Trans. Audio, Speech, Language Process. , vol. 29, pp. 1829 1843, 2021.

\[20\] H. Wang and D. Wang, Cross-domain speech enhancement with a neural cascade architecture, in ICASSP. IEEE, 2022, pp. 7862 7866.

\[21\] G. Zhang, L. Yu, C. Wang, and J. Wei, Multi-scale temporal frequency convolutional network with axial attention for speech enhancement, in ICASSP. IEEE, 2022, pp. 9122 9126.

\[22\] C. K. Reddy, V. Gopal, and R. Cutler, DNSMOS P. 835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors, in ICASSP. IEEE, 2022, pp. 886 890.

\[23\] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, Language modeling with gated convolutional networks, in  International conference on machine learning. PMLR, 2017, pp. 933 941.

\[24\] S. Zhang, Z. Wang, Y. Ju, Y. Fu, Y. Na, Q. Fu, and L. Xie, Personalized Acoustic Echo Cancellation for Full-duplex Communications, arXiv preprint arXiv:2205.15195, 2022.

\[25\] Y. Luo and N. Mesgarani, Conv-tasnet: Surpassing ideal time frequency magnitude masking for speech separation, IEEE/ACM Trans. Audio, Speech, Language Process. , vol. 27, no. 8, pp. 1256 1266, 2019.

\[26\] S. E. Eskimez, X. Wang, M. Tang, H. Yang, Z.n Zhu, Z. Chen, H. Wang, and T. Yoshioka, Human Listening and Live Captioning: Multi-Task Training for Speech Enhancement, in Interspeech, 2021, pp. 2686 2690.

\[27\] S. Zhang, Z. Wang, J. Sun, Y. Fu, B. Tian, Q. Fu, and L. Xie, Multi-task deep residual echo suppression with echo-aware loss, in ICASSP. IEEE, 2022, pp. 9127 9131.

\[28\] Q. Wang, I. L. Moreno, M. Saglam, K. Wilson, A. Chiao, R. Liu, Y. He, W. Li, J. Pelecanos, M. Nika, and A. Gruenstein, VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech Recognition, in Interspeech, 2020, pp. 2677 2681.

\[29\] H. Schr oter, T. Rosenkranz, A. Maier, et al., Deepfilternet2: Towards real-time speech enhancement on embedded devices for full-band audio, arXiv preprint arXiv:2205.05474, 2022.