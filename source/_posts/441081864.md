---
layout: post
title: "朴素贝叶斯分类"
date: "2023-01-27T14:14:48.013Z"
---
朴素贝叶斯分类
=======

一、朴素贝叶斯法原理
==========

1.基本原理
------

  朴素贝叶斯法(Naive Bayes)是一种基础分类算法，它的核心是贝叶斯定理+条件独立性假设。贝叶斯定理描述的是两个条件概率之间的关系，对两个事件A和B，由乘法法则易知$$(A∩B)=P(A)P(B│A)=P(B)P(A│B)$$  
  贝叶斯定理就是对这个关系式的变形，即

\\\[P(B│A)=\\frac{P(B)P(A|B)}{P(A)} \\\]

  若把样本特征和类别作为对应的条件和条件概率，则贝叶斯定理可以用来解决分类问题。如对样本\\(x=\\left( x\_1,x\_2,...,x\_n \\right)\\)，所属类别为\\(y\\)，那么该特征下对应该类别的概率代入贝叶斯公式就是$$P(y|x\_1,x\_2,...,x\_n)=\\frac{P(y)P(x\_1,x\_2,...,x\_n|y)}{P(x\_1,x\_2,...,x\_n)}$$  
  贝叶斯分类法的思想就是计算样本特征对应于各类别的概率，以概率最大的作为分类输出。分母部分是特征的联合概率，可以进一步由全概率公式展开；分子部分由于含复杂的条件概率，使得直接的计算较复杂，因此这里做一个条件独立性假设，即认为样本的各维特征间是相互独立的，这是一个较强的假设，朴素贝叶斯也由此得名。在该条件之下，分子便可化为$$P(y)\\prod\_{i=1}^{n}P(x\_i|y)$$  
  注意到，在用于分类决策时，分母部分的值对于所有的类别都是相同的，要找出最大概率对应的类别，只考察分子即可。因此，朴素贝叶斯分类器表示为$$\\hat{y}=\\arg max\_{y\_k}{P(y\_k)\\prod\_{i=1}^{n}P(x\_i|y\_k)}$$

2.平滑处理
------

  在离散特征的情形之下进行分类输出的概率计算，可能会出现概率为0的情况，如随机变量观测值的某一维并未在训练集中出现，那么它所属的条件概率为0，致使对应类别的后验概率为0，从而使分类产生偏差，这是不合理的，因此需进行一定的平滑处理。具体，就是在频率计算时，对每组统计的频数加上一个常数。  
先验概率：\\(P(y\_k)=\\frac{\\sum\_{i=1}^{N}{I(y\_i=y\_k)+\\lambda}}{N+K\\lambda}\\)  
条件概率：\\(P(x\_i|y\_k)=\\frac{\\sum\_{i=1}^{N}{I(x\_i,y\_i=y\_k)+\\lambda}}{\\sum\_{i=1}^{N}{I(y\_i=y\_k)+S\\lambda}}\\)  
  当\\(\\lambda=1\\)时，称为拉普拉斯平滑(Laplace smoothing)。

3.三个基本模型
--------

  根据特征随机变量的类型，分为伯努利朴素贝叶斯、多项式朴素贝叶斯、高斯朴素贝叶斯三种基本模型。  
(1) 伯努利朴素贝叶斯  
  若特征随机变量符合的是离散型的二项分布，也就是仅布尔值，那么此时的模型称为伯努利朴素贝叶斯。从统计的角度，分类器表达式分子中的连乘运算对应于n次独立试验。  
(2) 多项式朴素贝叶斯  
  若特征随机变量符合的是离散型的多项分布，那么此时的模型称为多项式朴素贝叶斯。同样地，分类器表达式分子中的连乘运算对应于n次独立试验。  
(3) 高斯朴素贝叶斯  
  若特征随机变量是连续型的（如身高、体重），即假定它是符合高斯分布的（正态分布），概率的计算就是由已知的数据计算出高斯分布的两个参数（均值、标准差），进而由密度函数确定对应的取值，代入公式计算。同样地，分类器表达式分子中的连乘运算对应于n次独立试验。

二、示例
====

  这里对多项式朴素贝叶斯分类模型举例。  
训练集：

样本特征向量X

类别Y

\[1, 1, 2, 3\]

1

\[1, 2, 2, 4\]

2

\[1, 2, 3, 3\]

2

\[1, 2, 4, 4\]

3

\[1, 3, 3, 4\]

3

\[2, 2, 3, 4\]

1

\[2, 1, 3, 3\]

3

测试样本：\[1, 2, 3, 4\]

则类别集合为\\(Y\\in\\left\\{ 1,2,3 \\right\\}\\) ,  
\\(P(Y=1)=\\frac{2}{7}\\),\\(P(Y=2)=\\frac{2}{7}\\),\\(P(Y=3)=\\frac{3}{7}\\),  
\\(P\\left( X\_1=1|Y=1 \\right)=\\frac{1}{2}\\),\\(P\\left( X\_2=2|Y=1 \\right)=\\frac{1}{2}\\),\\(P\\left( X\_3=3|Y=1 \\right)=\\frac{1}{2}\\),  
\\(P\\left( X\_4=4|Y=1 \\right)=\\frac{1}{2}\\),\\(P\\left( X\_1=1|Y=2 \\right)=1\\),\\(P\\left( X\_2=2|Y=2 \\right)=1\\),  
\\(P\\left( X\_3=3|Y=2 \\right)=\\frac{1}{2}\\),\\(P\\left( X\_4=4|Y=2 \\right)=\\frac{1}{2}\\),\\(P\\left( X\_1=1|Y=3 \\right)=\\frac{2}{3}\\),  
\\(P\\left( X\_2=2|Y=3 \\right)=\\frac{1}{3}\\),\\(P\\left( X\_3=3|Y=3 \\right)=\\frac{2}{3}\\),\\(P\\left( X\_4=4|Y=3 \\right)=\\frac{2}{3}\\),

归属于类别1的概率：

\\\[\\begin{equation\*} \\begin{aligned} &P(Y=1)P(X\_1=1|Y=1)P(X\_2=2|Y=1)P(X\_3=3|Y=1)P(X\_4=4|Y=1)\\\\ &=\\frac{2}{7}\\cdot\\frac{1}{2}\\cdot\\frac{1}{2}\\cdot\\frac{1}{2}\\cdot\\frac{1}{2}\\\\ &=\\frac{1}{56} \\end{aligned} \\end{equation\*} \\\]

归属于类别2的概率：

\\\[\\begin{equation\*} \\begin{aligned} &P(Y=2)P(X\_1=1|Y=2)P(X\_2=2|Y=2)P(X\_3=3|Y=2)P(X\_4=4|Y=2)\\\\ &=\\frac{2}{7}\\cdot1\\cdot1\\cdot\\frac{1}{2}\\cdot\\frac{1}{2}\\\\ &=\\frac{1}{14} \\end{aligned} \\end{equation\*} \\\]

归属于类别3的概率：

\\\[\\begin{equation\*} \\begin{aligned} &P(Y=3)P(X\_1=1|Y=3)P(X\_2=2|Y=3)P(X\_3=3|Y=3)P(X\_4=4|Y=3)\\\\ &=\\frac{3}{7}\\cdot\\frac{2}{3}\\cdot\\frac{1}{3}\\cdot\\frac{2}{3}\\cdot\\frac{2}{3}\\\\ &=\\frac{8}{189} \\end{aligned} \\end{equation\*} \\\]

归属于类别2的概率最大，因此分类输出为2。

三、Python实现
==========

(1) 伯努利朴素贝叶斯
------------

    '''
    sklearn实现伯努利朴素贝叶斯分类。
    '''
    
    import numpy as np
    from sklearn.naive_bayes import BernoulliNB
    
    ## 1.构造训练集和待测样本
    #训练集数据
    train_x=[
        [1, 1, 1, 1],
        [1, 1, 1, 1],
        [1, 1, 1, 0],
        [1, 1, 0, 0],
        [1, 0, 0, 0],
        [0, 0, 0, 0]
    ]
    #训练集数据标签
    train_y=[
        1,
        2,
        2,
        3,
        3,
        1
    ]
    
    #待测样本
    test_x = [
        [1, 2, 1, 2],
        [1, 1, 2, 2]
    ]
    
    #转为array形式
    train_x = np.array(train_x)
    train_y = np.array(train_y)
    test_x = np.array(test_x)
    
    ## 2.定义分类器
    bnbClf = BernoulliNB()
    
    ## 3.训练
    Fit_bnbClf = bnbClf.fit(train_x,train_y)
    
    ## 4.预测
    pre_y = Fit_bnbClf.predict(test_x)
    
    print('预测类别：')
    print(pre_y)
    

![](https://img2023.cnblogs.com/blog/2197714/202301/2197714-20230127220242621-249988221.png)

(2) 多项式朴素贝叶斯
------------

    '''
    sklearn实现多项式朴素贝叶斯分类。
    '''
    
    import numpy as np
    from sklearn.naive_bayes import ComplementNB
    
    ## 1.构造训练集和待测样本
    #训练集数据
    train_x=[
        [1, 1, 2, 3],
        [1, 2, 2, 4],
        [1, 2, 3, 3],
        [1, 2, 4, 4],
        [1, 3, 3, 4],
        [2, 2, 3, 4],
        [2, 1, 3, 3]
    ]
    #训练集数据标签
    train_y=[
        1,
        2,
        2,
        3,
        3,
        1,
        3
    ]
    
    
    #待测样本
    test_x = [
        [1, 2, 3, 4],
        [1, 1, 1, 4]
    ]
    
    #转为array形式
    train_x = np.array(train_x)
    train_y = np.array(train_y)
    test_x = np.array(test_x)
    
    ## 2.定义分类器
    cnbClf = ComplementNB()
    
    ## 3.训练
    Fit_cnbClf = cnbClf.fit(train_x,train_y)
    
    ## 4.预测
    pre_y = Fit_cnbClf.predict(test_x)
    
    print('预测类别：')
    print(pre_y)
    

![](https://img2023.cnblogs.com/blog/2197714/202301/2197714-20230127220314583-1892036620.png)

(3) 高斯朴素贝叶斯
-----------

    '''
    sklearn实现高斯朴素贝叶斯分类。
    '''
    
    import numpy as np
    from sklearn.naive_bayes import GaussianNB
    
    #训练集数据
    train_x=[
        [1.1, 2, 3, 4],
        [1, 2.2, 3, 4],
        [1, 2, 3.3, 4],
        [1, 2, 3, 4.4],
        [1.1, 2.2, 3, 4],
        [1, 2, 3.3, 4.4]
    ]
    #训练集数据标签
    train_y=[
        1,
        2,
        2,
        3,
        3,
        1
    ]
    
    #待测样本
    test_x = [
        [1.2, 2, 3, 4],
        [1, 2.3, 3, 4]
    ]
    
    #转为array形式
    train_x = np.array(train_x)
    train_y = np.array(train_y)
    test_x = np.array(test_x)
    
    ## 2.定义分类器
    gnbClf = GaussianNB()
    
    ## 3.训练
    Fit_gnbClf = gnbClf.fit(train_x,train_y)
    
    ## 4.预测
    pre_y = Fit_gnbClf.predict(test_x)
    
    print('预测类别：')
    print(pre_y)
    

![](https://img2023.cnblogs.com/blog/2197714/202301/2197714-20230127220342038-1234810720.png)  
  
  
**End.**