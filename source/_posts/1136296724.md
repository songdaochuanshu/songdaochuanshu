---
layout: post
title: "TIE: A Framework for Embedding-based Incremental Temporal Knowledge Graph Completion 增量时序知识图谱补全论文解读"
date: "2022-12-13T14:15:59.530Z"
---
TIE: A Framework for Embedding-based Incremental Temporal Knowledge Graph Completion 增量时序知识图谱补全论文解读
===================================================================================================

　　论文网址：[https://dl.acm.org/doi/10.1145/3404835.3462961](https://dl.acm.org/doi/10.1145/3404835.3462961)

　　Arxiv：[https://arxiv.org/abs/2104.08419](https://arxiv.org/abs/2104.08419)

　　论文提出一种用增量学习思想做时序知识图谱补全（Temporal Knowledge Graph Completion, TKGC）的学习框架——Time-aware Incremental Embedding (TIE)。看框架名是提出了一种学习知识图嵌入的方法。增量学习是为了缓解模型在学习新增数据时产生的对过去所学的灾难性遗忘问题。

　　时序知识图谱（Temporal Knowledge Graph, TKG）可以看**[这篇](https://blog.csdn.net/han54878/article/details/126574017)**简单了解。知识图谱（Knowledge Graph, KG）的事实表示为头实体、关系、尾实体 三元组$(h,r,t)$。TKG额外增加了时间戳，将事实表示为头实体、关系、尾实体、时间戳 四元组 $(s,r,o,t)$，从而能考虑时序动态。时间戳通常以年为计，如$(Obama, visit, China, 2014)$。TKG快照（snapshot）是指某个时间步下的TKG，它的包含的事实具有相同的时间戳。那么跨越一个个快照来推理缺失事实的任务，就称为TKGC。

引出
==

　　论文主要考虑过去的TKGC（Temporal KG Completion）模型学习模式的三个问题：

　　1、之前的方法并没有显式地将TKGC构造为一种增量学习问题——使模型能够适应训练数据的变化并能有效保留之前所学。它们将TKGC任务简单构造成KGC任务，仅仅在新的KG快照上对模型进行微调，导致灾难性遗忘。这点我不能苟同，既然在新的时间步下，过去的数据相对于当前已经无意义了。我就算把之前所学全都忘了，从新随机初始化模型在整个新快照上训练也行啊。我感觉作者想表达：不能有效利用之前时间步已经学到的与当前时间步相同的信息，导致过去训练的浪费。

　　2、以前的评价指标只能对整体的链接预测质量进行评估，例如Hits@10和MRR，而忽略了TKG的动态方面，缺少能评估模型对已删除事实的遗忘程度的指标。比如对于查询 (?, presidentOf, USA, 2021) ，我们希望模型对 Biden 的排名高于 Trump。

　　3、过去的方法在训练时使用了所有时间步中出现过的实体，不符合现实中实体从无到有的过程。

　　根据以上分析，本文做法如下：

　　1、将增量学习思想引入TKGC，提出incremental TKGC任务，并构建了TIE训练框架。

　　2、提出新的评估标准Deleted Facts Hits@10 (DF) 和 Reciprocal Rank Difference Measure (RRD)。

　　3、提出框架每次仅使用下一步快照更新的事实，而不是整个知识图进行训练，即增量学习。论文展示了它的训练效率（十倍于之前的微调模型），并且有与之前的训练方式相似的性能。

问题的定义和框架
========

问题定义
----

　　TKG被定义为KG的快照的集合，即$\\mathcal{G}=\\{G^1,G^2,...,G^T\\}$，$T$表示TKG的总时间步数。每个KG快照被表示为$G^t=\\{E^t,R^t,D^t\\}$。其中$E^t$，$R^t$和$D^t$分别表示该快照的实体集合、关系集合以及事实集合。事实由头实体、关系、尾实体、时间戳 四元组$(s,r,o,t)$表示。假设$\\bar{D}^t$为$t$时序下的真实事实四元组集合，我们已知$D^t$，则未知事实表示为$D\_{test}^t=\\bar{D}^t \\backslash D^t$。对于$(s,r,o,t)\\in D\_{test}^t$以及查询$(s,r,?,t)$，TKGC的目标就是使$o$的预测排名靠前。　　

编码器解码器框架
--------

　　论文提出的TIE框架分为为编码器和解码器两个部分。

　　编码器：编码器将实体和关系编码为$d$维的嵌入向量。论文本身并不提出新的嵌入向量的表示方法，它用两个之前提出的时序知识图嵌入方法来对TIE框架进行测试，DE和HyTE（[解读](https://blog.csdn.net/ngl567/article/details/114211828)）。为了引入时序信息，DE利用训练参数直接将其融入嵌入向量的一部分，HyTE则将关系表示投影到与时序相关的超平面。论文只列举了DE的计算方式，式 (1) 。

　　解码器：解码器就是某个事实的真实性的打分函数，文中直接用非时序KGC的打分方法，如式(2)所示。论文分别用ComplEx和TransE作为DE和HyTE的解码器。

评价指标
====

　　标准TKGC评价指标就是用Hits@k等，在某个KG快照上进行，如论文式 (3)。

　　而增量TKGC的评价指标需要评估两个方面：

　　1、Current and Historical Average Measure：也就是计算从$1$到$t$的所有时间步增量学习后的标准测试结果的平均。

　　2、Intransigence Measure：评估模型对已删除事实的识别能力。提出Deleted Facts Hits@10 (DF) 和 Reciprocal Rank Difference (RRD)。

　　DF如式（4）所示。评估模型是否将过去存在，而在当前$t$时间步已删除的事实排名靠前，指标越小越好。但是我感觉这个式子不对，首先，式子没排除$o'=o$的情况，而$(s,r,o)$不一定在$t$之前不存在。其次，即使$o'\\neq o$，$(s,r,o')$在$t$时间步也不一定不存在，因为可能会有实体一对多的情况。另外，我对DF这个指标本身的意义也有质疑。即使式子表达正确，为什么一定要把已删除的事实的排在$k$之外呢？排名一定存在前$k$个，如果已删除的事实排在$k$之后，除了正确事实之外，排在$k$以内的就剩下从未出现过的事实，难道把已删除的事实排在从未出现过的事实之后会使模型性能更好吗？也就是说：没必要。

　　RRD如式（5）所示。比较正确事实的预测排名和被删除事实的预测排名的倒数，由于正确事实应该相对于已删除事实排名靠前，指标越大越好。表达式感觉和DF有同样的问题。

TIE架构
=====

总体结构
----

　　图1展示了TIE的训练架构。依据不同的训练目标定义了5个损失。根据图1以及后面的定义，我们可以将训练数据流概括如下：

　　首先将训练数据的事实四元组按时序分为三类：重放缓冲区（Replay Buffer）$B^t$、旧事实（Old Facts）$D^{t-1}$、新事实（New Facts）$D^t$。其中新事实$D^t$就是当前$t$时间步的快照的事实集合，旧事实$D^{t-1}$是$t$前一步快照的事实集合。重放缓冲区$B^t$则是$t$步之前最近$\\tau$个快照的所有事实的集合：

$B^t=\\bigcup\\limits\_{i=\\max{(1,1-\\tau})}^{t-1}D^i$

　　然后从$B^t$、$D^{t-1}\\backslash D^t$和$D^{t}\\backslash D^{t-1}$中分别抽取重放事实$P^t$、删除事实$N^t$和新增事实$D^t\_{add}$来计算损失。其中$N^t$的抽样在图1中的表达和式(15)相冲突，式(15)是在$B^t \\backslash D^{t-1}$中抽取。抽取事实的实体和关系通过编码器得到嵌入，再输入解码器对事实进行打分，并计算相应的损失。

　　下面小节就是按顺序介绍图中的5个损失：$L\_{RKD},L\_{RCE},L\_{TR},L\_{del},L\_{CE}$。其中的一些关键技术包括：

　　1、利用增量学习的两个常用方法——experience replay和Temporal regularization techniques，来解决Fine-Tune的灾难性遗忘。

　　2、使用在最近快照中删除的事实作为训练的负样本来解决之前TKGC方法的intransigence问题，也就是过去的模型不能将随时间失效的知识抛弃。

　　3、只用每个时间步新增的事实来对模型进行训练，极大提升了训练效率。

经验重放（Experience Replay）
-----------------------

　　经验重放实际上就是抽取过去的数据继续参与训练，防止模型灾难性遗忘。论文考虑如何从重放缓冲区$B^t$中抽取重放事实$P^t$，并构建它们参与训练的方式。

### 重放事实的抽样策略

　　一种简单直接的抽样策略是均匀抽样。文中提出了一种依赖于事实成分出现频率和时序距离的抽样方式。

　　对于事实出现频率，论文首先定义了7个正则表达式来让事实进行匹配：

$P =\\{(s,r,o),(s,\*,o),(s,r,\*),(\*,r,o),(\*,r,\*),(s,\*,\*),(\*,\*,o)\\}$

　　其中$\*$代表任意匹配。然后定义所谓历史模式频率（Historical Pattern Frequency, HPF）和当下模式频率（Current Pattern Frequency, CPF）来表示某个事实在$P$的某个匹配模式下分别在$B^t$和$D^t$中的出现次数，如论文式(6)和式(7)所示。则定义某个事实$(s,r,o)$的频率依赖概率（frequency-dependent probability）$fp(s,r,o)$为文中式(8)：

$\\displaystyle fp(s,r,o)=\\sum\\limits\_{p\\in P}\\lambda\_p\\left\[\\log(h\_p^t+1)+\\gamma\\tau\\log(c\_p^t+1)\\right\]$

　　$\\lambda$是一些平衡数值的权重，$\\tau$是$B^t$的窗口大小，加对数是为了防止某些匹配数量过大。式子实际上就是表达了与这个事实有相同成分的所有事实（$B^t$和$D^t$中）的数量。

　　对于某个事实$(s,r,o,t')$的时序距离，论文表示为

$tp(t')=\\exp(\\frac{t'-t}{\\sigma})$

　　$\\sigma$是控制参数。则$(s,r,o,t')$被抽中的概率（未归一化）就被定义为

$ fp(s,r,o)tp(t')$

　　以上表示，事实所在时序与当前时序越接近，且其包含的成分（实体、关系）出现频率越高，该事实被抽中的概率越大。论文还提了一嘴相反的定义$\\frac{tp(t')}{fp(s,r,o)}$，在后面的实验中看到，其实和前者差别也不大╮(╯▽╰)╭。抽样策略算法如文中Algorithm 1所示。

### 学习策略

　　用以上策略抽取重放事实$P^t$后，就是考虑如何用这些事实参与训练。论文提出两个损失，重放交叉熵损失$L\_{RCE}$和重放知识精炼损失$L\_{RKD}$。

　　对于$L\_{RKD}$，论文将重放事实作为正例，使其打分最大化，并替换其实体生成相应的反例，使反例打分最小化。对于某个重放事实$(s,r,o,t')\\in P^t$，替换的实体集合定义如下：

 $D^-\_{s,r,t'}=\\{o'|o'\\in E^{t'}\_{known},(s,r,o',t')\\notin D^{t'}\\}$

　　其中$E^{t'}\_{known}$表示到$t'$时间步为止观察到的所有实体的集合。于是这个实体相关的损失被定义为文中式(10)，然后将$P^t$中所有实体相关的损失进行求和，如式(12)所示，就定义了所谓的$L\_{RKD}$。

　　对于$L\_{RCE}$，论文考虑到在$t$时间步对重放事实$P^t$的打分与前一时间步的打分应该变化不大。在训练$t$步之前，先保存在$t-1$步训练完成的模型在$P^t$上的损失分布。然后用KL散度来限制当前时间步对重放事实的过度训练，如式（11）所示，即$L\_{RCE}$。

时序正则化
-----

　　每当进入一个新的时间步$t$，TIE都使用前一时间步训练后的参数作为当前时间步的初始化训练参数，包括实体和关系的嵌入。如果出现新的实体或关系，其嵌入参数会进行随机初始化，如式（13）所示。

　　所谓时序正则化（Temporal Regularization），就是为了限制TIE对从之前时间步继承下来的参数的过度训练，防止灾难性遗忘。式（14）定义了相关损失$L\_{TR}$。

学习已删除事实
-------

　　$t$时间步的已删除事实定义为式（15），前面说过，定义与图1表示的不一样。相关损失$L\_{del}$定义为式（16），也就是最小化已删除事实的打分。

学习新增事实
------

　　TKG中每个时间步的事实总数以及新增事实数如图2所示。直接使用新时间步的所有事实进行训练的计算量很大，所以文中只用相对于前一步的新增事实进行训练，也就是增量学习的特点。相关损失$L\_{CE}$定义在式（17）和式（18），也就是最大化新增事实的打分，并最小化其反例的打分（前面定义了$q^t\_{s,r,o,t}$）。

最终损失
----

　　最终损失就是以上5个损失相加，论文式（19），实验时论文直接把所有$\\alpha$权重都设置为1。

实验
==

　　论文用YAGO11k和Wikidata12k进行实验，分别是YAGO和Wikidata的子集。使用的评价指标看论文6.2.2。实验结果看表1-4。

表1
--

　　表1进行了对比实验，比较5种不同的训练策略的性能。不同训练策略说明如下：

训练策略

FT

TR

TIE

FB

FB\_Future

说明

仅用$D^t\_{add}$进行微调，即损失只有$L\_{CE}$

FT的基础上+时序正则化，即$L\_{CE}+L\_{TR}$

TIE框架，即5个损失都用

用$B^t$和$D^t$的所有数据进行微调

用$1$到$T$时序的所有数据进行训练

　　可以看出TIE相对于用整个知识图谱，训练速度提升很大，并且有比较好的性能。DF和RRD这两个指标，对于知识图预测的有效性来说，感觉意义不大，就不说了。

表2
--

　　表2对经验重放的事实抽样策略进行了对比：

　　None：不用经验重放

　　Uniform：均匀抽样

　　Freq：抽样概率为$fp(s,r,o)tp(t')$

　　Inv\_Freq：抽样概率为$\\frac{tp(t')}{fp(s,r,o)}$

　　可以看出经验重放对性能有提升作用，但是不同的重放策略对性能看来影响不大。

表3
--

　　消融实验验证删除事实损失$L\_{del}$的有效性。可以看出这个损失对知识图谱补全的总体性能（C@10和A@10）是有害的。尽管能提升DF@10和RRD指标，但是这有什么意义呢？为了让已删除事实预测排名靠后，那么除了正确事实之外，排名挤到前头的就会是从未出现过的更错误的事实。感觉直接比较Hits@1我觉得更直接和有效。

表4
--

　　最后，分别使用微调（FT）和时序正则化（TR），对增量TKGC（Add）和原始TKGC（All）进行了对比。Add是只使用新增事实$D^t\_{add}$进行训练，All是使用当前时间步的所有事实$D^t$进行训练。可以看出，当使用时序正则化时，二者性能差异不大，而增量学习使训练数据量急剧减少，提升了训练效率。

总结
==

　　个人总结：论文提出了一个TKGC的增量学习框架，其表示学习主要通过定义5个损失函数来实现。另外还提出两个评价指标DF和RRD，来评估模型对已删除事实的识别能力。然而，如论文表3所示，损失$L\_{del}$对TKGC的总体性能（C@10和A@10）有害。我个人认为，与其让已删除事实排名靠后，不如直接就让真实事实排在第一，直接比较Hits@1更直接一些。因此我认为提出的这两个评价指标没有什么意义，并且损失$L\_{del}$可以去掉。最后，论文说这是首次将增量学习引入TKGC。增量学习的确在不损害较多性能的前提下很大程度地加快了TKGC的训练，这是TIE的优势之处。