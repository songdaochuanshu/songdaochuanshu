---
layout: post
title: "[论文][表情识别]Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin"
date: "2022-05-22T18:22:13.969Z"
---
\[论文\]\[表情识别\]Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin
===========================================================================================================

论文基本情况
======

发表时间及刊物/会议：2022 CVPR  
发表单位：西安电子科技大学， 香港中文大学，重庆邮电大学

问题背景
====

在大部分半监督学习方法中，一般而言，只有部分置信度高于提前设置的阈值的无标签数据被利用。由此说明，大部分半监督方法没有充分利用已有数据进行训练。

论文创新点
=====

设置了Adaptive Confidence Margin（自适应阈值）根据训练规律动态调整阈值，充分利用所有的无标签数据。

网络结构
====

![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522180618041-504540389.png)  
具体的训练步骤如下：  
初始基本设定：

1.  设置初始阈值，本文中，对于每个类别，阈值初始值为0.8。
2.  本模型借鉴Mean Teacher的思想，引入老师模型（ema\_model）。
3.  训练时有标签和无标签数据按1：1的比例输入网络
4.  模型backbone采用resent18，输出最后一层类别概率分布以及倒数第二层512维特征向量。

**学生网络**：  
对于有标签数据：  
有标签图片经过弱数据增强（WA）后进入模型，输出结果和真值对比，利用交叉熵函数作为损失函数，计算有标签损失\\(L^s\_{CE}\\)。  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522215139309-988042295.png)

对于无标签数据：  
无标签图片复制三份（a图, b图, c图），其中两份（a图，b图）经过弱数据增强（WA）后输入模型，再将输出的两个概率分布平均后得到最终的概率分布。记所得概率分布中的最大概率为\\(f\_{max}\\)，对应类别记作\\(c\\)。如果\\(f\_{max}\\)大于此类别\\(c\\)对应阈值，则将\\(c\\)作为此类别的真实标签，并将此类数据归为子集I(含“真实标签”)；否则，将此类数据归为子集II（无真实标签）。  
如果此图片属于子集I，则将c图经过强数据增强（SA）后送入网络，和标签\\(c\\)计算交叉熵损失\\(L^u\\)。  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522215107952-1284068405.png)

若此图片属于子集II，则\\(L^u=0\\)。并拼接a图，b图输入模型后得到的两个512特征向量，首先根据公式8 计算两个特征向量的相似度，再根据公式9计算SupConLoss \\(L^c\\) (具体计算方法见论文[Supervised Contrastive Learning](https://arxiv.org/pdf/2004.11362.pdf "Supervised Contrastive Learning"))。  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522215008412-253671085.png)

![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522214954907-1123126970.png)

总损失函数为:  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522215032169-1746002508.png)  
实验中\\(\\lambda\_1 = 0.5,\\lambda\_2=1,\\lambda\_3=0.1\\)。

**老师网络**  
学生模型根据损失函数更新模型参数后，老师网络在学生网络的基础上使用指数平均移动的方式更新参数。之后，将有标签数据输入老师网络，得到概率分布。  
对于一个batch的数据，记最大概率对应标签类别和真实类别相同的图片为集合\\(N\_{st}\\),记\\(N^c\_{st}\\)为最大概率对应标签类别和真实类别相同，且真实类别为\\(c\\)的图片张数，记\\(s\_i\\)为最大概率, \\(\\hat{y\_i}\\)为预测类别，按照以下公式计算一个类别的平均最大概率，记为\\(T\_c\\)。  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522220923318-1873863589.png)  
之后，考虑到置信值会随着epoch数逐步提高，再根据以下公式计算当前epoch各个类别的阈值。  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522221100584-150071756.png)

至此，一个iteration结束。

实验
==

表1 固定阈值和我们方法的比较，在RAF-DB， SFEW数据集上的结果，其中FT 表示使用FixMatch方法时取固定阈值的具体值，  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522222402189-1717254745.png)

表2 RAF-DB, SFEW 和AffectNet三个数据集上我们的方法和其他优秀的半监督方法对比  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522221936401-1300929311.png)

图3 自适应阈值调整方法，公式5中关于两个参数的值的消融实验  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522222131943-167647471.png)

表3 使用WideResNet-28-2作为backbone在RAFDB上实验结果  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522224707588-2025545834.png)

图4 使用2D t-SNE 可视化方法可视化得到的特征，从图中可以看出，我们的方法对各类表情提取特征的效果最好（不同类别的特征重合度最小）。  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522224828795-942556353.png)

表4 各个不同类别的数据集交叉验证结果。以下结果为在RAF-DB上训练，CK+数据集上进行测试所得结果  
![image](https://img2022.cnblogs.com/blog/2742224/202205/2742224-20220522225057443-1694150661.png)