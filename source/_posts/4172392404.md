---
layout: post
title: "分布式存储系统之Ceph基础"
date: "2022-10-01T22:21:43.169Z"
---
分布式存储系统之Ceph基础
==============

![分布式存储系统之Ceph基础](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922200654116-732945835.png) Ceph是一个对象式存储系统，所谓对象式存储是指它把每一个待管理的数据流（比如一个文件）切分成一到多个固定大小的对象数据，并以其为原子单元完成数据的存取；对象数据的底层存储服务由多个主机组成的存储集群；该集群被称之为RADOS（Reliable Automatic Distributed Object Store）集群；翻译成中文就是可靠的、自动化分布式对象存储系统；

　　Ceph基础概述

　　Ceph是一个对象式存储系统，所谓对象式存储是指它把每一个待管理的数据流（比如一个文件）切分成一到多个固定大小的对象数据，并以其为原子单元完成数据的存取；对象数据的底层存储服务由多个主机组成的存储集群；该集群被称之为RADOS（Reliable Automatic Distributed Object Store）集群；翻译成中文就是可靠的、自动化分布式对象存储系统；

　　Ceph架构

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922173918015-1744174570.png)

　　提示：librados是RADOS存储集群的API，它支持C、C++、java、python和php等变成语言；RADOSGW、RBD、CEPHFS都是RADOS存储服务的客户接口；它们分别把rados存储服务接口librados从不同角度做了进一步的抽象，因而各自适用于不同的应用场景；RADOSGW是将底层rados存储服务抽象为以RESTful风格接口提供对象存储服务，适用于存取对象数据的接口，比如web服务；RBD是将底层RADOS存储服务抽象为块设备的存储设备；主要用于虚拟化，比如给虚拟机提供硬盘；CEPHFS是将底层RADOS抽象为一个文件系统接口，供其他主机使用；

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922173857877-1660062042.png)

　　提示：RADOS集群主要由Monitors、Managers、Ceph OSDs、Ceph MDSs这几个组件组成；其中Monitor主要作用是监控整个集群的状态，健康与否等；它拥有整个集群的运行图（monitor map、manager map、OSD map、和CRUSH map）；除此之外，它还负责维护集群各组件之间以及客户端接入RADOS集群存取数据时的认证信息和实行认证；简单讲mon组件就是管理和维护其他组件状态以及接入RADOS集群的认证信息并实行认证，一旦mon组件所在主机宕机，那么整个集群将不可用；有点类似k8s里的etcd；所以为实现冗余和高可用性，通常在集群我们部署大于1的奇数个mon（因为它使用Paxos协议，为防止网络分区等原因，保证服务的正常可用）；manager组件主要负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率，当前性能指标和系统负载等，Ceph集群信息，包括基于web的Ceph管理器仪表板和REST API。高可用性通常需要至少两个mgr组件。OSD组件是存储数据，处理数据复制、恢复、再平衡，并提供一些监视信息的组件；Ceph通过检查其他Ceph OSD进程来监控和管理心跳；通常为了高可用和冗余，至少需要3个ceph osd（即3块硬盘，ceph为了每一个osd能够被单独使用和管理，每一个osd都会有一个单独的守护进程ceph-osd来管理，即服务器上有多少个osd,就会有多少个ceph-osd进程，一个ceph-osd进程就对应一个osd，一个osd就对应一块磁盘设备）；MDS是ceph元数据服务组件，主要实现分布式文件系统的控制层面，数据和元数据的存取依然由RADOS负责，即用户使用cephfs文件系统存取数据，用户存储的文件的元数据该怎么存放、怎么管理等都由MDS组件负责；当然如果我们没有使用cephfs文件系统的必要，对应mds组件也可以不用部署；所以mds组件不是必须组件；

　　Ceph数据抽象接口（客户端中间层）

　　Ceph存储集群提供了基础的对象数据存储服务，客户端可基于RADOS协议和librados API直接与存储系统交互进行对象数据存取； librados提供了访问RADOS存储集群支持异步通信的API接口，支持对集群中对象数据的直接并行访问，用户可通过支持的编程语言开发自定义客户端程序通过RADOS协议与存储系统进行交互；客户端应用程序必须与librados绑定方可连接到RADOS存储集群，因此，用户必须事先安装librados及其依赖后才能编写使用librados的应用程序； librados API本身是用C ++编写的，它额外支持C、Python、Java和PHP等开发接口；当然，并非所有用户都有能力自定义开发接口以接入RADOS存储集群的需要，为此，Ceph也原生提供了几个较高级别的客户端接口，它们分别是RADOS GateWay（RGW）、ReliableBlock Device（RBD）和MDS（MetaData Server），分别为用户提供RESTful、块和POSIX文件系统接口；

　　Ceph文件系统

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922184047912-361881719.png)

　　不同于传统文件系统的地方是，CephFS MDS在设计的初衷之一即高度可扩展的能力，其实现机制中，数据由客户端以分布式方式通过多路OSD直接存储于RADOS系统，而元数据则由MDS组织管理后仍然存储于RADOS系统之上； MDS仅是实现了分布式文件系统的控制平面，数据和元数据的存取依然由RADOS负责；CephFS依赖于独立运行的守护进程ceph-mds向客户端提供服务；

　　Ceph块设备

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922184138675-1260898091.png)

　　存储领域中，“块（block）”是进行数据存取的主要形式，块设备也于是成为了主流的设备形式，因此，RBD虚拟块设备也就成了Ceph之上广为人知及非常受欢迎的访问接口；RBD的服务接口无须依赖于特定的守护进程，只要客户端主机有对应内核模块librbd，就可以通过ceph RBD 接口使用；

　　Ceph对象网关

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922184442339-1036236296.png)

　　Ceph对象网关是一个建立在librados之上的对象存储接口为应用程序提供的Ceph存储集群的RESTful网关。Ceph对象存储支持两个接口s3和Swift；Ceph对象存储使用Ceph对象网关守护进程(radosgw)，它是一个HTTP服务器，用于与Ceph存储集群交互；RGW依赖于在RADOS集群基础上独立运行的守护进程（ceph-radosgw）基于http或https协议提供相关的API服务，不过，通常仅在需要以REST对象形式存取数据时才部署RGW；

　　管理节点（admin host）

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922185254779-1140369364.png)

　　Ceph的常用管理接口是一组命令行工具程序，例如rados、ceph、rbd等命令，管理员可以从某个特定的MON节点执行管理操作，但也有人更倾向于使用专用的管理节点；事实上，专用的管理节点有助于在Ceph相关的程序升级或硬件维护期间为管理员提供一个完整的、独立的并隔离于存储集群之外的操作环境，从而避免因重启或意外中断而导致维护操作异常中断；

　　存储池、PG（Placement Group）和OSD（Object Store Device）之间的关系

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922192600433-547937789.png)

　　提示：存储在Ceph存储系统之上的数据，都会先由Ceph将对应数据切分等额大小的对象数据，然后将这些数据存储到对应的存储池中；存储池主要作用是向外界展示集群的逻辑分区；对于每个存储池，我们可以定义一组规则，比如每个对象数据需要有多少个副本存在；PG是Placement Group的缩写，归置组；它是一个虚拟的概念，主要作用是用于将对象数据映射到osd上而存在的，对象数据具体通过那个pg存放在那个osd上，这个是根据ceph的crush算法动态映射的；我们可以理解为根据存储池中PG的数量，结合一致性hash将对象数据动态映射至PG上，然后PG根据osd数量结合一致性hash动态映射到不同的磁盘上；我们在创建存储池的时候就必须指定一定数量的PG；如上图所示，我们将数据存储到存储池B中，那么对应数据就会根据存储池B中的规则进行存储，即3个副本；在存储池B中只有2个归置组，两个归置组分别对应了不同的OSD；如果将数据存储到PG3上，那么对应数据就会在osd2\\6\\7分别存储一份以做备份；如果将数据存储到PG4上，这对应数据就会被分发到osd3\\4\\9上进行存储；

　　File Store  和 Blue Store

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922194107220-1423203189.png)

　　什么是file store呢？file store是传统ceph存储对象的方法，它依赖于xfs文件系统（ext4有bug和缺陷，会导致数据丢失，所以ceph只支持xfs上使用file store）；通过上述的描述，我们知道ceph不管是什么客户端提交的对象数据，最终都会存储到osd所在主机的硬盘上；那么问题来了，osd所在主机的磁盘是怎么被osd管理的呢？传统ceph是将对应磁盘格式化分区挂载在osd所在主机的文件系统，被osd所在主机以一个目录的形式表示；即用户存储的对象数据，最后会被存放为一个文件的形式存放在osd所在主机的磁盘上；这也意味着我们需要存放文件本身的元数据和数据；除此之外对象本身也有数据和元数据，那么对象本身的数据和元数据是怎么存放的呢？file store是是将对象的元数据存放在leveldb（早期ceph版本）中，数据存放在文件系统的数据区；简单讲file store就是将对象的元数据存放在leveldb中，数据存放在osd所在主机的文件系统的数据区中，中间有文件系统做转换的过程；而blue store将osd所在主机的磁盘不格式化分区，而是直接用裸设备硬盘被osd识别和管理；在osd所在主机对应osd进程会将自己管理的磁盘中一小部分格式化为bluefs文件系统，用于安装使用rocksdb；即客户端提交的对象数据，对象本身的元数据会被存放在rocksdb中，数据会被直接存放在磁盘上（由osd进程直接管理数据格式等，不会存放为文件）；rocksdb为了管理它自己本身的数据持久化，它也会维护一个日志文件，类似redis的aof；这样一来blue store的方式存储对象数据，在磁盘上就会存在三种数据，第一种是对象本身的数据，第二种是对象的元数据，第三种就是rocksdb的日志文件；ceph为了使存储的数据更高效，它支持将blue store方式的三种数据分别存放不同的磁盘，如下图

 ![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220922220751187-621575027.png)

　　提示：如果是单块硬盘，那没得说三种数据直接存入到一块磁盘上；如果是两块磁盘，我们可以将blue store的日志数据和对象元数据存放在一个高性能的磁盘上，如nvme，ssd上，将对象数据存放在一个大的机械硬盘上；当然也可以将blue store的日志文件存放在高性能磁盘上，对象的元数据和数据存放在机械硬盘上；如果有三块硬盘，我们可以将blue store的日志数据存放在高性能磁盘上，比如512M的nvme的磁盘；使用1G或2G的ssd来存储对象的元数据；用机械硬盘来存储对象数据；

作者：[Linux-1874](https://www.cnblogs.com/qiuhom-1874/)

出处：[https://www.cnblogs.com/qiuhom-1874/](https://www.cnblogs.com/qiuhom-1874/)

本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留追究法律责任的权利.