---
layout: post
title: "OpenAI Chat completion API 入门指南"
date: "2023-03-04T01:12:30.659Z"
---
OpenAI Chat completion API 入门指南
===============================

ChatGPT 由 OpenAI 最先进的语言模型 gpt-3.5-turbo 提供支持。

使用 OpenAI API，您可以使用 GPT-3.5-turbo 构建自己的程序来做一些如下的事情：

*   起草电子邮件或其他书面文件
*   编写 Python 代码
*   回答关于一组文档的问题
*   创建对话代理程序
*   为你的软件提供自然语言接口
*   充当导师辅导多学科
*   充当翻译
*   模拟游戏中的角色等等

**1.模型介绍**

GPT-3.5-turbo 模型是以一系列消息作为输入，并将模型生成的消息作为输出。

    # Note: you need to be using OpenAI Python v0.27.0 for the code below to work
    import openai
    
    openai.ChatCompletion.create(
      model="gpt-3.5-turbo",
      messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"},
            {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
            {"role": "user", "content": "Where was it played?"}
        ]
    )
    

消息是一个对象数组，其中每个对象都有一个角色，一共有三种角色。

*   **系统** 消息有助于设置助手的行为。在上面的例子中，助手被指示 “你是一个得力的助手”。
*   **用户** 消息有助于指导助手。 就是用户说的话，向助手提的问题。
*   **助手** 消息有助于存储先前的回复。这是为了持续对话，提供会话的上下文。

**2.建立持续会话**

![](https://img2023.cnblogs.com/blog/1323675/202303/1323675-20230303223957865-473730693.png)

在这个 ChatGPT 的会话场景中，第一行文本告诉模型 它是一个翻译家

然后，在交替的会话中，ChatGPT 会将用户发送的英文句子翻译成中文再响应给用户，这就是一个有上下文的持续会话。

GPT-3.5-turbo 模型是没有记忆的，不会记录之前的 **请求上下文**，所有相关信息都必须通过对话提供，这样才能保持持续的会话。

通常，对话的格式为先是系统消息，然后是交替的用户和助手消息。在 Chat completion API 接口中，我们可以实现这个上下文请求

    completion = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "你是一个翻译家"},
            {"role": "user", "content": "将我发你的英文句子翻译成中文，你不需要理解内容的含义作出回答。"},
            {"role": "user", "content": "Draft an email or other piece of writing."}
        ]
    )
    

助手响应输出

    {
        "id": "chatcmpl-6q0Kqgk2qlcpCGDYcLQnUmUVVrMd6",
        "object": "chat.completion",
        "created": 1677852364,
        "model": "gpt-3.5-turbo-0301",
        "usage": {
            "prompt_tokens": 69,
            "completion_tokens": 20,
            "total_tokens": 89
        },
        "choices": [
            {
                "message": {
                    "role": "assistant",
                    "content": "起草一封电子邮件或其他写作材料。"
                },
                "finish_reason": "stop",
                "index": 0
            }
        ]
    }
    

**3.管理 Token**

语言模型以称为 tokens 的块读取文本。在英语中，一个 token 可以短至一个字符或长至一个单词（例如，a 或 apple），在某些语言中，token 可以比一个字符更短，也可以比一个单词长。

例如，字符串 “ChatGPT is great！” 被编码成六个 token：\[“Chat”, “G”, “PT”, “ is”, “ great”, “!”\]。

API 调用中的 token 总数会影响：

*   API 调用成本：因为您需要为为每个 token 支付费用
*   API 调用响应时间：因为写入更多令牌需要更多时间
*   API 调用是否有效：因为令牌总数必须是 低于模型的最大限制（gpt-3.5-turbo-0301 为 4096 个令牌）

**4.Token 计费方式**

输入和输出标记都计入这些数量。例如，如果您的 API 调用在消息输入中使用了 10 个 token，并且在消息输出中收到了 20 个 token，您将被收取 30 个token 的费用。API 响应中的 usage 字段显示了本次调用使用了多少 token

    {
    		"usage": {
            "prompt_tokens": 69,
            "completion_tokens": 20,
            "total_tokens": 89
        }
    }
    

**5.计算 Token 消耗**

要在不调用 API 的情况下查看文本字符串中有多少个 token，请使用 OpenAI 的 tiktoken Python 库。 示例代码可以在 [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) 关于如何使用 tiktoken 计算令牌的指南中找到。

    import tiktoken
    
    def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):
        """Returns the number of tokens used by a list of messages."""
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")
        if model == "gpt-3.5-turbo-0301":  # note: future models may deviate from this
            num_tokens = 0
            for message in messages:
                num_tokens += 4  # every message follows <im_start>{role/name}\n{content}<im_end>\n
                for key, value in message.items():
                    num_tokens += len(encoding.encode(value))
                    if key == "name":  # if there's a name, the role is omitted
                        num_tokens += -1  # role is always required and always 1 token
            num_tokens += 2  # every reply is primed with <im_start>assistant
            return num_tokens
        else:
            raise NotImplementedError(f"""num_tokens_from_messages() is not presently implemented for model {model}.
    See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.""")
    
    messages = [
        {"role": "system", "content": "你是一个翻译家"},
        {"role": "user", "content": "将我发你的英文句子翻译成中文，你不需要理解内容的含义作出回答。"},
        {"role": "user", "content": "Draft an email or other piece of writing."}
    ]
    
    # example token count from the function defined above
    model = "gpt-3.5-turbo-0301"
    
    print(f"{num_tokens_from_messages(messages, model)} prompt tokens counted.")
    # output: 69 prompt tokens counted.
    

另请注意，非常长的对话更有可能收到不完整的回复。例如，一个长度为 4090 个 token 的 gpt-3.5-turbo 对话将在只回复了 6 个 token 后被截断。