---
layout: post
title: "基于Unet+opencv实现天空对象的分割、替换和美化"
date: "2022-12-21T19:12:53.372Z"
---
基于Unet+opencv实现天空对象的分割、替换和美化
----------------------------

![基于Unet+opencv实现天空对象的分割、替换和美化](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221113229733-1903827615.png) 传统图像处理算法进行“天空分割”存在精度问题且调参复杂，无法很好地应对云雾、阴霾等情况；本篇文章分享的“基于Unet+opencv实现天空对象的分割、替换和美化”，较好地解决了该问题，包括以下内容： 1、基于Unet语义分割的基本原理、环境构建、参数调节等 2、一种有效的天空分割数据集准备方法，并且获得数据集 3、基于OpenCV的Pytorch模型部署方法 4、融合效果极好的 SeamlessClone 技术 5、饱和度调整、颜色域等基础图像处理知识和编码技术 本文适合具备 OpenCV 和Pytorch相关基础，对“天空替换”感兴趣的人士。学完本文，可以获得基于Pytorch和OpenCV进行语义分割、解决实际问题的具体方法，提高环境构建、数据集准备、参数调节和运行部署等方面综合能力。

     原文地址：https://www.cnblogs.com/jsxyhelu/p/16995892.html

     传统图像处理算法进行“天空分割”存在精度问题且调参复杂，无法很好地应对云雾、阴霾等情况；本篇文章分享的“基于Unet+opencv实现天空对象的分割、替换和美化”，较好地解决了该问题，包括以下内容：

1、基于Unet语义分割的基本原理、环境构建、参数调节等
2、一种有效的天空分割数据集准备方法，并且获得数据集
3、基于OpenCV的Pytorch模型部署方法
4、融合效果极好的 SeamlessClone 技术
5、饱和度调整、颜色域等基础图像处理知识和编码技术

    本文适合具备 OpenCV 和Pytorch相关基础，对“天空替换”感兴趣的人士。学完本文，可以获得基于Pytorch和OpenCV进行语义分割、解决实际问题的具体方法，提高环境构建、数据集准备、参数调节和运行部署等方面综合能力。

 **一、传统方法和语义分割基础**

**1.1传统方法主要通过**“颜色域”来进行分割

比如，我们要找的是蓝天，那么在HSV域，就可以通过查表的方法找出蓝色区域。 

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221111845913-47476091.png)

在这张表中，蓝色的HSV的上下门限已经标注出来，我们编码实现。

    cvtColor(matSrc,temp,COLOR\_BGR2HSV);
    split(temp,planes);
    equalizeHist(planes\[2\],planes\[2\]);//对v通道进行equalizeHist
    merge(planes,temp);
    inRange(temp,Scalar(100,43,46),Scalar(124,255,255),temp);
    erode(temp,temp,Mat());//形态学变换，填补内部空洞
    dilate(temp,temp,Mat());
    imshow("原始图",matSrc);

在这段代码中，有两个小技巧，一个是对模板(MASK)进行了形态学变化，这个不展开说；一个是我们首先对HSV图进行了3通道分解，并且直方图增强V通道，而后将3通道合并回去。通过这种方法能够增强原图对比度，让蓝天更蓝、青山更青……大家可以自己调试看一下。 显示处理后识别为天空的结果（在OpenCV中，白色代表1也就是由数据，黑色代表0也就是没数据） 

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221111907364-560608191.png)

对于天坛这幅图来说，效果不错。虽然在右上角错误，而塔中间的一个很小的空洞，这些后期都是可以规避掉的错误。 

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112001233-1439155634.png)

但是对于阴霾图片来说，由于天空中没有蓝色，识别起来就很错误很多。

**1.2 语义分割基础**

图像语义分割（semantic segmentation），从字面意思上理解就是让计算机根据图像的语义来进行分割，例如让计算机在输入下面左图的情况下，能够输出右图。语义在语音识别中指的是语音的意思，在图像领域，语义指的是图像的内容，对图片意思的理解，比如左图的语义就是三个人骑着三辆自行车；分割的意思是从像素的角度分割出图片中的不同对象，对原图中的每个像素都进行标注，比如右图中粉红色代表人，绿色代表自行车。

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112014228-1302444302.png)

那么对于天空分割问题来说，主要目标就是找到像素级别的天空对象，使用语义分割模型就是有效的。

**二、Unet基本情况和环境构建**

Unet 发表于 2015 年，属于 FCN 的一种变体，Unet 的初衷是为了解决生物医学图像方面的问题，由于效果确实很好后来也被广泛的应用在语义分割的各个方向，比如卫星图像分割，工业瑕疵检测等。它也有很多变体，但是对于天空分割问题来看，Unet的能力已经够了。

Unet 跟 FCN 都是 Encoder-Decoder 结构，结构简单但很有效。Encoder 负责特征提取，你可以将自己熟悉的各种特征提取网络放在这个位置。由于在医学方面，样本收集较为困难，作者为了解决这个问题，应用了图像增强的方法，在数据集有限的情况下获得了不错的精度。

 ![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112039553-1929137977.png)

如上图，Unet 网络结构是对称的，形似英文字母 U 所以被称为 Unet。整张图都是由蓝/白色框与各种颜色的箭头组成，其中，蓝/白色框表示 feature map；蓝色箭头表示 3x3 卷积，用于特征提取；灰色箭头表示 skip-connection，用于特征融合；红色箭头表示池化 pooling，用于降低维度；绿色箭头表示上采样 upsample，用于恢复维度；青色箭头表示 1x1 卷积，用于输出结果。

在环境构建这块，我建议一定要结合自己的实际情况，构建专用的代码库，这样才能够通过不断迭代，在总体正确的前提下形成自己风格。

在我的库中，基于现有的Unet代码进行了修改

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112053424-173834107.png)

其中checkpoints、data保持数据；unet是模型的具体实现，未来可以扩充为多模型；utils是常用函数；alibaba.py和oss2helper.py是阿里云的辅助函数;export\_unet.py是输出函数；eveluate.py和train.py用于训练；predict.py用于本地测试；main.py是主要函数。

**三、数据集准备和增强**

3.1 数据集准备这块，我采取了增强的方法。由于个人习惯问题，采用的是OpenCV本地变换的方法

   

 getFiles("e:/template/Data\_sky/data", fileNames);
    string saveFile = "e:/template/Data\_sky/dataEX3/";
    for (int index = 0; index < fileNames.size(); index++)
    {
        Mat src \= imread(fileNames\[index\]);
        Mat dst;
        string fileName;
        getFileName(fileNames\[index\], fileName);
        resize(src, dst, cv::Size(512, 512));
        imwrite(saveFile \+ fileName + "\_512.jpg", dst);
        resize(src, dst, cv::Size(256, 256));
        imwrite(saveFile \+ fileName + "\_256.jpg", dst);
        resize(src, dst, cv::Size(128, 128));
        imwrite(saveFile \+ fileName + "\_128.jpg", dst);
        cout << fileName << endl;
    }
    fileNames.clear();
    getFiles("e:/template/Data\_sky/mask", fileNames);
    saveFile \= "e:/template/Data\_sky/maskEX3/";
    for (int index = 0; index < fileNames.size(); index++)
    {
        Mat src \= imread(fileNames\[index\], 0);
        Mat dst;
        string fileName;
        getFileName(fileNames\[index\], fileName);
        fileName \= fileName.substr(0, fileName.size() - 3);
        resize(src, dst, cv::Size(512, 512));
        imwrite(saveFile \+ fileName + "\_512\_gt.jpg", dst);
        resize(src, dst, cv::Size(256, 256));
        imwrite(saveFile \+ fileName + "\_256\_gt.jpg", dst);
        resize(src, dst, cv::Size(128, 128));
        imwrite(saveFile \+ fileName + "\_128\_gt.jpg", dst);
        cout << fileName << endl;
    }

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112137483-1188834795.png)

从而获得不同分辨率的目标数据，但是如何获得标注数据？我推荐一种方法。

3.2、通过对“阿里视觉智能开放平台”的研究，调用它的成果来进行训练。简单来说，它提供了天空分割的功能，但是要求数据的输入输出都保存在oss中，所以需要通过python来编写脚本。我对这段python代码进行了一些注释，放在这里。

\# -\*- coding: utf8 -\*-
from aliyunsdkcore.client import AcsClient
from aliyunsdkimageseg.request.v20191230 import SegmentSkyRequest
from aliyunsdkimageseg.request.v20191230.SegmentHDSkyRequest import SegmentHDSkyRequest
import oss2
import os
import json
import urllib


# 创建 AcsClient 实例
client \= AcsClient("LTAI5tQCCmMyKSfifwsFHLpC", "JyzNfHsCnUaVTeS6Xg3ylMjQFC8C6L", "cn-shanghai")
request \= SegmentSkyRequest.SegmentSkyRequest()
endpoint \= "https://oss-cn-shanghai.aliyuncs.com"
accesskey\_id \= "LTAI5tQCCmMyKSfifwsFHLpC"
accesskey\_secret \= "JyzNfHsCnUaVTeS6Xg3ylMjQFC8C6L"
bucket\_name \= "datasky2"
bucket\_name2 \= "viapi-cn-shanghai-dha-segmenter"

#本地文件保存路径前缀
download\_local\_save\_prefix \= "/home/helu/GOPytorchHelper/data/dataOss/"

'''
列举prefix全部文件
'''
def prefix\_all\_list(bucket,prefix):
    print("开始列举"+prefix+"全部文件");
    oss\_file\_size \= 0;
    for obj in oss2.ObjectIterator(bucket, prefix ='%s/'%prefix):
         print(' key : ' + obj.key)
         oss\_file\_size \= oss\_file\_size + 1;
         download\_to\_local(bucket, obj.key, obj.key);
    print(prefix +" file size " + str(oss\_file\_size));

'''
列举全部的根目录文件夹、文件
'''
def root\_directory\_list(bucket):
    # 设置Delimiter参数为正斜线（/）。
    for obj in oss2.ObjectIterator(bucket, delimiter='/'):
        # 通过is\_prefix方法判断obj是否为文件夹。
        if obj.is\_prefix():  # 文件夹
            print('directory: ' + obj.key);
            prefix\_all\_list(bucket,str(obj.key).strip("/")); #去除/
        else:  # 文件
            print('file: ' +obj.key)
            # 填写Object完整路径，例如exampledir/exampleobject.txt。Object完整路径中不能包含Bucket名称。
            object\_name \= obj.key
            # 生成下载文件的签名URL，有效时间为60秒。
            # 生成签名URL时，OSS默认会对Object完整路径中的正斜线（/）进行转义，从而导致生成的签名URL无法直接使用。
            # 设置slash\_safe为True，OSS不会对Object完整路径中的正斜线（/）进行转义，此时生成的签名URL可以直接使用。
            url \= bucket.sign\_url('GET', object\_name, 60, slash\_safe=True)     
            print('签名url的地址为：', url)
            ## 如下url替换为自有的上海region的oss文件地址
            request.set\_ImageURL(url)
            response \= client.do\_action\_with\_exception(request)
            print('response地址为：', response)
            user\_dict \= json.loads(response)
            for name in user\_dict.keys():
                if(name.title() == "Data"):
                    inner\_dict \= user\_dict\[name\]
                    for innerName in inner\_dict.keys():
                        if(innerName == "ImageURL"):
                            finalName \= inner\_dict\[innerName\]
                            print('finalName地址为：',str(finalName))
                            urllib.request.urlretrieve(str(finalName), download\_local\_save\_prefix+obj.key)
'''
下载文件到本地
'''
def download\_to\_local(bucket,object\_name,local\_file):
    url \= download\_local\_save\_prefix + local\_file;
    #文件名称
    file\_name \= url\[url.rindex("/")+1:\]
    file\_path\_prefix \= url.replace(file\_name, "")
    if False == os.path.exists(file\_path\_prefix):
        os.makedirs(file\_path\_prefix);
        print("directory don't not makedirs "+  file\_path\_prefix);
    # 下载OSS文件到本地文件。如果指定的本地文件存在会覆盖，不存在则新建。
    bucket.get\_object\_to\_file(object\_name, download\_local\_save\_prefix+local\_file);

if \_\_name\_\_ == '\_\_main\_\_':
    print("start \\n");
    # 阿里云主账号AccessKey拥有所有API的访问权限，风险很高。强烈建议您创建并使用RAM账号进行API访问或日常运维，请登录 https://ram.console.aliyun.com 创建RAM账号。
    auth = oss2.Auth(accesskey\_id,accesskey\_secret)
    # Endpoint以杭州为例，其它Region请按实际情况填写。
    bucket \= oss2.Bucket(auth,endpoint , bucket\_name)
    bucket2\= oss2.Bucket(auth,endpoint , bucket\_name2)
    #单个文件夹下载
    root\_directory\_list(bucket);
    print("end \\n");

**四、模型训练概要**

将数据集放入项目中，运行u2net\_train.py即可。

4.1读懂训练部分代码，其中在step5的地方，我添加了一段处理，用于float和int类型之间转换

 # 5. Begin training
    for epoch in range(epochs):
        net.train()
        epoch\_loss \= 0
        with tqdm(total\=n\_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:
            for batch in train\_loader:
                images \= batch\['image'\]
                true\_masks \= batch\['mask'\]

                assert images.shape\[1\] == net.n\_channels, \\
                    f'Network has been defined with {net.n\_channels} input channels, ' \\
                    f'but loaded images have {images.shape\[1\]} channels. Please check that ' \\
                    'the images are loaded correctly.'

                images \= images.to(device=device, dtype=torch.float32)
                true\_masks \= true\_masks.to(device=device, dtype=torch.long)
                ######
                one \= torch.ones\_like(true\_masks)
                zero \= torch.zeros\_like(true\_masks)
                true\_masks \= torch.where(true\_masks>0,one,zero)
                #####
    
                with torch.cuda.amp.autocast(enabled\=amp):
                    masks\_pred \= net(images)
                    loss \= criterion(masks\_pred, true\_masks) \\
                           \+ dice\_loss(F.softmax(masks\_pred, dim=1).float(),
                                       F.one\_hot(true\_masks, net.n\_classes).permute(0, 3, 1, 2).float(),
                                       multiclass\=True)

                optimizer.zero\_grad(set\_to\_none\=True)
                grad\_scaler.scale(loss).backward()
                grad\_scaler.step(optimizer)
                grad\_scaler.update()

                pbar.update(images.shape\[0\])
                global\_step += 1
                epoch\_loss += loss.item()
                
                pbar.set\_postfix(\*\*{'loss (batch)': loss.item()})

                # Evaluation round
                division\_step \= (n\_train // (10 \* batch\_size))
                if division\_step > 0:
                    if global\_step % division\_step == 0:
                        histograms \= {}
                        for tag, value in net.named\_parameters():
                            tag \= tag.replace('/', '.')
                           
                        val\_score \= evaluate(net, val\_loader, device)
                        scheduler.step(val\_score)

                        logging.info('Validation Dice score: {}'.format(val\_score))

        if save\_checkpoint:
            Path(dir\_checkpoint).mkdir(parents\=True, exist\_ok=True)
            torch.save(net.state\_dict(), str(dir\_checkpoint / 'checkpoint\_epoch{}.pth'.format(epoch + 1)))
            logging.info(f'Checkpoint {epoch + 1} saved!')

**4.2 推荐适当投资，**采购了autodl进行在线训练

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112220897-215412444.png)

通过predict生成模板结果，在Photoshop中进行比对发现边界已经比较贴合，最终在增强的数据集上，实现了DICE90%的目标。

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112230687-1976357073.png)

**五、基于OpenCV的Pytorch模型部署方法**

**这里为了进行总结，我对分别对目前使用Python和C++下的几种可行可用的推断方法进行汇总，并进一步比对。**

**5.1** **（python）****使用onnxruntime****方法进行推断**

session = onnxruntime.InferenceSession("转换的onnx文件")
input\_name \= session.get\_inputs()\[0\].name
label\_name \= session.get\_outputs()\[0\].name

img\_name\_list \= \['需要处理的图片'\]
image \= Image.open(img\_name\_list\[0\])
w, h \= image.size
dataset \= SalObjDataset(
    img\_name\_list\=img\_name\_list,
    lbl\_name\_list\=\[\],
    transform\=transforms.Compose(\[RescaleT(320), ToTensorLab(flag=0)\])
)
data\_loader \= DataLoader(
    dataset,
    batch\_size\=1,
    shuffle\=False,
    num\_workers\=1
)
im \= list(data\_loader)\[0\]\['image'\]
inputs\_test \= im
inputs\_test \= inputs\_test.type(torch.FloatTensor)
with torch.no\_grad():
    inputs\_test \= Variable(inputs\_test)
res \= session.run(\[label\_name\], {input\_name: inputs\_test.numpy().astype(np.float32)})
result \= torch.from\_numpy(res\[0\])
pred \= result\[:, 0, :, :\]
pred \= normPRED(pred)
pred \= pred.squeeze()
predict\_np \= pred.cpu().data.numpy()
im \= Image.fromarray(predict\_np \* 255).convert('RGB')
im \= im.resize((w, h), resample=Image.BILINEAR)
im.show()

**5.2** **（python）** **使用opencv方法**

import os
import argparse

from skimage import io, transform
import numpy as np
from PIL import Image
import cv2 as cv

parser \= argparse.ArgumentParser(description='Demo: U2Net Inference Using OpenCV')
parser.add\_argument('\--input', '\-i')
parser.add\_argument('\--model', '\-m', default\='u2net\_human\_seg.onnx')
args \= parser.parse\_args()

def normPred(d):
    ma \= np.amax(d)
    mi \= np.amin(d)
    return (d - mi)/(ma - mi)

def save\_output(image\_name, predict):
    img \= cv.imread(image\_name)
    h, w, \_ \= img.shape
    predict \= np.squeeze(predict, axis=0)
    img\_p \= (predict \* 255).astype(np.uint8)
    img\_p \= cv.resize(img\_p, (w, h))
    print('{}-result-opencv\_dnn.png-------------------------------------'.format(image\_name))
    cv.imwrite('{}-result-opencv\_dnn.png'.format(image\_name), img\_p)

def main():
    # load net
    net \= cv.dnn.readNet('saved\_models/sky\_split.onnx')
    input\_size \= 320 # fixed
    # build blob using OpenCV
    img \= cv.imread('test\_imgs/sky1.jpg')
    blob \= cv.dnn.blobFromImage(img, scalefactor=(1.0/255.0), size=(input\_size, input\_size), swapRB=True)
    # Inference
    net.setInput(blob)
    d0 \= net.forward('output')
    # Norm
    pred \= normPred(d0\[:, 0, :, :\])
    # Save
    save\_output('test\_imgs/sky1.jpg', pred)

if \_\_name\_\_ == '\_\_main\_\_':
    main()

**5.3 （****c++）****使用libtorch方法**

  

//    std::string strModelPath = "E:/template/u2net\_train.pt";
void  bgr\_u2net(cv::Mat& image\_src, cv::Mat& result, torch::jit::Module& model)
{
    //1.模型已经导入
    auto device = torch::Device("cpu");
    //2.输入图片，变换到320
    cv::Mat  image\_src1 = image\_src.clone();
    cv::resize(image\_src1, image\_src1, cv::Size(320, 320));
    cv::cvtColor(image\_src1, image\_src1, cv::COLOR\_BGR2RGB);
    // 3.图像转换为Tensor
    torch::Tensor tensor\_image\_src = torch::from\_blob(image\_src1.data, { image\_src1.rows, image\_src1.cols, 3 }, torch::kByte);
    tensor\_image\_src \= tensor\_image\_src.permute({ 2,0,1 }); // RGB -> BGR互换
    tensor\_image\_src = tensor\_image\_src.toType(torch::kFloat);
    tensor\_image\_src \= tensor\_image\_src.div(255);
    tensor\_image\_src \= tensor\_image\_src.unsqueeze(0); // 拿掉第一个维度  \[3, 320, 320\]
    //4.网络前向计算
    auto src = tensor\_image\_src.to(device);
    auto pred \= model.forward({ src }).toTuple()->elements()\[0\].toTensor();         //模型返回多个结果，用toTuple,其中elements()\[i-1\]获取第i个返回值                                                                                //d1,d2,d3,d4,d5,d6,d7= net(inputs\_test) //pred = d1\[:,0,:,:\]
    auto res\_tensor = (pred \* torch::ones\_like(src));
    res\_tensor \= normPRED(res\_tensor);
    //是否就是Tensor转换为图像
    res\_tensor = res\_tensor.squeeze(0).detach();
    res\_tensor \= res\_tensor.mul(255).clamp(0, 255).to(torch::kU8); //mul函数，表示张量中每个元素乘与一个数，clamp表示夹紧，限制在一个范围内输出
    res\_tensor = res\_tensor.to(torch::kCPU);
    //5.输出最终结果
    cv::Mat resultImg(res\_tensor.size(1), res\_tensor.size(2), CV\_8UC3);
    std::memcpy((void\*)resultImg.data, res\_tensor.data\_ptr(), sizeof(torch::kU8) \* res\_tensor.numel());
    cv::resize(resultImg, resultImg, cv::Size(image\_src.cols, image\_src.rows), cv::INTER\_LINEAR);
    result \= resultImg.clone();
}
 

**5.4 （****c++）****使用opencv方法**

#include "opencv2/dnn.hpp"
#include "opencv2/imgproc.hpp"
#include "opencv2/highgui.hpp"
 
#include <iostream>
 
#include "opencv2/objdetect.hpp"
 
using namespace cv;
using namespace std;
using namespace cv::dnn;
 
int main(int argc, char \*\* argv)
{
    Net net \= readNetFromONNX("E:/template/sky\_split.onnx");
 
    if (net.empty()) {
        printf("read  model data failure...\\n");
        return -1;
    }
 
   // load image data
    Mat frame = imread("e:/template/sky14.jpg");
    Mat blob;
    blobFromImage(frame, blob, 1.0 / 255.0, Size(320, 320), cv::Scalar(), true);
    net.setInput(blob);
    Mat prob \= net.forward("output");  
    Mat slice(cv::Size(prob.size\[2\], prob.size\[3\]), CV\_32FC1, prob.ptr<float\>(0, 0));
    normalize(slice, slice, 0, 255, NORM\_MINMAX, CV\_8U);
    resize(slice, slice, frame.size());
 
    return 0;
}

综合考虑后，选择opencv onnx的部署方式

import os
import torch
from unet import UNet  


def main():
    net \= UNet(n\_channels=3, n\_classes=2, bilinear=True)

    net.load\_state\_dict(torch.load("checkpoints/skyseg0113.pth", map\_location=torch.device('cpu')))
    net.eval()

    # \--------- model 序列化 ---------
    example \= torch.zeros(1, 3, 320, 320) #这里经过实验，最大是 example = torch.zeros(1, 3, 411, 411)
    
    torch\_script\_module \= torch.jit.trace(net, example)
    #torch\_script\_module.save('unet\_empty.pt')
    torch.onnx.export(net, example, 'checkpoints/skyseg0113.onnx', opset\_version=11)
    print('over')

if \_\_name\_\_ == "\_\_main\_\_":
    main()
 
int main()
{
    //参数和常量准备
    Net net = readNetFromONNX("E:/template/skyseg0113.onnx");
    if (net.empty()) {
        printf("read  model data failure...\\n");
        return -1;
    }
    // load image data
    Mat frame = imread("E:\\\\sandbox/sky4.jpg");
    pyrDown(frame, frame);
    Mat blob;
    blobFromImage(frame, blob, 1.0 / 255.0, Size(320, 320), cv::Scalar(), true);
    net.setInput(blob);
    Mat prob \= net.forward("473");//???对于Unet来说，example最大为(411,411)，原理上来说，值越大越有利于分割
    Mat slice(cv::Size(prob.size\[2\], prob.size\[3\]), CV\_32FC1, prob.ptr<float\>(0, 0));
    threshold(slice, slice, 0.1, 1, cv::THRESH\_BINARY\_INV);
    normalize(slice, slice, 0, 255, NORM\_MINMAX, CV\_8U);
    
    Mat mask;
    resize(slice, mask, frame.size());//制作mask
}

通过这种方法，就能够获得模型推断的模板对象，其中“473”是模型训练过程的层名，由于我们在训练的过程中没有指定，所以按照系统自己的名字给出。

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112345416-1655578021.png)

我们可以通过netron的方式查看获得这里的名称。

**六、结合SeamlessClone等图像处理方法，实现最终效果**

int main()
{
    //参数和常量准备
    Net net = readNetFromONNX("E:/template/skyseg0113.onnx");
    if (net.empty()) {
        printf("read  model data failure...\\n");
        return -1;
    }
    // load image data
    Mat frame = imread("E:\\\\sandbox/sky4.jpg");
    pyrDown(frame, frame);
    Mat blob;
    blobFromImage(frame, blob, 1.0 / 255.0, Size(320, 320), cv::Scalar(), true);
    net.setInput(blob);
    Mat prob \= net.forward("473");
    Mat slice(cv::Size(prob.size\[2\], prob.size\[3\]), CV\_32FC1, prob.ptr<float\>(0, 0));
    threshold(slice, slice, 0.1, 1, cv::THRESH\_BINARY\_INV);
    normalize(slice, slice, 0, 255, NORM\_MINMAX, CV\_8U);
    
    Mat mask;
    resize(slice, mask, frame.size());//制作mask
    Mat matSrc = frame.clone();
    VP maxCountour \= FindBigestContour(mask);
    Rect maxRect \= boundingRect(maxCountour);
    if (maxRect.height == 0 || maxRect.width == 0)
        maxRect \= Rect(0, 0, mask.cols, mask.rows);//特殊情况
    ////天空替换
    Mat matCloud = imread("E:/template/cloud/cloud1.jpg");
    resize(matCloud, matCloud, frame.size());
    //直接拷贝
    matCloud.copyTo(matSrc, mask);
    imshow("matSrc", matSrc);
    //seamless clone
    matSrc = frame.clone();
    Point center \= Point((maxRect.x + maxRect.width) / 2, (maxRect.y + maxRect.height) / 2);//中间位置为蓝天的背景位置
    Mat normal\_clone;
    Mat mixed\_clone;
    Mat monochrome\_clone;
    seamlessClone(matCloud, matSrc, mask, center, normal\_clone, NORMAL\_CLONE);
    seamlessClone(matCloud, matSrc, mask, center, mixed\_clone, MIXED\_CLONE);
    seamlessClone(matCloud, matSrc, mask, center, monochrome\_clone, MONOCHROME\_TRANSFER);
    imshow("normal\_clone", normal\_clone);
    imshow("mixed\_clone", mixed\_clone);
    imshow("monochrome\_clone", monochrome\_clone);
    waitKey();
    return 0;
}

在调用seamlessClone()的时候报错：

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112413778-1367994703.png)

报错原因：可以看seamlessClone源码（opencv/modules/photo/src/seamless\_cloning.cpp），在执行seamlessClone的时候，会先求mask内物体的boundingRect，然后会把这个最小框矩形复制到dst上，矩形中心对齐center

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112422706-693607328.png)

这个过程中可能矩形会超出dst的边界范围，就会报上面的roi边界错误。

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112438787-226050448.png)

这里错误的根源应该还是OpenCV 这块的代码有问题，其中roi\_s不应该适用BoundingRect进行处理。除了进行修改重新编译，或者直接进行PR解决之外，我们可以采取一些补救的。这里我采取了2手方法来避免异常：一个是在模板制作的过程中，除了获得的最大区域之外，主动地将其他区域涂黑，从而保证BoundingRect能够准确地框选天空区域；二个是在seamlessClone之前，对模板进行异常判断，对可能出现的情况进程处置。

通过添加opencv代码，进行系统联调：

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112453316-869617728.png)

修改后的代码为：

int main()
{
    //参数和常量准备
    Net net = readNetFromONNX("E:/template/skyseg0113.onnx");
    if (net.empty()) {
        printf("read  model data failure...\\n");
        return -1;
    }
    vector<string\> vecFilePaths;
    getFiles("e:/template/sky", vecFilePaths);
    string strSavePath = "e:/template/sky\_change\_result";
    for (int index = 0;index<vecFilePaths.size();index++)
    {
        try{
            string strFilePath = vecFilePaths\[index\];
            string strFileName;
            getFileName(strFilePath, strFileName);
            Mat frame \= imread(strFilePath);
            pyrDown(frame, frame);
            Mat blob;
            blobFromImage(frame, blob, 1.0 / 255.0, Size(320, 320), cv::Scalar(), true);
            net.setInput(blob);
            Mat prob \= net.forward("473");
            Mat slice(cv::Size(prob.size\[2\], prob.size\[3\]), CV\_32FC1, prob.ptr<float\>(0, 0));
            threshold(slice, slice, 0.1, 1, cv::THRESH\_BINARY\_INV);
            normalize(slice, slice, 0, 255, NORM\_MINMAX, CV\_8U);
            Mat mask; 
            resize(slice, mask, frame.size());//制作mask
            Mat matSrc = frame.clone();
            VP maxCountour \= FindBigestContour(mask);
            Rect maxRect \= boundingRect(maxCountour);
            if (maxRect.height == 0 || maxRect.width == 0)
                maxRect \= Rect(0, 0, mask.cols, mask.rows);//特殊情况
            Mat maskRedux(mask.size(), mask.type(), Scalar::all(0));
            Mat roi1 \= mask(maxRect);
            Mat roi2 \= maskRedux(maxRect);
            roi1.copyTo(roi2);
            ////天空替换
            Mat matCloud = imread("E:/template/cloud/cloud2.jpg");
            resize(matCloud, matCloud, frame.size());
            //直接拷贝
            matCloud.copyTo(matSrc, maskRedux);
            matSrc \= frame.clone();
            cv::Point center \= Point((maxRect.x + maxRect.width) / 2, (maxRect.y + maxRect.height) / 2);//中间位置为蓝天的背景位置
            Rect roi\_s = maxRect;
            Rect roi\_d(center.x \- roi\_s.width / 2, center.y - roi\_s.height / 2, roi\_s.width, roi\_s.height);
            if(! (0 <= roi\_d.x && 0 <= roi\_d.width && roi\_d.x + roi\_d.width <= matSrc.cols && 0 <= roi\_d.y && 0 <= roi\_d.height && roi\_d.y + roi\_d.height <= matSrc.rows))
                center \= Point(matSrc.cols / 2, matSrc.rows / 2);//这里错误的根源应该还是OpenCV 这块的代码有问题，其中roi\_s不应该适用BoundingRect进行处理.所以采取补救的方法
            Mat mixed\_clone;
            seamlessClone(matCloud, matSrc, maskRedux, center, mixed\_clone, MIXED\_CLONE);
            string saveFileName = strSavePath + "/" + strFileName + "\_cloud2.jpg";
            imwrite(saveFileName, mixed\_clone);
        }
        catch (Exception \* e)
        {
            continue;
        }
    }

**2022 0312 更新代码**

  

int main()
{
    Mat src \= imread("e:/template/tiantan.jpg");
    Mat matCloud \= imread("E:/template/cloud/cloud2.jpg");
    Mat mask \= imread("e:/template/tiantanmask2.jpg", 0);
    resize(matCloud, matCloud, src.size());
    resize(mask, mask, src.size());
    Mat matSrc \= src.clone();
    Mat board \= mask.clone();
    cvtColor(board, board, COLOR\_GRAY2BGR);
    //寻找模板最大轮廓
    VP maxCountour = FindBigestContour(mask);
    Rect maxRect \= boundingRect(maxCountour);
    //异常处理
    Mat maskCopy = mask.clone();
    copyMakeBorder(maskCopy, maskCopy, 1, 1, 1, 1, BORDER\_ISOLATED | BORDER\_CONSTANT, Scalar(0));
    Rect roi\_s \= boundingRect(maskCopy);
    if (roi\_s.empty()) return -1;
    cv::Point center \= Point((maxRect.x + maxRect.width) / 2, (maxRect.y + maxRect.height) / 2);
    Rect roi\_d(center.x \- roi\_s.width / 2, center.y - roi\_s.height / 2, roi\_s.width, roi\_s.height);
    if (!(0 <= roi\_d.x && 0 <= roi\_d.width && roi\_d.x + roi\_d.width <= matSrc.cols && 0 <= roi\_d.y && 0 <= roi\_d.height && roi\_d.y + roi\_d.height <= matSrc.rows))
        center \= Point(matSrc.cols / 2, matSrc.rows / 2);
    //融合
    Mat normal\_clone, mixed\_clone, monochrome\_clone;
    seamlessClone(matCloud, matSrc, mask, center, normal\_clone, NORMAL\_CLONE);
    seamlessClone(matCloud, matSrc, mask, center, mixed\_clone, MIXED\_CLONE);
    seamlessClone(matCloud, matSrc, mask, center, monochrome\_clone, MONOCHROME\_TRANSFER);
    waitKey();
    return 0;
}

**七、结果对比和小结**

效果是相当不错的，但是在部署过程中也可能会遇到一些问题；特别是如果用于手机端部署，必然有工具链的问题。

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112534554-1407001115.png)

 ![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112545767-699731341.png)

 ![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112628219-983675849.png)

我在hugginface上也实现了可以在线测试的效果。分别是skgseg和skgchange

https://huggingface.co/spaces/jsxyhelu/skyseg

![](https://img2023.cnblogs.com/blog/2917779/202212/2917779-20221221112614200-1512950247.png)

最后，“天空替换”整个问题，只是语义分割的一种应用，结果是美化的图片。这是价值比较有限的，必须要转换为量化的结果，用于定量计数，才能够推动生产实践。

此外，关于算法运行效率，也是部署应用的重要环节，在部署实现的时候也需要重点考虑。

posted on 2022-12-21 11:33  [jsxyhelu](https://www.cnblogs.com/jsxyhelu/)  阅读(194)  评论(1)  [编辑](https://i.cnblogs.com/EditPosts.aspx?postid=16995892)  [收藏](javascript:void(0))  [举报](javascript:void(0))