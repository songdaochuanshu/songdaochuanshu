---
layout: post
title: "ChatGPT 背后核心技术的白话版"
date: "2023-02-09T22:16:53.172Z"
---
ChatGPT 背后核心技术的白话版
==================

本文是关于ChatGPT 背后核心技术实现的一个通俗白话版，不涉及到的AI具体实现的技术细节哦。

在编排上增加了一些分割，内容具体如下：

LLMs（大型语言模型）
============

如果将ChatGPT比作是动物，它就像一只饥饿的毛毛虫一样，毛毛虫喜欢啃食树叶，并不断的长大。

LLMs（大型语言模型）也喜欢吞噬大量的文本数据，并利用这些数据来学习，然后变得更加的聪明，更加的强大。LLMs消耗的文本数据越多，它们对语言和词语之间的关系的理解就越深。

就如同自然界内，毛毛虫要变成美丽的蝴蝶一样，LLMs也会进化为强大的语言模型，它也可以理解并生成类似人类的反应。

语言模型被训练来预测一个序列中的下一个词，有两种常见的方法：下一个标记预测（next-token-prediction）和掩码语言建模（masked-language modeling）。

Next-token-prediction模型的样本，比如：

> "The cat sat on the...."

Next-token-prediction模型将被训练来预测 “The cat sat on the....”之后的下一个单词。给定输入“The cat sat on the....”，模型可以预测“mat”、“couch”或“chair”。

Masked-language-modeling模型的样本为：

> The quick brown \[MASK\] jumps over the lazy dog.

在这种情况下，模型将尝试预测缺失的单词“cat”。它通过使用周围单词的上下文来做到这一点，比如“quick”和“brown”，来理解它们之间的关系,并做出预测。Masked-language-modeling模型的目标是，训练模型以一种有意义，且语法正确的方式来填充句子中缺失的单词。

使用Long-Short-Term-Memory(LSTM)模型进行序列建模，只是预测序列中单词的一种方法，但它也有其局限性。

例如，模型不能给上下文中不同的单词赋予不同的权重，即使有时，就像在我们可爱的猫咪的例子中，一个单词可能比另一个单词更重要。而且输入数据是一步一步处理的，这意味着单词之间的关系可能是有限的。

Transformers
============

这就是为什么在2017年，Google Brain的一些聪明人想出了transformers。 Transformers与LSTM不同，因为它们可以一次处理所有的输入数据。他们使用了一种很酷的方法，称为 Self-attention，这意味着模型可以给输入数据的不同部分，赋予不同的权重。这使得单词之间的关系更加复杂，含义也更加的丰富。

GPT和Self-Attention GPT-1是openAI在2018年制作的第一个生成式预训练变压器模型。它在接下来的几年里不断发展，变成了GPT-2, GPT-3, InstructGPT，最后是ChatGPT。

在人类开始向ChatGPT提供反馈之前，GPT模型最大的变化是它们变得越来越快，这使得它们可以接受越来越多的数据训练。这让他们更有知识，能够做更广泛的任务。

像ChatGPT这样的GPT模型使用Transformer, Transformer有一个“编码器（encoder）”来处理输入，一个“解码器（decoder）” 来生成输出。编码器和解码器都使用 multi-head self-attention 理解单词和产生更准确的反应之间的关系。

Multi-head self-attention 就像给了机器人一种超能力，可以同时关注多个事情。

Self-attention 通过为每个令牌创建查询、键和值向量，然后使用softmax函数生成规范化权重，将令牌转换为表示其在输入序列中的重要性的向量。multi-head 机制来执行多次self-attention，使模型能够掌握输入数据中的复杂关系。

尽管GPT-3在自然语言处理方面带来了如此惊人的进步，但它在与用户意图一致方面仍有局限性。它可能会产生缺乏帮助的输出，对不存在或不正确的事实产生幻觉，缺乏可解释性，甚至有有毒或有偏见的内容，就像一个喝醉了的小机器人。

ChatGPT
=======

ChatGPT是InstructGPT的衍生产品，它引入了一种将人类反馈纳入训练的新方法，以更好地使模型的输出与用户的意图保持一致。

OpenAI在2022年的论文中详细介绍了从人类反馈中强化学习(RLHF)，并在这里进行了简化:

### 第1步：监督下的微调（SFT）模型

第1步是对GPT-3模型进行微调，雇用40个承包商来创建一个有监督的训练数据集，其中的输入有一个已知的输出，供模型学习。 从用户进入开放API的条目中收集提示信息，标注员写出适当的回应，以创建一个已知的输出。 然后用这个新的数据集对GPT-3模型进行微调，成为GPT-3.5，也被称为SFT模型。

为了最大限度地提高提示数据集的多样性，每个用户ID只允许有200个提示，任何带有长的普通前缀的提示都被删除。 此外，出于隐私安全的考虑，所有带有个人身份信息的提示都被删除。

标注员还被要求用最少的实际样本数据为类别创建示例提示，包括:

*   简单的提示：任何随机的问题。
*   少样本提示：具有多个查询/响应对的指令。
*   基于用户的提示：对应于OpenAI API所请求的特定用例。

在生成响应时，标注员尽力推断用户的指令。论文称，提示请求信息主要有三种方式:

*   直接：“跟我说说……”
*   写一篇相同主题的文章，并举例说明。
*   延续:完成一个故事的开头。

通过OpenAI API的提示和标签师的手写，监督模型有13000个输入/输出样本可以使用!

### 第2步：奖励模型

在这个过程的第2步中，模型被赋予了一种待遇。 训练一个奖励模型，以便它能学会如何对用户的提示做出最好的反应。 这个奖励模型将提示和响应作为输入，并给我们一个称为奖励的可爱的小缩放器值，作为输出。 有了这个奖励模型，我们就可以进行强化学习，让模型变得更棒，这就是强化学习。

为了训练奖励模型，我们请一些可爱的标注员们，将SFT模型的输出从最好到最差进行排序。我们把所有这些排名放在一起来训练模型，这样它就不会被所有的信息弄糊涂了。

我们曾经把每个组合作为一个单独的数据点，但这导致了过度拟合。 过度拟合就像一个只想玩自己的玩具的孩子，而不会注意其他东西。 为了防止过度拟合，我们给模型一个奖励，把数据点归为一个批次的数据点，这样它就能学会更灵活地适应新的情况啦。

### 第3步：强化学习模型

在第三步，是时候让强化学习模型大放异彩了。给模型一个提示，它就会摇尾巴来产生响应。响应是在模型在第2步中学习到的“策略”的帮助下做出的。这个策略就像一个秘密策略，模型想出了得到更多的奖励(又名最大化奖励)。然后根据步骤2中建立的奖励模型对模型进行奖励。这种奖励有助于模型的成长和进化，就像奖励让狗狗开心一样。

2017年，一些名叫Schulman等人的聪明人引入了一种有趣的方法来更新模型的策略，称为近端策略优化(PPO)。它使用SFT模型中称为每个令牌的Kullback-Leibler (KL)惩罚。KL发散就像比较两种不同的食物，它有助于确保响应与人类意图数据集没有太大差异，这样模型就不会因为追逐自己的尾巴而分心。

该模型在训练过程中通过将一些从未见过的数据放在一边进行评估。测试数据被用来检验该模型是否比旧的GPT-3模型更好。他们检查了它有多大帮助，有多真实，以及在多大程度上避免了刻薄。他们发现人们在85%的情况下更喜欢它，当被告知要友善时，它更真实，而不是刻薄。但当它被告知要刻薄时，它比GPT-3更刻薄。

写在最后
====

这就是你可爱的小朋友ChatGPT，产生的全部过程啦。

备注：本文翻译之外网，具体地址如下。

原文：[https://medium.com/@anixlynch/behind-the-scenes-the-tech-behind-chatgpt-super-friendly-ver-2a214c4dc284](https://medium.com/@anixlynch/behind-the-scenes-the-tech-behind-chatgpt-super-friendly-ver-2a214c4dc284)

关注公众号：架构未来 ，我们一起学习成长
--------------------

![架构未来](https://img2018.cnblogs.com/blog/34483/201912/34483-20191225100211896-1138251355.jpg)