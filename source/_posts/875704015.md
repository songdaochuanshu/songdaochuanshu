---
layout: post
title: "Hadoop（一）Hadoop核心架构与安装"
date: "2022-04-30T05:17:20.811Z"
---
Hadoop（一）Hadoop核心架构与安装
======================

Hadoop是什么
---------

大白话，Hadoop是个存储数据，计算数据的分布式框架。核心组件是HDFS、MapReduce、Yarn。

_**HDFS**_：分布式存储

_**MapReduce**_：分布式计算

_**Yarn**_：调度MapReduce

现在为止我们知道了HDFS、MapReduce、Yarn是干啥的，下面通过一张图再来看看他的整体架构。

![](https://img2022.cnblogs.com/blog/1033233/202204/1033233-20220429191535383-929426914.png)

HDFS
----

HDFS是Hadoop的存储系统，将庞大的数据存储在多台机器上，并通过数据副本冗余实现容错。HDFS两大核心组件是NameNode与DataNode。

_**NameNode**_：管理文件命名空间元数据；实现文件命名、打开关闭操作

_**SecondaryNameNode**_：帮助NameNode实现log与数据快照的合并

_**DataNode**_：根据客户请求实现文件的读写

![](https://img2022.cnblogs.com/blog/1033233/202204/1033233-20220429192225090-1651839104.png)

MapReduce
---------

MapReduce是基于Java开发的分布式计算。包含重要的两部分，Map和Reduce。

_**Map**_：将数据转成键值对

_**Reduce**_：将Map的输出数据聚合减少

![](https://img2022.cnblogs.com/blog/1033233/202204/1033233-20220429192548989-1050821278.png)

Yarn
----

通过对集群资源的监控，调度MapReduce的任务。核心组件有ResourceManager、NodeManager、ApplicationMaster 和 Container。

_**ResourceManager**_：处理客户端请求；监控NodeManager与ApplicationMaster；调度资源。

_**NodeManager**_：管理节点资源；与ResourceManager ApplicationMaster交互。

_**ApplicationMaster**_：为程序申请资源并将资源分配给任务；任务监控。

![](https://img2022.cnblogs.com/blog/1033233/202204/1033233-20220429193116838-1829147403.png)

安装Hadoop
--------

### 1.安装Jdk

下载[https://www.oracle.com/java/technologies/downloads/](https://www.oracle.com/java/technologies/downloads/ "https://www.oracle.com/java/technologies/downloads/")

解压

tar -zxvf jdk-8u331-linux-x64.tar.gz

加入环境变量

vi /etc/profile

#加入以下内容
JAVA\_HOME\=/usr/local/java18/jdk1.8.0\_331
JRE\_HOME\=$JAVA\_HOME/jre
PATH\=$PATH:$JAVA\_HOME/bin:$JRE\_HOME/bin
CLASSPATH\=.:$JAVA\_HOME/lib/dt.jar:$JAVA\_HOME/lib/tools.jar:$JRE\_HOME/lib
export JAVA\_HOME JRE\_HOME PATH CLASSPATH

//生效
source  /etc/profile

验证java

![](https://img2022.cnblogs.com/blog/1033233/202204/1033233-20220429193901835-1939833201.png)

### 2.安装伪分布式Hadoop

下载[https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz "https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz")

解压

tar xzf hadoop-3.2.3.tar.gz

配置本机ssh

ssh-keygen -t rsa -P '' -f ~/.ssh/id\_rsa
cat ~/.ssh/id\_rsa.pub >> ~/.ssh/authorized\_keys
chmod 0600 ~/.ssh/authorized\_keys

配置Hadoop环境变量

cat etc/hadoop/hadoop-env.sh  
  
export JAVA\_HOME\=/usr/local/java/jdk1.8

配置hdfs地址

cat etc/hadoop/core-site.xml

    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>

配置hafs分片数

cat etc/hadoop/hdfs-site.xml
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>

/etc/profile新增hadoop环境变量

export HADOOP\_HOME=/usr/local/hadoop/hadoop-3.2.3
export HADOOP\_MAPRED\_HOME\=$HADOOP\_HOME
export HADOOP\_COMMON\_HOME\=$HADOOP\_HOME
export HADOOP\_HDFS\_HOME\=$HADOOP\_HOME
export YARN\_HOME\=$HADOOP\_HOME
export HADOOP\_COMMON\_LIB\_NATIVE\_DIR\=$HADOOP\_HOME/lib/native
export PATH\=$PATH:$HADOOP\_HOME/sbin:$HADOOP\_HOME/bin
export HADOOP\_INSTALL\=$HADOOP\_HOME
export HADOOP\_CLASSPATH\=${JAVA\_HOME}/lib/tools.jar

//生效
source /etc/profile

配置mapreduce

vi etc/hadoop/mapred-site.xml

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP\_MAPRED\_HOME/share/hadoop/mapreduce/\*:$HADOOP\_MAPRED\_HOME/share/hadoop/mapreduce/lib/\*</value>
    </property>
</configuration>

配置yarn

vi etc/hadoop/yarn-site.xml

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce\_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env\-whitelist</name>
        <value>JAVA\_HOME,HADOOP\_COMMON\_HOME,HADOOP\_HDFS\_HOME,HADOOP\_CONF\_DIR,CLASSPATH\_PREPEND\_DISTCACHE,HADOOP\_YARN\_HOME,HADOOP\_HOME,PATH,LANG,TZ,HADOOP\_MAPRED\_HOME</value>
    </property>
</configuration>

配置相关user

//将sbin/start-dfs.sh，sbin/stop-dfs.sh两个文件顶部添加以下参数
HDFS\_DATANODE\_USER\=root
HADOOP\_SECURE\_DN\_USER\=hdfs
HDFS\_NAMENODE\_USER\=root
HDFS\_SECONDARYNAMENODE\_USER\=root

//将sbin/start-yarn.sh，sbin/stop-yarn.sh顶部也需添加以下
YARN\_RESOURCEMANAGER\_USER\=root
HADOOP\_SECURE\_DN\_USER\=yarn
YARN\_NODEMANAGER\_USER\=root

初始化hdfs

bin/hdfs namenode -format

启动yarn

sbin/start-yarn.sh

通过jps查看启动的进程

![](https://img2022.cnblogs.com/blog/1033233/202204/1033233-20220429195526390-443607387.png)

 启动hdfs

sbin/start-dfs.sh

通过jps查看进程

![](https://img2022.cnblogs.com/blog/1033233/202204/1033233-20220429195626063-912327443.png)

访问hadoopui验证安装是否成功

http://192.168.43.50:9870/dfshealth.html#tab-overview

![](https://img2022.cnblogs.com/blog/1033233/202204/1033233-20220429200136177-1121856580.png)

访问集群ui

http://192.168.43.50:8088/cluster/cluster

![](https://img2022.cnblogs.com/blog/1033233/202204/1033233-20220429200323633-955006196.png)