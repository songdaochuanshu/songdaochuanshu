---
layout: post
title: " 神经网络模型与算法疑问解答"
date: "2022-10-26T13:51:48.189Z"
---
神经网络模型与算法疑问解答
=============

![ 神经网络模型与算法疑问解答](https://img2022.cnblogs.com/blog/2192866/202210/2192866-20221026214000047-607364352.png) 神经网络模型与算法的复习总结。因为只是复习提纲，所以对理解分析内容不多，也不是学习笔记。精力所限，本人理解的也不深刻，只是把一些概念性的知识放上来。

神经网络模型与算法的复习总结。因为只是复习提纲，所以对理解分析内容不多，也不是学习笔记。精力所限，本人理解的也不深刻，只是把一些概念性的知识放上来。

* * *

参考资料：

*   上课PPT
*   邱锡鹏《神经网络与深度学习》
*   博客

* * *

00 课程概述
-------

### A 问题

1.  课程《神经网络模型与算法》计划介绍的模型包括\_\_\_\_\_、\_\_\_\_\_\_、\__****和****_。
2.  \_\_\_\_\_\_\_\_主要用于从有限样例中通过算法总结出一般性的规律，并应用到新的未知数据上。说说你的理解。
3.  神经网络模型有什么用？

### B 答案

第1题

*   前馈神经网络
*   卷积神经网络
*   循环神经网络
*   深度生成网络

第2题

神经网络模型

第3题

神经网络模型可实现对各类数据的高层次深度分析，广泛应用于图像视频处理、计算机视觉、自然语言处理、智能交通、智能家居、机器人、生物信息学和化学、电子游戏、搜索引擎、网络广告和金融等领域。

01-01 前馈神经网络模型基础
----------------

### A 问题

1.  请说说你对前馈神经网络中“前馈”二字的理解。
2.  记忆和知识是存储在\_\_\_\_\_上的。我们通常是通过逐渐改变\_\_\_\_\_来学习新知识。
3.  在一个人工神经元中，首先对输入信号进行\_\_\_\_，然后加上一个\_\_\_\_，最后经过\_\_\_\_\_得到输出信号。
4.  请简要说明生物神经元和人工神经元的相似之处。
5.  不同神经元之间的连接被赋予了不同的\_\_\_\_\_\_，用来表示一个神经元对另一个神经元的影响大小。
6.  请说出两种常用的激活函数。
7.  人工神经网络是由很多\_\_\_\_\_\_\_\_\_\_\_按照一定的\_\_\_\_\_\_\_\_\_结构组成的。
8.  人工神经网络中神经元通过权重（网络参数）连接，这些权重（网络参数）是怎么得到的吗？
9.  请简要说明前馈神经网络和人工神经网络的主要区别。
10.  前馈神经网络有多个层，层内神经元\_\_\_\_连接。邻层神经元\_\_\_\_\_连接。
11.  前馈神经网络可作为一个\_\_\_\_\_\_\_来使用，用于进行复杂的特征转换。
12.  说一说损失函数的作用，并举出两种常见的损失函数。

### B 答案

1.  前馈神经网络的层内无连接，邻层神经元亮亮链接，**信号从输入层向输出层的单向传播**。
    
2.  记忆和知识是存储在**生物神经元之间的连接上**。我们通过逐渐改变**生物神经元之间的连接强度**来学习新知识。
    
3.  在一个人工神经元中，首先对输入信号进行**加权求和**，然后加上一个**偏置**，最后经过**激活函数**得到输出信号。
    
4.  请简要说明生物神经元和人工神经元的相似之处。
    
    人工神经元是对生物神经元的功能和结构的模拟，是对生物神经的形式化描述，是对生物神经元信息处理过程的抽象。
    
5.  不同神经元之间的连接被赋予了不同的**权重**，用来表示一个神经元对另一个神经元的影响大小。
    
6.  请说出两种常用的激活函数。
    
    Sigmoid型函数（一类S型曲线函数）包括 Logistic函数、Tanh函数
    
    欧拉函数 ReLU。
    
7.  人工神经网络是由很多**人工神经元**按照一定的**拓扑**结构组成的。
    
8.  人工神经网络中神经元通过权重（网络参数）连接，这些权重（网络参数）是怎么得到的吗？
    
    这些权重（网络参数）需要**借助学习算法和训练数据来学习得到**。
    
9.  请简要说明前馈神经网络和人工神经网络的主要区别。
    
    人工神经网络是一个更笼统的概念，泛指使用人工神经元彼此来连接构成的网络。
    
    前馈神经网络是一种最简单的人工神经网络。
    
    前馈神经网络专指层间神经元无连接，邻层神经元两两连接、且信号从输入层向输出层的单向传播的网络模型。
    
10.  前馈神经网络有多个层，层内神经元**无**连接。邻层神经元**两两**连接。
    
11.  前馈神经网络可作为一个**万能函数**来使用，用于进行复杂的特征转换。
    
12.  说一说损失函数的作用，并举出两种常见的损失函数。
    
    **量化模型预测和真实标签之间的差异。**
    
    0-1损失函数、平方损失函数、交叉熵损失
    
    平方：
    

01-02 前馈神经网络参数学习
----------------

### A 问题

1.  梯度下降法是用于求解\_
    
2.  反向传播算法用于求解\_
    
3.  请说明梯度下降法和下山的基本思想的相似之处
    
4.  在梯度下降法中，设 xk 为第k次迭代点，gk为点 xk对应的梯度方向，ak为第k次迭代的步长因子，则第k次迭代完成后可得到新一轮的迭代点xk+1 = \_\_\_
    
5.  说说你对梯度下降法中步长因子 / 学习率的理解。
    
6.  ~10.
    
    ![revi1](Pic/revi1.png)
    

### B 答案

1.  神经网络的网络参数
    
2.  计算梯度下降法中每一步的梯度。
    
3.  最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快。下山的每一步的步子长度就对应梯度下降法中的步长/学习率。
    
    反复求取梯度，最后就能到达局部的最小值。
    
4.  xk+1\=xk\-akgk
    
5.  学习率简单讲就是下山中每一步的步长，也对应梯度下降法中每次迭代的程度。好的学习率可以更快地达到损失函数的最小值，保证收敛的loss值是全局最优解。
    
6.  **点xk的负梯度方向**是f的函数值下降最快的方向，沿这一方向的变化率就是**梯度的模**。
    
7.  **梯度下降，选所有的样本；随机梯度下降：随机选择一个样本；小批量随机梯度下降：随机选择一小批样本。**
    
8.  链式法则就是微积分中求复合函数导数的一种常用方法
    
9.  过拟合：能够在训练数据上拟合得很好，但是在训练集以外的数据集上不能很好的拟合数据。（可以画个图）
    
    方法：增加数据集的规模。改进神经网络模型。
    
    L1、L2正则化、进行特征选择与稀疏选择等。
    
10.  \\((\\frac{\\partial f}{\\partial x\_1},\\frac{\\partial f}{\\partial x\_2},...,\\frac{\\partial f}{\\partial x\_n})\\)
    

01-03 前馈神经网络网络优化
----------------

### A 问题

1.  请详细说说你对过拟合的一些理解以及如何减轻过拟合
2.  梯度表示

> 跟上面01-02节的9、10一样。

### B 答案

02-01 卷积神经网络模型基础
----------------

### A 问题

1.  请问用全连接神经网络处理大尺寸图像存在的主要问题是什么？
2.  请简要说明卷积神经网络中的“卷积”二字的含义。
3.  与全连接神经网络相比，请简要说明卷积神经网络能够降低网络参数的三个策略是什么。
4.  卷积神经网路的网络结构一般是由\_\_\_\_\_\_、\_\_\_\_\_\_和全连接层组成。
5.  请简要说说你对卷积层中的步长（stride）和感受野（Receptive field）两个概念的理解。
6.  请说说关于权重共享的理解。
7.  请说说卷积前对图像进行填充（Padding）有什么用。
8.  请说说你对特征图（feature map）和通道（channel）的理解。
9.  请问全连接神经网络和卷积神经网络的主要区别在哪里？
10.  请说说你对池化（Pooling）的一些理解。
11.  现有60000张手写数字图像，每张图像写有0到9中的一个数字。假定训练集为50000张，测试集为10000张。设计并训练一个卷积神经网络实现对测试集中图像进行分类。

### B 答案

1.  请问用全连接神经网络处理大尺寸图像存在的主要问题是什么？
    
    如果输入图像尺寸很大，会导致网络参数很多。全连接前馈网络很难提取局部不变特征。
    
2.  请简要说明卷积神经网络中的“卷积”二字的含义。
    
    卷积是一种数学运算。在卷积神经网络中指的就是在一个图像上滑动一个卷积核（滤波器）通过卷积操作得到一组新的特征。
    
    这里的卷积操作就是 计算权重矩阵和扫描所得的数据矩阵的乘积， 然后把结果汇总成一个输出像素。
    
3.  与全连接神经网络相比，请简要说明卷积神经网络能够降低网络参数的三个策略是什么。
    
    局部连接、权重共享、汇聚
    
4.  卷积神经网路的网络结构一般是由\_\_\_\_\_\_、\_\_\_\_\_\_和全连接层组成。
    
    卷积层、汇聚层、全连接层
    
5.  请简要说说你对卷积层中的步长（stride）和感受野（Receptive field）两个概念的理解。
    
    在卷积神经网络中,**感受野(Receptive Field)**是指特征图上的某个点能看到的输入图像的区域,即特征图上的点是由输入图像中感受野大小区域的计算得到的。一个神经元的感受野是指视网膜上的特定区域，只有这个区域的刺激才能激活该神经元。
    
    步长是卷积操作中卷积核在图像上每一次运算移动的长度。即每次滑动的行数或列数。
    
6.  请说说关于权重共享的理解。
    
    作为参数的卷积核wl对于第L层的所有神经元都是相同的。可以理解为一个卷积核只捕捉输入数据中的一种特定的局部特征。
    
7.  请说说卷积前对图像进行填充（Padding）有什么用。
    
    输入图像在进行卷积后损失了部分信息，输入图像的边缘被剪掉了，所以要对原矩阵进行边界填充。
    
8.  请说说你对特征图（feature map）和通道（channel）的理解。
    
    通道是特征图的另一种说法。是指卷积核对图像进行卷积操作后，得到的结果矩阵。
    
9.  请问全连接神经网络和卷积神经网络的主要区别在哪里？
    
    连接神经网络和卷积神经网络的唯一区别就是神经网络相邻两层的连接方式。
    
    在全连接神经网络中，每相邻两层之间的节点都有边相连。而对于卷积神经网络，相邻两层之间只有部分节点相连。
    
10.  请说说你对池化（Pooling）的一些理解。
    
    对卷积后得到的特征图采取某些方式进行降维压缩。比如最大池化，如果......
    
11.  现有60000张手写数字图像，每张图像写有0到9中的一个数字。假定训练集为50000张，测试集为10000张。设计并训练一个卷积神经网络实现对测试集中图像进行分类。
    

02-02 卷积神经网络程序学习
----------------

### A 问题

1.  请解释pytorch中下述程序的含义：self.fc2 = nn.Linear(512, 10)
2.  请解释pytorch中下述程序的含义：self.conv3 = nn.Conv2d(16, 32, 5, 1, 2)
3.  请解释pytorch中下述程序的含义：x = F.max\_pool2d(x, 2, 2)
4.  请解释pytorch中下述程序的含义：x=F.avg\_pool2d(x, 2, 2)
5.  请解释pytorch中下述程序的含义：optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.5)
6.  请解释pytorch中下述程序的含义：loss=F.cross\_entropy(output, target)
7.  请解释pytorch中下述程序的含义：loss.backward()
8.  请解释pytorch中下述程序的含义：nn.BatchNorm2d(32, momentum=0.95)
9.  请解释pytorch中下述程序的含义：nn.Dropout2d(0.25)
10.  请解释pytorch中下述程序的含义：x = x.view(-1, 7_7_32)
11.  请解释pytorch中下述程序的含义：x = F.relu(x)
12.  请解释pytorch中下述程序的含义：torch.save(model, "save/models/m\_middle.pt")
13.  定义如下网络Net。假设输入张量大小为16×3×28×28，请问网络输出结果x的尺寸为\_\_\_\_\_\_\_\_\_。

### B 答案

1.  请解释pytorch中下述程序的含义：self.fc2 = nn.Linear(512, 10)
    
    实现加权求和+偏置的过程。 \\(y = xA^T + b\\)
    
    512是输入样本向量大小，10是输出向量大小。
    
2.  请解释pytorch中下述程序的含义：self.conv3 = nn.Conv2d(16, 32, 5, 1, 2)
    
    输入图像的通道数
    
    卷积层输出的通道数
    
    卷积核的大小
    
    步长
    
    填充数（如填充数为2，那么32×32变为36×36）
    
3.  请解释pytorch中下述程序的含义：x = F.max\_pool2d(x, 2, 2)
    
    参数：输入向量
    
    卷积核大小
    
    步长大小
    
4.  请解释pytorch中下述程序的含义：x=F.avg\_pool2d(x, 2, 2)
    
    平均池化
    
5.  请解释pytorch中下述程序的含义：optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.5)
    
    参数1：一个求cost function以及cost function针对于权重的导数的函数
    
    优化器整个是。感觉没讲。
    
6.  请解释pytorch中下述程序的含义：loss=F.cross\_entropy(output, target)
    
    交叉熵。第一个参数：网络计算输出的向量，target是
    

02-03 卷积神经网络卷积扩展
----------------

### A 问题

1.  说说空洞卷积和反卷积的区别。
2.  列举几种增加神经元的感受野的方法。
3.  \_\_\_\_\_\_是通过给卷积核插入“空洞”来变相地增加感受野大小。
4.  \_\_\_\_\_\_可看做特殊的正向卷积。具体地，先按一定比例填充零扩大输入特征图尺寸，然后进行正向卷积，以扩大特征空间尺寸。

### B 答案

1.  **说说空洞卷积和反卷积的区别。（我觉得这个我理解的并不好**
    
    通过给卷积核插入“空洞”来变相地增加其大小。
    
    反卷积，又称转置卷积，可看做特殊的正向卷积，先按一定比例填充0扩大输入图像尺寸，后进行正向卷积。
    
2.  列举几种增加神经元的感受野的方法。
    
    *   增加卷积核的大小
    *   增加层数来实现
    *   在卷积之前进行汇聚操作
    *   空洞卷积（又称膨胀卷积）：通过给卷积核插入“空洞”来变相地增加其大小。
3.  \_\_\_\_\_\_是通过给卷积核插入“空洞”来变相地增加感受野大小。
    
4.  \_\_\_\_\_\_可看做特殊的正向卷积。具体地，先按一定比例填充零扩大输入特征图尺寸，然后进行正向卷积，以扩大特征空间尺寸。
    

02-04 卷积神经网络注意力机制
-----------------

### A 问题

1.  眼睛每秒钟都会发送千万比特的信息给视觉神经系统。人脑通过\_\_\_\_\_\_来解决上述信息超载问题。受此启发，人们提出了注意力机制（Attention Mechanism），其主要作用是\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_。
    
2.  列举两种注意力打分函数。
    
3.  下图给出了通道注意力的整个网络架构，详细说明每一步的含义（特别是特征图尺寸的变化情况）。
    
    ![revi3](Pic/revi3.png)
    

### B 答案

1.  眼睛每秒钟都会发送千万比特的信息给视觉神经系统。人脑通过\_\_注意力\_\_来解决上述信息超载问题。受此启发，人们提出了注意力机制（Attention Mechanism），其主要作用是 **通过信息选择机制过滤掉大量无关的信息**。
    
2.  如图所示：
    
    ![revi2](Pic/revi2.png)
    
3.  从输入X到U，是经典的卷积结构，U之后才是SENet的创新部分。
    
    上图中的画线部分就是squeeze的操作，具体来说他就是是对应一个全局平均池化的操作。将一个c通道，hxw的特征图，**压成**c通道1x1。于是这个得到的结果是能表示全局信息的。  
    ![img](https://pic1.zhimg.com/80/v2-e3fe4f6638467b4d6ca828f7c5edd544_720w.webp)
    
    在代码中，就是对应的自适应平均池化到1x1
    
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        y = self.avg_pool(x).view(b, c) #对应Squeeze操作
        
    
    ![img](https://pic3.zhimg.com/80/v2-0b652acb6408dec23127543e3701e8c2_720w.webp)
    
    ![img](https://pic2.zhimg.com/80/v2-48379fa9de92ec0f13df7b13f5489fe5_720w.webp)
    
    上图中的画线部分就是excitation的操作，具体来说包含两个全连接层。对squeeze全局池化后得到的结果（已经是可以看作一个C维向量），进行全连接，得到C/r维的向量，在进行Reulu激活，再对进行一次全连接，将C/r维的向量变回C维向量，再进行sigmoid激活（使得数值位于0-1之间），这便是得到了权重矩阵。
    
        self.fc = nn.Sequential(
              #第一次全连接，降低维度
                    nn.Linear(channel, channel // reduction, bias=False),
                    nn.ReLU(inplace=True),
              #第二次全连接，恢复维度
                    nn.Linear(channel // reduction, channel, bias=False),
              #sigmoid激活，使得值位于0-1之间
                    nn.Sigmoid()
        
        y = self.fc(y).view(b, c, 1, 1) #对应Excitation操作
        return x * y.expand_as(x)#把权重矩阵赋予到特征图
        
    
    最后的操作就是把这个权重矩阵和U进行相乘计算，把权重赋予上去。
    
    分开看完之后，再整合起来看就是如下图这样的操作过程。
    
    ![img](https://pic4.zhimg.com/80/v2-133f60d618baa6310e9e56aa8323d793_720w.webp)
    
    先全局池化，再全连接将低维度，再rulu激活，再全连接恢复维度，再sigmoid激活。
    
    > 本部分参考：[【注意力机制】通道上的注意力：SENet论文笔记 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/334349672)
    

02-05 卷积神经网络模型串讲
----------------

### A 问题

1.  为了用一个非线性单元（如一层或多层的卷积层）去逼近一个目标函数。我们可以将目标函数拆分成两部分：\_****和****。
2.  ResNet系列网络模型中的残差块的好处是\_\_\_\_\_\_\_\_。

### B 答案

1.  为了用一个非线性单元（如一层或多层的卷积层）去逼近一个目标函数。我们可以将目标函数拆分成两部分：**恒等函数**和**残差函数**。
    
2.  假如新加的这些层的学习效果非常差，那我们就可以通过一条残差边将这一部分直接“跳过”。
    
    添加的新网络层至少不会使效果比原来差，就可以较为稳定地通过加深层数来提高模型的效果了。**提高信息的传播效率**。
    
    另一个好处在于**可以缓解梯度消失的问题**。
    
    > 本部分参考：[ResNet网络 残差块的作用](https://blog.csdn.net/weixin_44492824/article/details/123189456)
    

03-01 深度生成网络模型基础
----------------

### A 问题

1.  说出两种深度生成网络模型\_\_\_\_\_
2.  判别模型和生成模型的区别在哪里
3.  变分自编码器生成数据样本的基本思想是什么？
4.  变分自编码器中的再参数化指的是什么？解决什么问题？
5.  生成对抗网络生成数据样本的基本思想是什么？
6.  生成对抗网络中的生成网络和判别网络的分别是用于干什么的？
7.  解释生成对抗网络的损失函数中各个参数的含义？
8.  解释变分自编码器的损失函数中各个参数的含义？

### B 答案

1.  说出两种深度生成网络模型\_\_\_\_\_
    
    变分自编码器 VAE
    
    生成对抗网络 GAN
    
2.  判别模型和生成模型的区别在哪里
    
    生成模型，源头导向，会尝试去找数据是怎么产生的，然后再对一个信号进行分类。基于你学习到的生成假设，判断哪个类别最有可能产生这个信号，这个样本就属于那个类别。
    
    判别模型：结果导向。并不关心样本数据是怎么生成的，它只关心样本之间的差别，然后用差别来简单对给定的一个样本进行分类。
    
    > 本部分参考：[生成模型和判别模型的区别](https://blog.csdn.net/qq_20011607/article/details/81744614)
    
3.  变分自编码器生成数据样本的基本思想是什么？
    
    变分自编码器是一种深度生成模型，思想是利用神经网络分别建模两个复杂的条件概率密度函数。
    
4.  变分自编码器中的再参数化指的是什么？解决什么问题？
    
    是将一个函数 \\(f(\\theta)\\)的参数\\(\\theta\\)用另外一组参数表示，即\\(\\theta = g(\\theta)\\)
    
    常用来将原始参数转换为另外一组具有特殊属性的参数。比如当θ是一个很大的矩阵时，可以使用两个低秩矩阵的乘积再参数化，从而减少参数量。
    
5.  生成对抗网络生成数据样本的基本思想是什么？
    
    通过对抗训练的方式使得生成网络产生的样本服从真实数据分布。在生成对抗网络中，有两个网络进行对抗训练。一个是生成网络，目标是尽量生成判别网络无法区分来源的样本，另一个是判别网络，目标是尽量准确的判断样本来自于真实数据还是由生成网络生成。
    
6.  生成对抗网络中的生成网络和判别网络的分别是用于干什么的？
    
    一个是生成网络，目标是尽量生成判别网络无法区分来源的样本，另一个是判别网络，目标是尽量准确的判断样本来自于真实数据还是由生成网络生成。
    
7.  解释生成对抗网络的损失函数中各个参数的含义？
    
    先看左边。左边这一部分的作用是保证判别器的基础判断能力：\\(E\_{x~pdata(x)}\[logD(x)\]\\)，这里的x是从真实数据分布pdata中采样得到的样本。要使得这个式子越大，意味着D(x)越大，即判别器越能准确将真实样本识别为真实样本。故有maxD
    
    再看右边，\\(E\_{z\\sim p\_z(z)}\[log(1-D(G(z)))\]\\)，z为某一特定分布pz中得到的采样，G(z)为生成器生成的虚假样本。要使得这项最大，意味着D(G(z)) 越小，即判别器能够正确区分样本，将其标为 False 故有maxD
    
    最后看生成器G的损失函数，到了训练生成器G的阶段，此时判别器D固定，如果G更强，那么判别器会误判，此时 D(G(z))会变大，右边那一项会接近于0，整个式子的值会更小，则有minG
    
    > 本部分参考资料：[生成对抗网络GAN损失函数loss的简单理解](https://blog.csdn.net/qq_40714949/article/details/120493934)
    
8.  解释变分自编码器的损失函数中各个参数的含义？
    

04-01 循环神经网络模型基础
----------------

### A 问题

![revi4](Pic/revi4.png)

### B 答案

1.  循环神经网络使用带自反馈的神经元，是一类具有**短期记忆**能力的神经网络，能够处理任意长度的时序数据。神经元不但可以接受其他神经元的信息，也可以接受**自身**的信息，形成具有环路的网络结构。
    
2.  循环神经网络的**长程依赖**问题
    
3.  引入**门控机制**来控制信息更新的方式
    
4.  **门控机制**、**长短期记忆网络**、**门控循环单元网络**
    
5.  长短期记忆网络、门控循环单元网络
    
6.  堆叠循环神经网络、双向循环神经网络
    
7.  一个完全连接的循环网络是**任何非线性动力系统**的近似器 。
    
8.  \\(h(t)=f(h\_{t-1},x\_t)\\)
    
    \\(h(t)=f(Uh\_{t-1}+Wx\_t+b)\\)
    
9.  循环神经网络的参数学习可以通过随时间反向传播算法来学习，随时间反向传播算法即按照时间的逆序将错误信息一步步地往前传递。  
    当输入序列较长时，会存在梯度爆炸和消失问题，也称长程依赖问题．  
    由于梯度爆炸或消失问题，实际上只能学习到短周期的依赖关系，很难建模长时间间隔（Long Range）的状态之间的依赖关系。这就是所谓的长程依赖问题。