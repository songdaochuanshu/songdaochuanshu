---
layout: post
title: "迁移学习（DIFEX）《Domain-invariant Feature Exploration for Domain Generalization》"
date: "2023-01-27T23:15:36.763Z"
---
迁移学习（DIFEX）《Domain-invariant Feature Exploration for Domain Generalization》
===========================================================================

论文信息
====

> 论文标题：Domain-invariant Feature Exploration for Domain Generalization  
> 论文作者：Wang Lu, Jindong Wang, Haoliang Li, Yiqiang Chen, Xing Xie  
> 论文来源：TMLR 2022  
> 论文地址：[download](https://arxiv.org/abs/2207.12020v1)   
> 论文代码：[download](https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG)  
> 引用次数：

1 前言
====

　　本文将介绍一种基于域不变特征挖掘的域泛化方法（$\\text{DIFEX}$）。近年来，领域泛化(Domain Generalization, DG) 受到了越来越多的关注，现有的 DG 方法可以粗略地分为三类：**数据操作**、**表示学习**、**学习策略**。本文聚焦于域泛化表示学习，针对现有表示学习中获取的不变特征不够充分的问题，尝试思考：什么是域不变特征？如何进一步改进DG的效果？首次将域不变特征分成域内不变特征（internally-invariant）和域间不变特征（mutually-invariant）两种类型，更多样、更充分地挖掘域不变特征。

2 介绍
====

　　数据操作：对数据输入输出进行操作，比如数据增量、数据生成；

　　表示学习：学习域不变特征或者对特征进行解耦，获取更有意义的泛化特征；

　　学习策略：设计一些特定的策略增强模型泛化能力，比如集成或元学习；

　　本文主要聚焦于**表示学习**，进行模型泛化能力的增强。已有的关于域不变特征学习方法的探索促使我们尝试思考这类方法的合理性：什么是域不变特征？如何获取域不变特征并更好获得泛化效果？现有一些工作表明，在域自适应领域，简单的域间特征对齐获取的特征是远远不够的，需要关注更多其它的方面。最近，在域泛化领域也出现了类似的结论，简单的对齐可能损害模型的分辨能力以及特征的多样性和充分性。针对这个问题，我们对于不变特征进行了深入的思考。

　　本文认为不变特征应该从**域内**和**域间**两个角度进行学习：

*   *   域内不变特征（internally-invariant features），与分类有关的特征，产生于域的内部，不受其他域的影响，主要抓取数据的内在语义信息；
    *   域间不变特征（mutually-invariant features），跨域迁移知识，通过多个域产生，共同学习的一些知识；

　　本文认为，把这两种特征有效充分地结合起来，可以得到泛化性更好的模型。注意我们的方法类似特征解耦，但是其实稍有区别，特征解耦通常将特征分为域专有特征和域共有特征，这里的域内不变特征和域专有特征有稍许区别，更关注于对分类有用的特征，可以理解为针对分类不变的特征，而后者强调与域关联的特征。广泛的实验表明，我们的方法能获取更多样、更充分的特征，从而构建泛化能力更强的机器学习模型。

3 问题定义
======

　　多源域数据集 $\\mathcal{S}=\\left\\{\\mathcal{S}^{i} \\mid i=1, \\cdots, M\\right\\}$ ，其中 $\\mathcal{S}^{i}=\\left\\{\\left(\\mathbf{x}\_{j}^{i}, y\_{j}^{i}\\right)\\right\\}\_{j=1}^{n\_{i}}$ 代表着第 $i$ 个源域，每个域的联合分布不一样 $P\_{X Y}^{i} \\neq   P\_{X Y}^{j}, 1 \\leq i \\neq j \\leq M$。域适应的目的是从 $M$ 个训练的源域学习到一个泛化函数 $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$ 应用到一个不可知的目标域 $\\mathcal{S}\_{\\text {test }}$，使得 $\\min \_{h} \\mathbb{E}\_{(\\mathbf{x}, y) \\in \\mathcal{S}\_{\\text {test }}}\[\\ell(h(\\mathbf{x}), y)\]$ 。所有域，包括源域和目标域，都具有相同的输入空间 $\\mathcal{X}^{1}=\\cdots=\\mathcal{X}^{M}=\\mathcal{X}^{T} \\in \\mathbb{R}^{m}$ 和输出空间 $\\mathcal{Y}^{1}=\\cdots=\\mathcal{Y}^{M}=\\mathcal{Y}^{T}=\\{1,2, \\cdots, C\\}$。

4 动机
====

　　已有的工作表明傅里叶相值（Phase）中包含更多的语义信息，不太容易受到域偏移的影响，而傅里叶幅值（Amplitude）信息主要包含低层次的统计信息，受域偏移影响较大。从下面的图中，可以看出，对于行走采集到的原始数据来说，傅里叶的相值信息的确更能代表类别，仅由相值恢复的数据的确包含更多的语义信息，比如周期性以及起伏。因此，我们把傅里叶相值作为域内不变特征。

　　![](https://img2023.cnblogs.com/blog/1664108/202301/1664108-20230126203941907-738545426.png)

5 方法
====

　　整体框架：

　　![](https://img2023.cnblogs.com/blog/1664108/202301/1664108-20230126204439330-629059835.png)

　　如上图所示，$\\text{DIFEX}$ 尝试同时学习域内不变特征和域间不变特征，并尝试将他们集合起来进行分类。注意，为了保持公平性，我们将最后一层特征一分为二，一部分进行域内不变特征学习，一部分进行域间特征学习；同时为了保证特征的多样性，我们提出了一个正则项，来让两种特征的差别尽量大。下面来具体看看两种特征的获取以及多样性的拓展。

5.1 域不变特征
---------

　　为了获取域内不变特征，主要采用一个简单的蒸馏框架来学习，注意这里的蒸馏方法虽然在训练时候引入了额外的训练代价，但是在预测时可以减少不必要的FFT计算，确保预测的整个过程可以端到端的直接进行。

　　![](https://img2023.cnblogs.com/blog/1664108/202301/1664108-20230126204617400-1332773779.png)

　　如上图所示，我们首先使用一个老师网络来利用傅里叶相值信息来学习分类模型，从而获取有用的与分类有关的傅里叶相值信息，训练之后，我们认为老师模型可以得到与分类有关的傅里叶相值信息，那么在学生模型训练的时候，便可以让它参考老师的这部分特征，进行域内不变特征学习。

　　　　$\\underset{\\theta\_{S}^{f}, \\theta\_{S}^{c}}{\\text{min }}      \\mathbb{E}\_{(\\mathbf{x}, y) \\sim P \\operatorname{tr}} \\ell\_{c}\\left(G\_{S}^{c}\\left(G\_{S}^{f}(\\mathbf{x})\\right), y\\right)+\\lambda\_{1} \\mathcal{L}\_{m s e}\\left(G\_{S}^{f}(\\mathbf{x}), G\_{T}^{f}(\\tilde{\\mathbf{x}})\\right) \\quad\\quad\\quad(4)$

补充：

　　对单通道二维数据 $x$ 的傅里叶变换 $\\mathcal{F}(\\mathbf{x})$ 表示为：

　　　　$\\mathcal{F}(\\mathbf{x})(u, v)=\\sum\\limits \_{h=1}^{H-1} \\sum\\limits\_{w=0}^{W-1} \\mathbf{x}(h, w) e^{-j 2 \\pi\\left(\\frac{h}{H} u+\\frac{w}{W} v\\right)}\\quad\\quad\\quad(1)$

　　其中 $u$ 和 $v$ 是指数。$H$ 和 $W$ 分别是高度和宽度。傅里叶变换可以用 FFT 算法有效地计算出来。相位分量随后表示为：

　　　　$\\mathcal{P}(x)(u, v)=\\arctan \\left\[\\frac{I(x)(u, v)}{R(x)(u, v)}\\right\]\\quad\\quad\\quad(2)$

　　其中，$R(x)$ 和 $I(x)$ 分别表示 $\\mathcal{F}(\\mathbf{x})$ 的实部和虚部。对于具有多个通道的数据，分别计算每个通道的傅里叶变换，得到相应的相位信息。我们将 $x$ 的傅里叶相位表示为 $\\tilde{\\mathbf{x}}$，然后，使用 $(\\tilde{\\mathbf{x}}, y)$ 训练教师网络：

　　　　$\\underset{\\theta\_{T}^{f}, \\theta\_{T}^{c}}{\\text{min}}  \\quad \\mathbb{E}\_{(\\mathbf{x}, y) \\sim P^{t r}} \\mathcal{L}\_{c l s}\\left(G\_{T}^{c}\\left(G\_{T}^{f}(\\tilde{\\mathbf{x}})\\right), y\\right)\\quad\\quad\\quad(3)$

　　一旦获得了教师网络 $G\_{T}$，我们就使用特征知识蒸馏来指导学生网络学习傅里叶信息。这种蒸馏方法的配方如下： 

　　　　$\\underset{\\theta\_{S}^{f}, \\theta\_{S}^{c}}{\\text{min }}      \\mathbb{E}\_{(\\mathbf{x}, y) \\sim P \\operatorname{tr}} \\ell\_{c}\\left(G\_{S}^{c}\\left(G\_{S}^{f}(\\mathbf{x})\\right), y\\right)+\\lambda\_{1} \\mathcal{L}\_{m s e}\\left(G\_{S}^{f}(\\mathbf{x}), G\_{T}^{f}(\\tilde{\\mathbf{x}})\\right) \\quad\\quad\\quad(4)$

　　其中，$\\theta\_{S}^{f}$ 和 $\\theta\_{S}^{c}$ 是学生网络的特征提取器 $G\_{S}^{f}$ 和分类层 $G\_{S}^{c}$ 的可学习参数。$\\lambda\_{1}$ 是一个权衡超参数，$\\mathcal{L}\_{m s e}$ 是 $\\text{MSE}$ 损失，它可以使学生网络的特征接近教师网络的特征。

5.2 互不变特征
---------

　　如前所述，仅凭傅里叶相位特征并不足以获得足够的鉴别特征来进行分类。因此，我们通过利用多个训练领域中包含的跨领域知识来探索互不变的特征。具体来说，给定两个域 $\\mathcal{S}^{i}$，$\\mathcal{S}^{i}$，我们使用相关性对齐方法对它们的二阶统计量（相关性）进行对齐：

　　　　$\\mathcal{L}\_{\\text {align }}=\\frac{2}{N \\times(N-1)} \\sum\\limits\_{i \\neq j}^{N}\\left\\|\\mathbf{C}^{i}-\\mathbf{C}^{j}\\right\\|\_{F}^{2}\\quad\\quad\\quad(5)$

　　其中，$\\mathbf{C}^{i}=\\frac{1}{n\_{i}-1}\\left(\\mathbf{X}^{i} \\mathbf{X}^{i}-\\frac{1}{n\_{i}}\\left(\\mathbf{1}^{T} \\mathbf{X}^{i}\\right)^{T}\\left(\\mathbf{1}^{T} \\mathbf{X}^{i}\\right)\\right)$ 是协方差矩阵，$\\|\\cdot\\|\_{F}$ 代表着 $F$ 范数。

　　由于域内不变特征和域间不变特征之间可能存在重复和冗余，我们期望两部分可以尽可能多地提取出不同的不变特征。这使得特征有更多的多样性，有利于泛化。为了实现这个目标，我们通过最大化内部不变（$z\_1$）和互不变（ $z\_1$）特征之间的距离，我们称之为  $\\text{exploration loss}$：

　　　　$\\mathcal{L}\_{\\exp }\\left(\\mathbf{z}\_{1}, \\mathbf{z}\_{2}\\right)=-d\\left(\\mathbf{z}\_{1}, \\mathbf{z}\_{2}\\right) \\quad\\quad\\quad(6)$

　　其中 $d(\\cdot, \\cdot)$ 是一个距离函数，为了简单，我们简单地使用 $L2$ 距离： $ \\mathcal{L}\_{e x p}=-\\left\\|\\mathbf{z}\_{1}-\\mathbf{z}\_{2}\\right\\|\_{2}^{2} $。

6 DIFEX 总结 
===========

　　综上所述，我们的方法被分为两个步骤。首先，我们优化了 $\\text{Eq.1}$。其次，优化了以下目标：

　　　　$\\underset{\\theta\_{f}, \\theta\_{c}}{\\text{min}} \\;\\mathbb{E}\_{(\\mathbf{x}, y) \\sim P^{t r}} \\mathcal{L}\_{c l s}\\left(G\_{c}\\left(G\_{f}(\\mathbf{x})\\right), y\\right)+\\lambda\_{1} \\mathcal{L}\_{m s e}\\left(\\mathbf{z}\_{1}, G\_{T}^{f}(\\tilde{\\mathbf{x}})\\right)+\\lambda\_{2} \\mathcal{L}\_{\\text {align }}+\\lambda\_{3} \\mathcal{L}\_{\\text {exp }}\\left(\\mathbf{z}\_{1}, \\mathbf{z}\_{2}\\right)$

因上求缘，果上努力~~~~ 作者：[加微信X466550探讨](https://www.cnblogs.com/BlairGrowing/)，转载请注明原文链接：[https://www.cnblogs.com/BlairGrowing/p/17067914.html](https://www.cnblogs.com/BlairGrowing/p/17067914.html)