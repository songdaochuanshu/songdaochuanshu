---
layout: post
title: "æ·±åº¦å­¦ä¹ ï¼ˆäº”ï¼‰ä¹‹åŸå‹ç½‘ç»œ"
date: "2022-05-08T04:25:39.197Z"
---
æ·±åº¦å­¦ä¹ ï¼ˆäº”ï¼‰ä¹‹åŸå‹ç½‘ç»œ
============

ç›®å½•

*   [å°æ ·æœ¬å­¦ä¹ å¼•å…¥](#å°æ ·æœ¬å­¦ä¹ å¼•å…¥)
    *   [å°æ ·æœ¬å­¦ä¹ æ˜¯ä»€ä¹ˆ](#å°æ ·æœ¬å­¦ä¹ æ˜¯ä»€ä¹ˆ)
    *   [å°æ ·æœ¬å­¦ä¹ æ–¹æ³•](#å°æ ·æœ¬å­¦ä¹ æ–¹æ³•)
*   [åŸå‹ç½‘ç»œï¼ˆPrototype Networkï¼‰](#åŸå‹ç½‘ç»œprototype-network)
    *   [åŸç†ç®€è¿°](#åŸç†ç®€è¿°)
    *   [ç®—æ³•æµç¨‹](#ç®—æ³•æµç¨‹)
*   [ç®—æ³•å®ç°](#ç®—æ³•å®ç°)
    *   [æ•°æ®é›†å¤„ç†](#æ•°æ®é›†å¤„ç†)
    *   [éšæœºäº§ç”ŸSupport set å’Œ Query set](#éšæœºäº§ç”Ÿsupport-set-å’Œ-query-set)
    *   [Embeddingç½‘ç»œæ„å»º](#embeddingç½‘ç»œæ„å»º)
    *   [æŸå¤±å‡½æ•°](#æŸå¤±å‡½æ•°)
    *   [å®éªŒç»“æœ](#å®éªŒç»“æœ)
*   [æ€»ç»“](#æ€»ç»“)
*   [References](#references)

åœ¨æœ¬æ–‡ä¸­ï¼Œå°†ä»‹ç»ä¸€äº›å…³äºå°æ ·æœ¬å­¦ä¹ çš„ç›¸å…³çŸ¥è¯†ï¼Œä»¥åŠä»‹ç»å¦‚ä½•ä½¿ç”¨pytorchæ„å»ºä¸€ä¸ªåŸå‹ç½‘ç»œ(**Prototypical Networks**[\[1\]](#fn1))ï¼Œå¹¶åº”ç”¨äºminiImageNet æ•°æ®é›†ã€‚

å®éªŒç¯å¢ƒï¼š

pytorchï¼š1.11.0  
ä»£ç åœ°å€ï¼š[https://github.com/xiaohuiduan/deeplearning-study/tree/main/å°æ ·æœ¬å­¦ä¹ ](https://github.com/xiaohuiduan/deeplearning-study/tree/main/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0)

å°æ ·æœ¬å­¦ä¹ å¼•å…¥
-------

åœ¨è¿™ä¸€èŠ‚å°†ç®€è¦çš„å¯¹å°æ ·æœ¬å­¦ä¹ (FSL)ç›¸å…³çš„çŸ¥è¯†è¿›è¡Œä»‹ç»ã€‚ç”±äºæˆ‘å¹¶ä¸æ˜¯ä¸“é—¨ç ”ç©¶å°æ ·æœ¬çš„ï¼ˆæˆ‘å­¦ä¹ FSLä¹Ÿåªæ˜¯ä¸ºäº†å®Œæˆæˆ‘çš„è¯¾ç¨‹ä½œä¸šï¼‰ï¼Œå› æ­¤ï¼Œå¦‚æœæœ¬æ–‡å­˜åœ¨ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿è¿›è¡Œæ‰¹è¯„æŒ‡æ­£ğŸ˜ã€‚

> **é‚®ç®±ğŸ“«ï¼šxiaohuiduan@hunnu.edu.cn**

é¦–å…ˆå°†æ¨¡å‹çœ‹æˆä¸€ä¸ªé»‘ç›’å­ï¼Œä¸å»å…³æ³¨å®ƒçš„å†…éƒ¨ç»“æ„ï¼Œè€Œæ˜¯å…³æ³¨å…¶**input**å’Œ**output**ã€‚

åœ¨åˆ†ç±»æ¨¡å‹[\[2\]](#fn2)ä¸­ï¼Œinputæ˜¯ä¸€å¼ çŒ«orç‹—çš„å›¾ç‰‡ï¼Œoutputåˆ™ä¸º0/1ï¼ˆä»£è¡¨å…¶ä¸ºçŒ«æˆ–è€…ç‹—ï¼›å®é™…ä¸Šè¾“å‡ºçš„æ˜¯ä¸¤è€…çš„é¢„æµ‹æ¦‚ç‡ï¼‰ã€‚

![](https://img2022.cnblogs.com/blog/1439869/202205/1439869-20220507211647541-248521631.png)

ä½†æ˜¯å…³äºä¸Šé¢çš„æ¨¡å‹ï¼Œå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œè®­ç»ƒè¿™æ ·çš„æ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®ï¼Œæ ¹æ®[@petewarden](https://twitter.com/petewarden)[\[3\]](#fn3)çš„è¯´æ³•ï¼Œè®­ç»ƒä¸€ä¸ªåˆ†ç±»å›¾ç‰‡çš„ç½‘ç»œï¼Œæ¯ä¸ªç±»åˆ«éœ€è¦å¤§çº¦1000å¼ ã€‚ä½†æ˜¯ï¼Œå¾ˆå¤šåœºæ™¯æˆ‘ä»¬å¹¶æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¹Ÿå°±æ˜¯è¯´æ•°æ®é›†çš„æ ·æœ¬æ¯”è¾ƒå°‘ï¼Œè¿™æ—¶å€™é’ˆå¯¹å°æ ·æœ¬æ•°æ®é›†å¯ä»¥æœ‰ä¸¤ç§å¤„ç†æ–¹å¼[\[4\]](#fn4)ï¼š

*   æ•°æ®å¢å¼ºï¼šæ¯”å¦‚è¯´å¯¹å›¾åƒè¿›è¡Œæ—‹è½¬ï¼Œè£å‰ªç­‰ç­‰ã€‚
    
*   æ•°æ®å»ºæ¨¡ï¼šå¦‚ä½¿ç”¨å°æ ·æœ¬å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œå¯¹æ•°æ®è¿›è¡Œå»ºæ¨¡ã€‚
    

### å°æ ·æœ¬å­¦ä¹ æ˜¯ä»€ä¹ˆ

ä»¥VGGæ¨¡å‹é¢„æµ‹çŒ«ç‹—åˆ†ç±»ä¸¾ä¾‹ï¼Œæ¨¡å‹å¯¹äºä¸€å¼ æ–°çš„å›¾ç‰‡çš„é¢„æµ‹å¯ä»¥å½¢è±¡çš„è§£é‡Šä¸ºï¼š

> å›¾ç‰‡ä¸­çš„åŠ¨ç‰©å› ä¸ºæœ‰ç€å°–å°–çš„è€³æœµï¼Œæœ‰ç€è¾ƒé•¿çš„èƒ¡é¡»ï¼Œé¼»å­é‚£ä¸€ä¸ªåœ°æ–¹ä¸æ˜¯å¾ˆçªå‡ºï¼Œå› æ­¤ï¼Œæˆ‘ï¼ˆVGGï¼‰åˆ¤æ–­å®ƒæ˜¯ä¸€åªçŒ«o(=â€¢ã‚§â€¢=)mã€‚

ä½†æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ ä¸­ï¼Œä¸æ˜¯è¿™æ ·çš„ï¼Œå°æ ·æœ¬å­¦ä¹ å¯¹äºä¸€å¼ æ–°çš„å›¾ç‰‡çš„é¢„æµ‹å¯ä»¥å½¢è±¡çš„è§£é‡Šä¸ºï¼š

> æˆ‘æ‰‹ä¸­æœ‰2å¼ å›¾ç‰‡ï¼Œå›¾ç‰‡Aå’Œå›¾ç‰‡Bã€‚å¯¹äºæ–°çš„å›¾ç‰‡ï¼Œæˆ‘ï¼ˆæ¨¡å‹ï¼‰ä¹Ÿä¸çŸ¥é“ä»–æ˜¯å•¥ï¼Œä½†æ˜¯æˆ‘å‘ç°å®ƒè·Ÿå›¾ç‰‡Bé•¿å¾—å¾ˆç›¸ä¼¼ï¼Œå› æ­¤æˆ‘ï¼ˆæ¨¡å‹ï¼‰åˆ¤æ–­è¿™å¼ æ–°çš„å›¾ç‰‡å’Œå›¾ç‰‡Bæ˜¯åŒä¸€ä¸ªç±»åˆ«ã€‚
> 
> åœ¨ä¸Šè¿°è§£é‡Šä¸­ï¼Œå›¾ç‰‡Aå’ŒBç§°ä¹‹ä¸º`support sets`ï¼Œè€Œæ–°çš„å›¾ç‰‡ç§°ä¹‹ä¸º`query sets`ã€‚

å°æ ·æœ¬çš„åˆ†ç±»æ¨¡å‹ä¸ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ åˆ†ç±»æ¨¡å‹ï¼ˆå¦‚VGGï¼‰æœ‰ç€ä¸åŒï¼Œè¿™é‡Œå¼•ç”¨è®ºæ–‡[\[1:1\]](#fn1)çš„ä¸€å¥è¯ï¼š

> Few-shot classification is a task in which a classifier must be adapted to accommodate  
> new classes not seen in training, given only a few examples of each of these classes. A naive approach,  
> such as re-training the model on the new data, would severely overfit.

ä¹Ÿå°±æ˜¯è¯´ï¼Œå°æ ·æœ¬åˆ†ç±»å¹¶ä¸æ˜¯åƒVGGä¸€æ ·ï¼Œtestçš„æ•°æ®æ˜¯ä»¥å‰è®­ç»ƒè¿‡çš„ç±»åˆ«ï¼Œå¯¹äºå°æ ·æœ¬åˆ†ç±»æ¥è¯´ï¼Œè¿›è¡Œtestçš„æ•°æ®æ˜¯ä¸€äº›æ–°çš„ç±»ï¼Œå¹¶ä¸”è¿™äº›ç±»åˆ«çš„æ ·æœ¬å¾ˆå°‘ï¼Œå› æ­¤ï¼Œæ²¡æ³•å¯¹å…¶è¿›è¡Œre-trainingï¼Œå¦åˆ™ä¼šé€ æˆè¿‡æ‹Ÿåˆã€‚

### å°æ ·æœ¬å­¦ä¹ æ–¹æ³•

å°æ ·æœ¬å­¦ä¹ å¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªN-way K-shotçš„åˆ†ç±»é—®é¢˜ï¼ˆ_ä¸ç¡®å®šæ˜¯ä¸æ˜¯æ‰€æœ‰çš„å°æ ·æœ¬åˆ†ç±»ä»»åŠ¡éƒ½è¢«è®¤ä¸ºæ˜¯N-way K-shotåˆ†ç±»é—®é¢˜_ï¼‰ã€‚

æ— è®ºå¯¹äº**æµ‹è¯•é›†**è¿˜æ˜¯**è®­ç»ƒé›†**ï¼Œéƒ½éœ€è¦è¿›è¡Œå¦‚ä¸‹çš„åˆ’åˆ†ï¼Œå°†æ•°æ®é›†åˆ’åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šå·¦è¾¹DataSetä»£è¡¨æ•°æ®é›†ï¼Œå³è¾¹åˆ†åˆ«ä»£è¡¨Support setï¼Œå³è¾¹ä»£è¡¨Query Setã€‚åœ¨**train**æˆ–è€…**test**æ•°æ®é›†ä¸­ï¼š

1.  é¦–å…ˆå¯¹äºæ‰€æœ‰ç±»åˆ«ï¼Œéšæœºé€‰æ‹©å…¶ä¸­Nï¼ˆå›¾ä¸­N=3ï¼‰ä¸ªç±»åˆ«ï¼ˆå›¾ä¸­ï¼Œé€‰æ‹©äº†ç±»åˆ«2ï¼Œç±»åˆ«3å’Œç±»åˆ«5ï¼‰ã€‚
2.  åœ¨step 1ä¸­é€‰æ‹©çš„ç±»åˆ«æ ·æœ¬ä¸­ï¼Œéšæœºé€‰æ‹©K(å›¾ä¸­K=3)ä¸ªæ ·æœ¬ï¼ˆç»¿è‰²çš„éƒ¨åˆ†ï¼‰ï¼Œæ„æˆSupport Setã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒSupport Setä¸­æ‹¥æœ‰K\*Nä¸ªæ ·æœ¬ã€‚
3.  ç„¶ååœ¨æ‰€é€‰æ‹©ç±»åˆ«çš„å‰©ä½™æ ·æœ¬ä¸­ï¼Œé€‰æ‹©Xï¼ˆè¿™é‡ŒX=1ï¼‰ä¸ªæ ·æœ¬ï¼ˆçº¢è‰²çš„éƒ¨åˆ†ï¼‰ï¼Œæ„æˆQuery Setã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒQuery Setä¸­æ‹¥æœ‰X\*Nä¸ªæ ·æœ¬ã€‚

åœ¨è®­ç»ƒé›†ä¸­ï¼Œä»¥ä¸Šæ­¥éª¤æ„æˆçš„Support Setå’ŒQuery Setä¼šè¢«inputåˆ°Modelä¸­è¿›è¡Œè®­ç»ƒï¼Œç§°ä¹‹ä¸ºä¸€ä¸ªeposidesï¼ˆç›¸å½“äºmini-batchï¼‰ã€‚

![](https://img2022.cnblogs.com/blog/1439869/202205/1439869-20220507211648034-1954084035.png)

å¯¹äºæ¨¡å‹æ¥è¯´ï¼Œå…¶ç›®çš„åˆ™ä¸º**åˆ¤æ–­Query Setä¸­çš„æ ·æœ¬ä¸å“ªä¸€ä¸ªæ”¯æŒé›†æœ€ç›¸ä¼¼**ã€‚

åŸå‹ç½‘ç»œï¼ˆPrototype Networkï¼‰
-----------------------

### åŸç†ç®€è¿°

Prototype Networkçš„åŸç†å¾ˆç®€å•ï¼Œå¯ä»¥ç®€å•çš„æ¦‚æ‹¬ä¸ºï¼šå°†support setä¸­çš„å›¾ç‰‡\\(data\_1,data\_2,\\cdots,data\_n\\)æ˜ å°„åˆ°æŸä¸€ä¸ªå‘é‡ç©ºé—´\\(c\_1,c\_2,\\cdots,c\_n\\)ï¼›å¯¹äºQuery setä¸­çš„**æŸä¸€å¼ **å›¾ç‰‡\\(query\_i\\)ä½¿ç”¨åŒä¸€ä¸ªæ˜ å°„å‡½æ•°ï¼Œä¹Ÿæ˜ å°„åˆ°åˆ°å‘é‡ç©ºé—´\\(x\_i\\)ï¼Œç„¶ååˆ¤æ–­\\(x\_i\\)ä¸\\(c\_1,c\_2,\\cdots,c\_n\\)çš„è·ç¦»ï¼ˆä½™å¼¦è·ç¦»oræ¬§æ°è·ç¦»ï¼‰ï¼Œé€‰æ‹©è·ç¦»æœ€è¿‘å‘é‡æ‰€å¯¹åº”çš„ç±»åˆ«ä½œä¸º\\(query\_i\\)æ‰€å±çš„ç±»åˆ«ã€‚

ç¤ºæ„å›¾[\[5\]](#fn5)å¦‚ä¸‹æ‰€ç¤ºï¼š

![](https://img2022.cnblogs.com/blog/1439869/202205/1439869-20220507211648691-1928172859.png)

å¦‚æœäº†è§£NLPä¸­word2vecçš„è¯ï¼Œä¼šå‘ç°ï¼Œå…¶ä¸word2vecçš„Embeddingæ€æƒ³æ˜¯å¾ˆç›¸ä¼¼çš„ã€‚

### ç®—æ³•æµç¨‹

ç®—æ³•æµç¨‹å›¾[\[1:2\]](#fn1)å¦‚ä¸‹æ‰€ç¤ºï¼Œçº¢è‰²æ¡†å’Œç»¿è‰²æ¡†ä¸­çš„è¿‡ç¨‹å·²ç»åœ¨å‰æ–‡è¿›è¡Œä»‹ç»ï¼Œè¿™é‡Œä¸»è¦æ˜¯æ¥ä»‹ç»ä¸€ä¸‹lossçš„è®¡ç®—æ–¹å¼ã€‚

![image-20220507144721234](https://img2022.cnblogs.com/blog/1439869/202205/1439869-20220507211649257-154870073.png)

å®é™…ä¸Šï¼Œlossçš„è®¡ç®—æ–¹å¼å°±æ˜¯ä¸€ä¸ªäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œpytorchä¸­CrossEntropyLossçš„è®¡ç®—æ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼Œclassä»£è¡¨\\(x\\)å®é™…æ‰€å±ç±»åˆ«\\(x\[j\]\\)ä»£è¡¨æ¨¡å‹å¯¹äº\\(x\\)æ‰€å±ç±»åˆ«\\(j\\)çš„æ¦‚ç‡é¢„æµ‹ã€‚

\\\[\\operatorname{loss}(x, \\text { class })=-\\log \\left(\\frac{\\exp (x\[\\text { class }\])}{\\sum\_{j} \\exp (x\[j\])}\\right)=-x\[\\text { class }\]+\\log \\left(\\sum\_{j} \\exp (x\[j\])\\right) \\\]

ä½†æ˜¯ï¼Œåœ¨ç®—æ³•æµç¨‹å›¾ä¸­ï¼Œå¤§å®¶ä¼šå‘ç°ï¼Œå…¶lossè®¡ç®—çš„æ­£è´Ÿå·åˆšå¥½ä¸ä¸Šé¢å…¬å¼ä¸­çš„ç›¸åï¼Œè§£é‡Šå¦‚ä¸‹ï¼š

> ä»¥æ¬§å¼è·ç¦»ä¸ºä¾‹ï¼Œè·ç¦»è¶Šè¿œ(\\(d\\)åˆ™è¶Šå¤§)ï¼Œåˆ™ä»£è¡¨ä¸¤è€…çš„ç›¸ä¼¼åº¦è¶Šä½ã€‚å¦‚æœä¸åŠ è´Ÿå·çš„è¯ï¼Œè¿›è¡Œsoftmaxè®¡ç®—ï¼Œè·ç¦»è¶Šè¿œçš„åˆ™predictæ¦‚ç‡è¶Šå¤§ï¼Œè¿™æ˜æ˜¾æ˜¯é”™è¯¯çš„ã€‚å› æ­¤ï¼ŒåŠ äº†ä¸€ä¸ªè´Ÿå·ä¹‹åï¼Œè·ç¦»è¶Šè¿œï¼Œè¿›è¡Œsoftmaxä¹‹åï¼Œè¾“å‡ºåˆ™è¶Šå°ï¼Œpredictçš„æ¦‚ç‡ä¹Ÿå˜å°ï¼Œè¿™æ‰æ˜¯åˆç†çš„ã€‚

ä»¥ä¸Šï¼Œä¾¿æ˜¯åŸå‹ç½‘ç»œçš„ç®—æ³•æµç¨‹ã€‚

ç®—æ³•å®ç°
----

### æ•°æ®é›†å¤„ç†

mini-Imagenetæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè®­ç»ƒå°æ ·æœ¬å­¦ä¹ çš„è®­ç»ƒé›†ï¼Œæ•°æ®é›†ä¸­ä¸€å…±æœ‰100ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«600å¼ å›¾ç‰‡ï¼Œä¸€å…±æœ‰60000å¼ å›¾ç‰‡ã€‚æ•°æ®é›†å¯ä»¥ä»[mini-ImageNet | Kaggles](https://www.kaggle.com/datasets/zcyzhchyu/mini-imagenet)ä¸Šé¢ä¸‹è½½ã€‚åœ¨ä¸‹è½½æ–‡ä»¶ä¸­ï¼Œä¸€å…±æœ‰4ä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªæ˜¯æ•°æ®é›†å›¾ç‰‡çš„å‹ç¼©åŒ…ï¼Œå¦å¤–3ä¸ªcsvæ–‡ä»¶åˆ†åˆ«ä»£è¡¨äº†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ç›¸å…³çš„ä¿¡æ¯ã€‚å…¶ä¸­è®­ç»ƒé›†æœ‰64ä¸ªç±»ï¼ŒéªŒè¯é›†16ä¸ªç±»ï¼Œæµ‹è¯•é›†20ä¸ªç±»ã€‚

csvçš„éƒ¨åˆ†æ•°æ®å¦‚ä¸‹æ‰€ç¤ºï¼Œfilenameä»£è¡¨äº†å›¾ç‰‡çš„åå­—ï¼Œlabelä»£è¡¨äº†å›¾ç‰‡å¯¹åº”çš„æ ‡ç­¾ã€‚

![](https://img2022.cnblogs.com/blog/1439869/202205/1439869-20220507211649710-1561766347.png)

å› æ­¤ï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªlabelæ‰€å¯¹åº”filenameçš„å­—å…¸

    def read_csv(csv_path):
        dict = collections.defaultdict(list)
        df = pd.read_csv(csv_path)
        for index,row in df.iterrows():
            dict[row["label"]].append(row["filename"])
        return dict
    train_dict = read_csv(train_csv_path)
    val_dict = read_csv(val_csv_path)
    test_dict = read_csv(test_csv_path)
    

åŒæ—¶ï¼Œæ„å»ºdataä¸labelsçš„å¯¹åº”å…³ç³»ï¼š

    from PIL import Image
    import numpy as np
    from torchvision import transforms
    from PIL import ImageFile
    ImageFile.LOAD_TRUNCATED_IMAGES = True
    
    resize_transform = transforms.Resize(84) # æå‰å¯¹å›¾ç‰‡è¿›è¡Œç¼©æ”¾ï¼Œä»¥èŠ‚çœå†…å­˜ç©ºé—´ï¼Œå°†æœ€çŸ­çš„è¾¹å˜æˆ84
    def build_data(data_dict):
        datas = []
        labels = []
        label_index = 0
        for label in data_dict.keys(): # å¯¹å›¾ç‰‡çš„æ ‡ç­¾è¿›è¡Œè¿­ä»£
            for path in data_dict[label]: # å¯¹æ ‡ç­¾å¯¹åº”çš„æ–‡ä»¶åè¿›è¡Œè¿­ä»£
                img_path = os.path.join(img_root_dir,path) 
                img = Image.open(img_path) # è¯»å–æ–‡ä»¶
                img = resize_transform(img) # è¿›è¡Œç¼©æ”¾
                datas.append(img) 
                labels.append(label_index)
    
            label_index += 1
        return {"datas":datas,"labels":labels}
    

### éšæœºäº§ç”ŸSupport set å’Œ Query set

åœ¨ä¸‹é¢ä»£ç ä¸­ï¼Œ`CategoriesSampler`çš„ä½œç”¨æ˜¯ä¸ºäº†äº§ç”Ÿindexï¼Œç„¶åä¾›ç»™dataloaderä½¿ç”¨ã€‚

    class CategoriesSampler():
        """
            ç›®çš„æ˜¯ä¸ºäº†éšæœºäº§ç”ŸK_way*(N_support+N_query)ä¸ªå›¾ç‰‡å¯¹åº”çš„index
        """
        def __init__(self, data, n_batch, K_way, N_per):
            self.n_batch = n_batch
            self.K_way = K_way
            self.N_per = N_per
            labels = np.array(data["labels"]) # [0,0,0,0,1,1,1,1,2,2,2,2â€¦â€¦]
            self.index = [] # è®°å½•labelå¯¹åº”çš„ç´¢å¼•ä½ç½®
            for i in range(max(labels)+1):
                ind = np.argwhere(labels == i).reshape(-1)
                self.index.append(torch.from_numpy(ind))   
    
        def __len__(self):
            return self.n_batch
        
        def __iter__(self):
            for i_batch in range(self.n_batch):  
                batch = []
                classes = torch.randperm(len(self.index))[:self.K_way] # éšæœºé€‰æ‹©Kä¸ªç±»åˆ«æ„æˆsupport setå’Œquery set
                for c in classes:
                    l = self.index[c] # ç±»åˆ«cå¯¹åº”çš„å›¾ç‰‡æ•°ç»„çš„ç´¢å¼•ï¼Œå¦‚ l = [5,6,7,8,9]
                    pos = torch.randperm(len(l))[:self.N_per] # å¦‚ pos = [4,1,0]
                    batch.append(l[pos]) # å¦‚ l[pos] = [9,6,5]
    
                batch = torch.stack(batch).reshape(-1)
                yield batch
    

åŒæ—¶ï¼Œå®šä¹‰Datasetç±»ï¼Œå¦‚ä¸‹ï¼š

    class MiniImageNet(Dataset):
    
        def __init__(self, data):
    
            self.datas = data["datas"]
            self.labels = data["labels"]
            self.transform = transforms.Compose([
                transforms.CenterCrop(84),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
            ])
    
        def __len__(self):
            return len(self.datas)
    
        def __getitem__(self, i):
            img, label = self.datas[i], self.labels[i]
            return self.transform(img), label
    

ä½¿ç”¨ç¤ºä¾‹å¦‚ä¸‹ï¼Œåœ¨MiniImageNetçš„`__getitem__`å‡½æ•°ä¸­ï¼Œå…¶å‚æ•°`i`ç”±`CategoriesSampler`çš„`__iter__`å‡½æ•°æ‰€äº§ç”Ÿã€‚

    batch_sampler = CategoriesSampler(datas,eval_step,K_way,N_shot+N_query)
    data_loader = DataLoader(dataset=data_set, batch_sampler=batch_sampler,
                                                num_workers=16, pin_memory=True)
    

å®é™…ä¸Šï¼Œä¸Šé¢çš„ä»£ç å°±æ˜¯ä¸ºäº†å®ç°å¦‚ä¸‹å›¾æ‰€ç¤ºçš„åŠŸèƒ½ï¼š

![](https://img2022.cnblogs.com/blog/1439869/202205/1439869-20220507211650154-875610629.png)

### Embeddingç½‘ç»œæ„å»º

å‰æ–‡è¯´åˆ°ï¼Œéœ€è¦å°†å›¾ç‰‡è¿›è¡Œå‘é‡åŒ–è¡¨ç¤ºï¼Œåœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œä¾¿å¯ä»¥å°†ä¸€å¼ å›¾ç‰‡(shape=\\(3\\times84\\times84\\))å˜æˆä¸€ä¸ª1600ç»´çš„å‘é‡ã€‚ï¼ˆç½‘ç»œç»“æ„æ¥è‡ªäºè®ºæ–‡ï¼‰

    class CNN_Net(nn.Module):
        """
            ç”¨äºç‰¹å¾æå–
        """
    
        def __init__(self, input_dim):
            super(CNN_Net, self).__init__()
            
            self.input_dim = input_dim
            def conv_block(in_channel,out_channel):
                return nn.Sequential(
                    nn.Conv2d(in_channel, out_channel, 3,padding=1),
                    nn.BatchNorm2d(out_channel),
                    nn.ReLU(),
                    nn.MaxPool2d(2)
                )
            self.encoder = nn.Sequential(
                conv_block(input_dim,64),
                conv_block(64,64),
                conv_block(64,64),
                conv_block(64,64),
            )
        def forward(self, x):
            x = self.encoder(x)
            x = x.view(x.size(0), -1)
            return x
    

### æŸå¤±å‡½æ•°

å…³äºè®¡ç®—æŸå¤±çš„å…³é”®å‡½æ•°å¦‚ä¸‹æ‰€ç¤ºï¼ˆå‚è€ƒäº†è®ºæ–‡ä½œè€…çš„æºä»£ç [\[6\]](#fn6)ï¼‰ï¼š

    def cal_euc_distance(self, query_z, center,K_way, N_query):
        """
            è®¡ç®—query_zä¸centerçš„è·ç¦»
            query_z : (K_way*N_query,z_dim)
            center : (K_way,z_dim)
        """
        center = center.unsqueeze(0).expand(
            K_way*N_query, K_way, self.z_dim)  # (K_way*N_query,K_way,z_dim)
        query_z = query_z.unsqueeze(1).expand(
            K_way*N_query, K_way, self.z_dim)  # (K_way*N_query,K_way,z_dim)
    
        return torch.pow(query_z-center, 2).sum(2)  # (K_way*N_query,K_way)
    
    def loss_acc(self, query_z, center, K_way, N_query):
        """
            è®¡ç®—losså’Œacc
            query_z : (K_way*N_query,z_dim)
            center : (K_way,z_dim)
        """
        target_inds = torch.arange(0, K_way).view(K_way, 1).expand(
            K_way, N_query).long().to(self.device) # shape=(K_way, N_query)
        
        distance = self.cal_euc_distance(query_z, center,K_way, N_query)    # (K_way*N_query,K_way) 
        predict_label = torch.argmin(distance, dim=1)  # (K_way*N_query) é¢„æµ‹å‡ºæ¥çš„label
    
        acc = torch.eq(target_inds.contiguous().view(-1),
                        predict_label).float().mean() # å‡†ç¡®ç‡
    
        loss = F.log_softmax(-distance, dim=1).view(K_way,
                                                    N_query, K_way)  # (K_way,N_query,K_way)
        loss = - \
            loss.gather(dim=2, index=target_inds.unsqueeze(2)).view(-1).mean()
        return loss, acc
    
    def set_forward_loss(self, K_way, N_shot, N_query,sample_datas):
        """
            sample_datasï¼š shape(K_way*(N_shot+N_query),3,84,84)
        """
    
        z = self.cnn_net(sample_datas) # shape=(K_way*(N_shot+N_query),z_dim) ï¼Œå°†support setå’Œquery setéƒ½è¿›è¡Œå‘é‡åŒ–è¡¨ç¤º
        z = z.view(K_way,N_shot+N_query,-1) # shape = (K_way,N_shot+N_query,1600)
        
        support_z = z[:,:N_shot] # support setçš„å‘é‡åŒ–è¡¨ç¤º shape=(K_way,N_shot,1600)
        query_z = z[:,N_shot:].contiguous().view(K_way*N_query,-1) # Query setçš„å‘é‡åŒ–è¡¨ç¤º shape=(K_way*N_query,1600)
        
        center = torch.mean(support_z, dim=1) # è®¡ç®—support setçš„å‘é‡å‡å€¼ï¼Œshape=(K_way,1600)
        return self.loss_acc(query_z, center,K_way,N_query)
    

å…³äºå®éªŒä¸­å…·ä½“çš„å‚æ•°è®¾è®¡ï¼Œå¯ä»¥å‚è€ƒè®ºæ–‡[\[1:3\]](#fn1)æˆ–è€…Githubä¸Šé¢çš„[æºä»£ç ](https://github.com/xiaohuiduan/deeplearning-study/tree/main/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0)ã€‚åœ¨åŸè®ºæ–‡ä¸­ï¼Œå¯¹äºå®éªŒçš„è®¾è®¡è®²å¾—éå¸¸æ¸…æ¥šã€‚

### å®éªŒç»“æœ

ä¸‹é¢çš„è¡¨æ ¼ä¸ºæµ‹è¯•é›†çš„accï¼ˆå½“éªŒè¯é›†accä¸ºæœ€å¤§å€¼æ—¶æµ‹è¯•é›†æ‰€å¯¹åº”çš„accï¼‰ï¼š

N-shot=1

N-shot=5

K\_way=5

0.4313

0.6684

![](https://img2022.cnblogs.com/blog/1439869/202205/1439869-20220507211650629-169357005.png)

æ€»ç»“
--

æ€»çš„æ¥è¯´ï¼ŒåŸå‹ç½‘ç»œæ˜¯ä¸€ä¸ªå®¹æ˜“ç†è§£çš„ç½‘ç»œæ¨¡å‹ï¼Œæ€æƒ³ç®€å•ï¼Œæ˜“äºå®ç°ã€‚

References
----------

* * *

1.  \[[1703.05175\] Prototypical Networks for Few-shot Learning (arxiv.org)](https://arxiv.org/abs/1703.05175) [â†©ï¸](#fnref1) [â†©ï¸](#fnref1:1) [â†©ï¸](#fnref1:2) [â†©ï¸](#fnref1:3)
    
2.  [æ·±åº¦å­¦ä¹ ï¼ˆäºŒï¼‰ä¹‹çŒ«ç‹—åˆ†ç±» - æ®µå°è¾‰ - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/xiaohuiduan/p/16032352.html) [â†©ï¸](#fnref2)
    
3.  [How many images do you need to train a neural network? Â« Pete Warden's blog](https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/) [â†©ï¸](#fnref3)
    
4.  åˆ˜é¢–, é›·ç ”åš, èŒƒä¹ä¼¦, ç‹å¯Œå¹³, å…¬è¡è¶…, ç”°å¥‡. åŸºäºå°æ ·æœ¬å­¦ä¹ çš„å›¾åƒåˆ†ç±»æŠ€æœ¯ç»¼è¿°. è‡ªåŠ¨åŒ–å­¦æŠ¥, 2021, 47(2): 297âˆ’315 [â†©ï¸](#fnref4)
    
5.  [ã€Pytorchã€‘prototypical networkåŸå‹ç½‘ç»œå°æ ·æœ¬å›¾åƒåˆ†ç±»ç®€è¿°åŠå…¶å®ç°\_Jnchinçš„åšå®¢-CSDNåšå®¢\_åŸå‹ç½‘ç»œå°æ ·æœ¬](https://blog.csdn.net/qq_38237214/article/details/120503914) [â†©ï¸](#fnref5)
    
6.  [jakesnell/prototypical-networks: Code for the NeurIPS 2017 Paper "Prototypical Networks for Few-shot Learning" (github.com)](https://github.com/jakesnell/prototypical-networks) [â†©ï¸](#fnref6)
    

  
ä½œè€…ï¼š [æ®µå°è¾‰](https://www.cnblogs.com/xiaohuiduan/)  
å‡ºå¤„ï¼š[https://www.cnblogs.com/xiaohuiduan/p/16244173.html](https://www.cnblogs.com/xiaohuiduan/p/16244173.html)

### é‚®ç®±ğŸ“«ï¼šxiaohuiduan@hunnu.edu.cn