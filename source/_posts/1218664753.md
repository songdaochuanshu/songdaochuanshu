---
layout: post
title: "自然语言处理NLP程序包（NLTK/spaCy）使用总结"
date: "2022-09-30T08:35:09.619Z"
---
自然语言处理NLP程序包（NLTK/spaCy）使用总结
============================

　　NLTK和SpaCy是NLP的Python应用，提供了一些现成的处理工具和数据接口。下面介绍它们的一些常用功能和特性，便于对NLP研究的组成形式有一个基本的了解。

NLTK
====

　　Natural Language Toolkit (NLTK) 由宾夕法尼亚大学开发，提供了超过50种语料库，以及一些常用的文本处理函数，例如分词（Tokenization）、词干（Stemming）、词性标记（Tagging）等。

　　下面主要介绍WordNet语料库，其它方法和接口等用到了再进行记录。

WordNet
-------

　　在WordNet中，每个词由于可以有多重词义，因此会被包含在多个同义词集中。每个同义词集中又包含多个这个意思下的所有词汇（Lemma, 词元）。WordNet还对所有词义（动词和名词）进行了包含与被包含关系的层次排序。因此词义之间得以组成一种树状结构。此外，WordNet还包含量化两个词之间相似度的方法。

### 同义词集

　　同义词集用三元组表示，如下展示了channel被包含的所有同义词集：

import nltk
from nltk.corpus import wordnet
a \= wordnet.synsets('channel')
print(a)

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220929152455662-657644021.png)

　　如channel.n.01表示channel的第一个名词词义。此外channel还有impart.v.03词义，表示impart的第三个动词词义。我们可以看同义词集的解释、例子和这个同义词下的所有词元：

a = wordnet.synsets('channel')\[0\] #获取一个同义词集，
# a = wordnet.synset('channel.n.01') # 或直接用 wordnet.synset('channel.n.01')
print(a.definition())
print(a.examples())
print(a.lemmas())

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220929153750228-406172899.png)

　　其中同义词下的词元用四元组表示。

### 反义词

　　由于一个词有多种意思，因此获取反义词需要通过四元组词元（lemma）。

synset1 = wordnet.synsets('good')\[1\]
lemma1 \= synset1.lemmas()\[0\]
print("Lemma 1: ", lemma1)
print("Meaning 1: ", synset1.definition())
lemma2 \= lemma1.antonyms()\[0\] 
synset2 \= lemma2.synset()
print("Lemma 2: ", lemma2)
print("Meaning 2: ", synset2.definition())

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220929162806713-1937782994.png)

### 上下位词

　　根据WordNet的树状结构可以获取某个词集的上下位词集，及其对应的根节点、到根节点的路径。当然只对名词和动词有效，形容词和副词没有上下级的类别层次关系。

syn = wordnet.synsets('knife')\[0\] #获取某个同义词集
print("Upper: ", syn.hypernyms(), end="\\n\\n") # 上位词
print("Lower: ", syn.hyponyms(), end="\\n\\n") # 下位词
print("Root: ", syn.root\_hypernyms(), end="\\n\\n") # 根节点
print("Path: ", syn.hypernym\_paths(), end="\\n\\n") # 到根节点的路径

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220929182410770-167303614.png)

　　我们可以画出树形结构示意图如下：

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220930091143002-981428311.png)

### 词之间的相似度

　　利用层次关系，WordNet可以计算名词、动词之间的相似性。

def similarity\_with\_upper(syn):
  a \= wordnet.synset(syn)
  b \= wordnet.synset(syn).hypernyms()\[0\]
  print("Distance: ", a.shortest\_path\_distance(b)) # 两个同义词集之间的距离
  print("Path similarity: ", a.path\_similarity(b)) # 路径相似度，由两个词集之间的距离决定
  print("Distance to root: ", b.hypernym\_paths()\[0\].\_\_len\_\_()) # 到根节点的路径
  print("Wup-Palmer similarity: ", a.wup\_similarity(b)) # Wu-Palmer相似度，由两个词集所属的共同最深上位词集的深度决定
  print("LCH similarity: ", a.lch\_similarity(b))  # LCH相似度，由以上两个指标决定
  print()

similarity\_with\_upper('human.n.01')
similarity\_with\_upper('animal.n.01') 

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220929201251555-388173710.png)

　　以上分别计算了路径相似度、Wu-Palmer相似度和LCH相似度。可以看出，两个词集所属的共同最深上位词集越深，它们的共同含义越具体，Wup-Palmer相似度越大。另外，经过测试，发现动词集与名词集分别属于不同的树，因此它们之间无法计算相似性。

### 形容词簇

　　对于形容词，WordNet将它们的词义分为中心 (Head, a) 同义词集与卫星 (Satellite, s) 同义词集。一个中心通常围绕一个或多个卫星同义词集，从而形成一个簇。而中心同义词集则表示这个整个簇的基本概念。可以通过similar\_tos()获取一个中心同义词集的所有卫星。

a = wordnet.synset("glorious.a.01")
a.similar\_tos()

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220929210255603-973959012.png)

　　结构图如下：

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220929211503649-940201645.png)

### 句子词性标注

　　NLTK用了一些当前推荐的方法来对词性进行标注，内部应该是用了一些人为定义的规则加上神经网络。

s = "A man struck my video camera with a hammer."
text \= nltk.word\_tokenize(s) # 将句子划分为词列表，英语很简单类似于.split(' ')，但其他语言如中文就不一定了
tag1 = nltk.pos\_tag(text)
tag2 \= nltk.pos\_tag(text, tagset='universal')
print("Tag1: ", tag1, end='\\n\\n')
print("Tag2: ", tag2)

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220930093539285-417166124.png)

　　加上universal参数是直接分析单个词的基本词性，不加则默认进行更细粒度的Penn Treebank POS Tags划分。

SpaCy
=====

　　SpaCy提供了分词（Tokenization）、词性标注（Part-Of-Speech Tagging）、依赖关系分析（Dependency Parsing）、命名实体识别（Named Entity Recognization）、停用词识别（Stop Words）、名词短语提取（Noun Chunks）等方法，以及展示句子依赖关系的可视化工具。

获取神经网络模型
--------

　　SpaCy的NLP方法主要通过神经网络实现，因此在使用前需要下载安装相应的神经网络模型。SpaCy内部并没有提供模型的下载接口，需要我们通过python调用命令下载安装。各类语言模型的介绍可以通过以下网址查看：

　　[https://github.com/explosion/spacy-models/releases](https://github.com/explosion/spacy-models/releases)

　　在SpaCy安装好后（pip直接安装），通过如下命令安装所需的神经网络模型：

python -m spacy download en\_core\_web\_lg

　　其中en\_core\_web\_lg为模型的名称：en表示英语，如zh表示中文；web表示该模型利用网页数据进行预训练，另外还有news等；lg表示下载大模型，如sm为小模型，trf则为基于transformer的模型。

基本功能
----

　　下载好语言模型后导入。然后将需要进行处理的文本输入模型中：

import spacy
model \= spacy.load("zh\_core\_web\_lg")
text \= '无法改变想法的人，什么事情也改变不了。——萧伯纳'
output \= model(text)

　　分词、词性标注、依赖关系分析、命名实体识别、停用词识别：

print('分词:    ', end='\\t')
for i in output:
  print(i.text, end='\\t')
print('\\n粗粒度词性:  ', end='\\t')
for i in output:
  print(i.pos\_, end='\\t')
print('\\n细粒度词性:  ', end='\\t')
for i in output:
  print(i.tag\_, end='\\t')
print('\\n依赖关系:', end='\\t')
for i in output:
  print(i.dep\_, end='\\t')
print('\\n命名实体识别:', end='\\t')
for i in output:
  print(i.ent\_type\_, end='\\t')
print('\\n停用词识别:', end='\\t')
for i in output:
  print(i.is\_stop, end='\\t')

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220930155943577-1340513443.png)

　　名词短语提取在中文模型中没有，英文模型中用output.noun\_chunks获取。

　　另外，可视化方法如下：

from spacy import displacy
html \= displacy.render(output, style="ent", jupyter=False) #命名实体可视化
f = open('html1.html', 'w')
f.write(html)
html \= displacy.render(output, style="dep", jupyter=False) #依赖关系可视化
f = open('html2.html', 'w')
f.write(html)

　　输出html矢量图

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220930161751987-354078555.png)

![](https://img2022.cnblogs.com/blog/1908255/202209/1908255-20220930161739789-737827408.png)

StanfordNLP
===========

　　另外还有斯坦福的库，以后要用再记录。。特性：支持中文等53种语言、内置许多NLP神经网络模型、包含大量语义分析工具。