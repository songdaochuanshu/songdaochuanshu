---
layout: post
title: "神经网络前向和后向传播推导（二）：全连接层"
date: "2022-06-07T20:20:16.904Z"
---
神经网络前向和后向传播推导（二）：全连接层
=====================

![神经网络前向和后向传播推导（二）：全连接层](https://img2022.cnblogs.com/blog/419321/202206/419321-20220607191049321-1960358126.png) 大家好~本文推导全连接层的前向传播、后向传播、更新权重和偏移的数学公式，其中包括两种全连接层：作为输出层的全连接层、作为隐藏层的全连接层

大家好~本文推导全连接层的前向传播、后向传播、更新权重和偏移的数学公式，其中包括两种全连接层：作为输出层的全连接层、作为隐藏层的全连接层。

[神经网络前向和后向传播推导（一）：前向传播和梯度下降](https://www.cnblogs.com/chaogex/p/16343664.html)  
[神经网络前向和后向传播推导（二）：全连接层](https://www.cnblogs.com/chaogex/p/16350439.html)

目录

*   [构建神经网络](#构建神经网络)
*   [推导前向传播](#推导前向传播)
*   [推导后向传播](#推导后向传播)
*   [推导权重和偏移更新](#推导权重和偏移更新)
*   [总结](#总结)
*   [参考资料](#参考资料)

构建神经网络
======

我们构建一个三层神经网络，由一层输入层+两层全连接层组成：  
![image](https://img2022.cnblogs.com/blog/419321/202206/419321-20220607190840823-1412379381.png)

输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的两个节点，编号依次为4、5；输出层的两个节点编号为6、7。因为我们这个神经网络是全连接网络，所以可以看到每个节点都和上一层的所有节点有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为\\(w\_{41}, w\_{42}, w\_{43}\\)  
（注意：权重的序号的命名规则是下一层的序号在上一层的序号之前，如为\\(w\_{41}\\)而不是\\(w\_{14}\\)）

推导前向传播
======

节点4的输出值\\(y\_4\\)的计算公式为：

\\\[y\_4=f(\\vec{w\_4}^T\\cdot\\vec{x}) \\\]

其中：

\\\[\\vec{w\_4} = \[w\_{b\_4}, w\_{41}, w\_{42}, w\_{43}\] \\\]

\\\[\\vec{x} = \\begin{bmatrix} 1 \\\\ x\_1\\\\ x\_2\\\\ x\_3\\\\ \\end{bmatrix} \\\]

\\\[f为激活函数 \\\]

**推导隐藏层的前向传播**  
我们把隐藏层的权重向量组合在一起成为矩阵，就推导出隐藏层的前向传播计算公式了：

\\\[\\overrightarrow{y\_{隐藏层}}=f(W\_{隐藏层}\\cdot\\vec{x}) \\\]

其中：

\\\[W\_{隐藏层} = \\begin{bmatrix} \\vec{w\_4} \\\\ \\vec{w\_5} \\\\ \\end{bmatrix} =\\begin{bmatrix} w\_{b\_4}, w\_{41}, w\_{42}, w\_{43} \\\\ w\_{b\_5}, w\_{51}, w\_{52}, w\_{53} \\\\ \\end{bmatrix} \\\]

\\\[\\vec{x} = \\begin{bmatrix} 1 \\\\ x\_1\\\\ x\_2\\\\ x\_3\\\\ \\end{bmatrix} \\\]

\\\[\\overrightarrow{y\_{隐藏层}}=\\begin{bmatrix} y\_4 \\\\ y\_5 \\\\ \\end{bmatrix} \\\]

**推导输出层的前向传播**  
同理，可推出输出层的前向传播计算公式：

\\\[\\overrightarrow{y\_{输出层}}=f(W\_{输出层}\\cdot\\overrightarrow{y\_{隐藏层}}) \\\]

其中：

\\\[W\_{输出层} = \\begin{bmatrix} \\vec{w\_6} \\\\ \\vec{w\_7} \\\\ \\end{bmatrix} =\\begin{bmatrix} w\_{b\_6}, w\_{64}, w\_{65} \\\\ w\_{b\_7}, w\_{74}, w\_{75} \\\\ \\end{bmatrix} \\\]

\\\[\\overrightarrow{y\_{隐藏层}} = \\begin{bmatrix} y\_4 \\\\ y\_5 \\\\ \\end{bmatrix} \\\]

\\\[\\overrightarrow{y\_{输出层}}=\\begin{bmatrix} y\_6 \\\\ y\_7 \\\\ \\end{bmatrix} \\\]

推导后向传播
======

我们先来看下输出层的梯度下降算法公式：

\\\[w\_{kj}=w\_{kj}-\\eta\\frac{dE}{dw\_{kj}} \\\]

其中：\\(k\\)是输出层的节点序号，\\(j\\)是隐藏层的节点序号，\\(w\_{kj}\\)是输出层的权重矩阵\\(W\_{输出层}\\)的权重值，\\(\\frac{dE}{dw\_{kj}}\\)是节点k的梯度

设\\(net\_k\\)函数是节点k的加权输入：

\\\[net\_k=\\overrightarrow{w\_k}^T\\cdot\\overrightarrow{y\_{隐藏层}} = \\sum\_{j} w\_{kj}y\_j \\\]

因为\\(E\\)是\\(\\overrightarrow{y\_{输出层}}\\)的函数，\\(\\overrightarrow{y\_{输出层}}\\)是\\(net\_k\\)的函数，\\(net\_k\\)是\\(w\_{kj}\\)的函数，所以根据链式求导法则，可以得到：

\\\[\\begin{aligned} \\frac{dE}{dw\_{kj}} & = \\frac{dE}{dnet\_k}\\frac{dnet\_k}{dw\_{kj}} \\\\ & = \\frac{dE}{dnet\_k}\\frac{d\\sum\_{j} w\_{kj}y\_{j}}{dw\_{kj}} \\\\ & = \\frac{dE}{dnet\_k}y\_{j} \\\\ \\end{aligned} \\\]

定义节点k的误差项\\(\\delta\_k\\)为：

\\\[\\delta\_k = \\frac{dE}{dnet\_k} \\\]

因为\\(y\_{j}\\)已知，所以只要求出\\(\\delta\_k\\)，就能计算出节点k的梯度

同理，对于隐藏层，可以得到下面的公式：

\\\[w\_{ji}=w\_{ji}-\\eta\\frac{dE}{dw\_{ji}} \\\\ \\frac{dE}{dw\_{ji}} = \\frac{dE}{dnet\_j}x\_{i} \\\\ \\delta\_j = \\frac{dE}{dnet\_j} \\\]

其中：\\(j\\)是隐藏层的节点序号，\\(i\\)是输入层的节点序号，\\(w\_{ji}\\)是隐藏层的权重矩阵\\(W\_{隐藏层}\\)的权重值，\\(\\frac{dE}{dw\_{ji}}\\)是节点j的梯度

因为\\(x\_{i}\\)已知，所以只要求出\\(\\delta\_j\\)，就能计算出节点j的梯度

**推导输出层的\\(\\delta\_k\\)**

因为节点k的输出值\\(y\_k\\)作为\\(\\overrightarrow{y\_{输出层}}\\)的一个值，并没有影响\\(\\overrightarrow{y\_{输出层}}\\)的其它值，所以节点k直接影响了\\(E\\)。也就说\\(E\\)是\\(y\_k\\)的函数，\\(y\_k\\)是\\(net\_k\\)的函数，所以根据链式求导法则，可以得到：

\\\[\\delta\_k=\\frac{dE}{dnet\_k} = \\frac{dE}{dy\_k}\\frac{dy\_k}{dnet\_k} \\\]

考虑上式的第一项：

\\\[\\frac{dE}{dy\_k} = \\frac{dE(\\overrightarrow{y\_{输出层}})}{dy\_k} \\\]

上式的第二项即为求激活函数\\(f\\)的导数：

\\\[\\frac{dy\_k}{dnet\_k} = \\frac{df(net\_k)}{dnet\_k} \\\]

将第一项和第二项带入\\(\\frac{dE}{dnet\_k}\\)，得到：

\\\[\\delta\_k = \\frac{dE(\\overrightarrow{y\_{输出层}})}{dy\_k}\\frac{df(net\_k)}{dnet\_k} \\\]

只要确定了\\(E\\)和激活函数\\(f\\)，就可以求出\\(\\delta\_k\\)  
一般来说，\\(E\\)可以为\\(softmax\\)，\\(f\\)可以为\\(relu\\)

**推导隐藏层的\\(\\delta\_j\\)**

因为节点j的输出值\\(y\_j\\)作为输出层所有节点的一个输入值，影响了\\(\\overrightarrow{y\_{输出层}}\\)的每个值，所以节点j通过输出层所有节点影响了\\(E\\)。也就说\\(E\\)是输出层所有节点的\\(net\\)的函数，每个\\(net\\)函数\\(net\_k\\)都是\\(net\_j\\)的函数，所以根据全导数公式，可以得到：

\\\[\\begin{aligned} \\delta\_j =\\frac{dE}{dnet\_j} & = \\sum\_{k\\in{输出层}} \\quad\\frac{dE}{dnet\_k}\\frac{dnet\_k}{dnet\_j} \\\\ & = \\sum\_{k\\in{输出层}} \\quad\\delta\_k\\frac{dnet\_k}{dnet\_j} \\end{aligned} \\\]

因为\\(net\_k\\)是\\(y\_{j}\\)的函数，\\(y\_{j}\\)是\\(net\_j\\)的函数，所以根据链式求导法则，可以得到：

\\\[\\begin{aligned} \\frac{dnet\_k}{dnet\_j} & = \\frac{dnet\_k}{dy\_{j}}\\frac{dy\_{j}}{dnet\_j} \\\\ & = \\frac{d\\sum\_{j} w\_{kj}y\_{j}}{dy\_{j}}\\frac{dy\_{j}}{dnet\_j} \\\\ & = w\_{kj}\\frac{dy\_{j}}{dnet\_j} \\\\ & = w\_{kj}\\frac{df(net\_j)}{dnet\_j} \\\\ \\end{aligned} \\\]

代入，得：

\\\[\\delta\_j = \\sum\_{k\\in{输出层}} \\quad\\delta\_k w\_{kj}\\frac{df(net\_j)}{dnet\_j} \\\]

只要确定了激活函数\\(f\\)和得到了每个\\(\\delta\_k\\)，就可以求出\\(\\delta\_j\\)

**后向传播算法**

通过上面的推导，得知要推导隐藏层的\\(\\delta\_j\\)，需要先得到出下一层（也就是输出层）每个节点的误差项\\(\\delta\_k\\)  
这就是反向传播算法：需要先计算输出层的误差项，然后反向依次计算每层的误差项，直到与输入层相连的层

推导权重和偏移更新
=========

经过上面的推导，可以得出输出层的更新公式为：

\\\[\\begin{aligned} w\_{kj} =w\_{kj}-\\eta\\delta\_k y\_j \\end{aligned} \\\]

隐藏层的更新公式为：

\\\[\\begin{aligned} w\_{ji} & =w\_{ji}-\\eta\\delta\_j x\_i \\end{aligned} \\\]

总结
==

我们在推导隐藏层的误差项时，应用了全导数公式，这是一个难点

参考资料
====

[零基础入门深度学习 | 第三章：神经网络和反向传播算法](https://cloud.tencent.com/developer/article/1056167?from=10680)

欢迎来到Wonder~

扫码加入我的QQ群：

![](https://img2020.cnblogs.com/blog/419321/202012/419321-20201228104448953-1235302601.png)

扫码加入免费知识星球-YYC的Web3D旅程：

![](https://img2018.cnblogs.com/blog/419321/201912/419321-20191203125111510-1737718475.png)