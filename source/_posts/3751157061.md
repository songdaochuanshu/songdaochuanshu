---
layout: post
title: "TCNä»£ç è¯¦è§£-Torch (è¯¯å¯¼çº æ­£)"
date: "2022-11-18T21:16:10.468Z"
---
TCNä»£ç è¯¦è§£-Torch (è¯¯å¯¼çº æ­£)
====================

è¯¦ç»†è§£é‡ŠTCNç»“æ„ï¼Œå›¾è§£ä¸ä»£ç è§£é‡Š

TCNä»£ç è¯¦è§£-Torch (è¯¯å¯¼çº æ­£)
====================

1\. ç»ªè®º
------

TCNç½‘ç»œç”±Shaojie Baiï¼Œ J. Zico Kolterï¼Œ Vladlen Koltun ä¸‰äººäº2018æå‡ºã€‚å¯¹äºåºåˆ—é¢„æµ‹è€Œè¨€ï¼Œé€šå¸¸è€ƒè™‘å¾ªç¯ç¥ç»ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚RNNã€LSTMã€GRUç­‰ã€‚ä»–ä»¬ä¸‰ä¸ªäººçš„ç ”ç©¶å»ºè®®æˆ‘ä»¬ï¼Œå¯¹äºæŸäº›åºåˆ—é¢„æµ‹ï¼ˆéŸ³é¢‘åˆæˆã€å­—çº§è¯­è¨€å»ºæ¨¡å’Œæœºå™¨ç¿»è¯‘ï¼‰ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨å·ç§¯ç½‘ç»œç»“æ„ã€‚

å…³äºTCNåŸºæœ¬æ„æˆå’Œä»–ä»¬çš„åŸç†æœ‰ç›¸å½“å¤šçš„åšå®¢å·²ç»è§£é‡Šçš„å¾ˆè¯¦ç»†çš„äº†ã€‚æ€»ç»“ä¸€å¥è¯ï¼šTCN = 1D FCN + å› æœå·ç§¯ã€‚ä¸‹é¢çš„åšå®¢å¯¹å› æœå·ç§¯å’Œå­”æ´å·ç§¯æœ‰è¯¦ç»†çš„è§£é‡Šã€‚

*   [æ—¶é—´å·ç§¯ç½‘ç»œ(TCN)ï¼šç»“æ„+pytorchä»£ç ](https://blog.csdn.net/Leon_winter/article/details/100124146)
*   [TCNè®ºæ–‡åŠä»£ç è§£è¯»æ€»ç»“](https://blog.csdn.net/qq_33331451/article/details/104810419)
*   [æ—¶é—´åºåˆ—åˆ†æï¼ˆ5ï¼‰ TCN](https://zhuanlan.zhihu.com/p/69919158)

ä½†æ˜¯ï¼ŒåŒ…æ‹¬TCNåŸæ–‡ä½œè€…ï¼Œä¸Šé¢è¿™äº›åšå®¢å¯¹TCNç½‘ç»œç»“æ„çš„é˜é‡Šæ— ä¸€ä¾‹å¤–éƒ½æ˜¯ä½¿ç”¨ä¸‹é¢è¿™å¼ å›¾ç‰‡ã€‚è€Œé—®é¢˜åœ¨äºï¼Œå¦‚æœ**ä¸ç†Ÿæ‚‰Torchæ“ä½œ**å’Œ**åŸºæœ¬çš„å·ç§¯ç½‘ç»œæ“ä½œ**ï¼Œè¿™å¼ å›¾ç‰‡å…·æœ‰å¾ˆå¤§çš„è¯¯å¯¼æ€§ã€‚  
![image](https://img2022.cnblogs.com/blog/2605793/202211/2605793-20221118152555866-949309674.png)  
å›¾1 è†¨èƒ€å› æœå·ç§¯(è†¨èƒ€å› å­d = 1,2,4ï¼Œæ»¤æ³¢å™¨å¤§å°k = 3)

ç»“åˆä¸Šå›¾å’Œä¸Šé¢åˆ—ä¸¾çš„åšå®¢ï¼Œæˆ‘ä»¬å¯ä»¥å¤§è‡´ç†è§£åˆ°ï¼ŒTCNå°±æ˜¯åœ¨åºåˆ—ä¸Šä½¿ç”¨ä¸€ç»´å·ç§¯æ ¸ï¼Œæ²¿ç€æ—¶é—´æ–¹å‘ï¼ŒæŒ‰ç…§ç©ºæ´å·ç§¯çš„æ–¹å¼ï¼Œä¾æ¬¡è®¡ç®—ã€‚  
ä¾‹å¦‚ï¼Œä¸Šå›¾ä¸­ï¼Œ

1.  ç¬¬ä¸€ä¸ªhiddenå±‚æ˜¯ç”± \\(d=1\\) çš„ç©ºæ´å·ç§¯ï¼Œå·ç§¯è€Œæ¥ï¼Œé€€åŒ–ä¸ºåŸºæœ¬çš„ä¸€ç»´å·ç§¯æ“ä½œï¼›
2.  ç¬¬äºŒä¸ªhiddenå±‚æ˜¯ç”± \\(d=2\\) çš„ç©ºæ´å·ç§¯ï¼Œå·ç§¯è€Œæ¥ï¼Œå·ç§¯æ¯ä¸ªå€¼æ—¶éš”å¼€äº†ä¸€ä¸ªå€¼ï¼›
3.  ç¬¬äºŒä¸ªhiddenå±‚æ˜¯ç”± \\(d=4\\) çš„ç©ºæ´å·ç§¯ï¼Œå·ç§¯è€Œæ¥ï¼Œå·ç§¯æ¯ä¸ªå€¼æ—¶éš”å¼€äº†ä¸‰ä¸ªå€¼ï¼›

ç”±æ­¤ï¼Œä¸Šå›¾ä¸­ç½‘ç»œæ·±åº¦ä¸º3ï¼Œæ¯ä¸€å±‚æœ‰1ä¸ªå·ç§¯æ“ä½œã€‚

å¦‚æœä½ ä¹Ÿæ˜¯è¿™ä¹ˆç†è§£ï¼Œæ­å–œä½ ï¼ŒæˆåŠŸçš„è¢«æˆ‘å¸¦è·‘åäº†ğŸ˜ˆã€‚

2\. TCNç»“æ„å†æ¬¡å›¾è§£
-------------

ä¸Šå›¾ä¸­ç½‘ç»œæ·±åº¦ç¡®å®ä¸º3ï¼Œä½†æ˜¯æ¯ä¸€å±‚å¹¶ä¸æ˜¯åªæœ‰1ä¸ªå·ç§¯æ“ä½œã€‚è¿™æ—¶å€™å°±è¦æ‹¿å‡ºåŸè®ºæ–‡ä¸­ç¬¬2ä¸ªå›¾äº†ã€‚  
![image](https://img2022.cnblogs.com/blog/2605793/202211/2605793-20221118152641079-795752770.png)

å›¾2 TCNæ ¸å¿ƒç»“æ„

è¿™å¼ å›¾å·¦è¾¹å±•ç¤ºäº†TCNç»“æ„çš„æ ¸å¿ƒï¼Œå·ç§¯+æ®‹å·®ï¼Œä½œè€…æŠŠå®ƒå‘½åä¸ºResidual blockã€‚æˆ‘è¿™é‡Œç®€ç§°ä¸ºblockã€‚  
å¯ä»¥å‘ç°ä¸€ä¸ªblockæœ‰ä¸¤ä¸ªå·ç§¯æ“ä½œå’Œä¸€ä¸ªæ®‹å·®æ“ä½œã€‚å› æ­¤ï¼Œå›¾1ä¸­æ¯åˆ°ä¸‹ä¸€å±‚ï¼Œéƒ½ä¼šæœ‰**ä¸¤ä¸ªå·ç§¯æ“ä½œå’Œä¸€ä¸ªæ®‹å·®æ“ä½œï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªå·ç§¯æ“ä½œ**ã€‚å†æ¬¡æé†’ï¼Œ_å½“ \\(d=1\\) æ—¶ï¼Œç©ºæ´å·ç§¯é€€åŒ–ä¸ºæ™®é€šçš„å·ç§¯_ï¼Œæ­£å¦‚å›¾2å³å›¾å±•ç¤ºçš„ã€‚

å› æ­¤ï¼Œå¯¹äºå›¾1ä¸­ç”±åŸå§‹åºåˆ—åˆ°ç¬¬ä¸€å±‚hiddençš„çœŸå®ç»“æ„ä¸ºï¼š  
![image](https://img2022.cnblogs.com/blog/2605793/202211/2605793-20221118152655326-840247642.png)

3\. ç»“åˆåŸæ–‡çš„torchä»£ç è§£é‡Š
------------------

å¾ˆå¤šåšå®¢å†æºä»£ç è§£é‡Šæ—¶ï¼ŒåŸºæœ¬éƒ½æ˜¯ä¸€ä¸ªæ¨¡å­ï¼Œæ²¡æœ‰çœŸæ­£è§£é‡Šå…³é”®å‚æ•°çš„å«ä¹‰ï¼Œä»¥åŠä»–ä»¬å¦‚ä½•é€šè¿‡torchçš„tensorä½œç”¨çš„ã€‚

é¢„äº†è§£TCNç»“æ„ï¼Œé¡»æ˜ç™½åŸè®ºæ–‡ä¸­ä½œè€…æè¿°çš„è¿™æ ·ä¸€å¥è¯ï¼š

> Since a TCNâ€™s receptive field depends on the network depth n as well as filter size k and dilation factor d, stabilization of deeper and larger TCNs becomes important.

ç¿»è¯‘æ˜¯ï¼š

> ç”±äºTCNçš„æ„Ÿå—é‡ä¾èµ–äº**ç½‘ç»œæ·±åº¦n**ã€**æ»¤æ³¢å™¨å¤§å°k**å’Œ**æ‰©å¼ å› å­d**ï¼Œå› æ­¤æ›´å¤§æ›´æ·±çš„TCNçš„ç¨³å®šå˜å¾—å¾ˆé‡è¦ã€‚

ä¸‹é¢ç»“åˆä½œè€…æºä»£ç ï¼Œå¯¹è¿™ä¸‰ä¸ªå‚æ•°è§£é‡Šã€‚

### 3.1 TemporalConvNet

ç½‘ç»œæ·±åº¦nå°±æ˜¯æœ‰å¤šå°‘ä¸ªblockï¼Œååº”åˆ°æºä»£ç çš„å˜é‡ä¸º_**num\_channels**_çš„é•¿åº¦ï¼Œå³ \\(len(num\_channels)\\)ã€‚

    class TemporalConvNet(nn.Module):
        def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):
            super(TemporalConvNet, self).__init__()
            """
            :param num_inputs: int,  è¾“å…¥é€šé“æ•°æˆ–è€…ç‰¹å¾æ•°
            :param num_channels: list, æ¯å±‚çš„hidden_channelæ•°. ä¾‹å¦‚[5,12,3], ä»£è¡¨æœ‰3ä¸ªblock, 
                                    block1çš„è¾“å‡ºchannelæ•°é‡ä¸º5; 
                                    block2çš„è¾“å‡ºchannelæ•°é‡ä¸º12;
                                    block3çš„è¾“å‡ºchannelæ•°é‡ä¸º3.
            :param kernel_size: int, å·ç§¯æ ¸å°ºå¯¸
            :param dropout: float, drop_outæ¯”ç‡
            """
            layers = []
            num_levels = len(num_channels)
    		# å¯è§ï¼Œå¦‚æœnum_channels=[5,12,3]ï¼Œé‚£ä¹ˆ
    		# block1çš„dilation_size=1
    		# block2çš„dilation_size=2
    		# block3çš„dilation_size=4
            for i in range(num_levels):
                dilation_size = 2 ** i
                in_channels = num_inputs if i == 0 else num_channels[i-1]
                out_channels = num_channels[i]
                layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,
                                         padding=(kernel_size-1) * dilation_size, dropout=dropout)]
    
            self.network = nn.Sequential(*layers)
    
        def forward(self, x):
            return self.network(x)
    

### 3.2 TemporalBlock

å‚æ•°dilationçš„è§£é‡Šï¼Œç»“åˆä¸Šé¢å’Œä¸‹é¢çš„ä»£ç ã€‚

    class TemporalBlock(nn.Module):
        def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
            super(TemporalBlock, self).__init__()
            """
            æ„æˆTCNçš„æ ¸å¿ƒBlock, åŸä½œè€…åœ¨å›¾ä¸­æˆä¸ºResidual block, æ˜¯å› ä¸ºå®ƒå­˜åœ¨æ®‹å·®è¿æ¥.
            ä½†æ³¨æ„, è¿™ä¸ªæ¨¡å—åŒ…å«äº†2ä¸ªConv1d.
    
            :param n_inputs: int, è¾“å…¥é€šé“æ•°æˆ–è€…ç‰¹å¾æ•°
            :param n_outputs: int, è¾“å‡ºé€šé“æ•°æˆ–è€…ç‰¹å¾æ•°
            :param kernel_size: int, å·ç§¯æ ¸å°ºå¯¸
            :param stride: int, æ­¥é•¿, åœ¨TCNå›ºå®šä¸º1
            :param dilation: int, è†¨èƒ€ç³»æ•°. ä¸è¿™ä¸ªResidual block(æˆ–è€…è¯´, éšè—å±‚)æ‰€åœ¨çš„å±‚æ•°æœ‰å…³ç³». 
                                    ä¾‹å¦‚, å¦‚æœè¿™ä¸ªResidual blockåœ¨ç¬¬1å±‚, dilation = 2**0 = 1;
                                          å¦‚æœè¿™ä¸ªResidual blockåœ¨ç¬¬2å±‚, dilation = 2**1 = 2;
                                          å¦‚æœè¿™ä¸ªResidual blockåœ¨ç¬¬3å±‚, dilation = 2**2 = 4;
                                          å¦‚æœè¿™ä¸ªResidual blockåœ¨ç¬¬4å±‚, dilation = 2**3 = 8 ......
            :param padding: int, å¡«å……ç³»æ•°. ä¸kernel_sizeå’Œdilationæœ‰å…³. 
            :param dropout: float, dropoutæ¯”ç‡
            """
            self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,
                                               stride=stride, padding=padding, dilation=dilation))
    
            # å› ä¸º padding çš„æ—¶å€™, åœ¨åºåˆ—çš„å·¦è¾¹å’Œå³è¾¹éƒ½æœ‰å¡«å……, æ‰€ä»¥è¦è£å‰ª
            self.chomp1 = Chomp1d(padding)
            self.relu1 = nn.ReLU()
            self.dropout1 = nn.Dropout(dropout)
    
            self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,
                                               stride=stride, padding=padding, dilation=dilation))
            self.chomp2 = Chomp1d(padding)
            self.relu2 = nn.ReLU()
            self.dropout2 = nn.Dropout(dropout)
    
            self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,
                                     self.conv2, self.chomp2, self.relu2, self.dropout2)
    
            # 1Ã—1çš„å·ç§¯. åªæœ‰åœ¨è¿›å…¥Residual blockçš„é€šé“æ•°ä¸å‡ºResidual blockçš„é€šé“æ•°ä¸ä¸€æ ·æ—¶ä½¿ç”¨.
            # ä¸€èˆ¬éƒ½ä¼šä¸ä¸€æ ·, é™¤énum_channelsè¿™ä¸ªé‡Œé¢çš„æ•°, ä¸num_inputsç›¸ç­‰. ä¾‹å¦‚[5,5,5], å¹¶ä¸”num_inputsä¹Ÿæ˜¯5
            self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
    
            # åœ¨æ•´ä¸ªResidual blockä¸­æœ‰éçº¿æ€§çš„æ¿€æ´». è¿™ä¸ªå®¹æ˜“å¿½ç•¥!
            self.relu = nn.ReLU()
            self.init_weights()
    
        def init_weights(self):
            self.conv1.weight.data.normal_(0, 0.01)
            self.conv2.weight.data.normal_(0, 0.01)
            if self.downsample is not None:
                self.downsample.weight.data.normal_(0, 0.01)
    
        def forward(self, x):
            out = self.net(x)
            res = x if self.downsample is None else self.downsample(x)
            return self.relu(out + res)
    

### 3.3 Chomp1d

è£å‰ªæ¨¡å—ã€‚è¿™é‡Œæ³¨æ„ï¼Œpaddingçš„æ—¶å€™å¯¹æ•°æ®åˆ—é¦–å°¾éƒ½æ·»åŠ äº†ï¼Œtorch[å®˜æ–¹è§£é‡Š](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)å¦‚ä¸‹ï¼š

> padding controls the amount of padding applied to the input. It can be either a string {â€˜validâ€™, â€˜sameâ€™} or a tuple of ints giving the amount of implicit padding applied on **both sides**.

æ³¨æ„è¿™é‡Œæ˜¯**both sides**ã€‚ä¾‹å¦‚ï¼Œè¿˜æ˜¯ä¸Šè¿°ä»£ç ä¸­çš„ä¾‹å­ï¼Œkernel\_size = 3ï¼Œåœ¨ç¬¬ä¸€å±‚(å¯¹äºç¬¬ä¸€ä¸ªblock)ï¼Œpadding = 2ã€‚å¯¹äºé•¿åº¦ä¸º20çš„åºåˆ—ï¼Œå…ˆpaddingï¼Œé•¿åº¦ä¸º\\(20+2\\times2=24\\)ï¼Œå†å·ç§¯ï¼Œé•¿åº¦ä¸º\\((24-3)+1=22\\)ã€‚æ‰€ä»¥è¦è£æ‰ï¼Œä¿è¯è¾“å‡ºåºåˆ—ä¸è¾“å…¥åºåˆ—ç›¸ç­‰ã€‚

    class Chomp1d(nn.Module):
        def __init__(self, chomp_size):
            super(Chomp1d, self).__init__()
            self.chomp_size = chomp_size
    
        def forward(self, x):
            return x[:, :, :-self.chomp_size].contiguous()
    

4\. éªŒè¯TCNçš„è¾“å…¥è¾“å‡º
==============

æ ¹æ®ä¸Šè¿°ä»£ç çš„è§£é‡Šå’Œç†è§£ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿çš„éªŒè¯å…¶è¾“å…¥å’Œè¾“å‡ºã€‚

    # è¾“å…¥27ä¸ªé€šé“ï¼Œæˆ–è€…ç‰¹å¾
    # æ„å»º1å±‚çš„TCNï¼Œæœ€åè¾“å‡ºä¸€ä¸ªé€šé“ï¼Œæˆ–è€…ç‰¹å¾
    model2 = TemporalConvNet(num_inputs=27, num_channels=[32,16,4,1], kernel_size=3, dropout=0.3)
    
    import torch
    
    # æ£€æµ‹è¾“å‡º
    with torch.no_grad():
    	# æ¨¡å‹è¾“å…¥ä¸€å®šæ˜¯ ï¼ˆbatch_size, channels, lengthï¼‰
        model2.eval() 
        print(model2(torch.randn(16,27,20)).shape) 
    

æ‰“å°ç»“æœä¸º(16, 1, 20) ã€‚é€šé“æ•°é™ä¸º1ã€‚è¾“å…¥åºåˆ—é•¿åº¦20ï¼Œ è¾“å‡ºåºåˆ—é•¿åº¦ä¹Ÿæ˜¯20ã€‚

ä½œè€…ï¼š[Aidan](https://www.cnblogs.com/AidanLee/)

å‡ºå¤„ï¼š[http://www.cnblogs.com/AidanLee/](https://www.cnblogs.com/AidanLee/)

\-------------------------------------------

ä¸ªæ€§ç­¾åï¼šç‹¬å­¦è€Œæ— å‹ï¼Œåˆ™å­¤é™‹è€Œå¯¡é—»ã€‚åšä¸€ä¸ªçµé­‚æœ‰è¶£çš„äººï¼

å¦‚æœè§‰å¾—è¿™ç¯‡æ–‡ç« å¯¹ä½ æœ‰å°å°çš„å¸®åŠ©çš„è¯ï¼Œè®°å¾—åœ¨å³ä¸‹è§’ç‚¹ä¸ªâ€œæ¨èâ€å“¦ï¼Œåšä¸»åœ¨æ­¤æ„Ÿè°¢ï¼

ä¸‡æ°´åƒå±±æ€»æ˜¯æƒ…ï¼Œæ‰“èµä¸€åˆ†è¡Œä¸è¡Œï¼Œæ‰€ä»¥å¦‚æœä½ å¿ƒæƒ…è¿˜æ¯”è¾ƒé«˜å…´ï¼Œä¹Ÿæ˜¯å¯ä»¥æ‰«ç æ‰“èµåšä¸»ï¼Œå“ˆå“ˆå“ˆ(ã£â€¢Ì€Ï‰â€¢Ì)ã£âœâ¾â¾ï¼