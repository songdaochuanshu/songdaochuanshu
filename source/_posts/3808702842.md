---
layout: post
title: "强化学习-学习笔记13 | 多智能体强化学习"
date: "2022-07-10T11:14:29.936Z"
---
强化学习-学习笔记13 | 多智能体强化学习
======================

![强化学习-学习笔记13 | 多智能体强化学习](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220710003248937-326938437.png) 这一篇介绍重头戏：多智能体强化学习。多智能体要比之前的单智能体复杂很多。但也更有意思。

这一篇介绍重头戏：多智能体强化学习。多智能体要比之前的单智能体复杂很多。但也更有意思。

13\. Multi-Agent-Reiforcement-Learning
--------------------------------------

### 13.1 多智能体关系设定

1.  合作关系 Full Cooperative Setting
2.  竞争关系 Full Competitive Setting
3.  合作和竞争的混合 Mixed Cooperative & Competitive
4.  利己主义 Self-Interested

#### a. 完全合作关系

*   agents 的利益一致，合作去获取共同的回报；
*   如工业机器人共同装配；

#### b. 完全竞争关系

*   一个 agent 的收益是另一个 agent 的损失；
*   如机器人搏斗，零和博弈；

#### c. 合作竞争混合

*   既有合作，也有竞争；
*   如足球机器人；

#### d. 利己主义

*   每个 agent 只考虑最大化自身利益，不关心别人的利益；
*   比如股票的自动交易；

### 13.2 专业术语

下面在多智能体的背景下更新一下先前的概念：

#### a. state / action / state transition

*   假设系统中有 n 个 agents，S 表示状态，用 \\(A^i\\) 表示 第 i 个agent 的动作
    
*   状态转移函数 : \\(p(s'|s,a^1,...,a^n)=\\mathbb{p}(S'=s'|S'= s',A^1=a^1,...,A^n = a^n)\\)
    
    这个函数是隐藏的，即只有环境知道，而人不知道。
    
*   多智能体问题的难点就是，下一状态 S' 会受到所有 agents 的 动作的影响。
    

#### b. Rewards

*   有 n 个 agents ，每一轮就会有 n 个奖励，用 \\(R^i\\) 表示第 i 个 agent 的奖励；
*   在合作的情境下：每个 agent 的奖励都相等；
*   在竞争的情境下：一个 agent 的奖励是另一个 agent 奖励的损失：\\(R^1 \\propto -R^2\\)
*   agent 获得的奖励不仅仅取决于本身的动作，还取决于 其他 agent 的动作；

#### c. Returns

*   用 \\(R^i\_t\\) 来表示第 i 个 agent 在 t 时刻获得的奖励；
    
*   则 第 i 个 agent 在时刻 t 的回报 Return 表示为：
    
    \\(U\_t^i=R\_t^i+R^i\_{t+1}+...+...\\)
    
*   折扣回报是加权和：
    
    \\(U\_t^i = R^i\_t+\\gamma \\cdot R\_{t+1}^i +\\gamma^2\\cdot R\_{t+2}^i + ...\\)
    

#### d. Policy Network 策略网络

*   用神经网络近似策略函数 \\(pi\\)；
*   第 i 个 agent 的策略网络记为： \\(\\pi(a^i|s;\\theta^i)\\)，所有网络结构可以相同；
*   在一些情况下，不同agent的网络参数可能一样，因为它们彼此是可以替换的；
*   在更多场景中，策略网络不能互换，不同 agent 的功能不同。

#### e. 奖励和回报 随机性的来源

*   奖励 \\(R\_t^i\\) 依赖于当前状态 \\(S\_t\\) 和 所有 agent 当前的动作 \\(A\_t^1,A\_t^2,...,A\_t^n\\)
*   状态的随机性来自于状态转移函数 p；
*   \\(A\_t^i\\) 的随机性 来自于 策略函数，具体问题中是策略网络 \\(\\pi\\)；
*   回报 \\(U\_t^i\\) 依赖于t 时刻开始所有的 奖励：
    *   未来所有的状态 \\({S\_t,S\_{t+1},...}\\)
    *   未来所有的动作 \\(A\_t^i,A\_{t+1}^i,A\_{t+2}^i,...\\)

#### f. 状态价值函数

*   第 i 个 agent 的状态价值函数 是:\\(V^i(s\_t;\\theta^1,...,\\theta^n)=\\mathbb{E}\[U^i\_t | S\_t=s\_t\]\\)
    
*   对 \\(U\_t\\) 求期望后，就消除掉了未来的状态以及所有 agent 的动作，这样 \\(V^i\\) **只依赖于** 当前状态 \\(s\_t\\)
    
*   动作 \\(A^j\_t\\) 是随机的，根据策略函数 \\(\\pi\\) 来随机抽样选择：\\(A^j\_t\\sim\\pi(\\cdot | s\_t;\\theta^j)\\)；
    
*   所以第 j 个策略网络会影响状态价值函数 \\(V^i\\)
    
    > 解释：
    > 
    > *   道理其实很好理解，但上面那么说可能有点绕；意思是，第 i 个 agent 的状态价值函数 \\(V^i\\) 依赖于所有 agent 的 策略函数；
    > *   因为agent 并不独立嘛，很好理解。如果一个 agent 的策略发生了变化，那么所有的 agent 的状态价值函数都会发生变化；
    

### 13.3 多智能体策略学习的收敛问题

Convergence , 收敛.

*   收敛的意思是，无法通过改变策略，来获得更大的价值回报；如果所有的 agent 都找不到更好的策略，说明已经收敛，可以停止训练；

#### a. 单智能体的策略学习

*   单智能体的策略网络只有一个：\\(\\pi(a|s;\\theta)\\)
    
*   状态价值函数：\\(V(s;\\theta)\\)；
    
*   对 V 关于状态 s 求期望，得到目标函数： \\(J(\\theta)=E\_s\[V(s;\\theta)\]\\)
    
    消掉了 状态 s；因为只依赖于 θ，所以可以用于评价策略好坏， J 越大，则说明策略越好。
    
*   策略网络的**参数学习方式**为最大化目标函数 J：$ \\max\\limits\_{\\theta} J(\\theta)$
    
    > 具体参见：[策略学习](https://www.cnblogs.com/Roboduster/p/16445811.html)
    
*   策略网络的**收敛条件**为**目标函数不再增加**。
    

#### b. 多智能体的策略学习

如果有多个 agents，判断收敛的条件就是 **纳什均衡**。

> 纳什均衡：
> 
> 当其他 agents 都不改变策略时，一个 agent 改变策略，无法让自己获得更高的回报。
> 
> 解释：
> 
> *   一个 agent 制定策略时，需要考虑其他 agents 的策略，在达到纳什均衡的状态下，**每个**agent 都在以最优的动作应对其他各方的策略；
> *   如果所有的 agents 都是理性的，在达到纳什均衡时，没有理由改变改变自己的策略，因为改变不会再增加自己的收益；
> *   这达到了一种平衡，收敛了。

在多智能体问题上直接应用单智能体的 算法 并不好，可能会不收敛，原因：

#### c. 在 m-agents 问题上应用 s-agent 方法

![image](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220710002927529-949015563.png)

系统中有 n 个 agent，假设独立和环境交互，即每个 agent 都可以独立观测到 环境的状态 s、接收环境给的奖励 ri，进而计算 ai并执行；

接着用策略梯度算法更新各自的策略网络，就相当于 n 个 agents 的策略学习的叠加，并且彼此之间没有直接联系。

下面说明一下这种思路为什么不行：

*   假设第 i 个智能体的策略网络为： \\(\\pi(a^i|s;\\theta^i)\\)
    
*   第 i 个智能体的状态价值函数为：\\(V(s;\\theta^1,...,\\theta^n)\\)
    
*   目标函数为：\\(J(\\theta^1,\\theta^2,...,\\theta^n)=\\mathbb{E}\[V(s;\\theta^1,\\theta^2,...,\\theta^n)\]\\)
    
*   当 \\(agent^i\\) 要提高自己的回报，即学习第 i 个策略网络的参数，就是最大化目标函数：\\(\\max\\limits\_{\\theta^i}J^i(\\theta^1,..,\\theta^n)\\)
    
    注意这里的目标函数 \\(\\max\\limits\_{\\theta^i}J^i(\\theta^1,..,\\theta^n)\\)，对于每个 agent 都不相同。
    
*   当一个智能体通过策略学习更新了策略，会通过环境影响其他智能体的目标函数，**这样整体的策略学习可能永远无法收敛**；
    
    > 假设 第 i 个智能体找到了最优策略：\\(\\theta^i\_\*J^i(\\theta^1,\\theta^2,...,\\theta^n)\\)
    > 
    > 其余 agent 改变自己的策略时，第 i 个智能体的最优策略就又改变了。
    

即，每个 agent 都不是独立的，每个 agent 都影响了下一个状态，下一个状态的改变反过来又改变了 agent 的策略。

那么我们应当如何处理多智能体的强化学习呢？

### 13.4 多智能体学习方法

因为 agents 之间会互相影响，所以最好在 agents 之间做**通信**来共享信息，而 agents 之间的**通信方式**主要分为 **中心式** 和 **去中心式**。

#### a. 去中心化

Fully decentralized. 即 agents 都是独立的个体，每个 agent 独立与系统交互，用自己观测到的状态和奖励更新自己的策略；彼此之间不交流，不知道别人的动作。

**13.3** 中已经介绍了这种方式的不足。

#### b. 中心化

所有 agent 都把信息传给中央控制器，中央控制器收集所有的状态和奖励，由中央统一做出决策，agent 自己不做决策，即 **定于一尊**。

#### c. 中心化训练 & 去中心化执行

这种方式 agents 各有各的策略网络；而训练时有中央控制器，中央统一收集信息帮助 agents 训练，训练结束后就由各自的策略网络作决策，不再需要中央控制器。

下面以比较常用的 Actor-Critic 来介绍多智能体强化学习的实现细节。

### 13.5 不完全观测

Partial Observation.在多智能体强化学习中，通常假设智能体是不完全观测的，即只能观测到局部状态，不能观测到全局的状态。

*   把 \\(agent^i\\) 观测到的状态记为 \\(o^i\\)，在不完全观测时， \\(o^i\\neq s\\)
*   完全观测时，\\(o^1=...=o^n=s\\)

### 13.6 完全去中心化

每个 agent 独立与环境进行交互，独立训练自己的策略网络，跟之前的单智能体强化学习基本相同，训练结束后，每个 agent 用自己的策略网络来作决策。把观测到的 \\(o^i\\) 输入，输出动作的概率分布，抽样得到动作并执行 \\(a^i\\)。

![image](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220710002959003-666539951.png)

> 本质还是单智能体强化学习，而不是多智能体强化学习；

**如何用去中心化实现 Actor-Critic** ？

*   每个 agent 上都搭载了计算设备，如 CPU、GPU；
*   在每个 agent 上都部署策略网络 Actor \\(\\pi(a^i|o^i;\\theta^i)\\) 和价值网络 Critic \\(q(o^i,a^i;w^i)\\) ；训练思路与此前的 [Actor-Critic](https://www.cnblogs.com/Roboduster/p/16448038.html) 相同。
*   agent 独立运行，不做通信;
*   但 agents 之间的联系不能忽略，这样做效果不好。

### 13.7 完全中心化

*   n 个 agents 与环境交互，将观测到的 状态和奖励 都上报给 中央控制器，由中央的策略网络来作决策，中央把决策发给每个 agent 。
    
*   agents上面没有策略网络，不能自己作决策，只听中央控制器的。
    

![image](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220710003010633-1198984834.png)

*   训练也是在中央控制器进行，用**所有观测到的的状态和奖励**来训练策略网络。
    
*   执行时，也需要中央控制器训练出的 n 个策略网络，网络结构相同，具体参数可能不同。
    
    > 由于输入需要时所有的状态，所以策略网络不能部署到具体的agent，只能放在中央控制器。
    

**如何用中心化实现 Actor-Critic** ？

*   中央控制器接收到 所有的动作 \\(a\\) 和 状态 \\(o\\)，以及所有的奖励；
*   中央控制器上有 n 个策略网络和 n 个价值网络，对应 n 个agents；
    *   策略网络 \\(\\pi(a^i|o\_{all};\\theta^i)\\)，输入是所有的观测值 o，输出是动作概率值；通过概率抽样执行动作；
    *   价值网络 \\(q(o,a;w^i)\\)，评价对应的策略网络的决策好坏；
*   用策略梯度算法来训练策略网络；
*   用 TD算法 训练 价值网络；
*   结束训练后，中央控制器用策略网络来作决策：
    *   agents 上报 状态\\(o\_{all}\\)，中央输入 策略网络，抽样得到动作 \\(a^i\\)
    *   把 \\(a^i\\) 传达到 第 i 个 agent，命令其执行；

中心化的好处是收集全局的信息，可以面向所有 agents 做出好的决策。但**缺点主要在于执行速度慢，无法做到实时决策**。

### 13.8 中心化训练 & 去中心化执行

*   每个 agent 都有策略网络，训练的时候使用 **13.7** 中心化训练的方式，执行时不需要中央控制器，用自己的策略网络，基于局部观测来做出决策。
*   这个方式目前比较流行，模型也有很多种，下面介绍一种 Actor-Critic 方法：
    *   参考文献：
        1.  [Multi-agent actor-critic for mixed cooperative-competitive environments](https://proceedings.neurips.cc/paper/2017/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html)
        2.  [Counterfactual multi-agent policy gradients](https://ojs.aaai.org/index.php/AAAI/article/view/11794)
    *   每个agent上都布置自己的策略网络 \\(\\pi(a^i|o^i;\\theta^i)\\)，输入是agent自己的局部观测 状态 \\(o^i\\) ,不依赖其他 agents。
    *   中央控制器上有 n 个价值网络 \\(q(o,a;w^i)\\)，对对应的策略网络进行评价，帮助训练策略网络；输入是所有的动作和状态；价值网络的结构相同，但是参数不同；
    *   训练时中央控制器收集所有的观测、动作和奖励；
    *   完成训练，每个 agent 独立作决策。

![image](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220710003028226-1018785795.png)

**训练方式：**

*   中央控制器上训练的价值网络，使用TD算法进行更新，输入：
    
    *   \\(a=\[a^1,a^2,...,a^n\]\\)
    *   \\(o=\[o^1,o^2,...,o^n\]\\)
    *   注意，只需一个奖励 \\(r^i\\)
    
    输出用TD算法拟合 TD target，即为 \\(q^i\\).
    
*   agent 端训练的策略网络在中央控制器的价值网络提供的 q 下进行训练；输入为：
    
    *   \\(a^i\\)、\\(o^i\\)、\\(q^i\\)
    *   不需要其他 agents 的信息。
    
    用策略梯度算法 更新 \\(\\theta\_i\\)；
    

**执行过程**：

![image](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220710003038886-1702112682.png)

不再需要中央控制器，只基于各自的局部观测与策略网络来做出决策。

### 13.9 参数共享

在本文举例的 Actor-Critic 中，有：

*   n个策略网络：\\(\\pi(a^i|o^i;\\theta^i)\\)
*   n个价值网络：\\(q(o,a;w^i)\\)
*   训练的参数是 \\(\\theta,w\\)
*   第 i 和第 j 两个神经网络，共享参数的意思是，\\(\\theta^i=\\theta^j,w^i=w^j\\)
*   是否共享参数取决于情境
    *   功能不同的 agents 之间不能共享：如足球机器人
    *   功能相同的 agents 之间可以共享：如无人车

### 13.10 总结

学习方式

策略网络（actor）

价值网络（critic）

完全去中心化

\\(\\pi(a^i,o^i;\\theta^i)\\)

\\(q(o^i,a^i;w^i)\\)

完全中心化

\\(\\pi(a^i,o;\\theta^i)\\)

\\(q(o,a;w^i)\\)

中心化训练 & 去中心化执行

\\(\\pi(a^i,o^i;\\theta^i)\\)

\\(q(o,a;w^i)\\)

> 看似一样，不同的是全局与局部。

x. 参考教程
-------

*   视频课程：[深度强化学习（全）\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1rv41167yx)
*   视频原地址：[https://www.youtube.com/user/wsszju](https://www.youtube.com/user/wsszju)
*   课件地址：[https://github.com/wangshusen/DeepLearning](https://github.com/wangshusen/DeepLearning)