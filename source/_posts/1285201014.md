---
layout: post
title: "çˆ¬è™«ï¼ˆ9ï¼‰ - Scrapyæ¡†æ¶(1) | Scrapy å¼‚æ­¥ç½‘ç»œçˆ¬è™«æ¡†æ¶"
date: "2022-07-05T09:18:09.345Z"
---
çˆ¬è™«ï¼ˆ9ï¼‰ - Scrapyæ¡†æ¶(1) | Scrapy å¼‚æ­¥ç½‘ç»œçˆ¬è™«æ¡†æ¶
=====================================

ä»€ä¹ˆæ˜¯Scrapy
---------

*   åŸºäºTwistedçš„å¼‚æ­¥å¤„ç†æ¡†æ¶
*   çº¯pythonå®ç°çš„çˆ¬è™«æ¡†æ¶
*   åŸºæœ¬ç»“æ„ï¼š5+2æ¡†æ¶ï¼Œ5ä¸ªç»„ä»¶ï¼Œ2ä¸ªä¸­é—´ä»¶

Â ![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220622144950841-1472507199.png)

**5ä¸ªç»„ä»¶ï¼š**

*   **Scrapy Engineï¼š**å¼•æ“ï¼Œè´Ÿè´£å…¶ä»–éƒ¨ä»¶é€šä¿¡ è¿›è¡Œä¿¡å·å’Œæ•°æ®ä¼ é€’ï¼›è´Ÿè´£Schedulerã€Downloaderã€Spidersã€Item Pipelineä¸­é—´çš„é€šè®¯ä¿¡å·å’Œæ•°æ®çš„ä¼ é€’ï¼Œæ­¤ç»„ä»¶ç›¸å½“äºçˆ¬è™«çš„â€œå¤§è„‘â€ï¼Œæ˜¯æ•´ä¸ªçˆ¬è™«çš„è°ƒåº¦ä¸­å¿ƒ
*   **Schedulerï¼š**è°ƒåº¦å™¨ï¼Œå°†requestè¯·æ±‚æ’åˆ—å…¥é˜Ÿï¼Œå½“å¼•æ“éœ€è¦äº¤è¿˜ç»™å¼•æ“ï¼Œé€šè¿‡å¼•æ“å°†è¯·æ±‚ä¼ é€’ç»™Downloaderï¼›ç®€å•åœ°è¯´å°±æ˜¯ä¸€ä¸ªé˜Ÿåˆ—ï¼Œè´Ÿè´£æ¥æ”¶å¼•æ“å‘é€è¿‡æ¥çš„ requestè¯·æ±‚ï¼Œç„¶åå°†è¯·æ±‚æ’é˜Ÿï¼Œå½“å¼•æ“éœ€è¦è¯·æ±‚æ•°æ®çš„æ—¶å€™ï¼Œå°±å°†è¯·æ±‚é˜Ÿåˆ—ä¸­çš„æ•°æ®äº¤ç»™å¼•æ“ã€‚åˆå§‹çš„çˆ¬å–URLå’Œåç»­åœ¨é¡µé¢ä¸­è·å–çš„å¾…çˆ¬å–çš„URLå°†æ”¾å…¥è°ƒåº¦å™¨ä¸­ï¼Œç­‰å¾…çˆ¬å–ï¼ŒåŒæ—¶è°ƒåº¦å™¨ä¼šè‡ªåŠ¨å»é™¤é‡å¤çš„URLï¼ˆå¦‚æœç‰¹å®šçš„URLä¸éœ€è¦å»é‡ä¹Ÿå¯ä»¥é€šè¿‡è®¾ç½®å®ç°ï¼Œå¦‚postè¯·æ±‚çš„URLï¼‰
*   **Downloaderï¼š**ä¸‹è½½å™¨ï¼Œå°†å¼•æ“engineå‘é€çš„requestè¿›è¡Œæ¥æ”¶ï¼Œå¹¶å°†responseç»“æœäº¤è¿˜ç»™å¼•æ“engineï¼Œå†ç”±å¼•æ“ä¼ é€’ç»™Spiderså¤„ç†
*   **Spidersï¼š**è§£æå™¨ï¼Œå®ƒè´Ÿè´£å¤„ç†æ‰€æœ‰responsesï¼Œä»ä¸­åˆ†ææå–æ•°æ®ï¼Œè·å–Itemå­—æ®µéœ€è¦çš„æ•°æ®ï¼Œå¹¶å°†éœ€è¦è·Ÿè¿›çš„URLæäº¤ç»™å¼•æ“ï¼Œå†æ¬¡è¿›å…¥Scheduler(è°ƒåº¦å™¨)ï¼›åŒæ—¶ä¹Ÿæ˜¯å…¥å£URLçš„åœ°æ–¹
*   **Item Pipelineï¼š**æ•°æ®ç®¡é“ï¼Œå°±æ˜¯æˆ‘ä»¬å°è£…å»é‡ç±»ã€å­˜å‚¨ç±»çš„åœ°æ–¹ï¼Œè´Ÿè´£å¤„ç† Spidersä¸­è·å–åˆ°çš„æ•°æ®å¹¶ä¸”è¿›è¡ŒåæœŸçš„å¤„ç†ï¼Œè¿‡æ»¤æˆ–è€…å­˜å‚¨ç­‰ç­‰ã€‚å½“é¡µé¢è¢«çˆ¬è™«è§£ææ‰€éœ€çš„æ•°æ®å­˜å…¥Itemåï¼Œå°†è¢«å‘é€åˆ°é¡¹ç›®ç®¡é“(Pipeline)ï¼Œå¹¶ç»è¿‡å‡ ä¸ªç‰¹å®šçš„æ¬¡åºå¤„ç†æ•°æ®ï¼Œæœ€åå­˜å…¥æœ¬åœ°æ–‡ä»¶æˆ–å­˜å…¥æ•°æ®åº“

**2ä¸ªä¸­é—´ä»¶ï¼š**

*   **Downloader Middlewaresï¼š**ä¸‹è½½ä¸­é—´ä»¶ï¼Œå¯ä»¥å½“åšæ˜¯ä¸€ä¸ªå¯è‡ªå®šä¹‰æ‰©å±•ä¸‹è½½åŠŸèƒ½çš„ç»„ä»¶ï¼Œæ˜¯åœ¨å¼•æ“åŠä¸‹è½½å™¨ä¹‹é—´çš„ç‰¹å®šé’©å­(specific hook)ï¼Œå¤„ç†Downloaderä¼ é€’ç»™å¼•æ“çš„responseã€‚é€šè¿‡è®¾ç½®ä¸‹è½½å™¨ä¸­é—´ä»¶å¯ä»¥å®ç°çˆ¬è™«è‡ªåŠ¨æ›´æ¢user-agentã€IPç­‰åŠŸèƒ½ã€‚
*   **Spider Middlewaresï¼š**çˆ¬è™«ä¸­é—´ä»¶ï¼ŒSpiderä¸­é—´ä»¶æ˜¯åœ¨å¼•æ“åŠSpiderä¹‹é—´çš„ç‰¹å®šé’©å­(specific hook)ï¼Œå¤„ç†spiderçš„è¾“å…¥(response)å’Œè¾“å‡º(itemsåŠrequests)ã€‚è‡ªå®šä¹‰æ‰©å±•ã€å¼•æ“å’ŒSpiderä¹‹é—´é€šä¿¡åŠŸèƒ½çš„ç»„ä»¶ï¼Œé€šè¿‡æ’å…¥è‡ªå®šä¹‰ä»£ç æ¥æ‰©å±•ScrapyåŠŸèƒ½ã€‚

Â **Scrapyæ“ä½œæ–‡æ¡£(ä¸­æ–‡çš„)ï¼š[https://www.osgeo.cn/scrapy/topics/spider-middleware.html](https://www.osgeo.cn/scrapy/topics/spider-middleware.html)**

Scrapyæ¡†æ¶çš„å®‰è£…
-----------

cmdçª—å£ï¼Œpipè¿›è¡Œå®‰è£…

pip install scrapy

![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220622144115902-591510172.png)

**Scrapyæ¡†æ¶å®‰è£…æ—¶å¸¸è§çš„é—®é¢˜**

æ‰¾ä¸åˆ°win32apiæ¨¡å—----windowsç³»ç»Ÿä¸­å¸¸è§

pip install pypiwin32

åˆ›å»ºScrapyçˆ¬è™«é¡¹ç›®
------------

### æ–°å»ºé¡¹ç›®

scrapy startproject xxxé¡¹ç›®åç§°

**å®ä¾‹:**

scrapy startproject tubatu\_scrapy\_project

**![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220622154649893-1468217369.png)**

### é¡¹ç›®ç›®å½•

Â ![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220622155204402-1254423692.png)

**scrapy.cfgï¼šé¡¹ç›®çš„é…ç½®æ–‡ä»¶ï¼Œå®šä¹‰äº†é¡¹ç›®é…ç½®æ–‡ä»¶çš„è·¯å¾„ç­‰é…ç½®ä¿¡æ¯**

*   ã€settingsã€‘ï¼šå®šä¹‰äº†é¡¹ç›®çš„é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼Œå³./tubatu\_scrapy\_project/settingsæ–‡ä»¶
*   ã€deployã€‘ï¼šéƒ¨ç½²ä¿¡æ¯

![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220622160214454-1616271586.png)

*   **items.pyï¼š**å°±æ˜¯æˆ‘ä»¬å®šä¹‰itemæ•°æ®ç»“æ„çš„åœ°æ–¹ï¼›ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬æƒ³è¦æŠ“å–å“ªäº›å­—æ®µï¼Œæ‰€æœ‰çš„itemå®šä¹‰éƒ½å¯ä»¥æ”¾åˆ°è¿™ä¸ªæ–‡ä»¶ä¸­
*   **pipelines.pyï¼š**é¡¹ç›®çš„ç®¡é“æ–‡ä»¶ï¼Œå°±æ˜¯æˆ‘ä»¬è¯´çš„æ•°æ®å¤„ç†ç®¡é“æ–‡ä»¶ï¼›ç”¨äºç¼–å†™æ•°æ®å­˜å‚¨ï¼Œæ¸…æ´—ç­‰é€»è¾‘ï¼Œæ¯”å¦‚å°†æ•°æ®å­˜å‚¨åˆ°jsonæ–‡ä»¶ï¼Œå°±å¯ä»¥åœ¨è¿™è¾¹ç¼–å†™é€»è¾‘
*   **settings.pyï¼š**é¡¹ç›®çš„è®¾ç½®æ–‡ä»¶ï¼Œå¯ä»¥å®šä¹‰é¡¹ç›®çš„å…¨å±€è®¾ç½®ï¼Œæ¯”å¦‚è®¾ç½®çˆ¬è™«çš„Â USER\_AGENTÂ ï¼Œå°±å¯ä»¥åœ¨è¿™é‡Œè®¾ç½®ï¼›å¸¸ç”¨é…ç½®é¡¹å¦‚ä¸‹ï¼š
    *   ROBOTSTXT\_OBEYÂ ï¼šæ˜¯å¦éµå¾ªROBTSåè®®ï¼Œä¸€èˆ¬è®¾ç½®ä¸ºFalse
    *   CONCURRENT\_REQUESTSÂ ï¼šå¹¶å‘é‡ï¼Œé»˜è®¤æ˜¯32ä¸ªå¹¶å‘
    *   COOKIES\_ENABLEDÂ ï¼šæ˜¯å¦å¯ç”¨cookiesï¼Œé»˜è®¤æ˜¯False
    *   DOWNLOAD\_DELAYÂ ï¼šä¸‹è½½å»¶è¿Ÿ
    *   DEFAULT\_REQUEST\_HEADERSÂ ï¼šé»˜è®¤è¯·æ±‚å¤´
    *   SPIDER\_MIDDLEWARESÂ ï¼šæ˜¯å¦å¯ç”¨spiderä¸­é—´ä»¶
    *   DOWNLOADER\_MIDDLEWARESÂ ï¼šæ˜¯å¦å¯ç”¨downloaderä¸­é—´ä»¶
    *   å…¶ä»–è¯¦è§[é“¾æ¥](https://blog.csdn.net/weixin_44634704/article/details/109013695?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-6-109013695-null-null.pc_agg_new_rank&utm_term=scrapy%E7%9A%84settings%E8%AE%BE%E7%BD%AE%E6%97%A5%E5%BF%97%E7%AD%89%E7%BA%A7&spm=1000.2123.3001.4430)
*   **spidersç›®å½•ï¼š**åŒ…å«æ¯ä¸ªçˆ¬è™«çš„å®ç°ï¼Œæˆ‘ä»¬çš„è§£æè§„åˆ™å†™åœ¨è¿™ä¸ªç›®å½•ä¸‹ï¼Œå³çˆ¬è™«çš„è§£æå™¨å†™åœ¨è¿™ä¸ªç›®å½•ä¸‹
*   **middlewares.pyï¼š**å®šä¹‰äº†Â SpiderMiddlewareå’ŒDownloaderMiddlewareÂ ä¸­é—´ä»¶çš„è§„åˆ™ï¼›è‡ªå®šä¹‰è¯·æ±‚ã€è‡ªå®šä¹‰å…¶ä»–æ•°æ®å¤„ç†æ–¹å¼ã€ä»£ç†è®¿é—®ç­‰

### è‡ªåŠ¨ç”Ÿæˆspidersæ¨¡æ¿æ–‡ä»¶

cdåˆ°spidersç›®å½•ä¸‹ï¼Œè¾“å‡ºå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆçˆ¬è™«æ–‡ä»¶ï¼š

scrapy genspider æ–‡ä»¶å çˆ¬å–çš„åœ°å€

![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220622194542701-501268431.png)

![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220622194705119-806132516.png)

### è¿è¡Œçˆ¬è™«

**æ–¹å¼ä¸€ï¼šcmdå¯åŠ¨**

cdåˆ°spidersç›®å½•ä¸‹ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨çˆ¬è™«ï¼š

scrapy crawl çˆ¬è™«å

![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220622200255323-134289728.png)

**æ–¹å¼äºŒï¼špyæ–‡ä»¶å¯åŠ¨**

åœ¨é¡¹ç›®ä¸‹åˆ›å»ºmain.pyæ–‡ä»¶ï¼Œåˆ›å»ºå¯åŠ¨è„šæœ¬ï¼Œæ‰§è¡Œmain.pyå¯åŠ¨æ–‡ä»¶ï¼Œä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š

code-çˆ¬è™«æ–‡ä»¶ğŸ‘‡

import scrapy

class TubatuSpider(scrapy.Spider):
    #åç§°ä¸èƒ½é‡å¤
    name = 'tubatu'
    #å…è®¸çˆ¬è™«å»æŠ“å–çš„åŸŸå
    allowed\_domains = \['xiaoguotu.to8to.com'\]
    #é¡¹ç›®å¯åŠ¨ä¹‹åè¦å¯åŠ¨çš„çˆ¬è™«æ–‡ä»¶
    start\_urls = \['https://xiaoguotu.to8to.com/pic\_space1?page=1'\]

    #é»˜è®¤çš„è§£ææ–¹æ³•
    def parse(self, response):
        print(response.text)

code-å¯åŠ¨æ–‡ä»¶ğŸ‘‡

from scrapy import cmdline

#åœ¨æˆ‘ä»¬scrapyé¡¹ç›®é‡Œé¢ï¼Œä¸ºäº†æ–¹ä¾¿è¿è¡Œscrapyçš„é¡¹ç›®çš„æ—¶å€™åˆ›å»ºçš„æ–‡ä»¶
#ä½¿ç”¨cmdlie.execute()æ–¹æ³•æ‰§è¡Œçˆ¬è™«å¯åŠ¨å‘½ä»¤ï¼šscrapy crawl çˆ¬è™«å
cmdline.execute("scrapy crawl tubatu".split())  #executeæ–¹æ³•éœ€è¦è¿è¡Œçš„æ¯ä¸€ä¸ªå‘½ä»¤ä¸ºå•ç‹¬çš„ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¦‚ï¼šcmdline.execute(\['scrapy', 'crawl', 'tubatu'\])ï¼Œæ‰€ä»¥å¦‚æœå‘½ä»¤ä¸ºä¸€æ•´ä¸ªå­—ç¬¦ä¸²æ—¶ï¼Œéœ€è¦split( )è¿›è¡Œåˆ†å‰²ï¼›#

code-è¿è¡Œç»“æœğŸ‘‡

![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220622201420445-574844410.png)

ç¤ºä¾‹é¡¹ç›®
----

çˆ¬å–åœŸå·´å…”è£…ä¿®ç½‘ç«™ä¿¡æ¯ã€‚å°†çˆ¬å–åˆ°çš„æ•°æ®å­˜å…¥åˆ°æœ¬åœ°MongoDBæ•°æ®åº“ä¸­ï¼›

ä¸‹å›¾ğŸ‘‡ä¸ºé¡¹ç›®æœºæ„ï¼Œæ ‡è“çš„æ–‡ä»¶å°±æ˜¯æ­¤æ¬¡codeçš„ä»£ç 

![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220627204237933-424503324.png)

### tubatu.py

 1 import scrapy 2 from tubatu\_scrapy\_project.items import TubatuScrapyProjectItem 3 import re 4 
 5 class TubatuSpider(scrapy.Spider): 6 
 7   **  #åç§°ä¸èƒ½é‡å¤**
 8     name = 'tubatu'
 9     **#å…è®¸çˆ¬è™«å»æŠ“å–çš„åŸŸå,è¶…è¿‡è¿™ä¸ªç›®å½•å°±ä¸å…è®¸æŠ“å–**
10     allowed\_domains = \['xiaoguotu.to8to.com','wx.to8to.com','sz.to8to.com'\]
11     #é¡¹ç›®å¯åŠ¨ä¹‹åè¦å¯åŠ¨çš„çˆ¬è™«æ–‡ä»¶
12     start\_urls = \['https://xiaoguotu.to8to.com/pic\_space1?page=1'\]
13 
14 
15     #é»˜è®¤çš„è§£ææ–¹æ³•
16     def parse(self, response):
17         **\# responseåé¢å¯ä»¥ç›´æ¥ä½¿ç”¨xpathæ–¹æ³•**
18         **\# responseå°±æ˜¯ä¸€ä¸ªHtmlå¯¹è±¡**
19         pic\_item\_list = response.xpath("//div\[@class='item'\]")
20         for item in pic\_item\_list\[1:\]:
21             info = {}
22             **\# è¿™é‡Œæœ‰ä¸€ä¸ªç‚¹ä¸è¦ä¸¢äº†ï¼Œæ˜¯è¯´æ˜åœ¨å½“å‰Itemä¸‹é¢å†æ¬¡ä½¿ç”¨xpath**
23             # è¿”å›çš„ä¸ä»…ä»…æ˜¯xpathå®šä½ä¸­çš„text()å†…å®¹ï¼Œéœ€è¦å†è¿‡æ»¤ï¼›è¿”å›å¦‚ï¼š\[<Selector xpath='.//div/a/text()' data='0å…ƒæå®šè®¾è®¡æ–¹æ¡ˆï¼Œé™åé¢é¢†å–'>\] <class 'scrapy.selector.unified.SelectorList'>
24             # content\_name = item.xpath('.//div/a/text()')
25 
26            ** #ä½¿ç”¨extract()æ–¹æ³•è·å–itemè¿”å›çš„dataä¿¡æ¯ï¼Œè¿”å›çš„æ˜¯åˆ—è¡¨**
27             # content\_name = item.xpath('.//div/a/text()').extract()
28 
29             **#ä½¿ç”¨extract\_first()æ–¹æ³•è·å–åç§°ï¼Œæ•°æ®;è¿”å›çš„æ˜¯strç±»å‹**
30             #è·å–é¡¹ç›®çš„åç§°ï¼Œé¡¹ç›®çš„æ•°æ®
31             info\['content\_name'\] = item.xpath(".//a\[@target='\_blank'\]/@data-content\_title").extract\_first()
32 
33             #è·å–é¡¹ç›®çš„URL
34             info\['content\_url'\] = "https:"\+ item.xpath(".//a\[@target='\_blank'\]/@href").extract\_first()
35 
36             #é¡¹ç›®id
37             content\_id\_search = re.compile(r"(\\d+)\\.html")
38             info\['content\_id'\] = str(content\_id\_search.search(info\['content\_url'\]).group(1))
39 
40             **#ä½¿ç”¨yieldæ¥å‘é€å¼‚æ­¥è¯·æ±‚ï¼Œä½¿ç”¨çš„æ˜¯scrapy.Request()æ–¹æ³•è¿›è¡Œå‘é€ï¼Œè¿™ä¸ªæ–¹æ³•å¯ä»¥ä¼ cookieç­‰ï¼Œå¯ä»¥è¿›åˆ°è¿™ä¸ªæ–¹æ³•é‡Œé¢æŸ¥çœ‹**
41            ** #å›è°ƒå‡½æ•°callbackï¼Œåªå†™æ–¹æ³•åç§°ï¼Œä¸è¦è°ƒç”¨æ–¹æ³•**
42             yield scrapy.Request(url=info\['content\_url'\],callback=self.handle\_pic\_parse,meta=info)
43 
44         if response.xpath("//a\[@id='nextpageid'\]"):
45             now\_page = int(response.xpath("//div\[@class='pages'\]/strong/text()").extract\_first())
46             next\_page\_url="https://xiaoguotu.to8to.com/pic\_space1?page=%d" %(now\_page+1)
47             yield scrapy.Request(url=next\_page\_url,callback=self.parse)
48 
49 
50     def handle\_pic\_parse(self,response):
51         tu\_batu\_info = TubatuScrapyProjectItem()
52         #å›¾ç‰‡çš„åœ°å€
53         tu\_batu\_info\["pic\_url"\]=response.xpath("//div\[@class='img\_div\_tag'\]/img/@src").extract\_first()
54         #æ˜µç§°
55         tu\_batu\_info\["nick\_name"\]=response.xpath("//p/i\[@id='nick'\]/text()").extract\_first()
56         #å›¾ç‰‡çš„åç§°
57         tu\_batu\_info\["pic\_name"\]=response.xpath("//div\[@class='pic\_author'\]/h1/text()").extract\_first()
58         #é¡¹ç›®çš„åç§°
59         tu\_batu\_info\["content\_name"\]=response.request.meta\['content\_name'\]
60         # é¡¹ç›®id
61         tu\_batu\_info\["content\_id"\]=response.request.meta\['content\_id'\]
62         #é¡¹ç›®çš„URL
63         tu\_batu\_info\["content\_url"\]=response.request.meta\['content\_url'\]
64        ** #yieldåˆ°piplinesï¼Œæˆ‘ä»¬é€šè¿‡settings.pyé‡Œé¢å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨ï¼Œå°†æ— æ³•ä½¿ç”¨**
65         yield tu\_batu\_info

### items.py

 1 # Define here the models for your scraped items
 2 #
 3 # See documentation in:
 4 # https://docs.scrapy.org/en/latest/topics/items.html
 5 
 6 import scrapy 7 
 8 
 9 class TubatuScrapyProjectItem(scrapy.Item):
10     # define the fields for your item here like:
11     # name = scrapy.Field()
12 
13     #è£…ä¿®åç§°
14     content\_name=scrapy.Field()
15     #è£…ä¿®id
16     content\_id = scrapy.Field()
17     #è¯·æ±‚url
18     content\_url=scrapy.Field()
19     #æ˜µç§°
20     nick\_name=scrapy.Field()
21     #å›¾ç‰‡çš„url
22     pic\_url=scrapy.Field()
23     #å›¾ç‰‡çš„åç§°
24     pic\_name=scrapy.Field()

### piplines.py

 1 # Define your item pipelines here
 2 #
 3 # Don't forget to add your pipeline to the ITEM\_PIPELINES setting
 4 # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
 5 
 6 
 7 # useful for handling different item types with a single interface
 8 from itemadapter import ItemAdapter 9 
10 from pymongo import MongoClient
11 
12 class TubatuScrapyProjectPipeline:
13 
14     def \_\_init\_\_(self):
15         client = MongoClient(host="localhost",
16                              port=27017,
17                              username="admin",
18                              password="123456")
19         mydb=client\['db\_tubatu'\]
20         self.mycollection = mydb\['collection\_tubatu'\]
21 
22     def process\_item(self, item, spider):
23         data = dict(item)
24 self.mycollection.insert\_one(data)
25         return item

### settings.py

![](https://img2022.cnblogs.com/blog/2281865/202206/2281865-20220627204550658-625647551.png)

### main.py

1 from scrapy import cmdline
2 
3 #åœ¨æˆ‘ä»¬scrapyé¡¹ç›®é‡Œé¢ï¼Œä¸ºäº†æ–¹ä¾¿è¿è¡Œscrapyçš„é¡¹ç›®çš„æ—¶å€™åˆ›å»ºçš„æ–‡ä»¶
4 #ä½¿ç”¨cmdlie.execute()æ–¹æ³•æ‰§è¡Œçˆ¬è™«å¯åŠ¨å‘½ä»¤ï¼šscrapy crawl çˆ¬è™«å
5 cmdline.execute("scrapy crawl tubatu".split())  **#executeæ–¹æ³•éœ€è¦è¿è¡Œçš„æ¯ä¸€ä¸ªå‘½ä»¤ä¸ºå•ç‹¬çš„ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¦‚ï¼šcmdline.execute(\['scrapy', 'crawl', 'tubatu'\])ï¼Œæ‰€ä»¥å¦‚æœå‘½ä»¤ä¸ºä¸€æ•´ä¸ªå­—ç¬¦ä¸²æ—¶ï¼Œéœ€è¦split( )è¿›è¡Œåˆ†å‰²ï¼›#**