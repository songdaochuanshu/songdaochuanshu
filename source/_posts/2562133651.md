---
layout: post
title: "AI éŸ³è¾¨ä¸–ç•Œï¼šè‰ºæœ¯å°ç™½çš„æˆ‘ï¼Œé è¿™ä¸ªAIæ¨¡å‹ï¼Œé€Ÿè¯†éŸ³ä¹æµæ´¾é€‰æ‹©éŸ³ä¹"
date: "2022-10-26T17:24:38.445Z"
---
AI éŸ³è¾¨ä¸–ç•Œï¼šè‰ºæœ¯å°ç™½çš„æˆ‘ï¼Œé è¿™ä¸ªAIæ¨¡å‹ï¼Œé€Ÿè¯†éŸ³ä¹æµæ´¾é€‰æ‹©éŸ³ä¹
=================================

![AI éŸ³è¾¨ä¸–ç•Œï¼šè‰ºæœ¯å°ç™½çš„æˆ‘ï¼Œé è¿™ä¸ªAIæ¨¡å‹ï¼Œé€Ÿè¯†éŸ³ä¹æµæ´¾é€‰æ‹©éŸ³ä¹](https://img2022.cnblogs.com/blog/2637458/202210/2637458-20221026173107599-1886403369.png) éŸ³ä¹é¢†åŸŸï¼Œå€ŸåŠ©äºæ­Œæ›²ç›¸å…³ä¿¡æ¯ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®æ­Œæ›²çš„éŸ³é¢‘å’Œæ­Œè¯ç‰¹å¾ï¼Œå°†æ­Œæ›²ç²¾å‡†è¿›è¡Œæµæ´¾åˆ†ç±»ã€‚æœ¬æ–‡è®²è§£å¦‚ä½•åŸºäºæœºå™¨å­¦ä¹ å®Œæˆå¯¹éŸ³ä¹çš„è¯†åˆ«åˆ†ç±»ã€‚

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4d583d26982e47e4a7c786f9fda81ece~tplv-k3u1fbpfcp-zoom-1.image)

> ğŸ’¡ ä½œè€…ï¼š[éŸ©ä¿¡å­](https://github.com/HanXinzi-AI)@[ShowMeAI](https://www.showmeai.tech/)  
> ğŸ“˜ [æ•°æ®åˆ†æå®æˆ˜ç³»åˆ—](https://www.showmeai.tech/tutorials/40)ï¼š[https://www.showmeai.tech/tutorials/40](https://www.showmeai.tech/tutorials/40)  
> ğŸ“˜ [æœºå™¨å­¦ä¹ å®æˆ˜ç³»åˆ—](https://www.showmeai.tech/tutorials/41)ï¼š[https://www.showmeai.tech/tutorials/41](https://www.showmeai.tech/tutorials/41)  
> ğŸ“˜ [æœ¬æ–‡åœ°å€](https://www.showmeai.tech/article-detail/309)ï¼š[https://www.showmeai.tech/article-detail/309](https://www.showmeai.tech/article-detail/309)  
> ğŸ“¢ å£°æ˜ï¼šç‰ˆæƒæ‰€æœ‰ï¼Œè½¬è½½è¯·è”ç³»å¹³å°ä¸ä½œè€…å¹¶æ³¨æ˜å‡ºå¤„  
> ğŸ“¢ æ”¶è—[ShowMeAI](https://www.showmeai.tech/)æŸ¥çœ‹æ›´å¤šç²¾å½©å†…å®¹

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/be628a0428444294a550f1647df8b2e1~tplv-k3u1fbpfcp-zoom-1.image)

åªè¦ç»™åˆ°è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯ï¼ŒAIæ¨¡å‹å¯ä»¥è¿…é€Ÿå­¦ä¹ ä¸€ä¸ªæ–°çš„é¢†åŸŸé—®é¢˜ï¼Œå¹¶æ„å»ºèµ·å¾ˆå¥½çš„çŸ¥è¯†å’Œé¢„ä¼°ç³»ç»Ÿã€‚æ¯”å¦‚éŸ³ä¹é¢†åŸŸï¼Œå€ŸåŠ©äºæ­Œæ›²ç›¸å…³ä¿¡æ¯ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®æ­Œæ›²çš„éŸ³é¢‘å’Œæ­Œè¯ç‰¹å¾å°†æ­Œæ›²ç²¾å‡†è¿›è¡Œæµæ´¾åˆ†ç±»ã€‚åœ¨æœ¬ç¯‡å†…å®¹ä¸­ [ShowMeAI](https://www.showmeai.tech/) å°±å¸¦å¤§å®¶ä¸€èµ·æ¥çœ‹çœ‹ï¼Œå¦‚ä½•åŸºäºæœºå™¨å­¦ä¹ å®Œæˆå¯¹éŸ³ä¹çš„è¯†åˆ«åˆ†ç±»ã€‚

æœ¬ç¯‡å†…å®¹ä½¿ç”¨åˆ°çš„æ•°æ®é›†ä¸º ğŸ†[**SpotifyéŸ³ä¹æ•°æ®é›†**](https://www.kaggle.com/datasets/imuhammad/audio-features-and-lyrics-of-spotify-songs)ï¼Œå¤§å®¶ä¹Ÿå¯ä»¥é€šè¿‡ [ShowMeAI](https://www.showmeai.tech/) çš„ç™¾åº¦ç½‘ç›˜åœ°å€å¿«é€Ÿä¸‹è½½ã€‚

> ğŸ† **å®æˆ˜æ•°æ®é›†ä¸‹è½½ï¼ˆç™¾åº¦ç½‘ç›˜ï¼‰**ï¼šå…¬ä¼—å·ã€ShowMeAIç ”ç©¶ä¸­å¿ƒã€å›å¤ã€**å®æˆ˜**ã€ï¼Œæˆ–è€…ç‚¹å‡» [**è¿™é‡Œ**](https://www.showmeai.tech/article-detail/305) è·å–æœ¬æ–‡ [\[18\]éŸ³ä¹æµæ´¾è¯†åˆ«çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿæ­å»ºä¸è°ƒä¼˜](https://www.showmeai.tech/article-detail/309) ã€**Spotify éŸ³ä¹æ•°æ®é›†**ã€

> â­ **ShowMeAIå®˜æ–¹GitHub**ï¼š[https://github.com/ShowMeAI-Hub](https://github.com/ShowMeAI-Hub)

æˆ‘ä»¬åœ¨æœ¬ç¯‡å†…å®¹ä¸­å°†ç”¨åˆ°æœ€å¸¸ç”¨çš„ boosting é›†æˆå·¥å…·åº“ LightGBMï¼Œå¹¶ä¸”å°†ç»“åˆ optuna å·¥å…·åº“å¯¹å…¶è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ï¼Œä¼˜åŒ–æ¨¡å‹æ•ˆæœã€‚

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c508905c49f041078722f8e499b4e39d~tplv-k3u1fbpfcp-zoom-1.image)

> å…³äº LightGBM çš„æ¨¡å‹åŸç†å’Œä½¿ç”¨è¯¦ç»†è®²è§£ï¼Œæ¬¢è¿å¤§å®¶æŸ¥é˜… [ShowMeAI](https://www.showmeai.tech/) çš„æ–‡ç« ï¼š
> 
> ğŸ“˜[**å›¾è§£æœºå™¨å­¦ä¹ ç®—æ³•(11) | LightGBMæ¨¡å‹è¯¦è§£**](https://www.showmeai.tech/article-detail/195)
> 
> ğŸ“˜[**æœºå™¨å­¦ä¹ å®æˆ˜(5) | LightGBMå»ºæ¨¡åº”ç”¨è¯¦è§£**](https://www.showmeai.tech/article-detail/205)

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8bc13f8d2d64465c868abb115cbe33fb~tplv-k3u1fbpfcp-zoom-1.image)

**æœ¬ç¯‡æ–‡ç« åŒ…å«ä»¥ä¸‹å†…å®¹æ¿å—ï¼š**

*   æ•°æ®æ¦‚è§ˆå’Œé¢„å¤„ç†
*   EDAæ¢ç´¢æ€§æ•°æ®åˆ†æ
*   æ­Œè¯ç‰¹å¾&æ•°æ®é™ç»´
*   å»ºæ¨¡å’Œè¶…å‚æ•°ä¼˜åŒ–
*   æ€»ç»“&ç»éªŒ

ğŸ’¡ æ•°æ®æ¦‚è§ˆå’Œé¢„å¤„ç†
===========

æœ¬æ¬¡ä½¿ç”¨çš„æ•°æ®é›†åŒ…å«è¶…è¿‡ 18000 é¦–æ­Œæ›²çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¶éŸ³é¢‘ç‰¹å¾ä¿¡æ¯ï¼ˆå¦‚æ´»åŠ›åº¦ï¼Œæ’­æ”¾é€Ÿåº¦æˆ–è°ƒæ€§ç­‰ï¼‰ï¼Œä»¥åŠæ­Œæ›²çš„æ­Œè¯ã€‚

æˆ‘ä»¬è¯»å–æ•°æ®å¹¶åšä¸€ä¸ªé€Ÿè§ˆå¦‚ä¸‹ï¼š

    import pandas as pd
    # è¯»å–æ•°æ®
    data = pd.read_csv("spotify_songs.csv")
    # æ•°æ®é€Ÿè§ˆ
    data.head()
    

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c5f86fb4c2654d219910f1aaac04aaba~tplv-k3u1fbpfcp-zoom-1.image)

    # æ•°æ®åŸºæœ¬ä¿¡æ¯
    data.info()
    

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e2996a6a13034dbd992129c9518fbe6f~tplv-k3u1fbpfcp-zoom-1.image)

å­—æ®µè¯´æ˜å¦‚ä¸‹ï¼š

å­—æ®µ

å«ä¹‰

track\_id

æ­Œæ›²å”¯ä¸€ID

track\_name

æ­Œæ›²åç§°

track\_artist

æ­Œæ‰‹

lyrics

æ­Œè¯

track\_popularity

å”±ç‰‡çƒ­åº¦

track\_album\_id

å”±ç‰‡çš„å”¯ä¸€ID

track\_album\_name

å”±ç‰‡åå­—

track\_album\_release\_date

å”±ç‰‡å‘è¡Œæ—¥æœŸ

playlist\_name

æ­Œå•åç§°

playlist\_id

æ­Œå•ID

playlist\_genre

æ­Œå•é£æ ¼

playlist\_subgenre

æ­Œå•å­é£æ ¼

danceability

èˆè¹ˆæ€§æè¿°çš„æ˜¯æ ¹æ®éŸ³ä¹å…ƒç´ çš„ç»„åˆï¼ŒåŒ…æ‹¬é€Ÿåº¦ã€èŠ‚å¥çš„ç¨³å®šæ€§ã€èŠ‚æ‹çš„å¼ºåº¦å’Œæ•´ä½“çš„è§„å¾‹æ€§ï¼Œæ¥è¡¡é‡ä¸€é¦–æ›²ç›®æ˜¯å¦é€‚åˆè·³èˆã€‚0.0çš„å€¼æ˜¯æœ€ä¸é€‚åˆè·³èˆçš„ï¼Œ1.0æ˜¯æœ€é€‚åˆè·³èˆçš„ã€‚

energy

èƒ½é‡æ˜¯ä¸€ä¸ªä»0.0åˆ°1.0çš„åº¦é‡ï¼Œä»£è¡¨å¼ºåº¦å’Œæ´»åŠ¨çš„æ„ŸçŸ¥åº¦ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæœ‰èƒ½é‡çš„æ›²ç›®ç»™äººçš„æ„Ÿè§‰æ˜¯å¿«é€Ÿã€å“äº®ã€‚ä¾‹å¦‚ï¼Œæ­»äº¡é‡‘å±æœ‰å¾ˆé«˜çš„èƒ½é‡ï¼Œè€Œå·´èµ«çš„å‰å¥æ›²åœ¨è¯¥é‡è¡¨ä¸­å¾—åˆ†è¾ƒä½ã€‚

key

éŸ³è½¨çš„ä¼°æµ‹æ€»è°ƒã€‚ç”¨æ ‡å‡†çš„éŸ³é˜¶ç¬¦å·å°†æ•´æ•°æ˜ å°„ä¸ºéŸ³é«˜ã€‚ä¾‹å¦‚ï¼Œ0=Cï¼Œ1=Câ™¯/Dâ™­ï¼Œ2=Dï¼Œä»¥æ­¤ç±»æ¨ã€‚å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°éŸ³è°ƒï¼Œåˆ™æ•°å€¼ä¸º-1ã€‚

loudness

è½¨é“çš„æ•´ä½“å“åº¦ï¼Œå•ä½æ˜¯åˆ†è´ï¼ˆdBï¼‰ã€‚å“åº¦å€¼æ˜¯æ•´ä¸ªéŸ³è½¨çš„å¹³å‡å€¼ï¼Œå¯¹äºæ¯”è¾ƒéŸ³è½¨çš„ç›¸å¯¹å“åº¦éå¸¸æœ‰ç”¨ã€‚

mode

æ¨¡å¼è¡¨ç¤ºéŸ³è½¨çš„è°ƒå¼ï¼ˆå¤§è°ƒæˆ–å°è°ƒï¼‰ï¼Œå³å…¶æ—‹å¾‹å†…å®¹æ‰€æ¥è‡ªçš„éŸ³é˜¶ç±»å‹ã€‚å¤§è°ƒç”¨1è¡¨ç¤ºï¼Œå°è°ƒç”¨0è¡¨ç¤ºã€‚

speechiness

è¨€è¯­æ€§æ£€æµ‹éŸ³è½¨ä¸­æ˜¯å¦æœ‰å£è¯­ã€‚å½•éŸ³è¶Šæ˜¯å®Œå…¨ç±»ä¼¼äºè¯­éŸ³ï¼ˆå¦‚è„±å£ç§€ã€è¯´å”±ã€è¯—æ­Œï¼‰ï¼Œå±æ€§å€¼å°±è¶Šæ¥è¿‘1.0ã€‚

acousticness

è¡¡é‡éŸ³è½¨æ˜¯å¦ä¸ºå£°å­¦çš„ä¿¡å¿ƒæŒ‡æ•°ï¼Œä»0.0åˆ°1.0ã€‚1.0è¡¨ç¤ºè¯¥æ›²ç›®ä¸ºåŸå£°çš„é«˜ç½®ä¿¡åº¦ã€‚

instrumentalness

é¢„æµ‹ä¸€ä¸ªéŸ³è½¨æ˜¯å¦åŒ…å«äººå£°ã€‚è¶Šæ¥è¿‘1.0è¯¥æ›²ç›®å°±è¶Šæœ‰å¯èƒ½ä¸åŒ…å«äººå£°å†…å®¹ã€‚

liveness

æ£€æµ‹å½•éŸ³ä¸­æ˜¯å¦æœ‰å¬ä¼—å­˜åœ¨ã€‚è¶Šæ¥è¿‘ç°åœºæ¼”å‡ºæ•°å€¼è¶Šå¤§ã€‚

valence

0.0åˆ°1.0ï¼Œæè¿°äº†ä¸€ä¸ªéŸ³è½¨æ‰€ä¼ è¾¾çš„éŸ³ä¹ç§¯ææ€§ï¼Œæ¥è¿‘1çš„æ›²ç›®å¬èµ·æ¥æ›´ç§¯æï¼ˆå¦‚å¿«ä¹ã€æ¬¢å¿«ã€å…´å¥‹ï¼‰ï¼Œè€Œæ¥è¿‘0çš„æ›²ç›®å¬èµ·æ¥æ›´æ¶ˆæï¼ˆå¦‚æ‚²ä¼¤ã€å‹æŠ‘ã€æ„¤æ€’ï¼‰ã€‚

tempo

è½¨é“çš„æ•´ä½“ä¼°è®¡é€Ÿåº¦ï¼Œå•ä½æ˜¯æ¯åˆ†é’ŸèŠ‚æ‹ï¼ˆBPMï¼‰ã€‚

duration\_ms

æ­Œæ›²çš„æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

language

æ­Œè¯çš„è¯­è¨€è¯­ç§

åŸå§‹çš„æ•°æ®æœ‰ç‚¹æ‚ä¹±ï¼Œæˆ‘ä»¬å…ˆè¿›è¡Œè¿‡æ»¤å’Œæ•°æ®æ¸…æ´—ã€‚

    # æ•°æ®å·¥å…·åº“
    import pandas as pd
    import re
    
    # æ­Œè¯å¤„ç†çš„nlpå·¥å…·åº“
    import nltk
    from nltk.corpus import stopwords
    from collections import Counter
    # nltk.download('stopwords')
    
    # è¯»å–æ•°æ®
    data = pd.read_csv("spotify_songs.csv")
    # å­—æ®µé€‰æ‹©
    keep_cols = [x for x in data.columns if not x.startswith("track") and not x.startswith("playlist")]
    keep_cols.append("playlist_genre")
    df = data[keep_cols].copy()
    # åªä¿ç•™è‹±æ–‡æ­Œæ›²
    subdf = df[(df.language == "en") & (df.playlist_genre != "latin")].copy().drop(columns = "language")
    
    
    # æ­Œè¯è§„æ•´åŒ–ï¼Œå…¨éƒ¨å°å†™
    pattern = r"[^a-zA-Z ]"
    subdf.lyrics = subdf.lyrics.apply(lambda x: re.sub(pattern, "", x.lower()))
    
    # ç§»é™¤åœç”¨è¯
    subdf.lyrics = subdf.lyrics.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words("english"))]))
    
    # æŸ¥çœ‹æ­Œè¯ä¸­çš„è¯æ±‡å‡ºç°çš„é¢‘æ¬¡
    
    # è¿æ¥æ‰€æœ‰æ­Œè¯
    all_text = " ".join(subdf.lyrics)
    # ç»Ÿè®¡è¯é¢‘
    word_count = Counter(all_text.split())
    # å¦‚æœä¸€ä¸ªè¯åœ¨200é¦–ä»¥ä¸Šçš„æ­Œé‡Œéƒ½å‡ºç°ï¼Œåˆ™ä¿ç•™ï¼Œå¦åˆ™è§†ä½œä½é¢‘è¿‡æ»¤æ‰
    keep_words = [k for k, v in word_count.items() if v > 200]
    # æ„å»ºä¸€ä¸ªå‰¯æœ¬
    lyricdf = subdf.copy().reset_index(drop=True)
    # å­—æ®µåç§°è§„èŒƒåŒ–
    lyricdf.columns = ["audio_"+ x if not x in ["lyrics", "playlist_genre"] else x for x in lyricdf.columns]
    # æ­Œè¯å†…å®¹
    lyricdf.lyrics = lyricdf.lyrics.apply(lambda x: Counter([word for word in x.split() if word in keep_words]))
    # æ„å»ºè¯æ±‡è¯é¢‘Dataframe
    unpacked_lyrics = pd.DataFrame.from_records(lyricdf.lyrics).add_prefix("lyrics_")
    # ç¼ºå¤±å¡«å……ä¸º0
    unpacked_lyrics = unpacked_lyrics.fillna(0) 
    # æ‹¼æ¥å¹¶åˆ é™¤åŸå§‹æ­Œè¯åˆ—
    lyricdf = pd.concat([lyricdf, unpacked_lyrics], axis = 1).drop(columns = "lyrics")
    # æ’åº
    reordered_cols = [col for col in lyricdf.columns if not col.startswith("lyrics_")] + sorted([col for col in lyricdf.columns if col.startswith("lyrics_")])
    lyricdf = lyricdf[reordered_cols]
    
    # å­˜å‚¨ä¸ºæ–°çš„csvæ–‡ä»¶
    lyricdf.to_csv("music_data.csv", index = False)
    

ä¸»è¦çš„æ•°æ®é¢„å¤„ç†åœ¨ä¸Šè¿°ä»£ç çš„æ³¨é‡Šé‡Œå¤§å®¶å¯ä»¥çœ‹åˆ°ï¼Œæ ¸å¿ƒæ­¥éª¤æ¦‚è¿°å¦‚ä¸‹ï¼š

*   è¿‡æ»¤æ•°æ®ä»¥ä»…åŒ…å«è‹±è¯­æ­Œæ›²å¹¶åˆ é™¤â€œæ‹‰ä¸â€ç±»å‹çš„æ­Œæ›²ï¼ˆå› ä¸ºè¿™äº›æ­Œæ›²å‡ ä¹å®Œå…¨æ˜¯è¥¿ç­ç‰™è¯­ï¼Œæ‰€ä»¥ä¼šäº§ç”Ÿä¸¥é‡çš„ç±»ä¸å¹³è¡¡ï¼‰ã€‚
*   é€šè¿‡å°†æ­Œè¯è®¾ä¸ºå°å†™ã€åˆ é™¤æ ‡ç‚¹ç¬¦å·å’Œåœç”¨è¯æ¥æ•´ç†æ­Œè¯ã€‚è®¡ç®—æ¯ä¸ªå‰©ä½™å•è¯åœ¨æ­Œæ›²æ­Œè¯ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œç„¶åè¿‡æ»¤æ‰æ‰€æœ‰æ­Œæ›²ä¸­å‡ºç°é¢‘ç‡æœ€ä½çš„å•è¯ï¼ˆæ··ä¹±çš„æ•°æ®/å™ªéŸ³ï¼‰ã€‚
*   æ¸…ç†ä¸æ’åºã€‚

ğŸ’¡ EDAæ¢ç´¢æ€§æ•°æ®åˆ†æ
=============

å’Œè¿‡å¾€æ‰€æœ‰çš„é¡¹ç›®ä¸€æ ·ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦å…ˆå¯¹æ•°æ®åšä¸€äº›åˆ†æå’Œæ›´è¿›ä¸€æ­¥çš„ç†è§£ï¼Œä¹Ÿå°±æ˜¯EDAæ¢ç´¢æ€§æ•°æ®åˆ†æè¿‡ç¨‹ã€‚

> EDAæ•°æ®åˆ†æéƒ¨åˆ†æ¶‰åŠçš„å·¥å…·åº“ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒ[ShowMeAI](https://www.showmeai.tech/)åˆ¶ä½œçš„å·¥å…·åº“é€ŸæŸ¥è¡¨å’Œæ•™ç¨‹è¿›è¡Œå­¦ä¹ å’Œå¿«é€Ÿä½¿ç”¨ã€‚  
> ğŸ“˜[**æ•°æ®ç§‘å­¦å·¥å…·åº“é€ŸæŸ¥è¡¨ | Pandas é€ŸæŸ¥è¡¨**](https://www.showmeai.tech/article-detail/101)  
> ğŸ“˜[**å›¾è§£æ•°æ®åˆ†æï¼šä»å…¥é—¨åˆ°ç²¾é€šç³»åˆ—æ•™ç¨‹**](https://www.showmeai.tech/tutorials/33)

é¦–å…ˆæˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æˆ‘ä»¬çš„æ ‡ç­¾ï¼ˆæµæ´¾ï¼‰çš„**ç±»åˆ†å¸ƒå’Œå¹³è¡¡**ã€‚

    # åˆ†ç»„ç»Ÿè®¡
    by_genre = data.groupby("playlist_genre")["audio_key"].count().reset_index()
    fig, ax = plt.subplots()
    
    # ç»˜å›¾
    ax.bar(by_genre.playlist_genre, by_genre.audio_key)
    ax.set_ylabel("Number of Observations")
    ax.set_xlabel("Genre")
    ax.set_title("Observations per Class")
    ax.set_ylim(0, 4000)
    
    # æ¯ä¸ªæŸ±å­ä¸Šæ ‡æ³¨æ•°é‡
    rects = ax.patches
    for rect in rects:
        height = rect.get_height()
        ax.text(
            rect.get_x() + rect.get_width() / 2, height + 5, height, ha="center", va="bottom"
        )
    

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ab478070b2dd477da93834dc893d2cfe~tplv-k3u1fbpfcp-zoom-1.image)

å­˜åœ¨è½»å¾®çš„**ç±»åˆ«ä¸å¹³è¡¡**ï¼Œé‚£åç»­æˆ‘ä»¬åœ¨äº¤å‰éªŒè¯å’Œè®­ç»ƒæµ‹è¯•æ‹†åˆ†æ—¶å€™æ³¨æ„**æ•°æ®åˆ†å±‚ï¼ˆä¿æŒæ¯”ä¾‹åˆ†å¸ƒï¼‰** å³å¯ã€‚

    # æŠŠæ‰€æœ‰å­—æ®µåˆ‡åˆ†ä¸ºéŸ³é¢‘å’Œæ­Œè¯åˆ—
    audio = data[[x for x in data.columns if x.startswith("audio")]]
    lyric = data[[x for x in data.columns if x.startswith("lyric")]]
    # è®©å­—æ®µå‘½åæ›´ç®€å•ä¸€äº›
    audio.columns = audio.columns.str.replace("audio_", "")
    lyric.columns = lyric.columns.str.replace("lyric_", "")
    

ğŸ’¡ æ­Œè¯ç‰¹å¾&æ•°æ®é™ç»´
============

æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å¤„ç†é«˜ç»´æ•°æ®çš„æ—¶å€™ï¼Œå¯èƒ½ä¼šæœ‰ä¸€äº›æ€§èƒ½é—®é¢˜ï¼Œæœ‰æ—¶å€™æˆ‘ä»¬ä¼šå¯¹æ•°æ®è¿›è¡Œé™ç»´å¤„ç†ã€‚

> é™ç»´çš„æœ¬è´¨æ˜¯å°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°ä½ç»´å­ç©ºé—´ä¸­ï¼ŒåŒæ—¶å°½å¯èƒ½å¤šåœ°ä¿ç•™æ•°æ®ä¸­çš„ä¿¡æ¯ã€‚å…³äºé™ç»´å¤§å®¶å¯ä»¥æŸ¥çœ‹ [ShowMeAI](https://www.showmeai.tech/) çš„ç®—æ³•åŸç†è®²è§£æ–‡ç«  ğŸ“˜**å›¾è§£æœºå™¨å­¦ä¹  | é™ç»´ç®—æ³•è¯¦è§£**

æˆ‘ä»¬æ¢ç´¢ä¸€ä¸‹é™ç»´ç®—æ³•ï¼ˆPCA å’Œ t-SNEï¼‰åœ¨æˆ‘ä»¬çš„æ­Œè¯æ•°æ®ä¸Šé™ç»´æ˜¯å¦åˆé€‚ï¼Œå¹¶åšä¸€ç‚¹è°ƒæ•´ã€‚

ğŸ“Œ PCAä¸»æˆåˆ†åˆ†æ
-----------

PCAæ˜¯æœ€å¸¸ç”¨çš„é™ç»´ç®—æ³•ä¹‹ä¸€ï¼Œæˆ‘ä»¬å€ŸåŠ©è¿™ä¸ªç®—æ³•å¯ä»¥å¯¹æ•°æ®è¿›è¡Œé™ç»´ï¼Œå¹¶ä¸”çœ‹åˆ°å®ƒä¿ç•™å¤§æ¦‚å¤šå°‘çš„åŸå§‹ä¿¡æ¯é‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬å½“å‰åœºæ™¯ä¸­ï¼Œå¦‚æœ**å°†æ­Œè¯å‡å°‘åˆ°400 ç»´**ï¼Œæˆ‘ä»¬ä»ç„¶**ä¿ç•™äº†æ­Œè¯ä¸­60% çš„ä¿¡æ¯ï¼ˆæ–¹å·®ï¼‰** ï¼›å¦‚æœé™ç»´åˆ°800ç»´ï¼Œåˆ™å¯ä»¥è¦†ç›– 80% çš„åŸå§‹ä¿¡æ¯ï¼ˆæ–¹å·®ï¼‰ã€‚æ­Œè¯æœ¬èº«æ˜¯å¾ˆç¨€ç–çš„ï¼Œæˆ‘ä»¬å¯¹å…¶é™ç»´ä¹Ÿèƒ½è®©æ¨¡å‹æ›´å¥½åœ°å»ºæ¨¡ã€‚

    # å¸¸è§„æ•°æ®å·¥å…·åº“
    import pandas as pd
    import numpy as np
    # ç»˜å›¾
    import matplotlib.pyplot as plt
    import matplotlib.ticker as mtick
    # æ•°æ®å¤„ç†
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.decomposition import PCA
    
    # è¯»å–æ•°æ®
    data = pd.read_csv("music_data.csv")
    # åˆ‡åˆ†ä¸ºéŸ³é¢‘ä¸æ­Œè¯
    audio = data[[x for x in data.columns if x.startswith("audio")]]
    lyric = data[[x for x in data.columns if x.startswith("lyric")]]
    # ç‰¹å¾å­—æ®µ
    y = data.playlist_genre
    
    # æ•°æ®å¹…åº¦ç¼©æ”¾ + PCAé™ç»´
    scaler = MinMaxScaler()
    audio_features = scaler.fit_transform(audio)
    lyric_features = scaler.fit_transform(lyric)
    
    pca = PCA()
    lyric_pca  = pca.fit_transform(lyric_features)
    var_explained_ratio = pca.explained_variance_ratio_
       
    # Plot graph
    fig, ax = plt.subplots()
    # Reduce margins
    plt.margins(x=0.01)
    # Get cumuluative sum of variance explained
    cum_var_explained = np.cumsum(var_explained_ratio)
    # Plot cumulative sum
    ax.fill_between(range(len(cum_var_explained)), cum_var_explained,
                    alpha = 0.4, color = "tab:orange",
                    label = "Cum. Var.")
    ax.set_ylim(0, 1)
    # Plot actual proportions
    ax2 = ax.twinx()
    ax2.plot(range(len(var_explained_ratio)), var_explained_ratio,
             alpha = 1, color = "tab:blue", lw  = 4, ls = "--",
             label = "Var per PC")
    ax2.set_ylim(0, 0.005)
    
    # Add lines to indicate where good values of components may be
    ax.hlines(0.6, 0, var_explained_ratio.shape[0], color = "tab:green", lw = 3, alpha = 0.6, ls=":")
    ax.hlines(0.8, 0, var_explained_ratio.shape[0], color = "tab:green", lw = 3, alpha = 0.6, ls=":")
    # Plot both legends together
    lines, labels = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax2.legend(lines + lines2, labels + labels2)
    # Format axis as percentages
    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))
    ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1)) 
    
    # Add titles and labels
    ax.set_ylabel("Cum. Prop. of Variance Explained")
    ax2.set_ylabel("Prop. of Variance Explained per PC", rotation = 270, labelpad=30)
    ax.set_title("Variance Explained by Number of Principal Components")
    ax.set_xlabel("Number of Principal Components")
    

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a07680fb94e94c8e8fdfc51415501bbe~tplv-k3u1fbpfcp-zoom-1.image)

ğŸ“Œ t-SNEå¯è§†åŒ–
-----------

æˆ‘ä»¬è¿˜å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼Œå¯è§†åŒ–æ•°æ®åœ¨ä¸€ç³»åˆ—é™ç»´è¿‡ç¨‹ä¸­çš„å¯åˆ†ç¦»æ€§ã€‚t-SNEç®—æ³•æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ•ˆçš„éçº¿æ€§é™ç»´å¯è§†åŒ–æ–¹æ³•ï¼Œå€ŸåŠ©äºå®ƒï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ•°æ®ç»˜åˆ¶åœ¨äºŒç»´å¹³é¢è§‚å¯Ÿå…¶åˆ†æ•£ç¨‹åº¦ã€‚ä¸‹é¢çš„t-SNEå¯è§†åŒ–å±•ç¤ºäº†å½“æˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰1806ä¸ªç‰¹å¾æˆ–å°†å…¶å‡å°‘ä¸º 1000ã€500ã€100 ä¸ªä¸»æˆåˆ†æ—¶ï¼Œå¦‚æœå°†æ­Œè¯æ•°æ®æŠ•å½±åˆ°äºŒç»´ç©ºé—´ä¸­ä¼šæ˜¯ä»€ä¹ˆæ ·å­ã€‚

ä»£ç å¦‚ä¸‹ï¼š

    from sklearn.manifold import TSNE
    import seaborn as sns
    
    # Merge numeric labels with normalised audio data and lyric principal components
    tsne_processed = pd.concat([
        pd.Series(y, name = "genre"),
        pd.DataFrame(audio_features, columns=audio.columns),
        # Add prefix to make selecting pcs easier later on
        pd.DataFrame(lyric_pca).add_prefix("lyrics_pc_")
              ], axis = 1)
    
    # Get t-SNE values for a range of principal component cutoffs, 1806 is all PCs
    all_tsne = pd.DataFrame()
    for cutoff in ["1806", "1000", "500", "100"]:
        # Create t-SNE object
        tsne = TSNE(init = "random", learning_rate = "auto")
        # Fit on normalised features (excluding the y/label column)
        tsne_results = tsne.fit_transform(tsne_processed.loc[:, "audio_danceability":f"lyrics_pc_{cutoff}"])
        
        # neater graph
        if cutoff == "1806":
            cutoff = "All 1806"
        # Get results
        tsne_df = pd.DataFrame({"y":y,
                            "tsne-2d-one":tsne_results[:,0],
                           "tsne-2d-two":tsne_results[:,1],
                               "Cutoff":cutoff})
        # Store results
        all_tsne = pd.concat([all_tsne, tsne_df], axis = 0)
        
    # Plot gridplot
    g = sns.FacetGrid(all_tsne, col="Cutoff", hue = "y",
                    col_wrap = 2, height = 6,
                    palette=sns.color_palette("hls", 4),
                   )
    # Add plots
    g.map(sns.scatterplot, "tsne-2d-one", "tsne-2d-two", alpha = 0.3)
    # Add titles/legends
    g.fig.suptitle("t-SNE Plots vs Number of Principal Components Included", y = 1)
    g.add_legend()
    

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/665e536cda074b61b29b9634b33ce08b~tplv-k3u1fbpfcp-zoom-1.image)

ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›çœ‹åˆ°çš„æ˜¯ï¼Œåœ¨é™ç»´åˆ°æŸäº›ä¸»æˆåˆ†æ•°é‡ï¼ˆä¾‹å¦‚ cutoff = 1000ï¼‰æ—¶ï¼Œæµæ´¾å˜å¾—æ›´åŠ å¯åˆ†ç¦»ã€‚

ç„¶è€Œï¼Œä¸Šè¿° t-SNE å›¾çš„ç»“æœæ˜¾ç¤ºï¼ŒPCA è¿™ä¸€æ­¥ä¸åŒæ•°é‡çš„ä¸»æˆåˆ†å¹¶æ²¡æœ‰å“ªä¸ªä¼šè®©æ•°æ®æ ‡ç­¾æ›´å¯åˆ†ç¦»ã€‚

ğŸ“Œ è‡ªç¼–ç å™¨é™ç»´
---------

å®é™…ä¸Šæˆ‘ä»¬æœ‰ä¸åŒçš„æ–¹å¼å¯ä»¥å®Œæˆæ•°æ®é™ç»´ä»»åŠ¡ï¼Œåœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬æä¾›äº† PCAã€æˆªæ–­ SVD å’Œ Keras è‡ªç¼–ç å™¨ä¸‰ç§æ–¹å¼ä½œä¸ºå€™é€‰ï¼Œè°ƒæ•´é…ç½®å³å¯è¿›è¡Œé€‰æ‹©ã€‚

ä¸ºç®€æ´èµ·è§ï¼Œè‡ªåŠ¨ç¼–ç å™¨çš„ä»£ç å·²è¢«çœç•¥ï¼Œä½†å¯ä»¥åœ¨ `autoencode` å†…çš„åŠŸèƒ½ `custom_functions.py` ä¸­çš„æ–‡ä»¶åº“ã€‚

    # é€šç”¨åº“
    import pandas as pd
    import numpy as np
    # å»ºæ¨¡åº“
    from sklearn.model_selection import train_test_split
    from sklearn.decomposition import PCA, TruncatedSVD
    from sklearn.preprocessing import LabelEncoder, MinMaxScaler
    # ç¥ç»ç½‘ç»œ
    from keras.layers import Dense, Input, LeakyReLU, BatchNormalization
    from keras.callbacks import EarlyStopping
    from keras import Model
    
    # å®šä¹‰è‡ªç¼–ç å™¨
    def autoencode(lyric_tr, n_components):
        """Build, compile and fit an autoencoder for
        lyric data using Keras. Uses a batch normalised,
        undercomplete encoder with leaky ReLU activations.
        It will take a while to train.
        --------------------------------------------------
        lyric_tr = df of lyric training data
        n_components = int, number of output dimensions
        from encoder
        """
        n_inputs = lyric_tr.shape[1]
        # å®šä¹‰encoder
        visible = Input(shape=(n_inputs,))
    
        # encoderæ¨¡å—1
        e = Dense(n_inputs*2)(visible)
        e = BatchNormalization()(e)
        e = LeakyReLU()(e)
        # encoderæ¨¡å—2
        e = Dense(n_inputs)(e) 
        e = BatchNormalization()(e)
        e = LeakyReLU()(e)
        bottleneck = Dense(n_components)(e)
    
        # decoderæ¨¡å—1
        d = Dense(n_inputs)(bottleneck)
        d = BatchNormalization()(d)
        d = LeakyReLU()(d)
        # decoderæ¨¡å—2
        d = Dense(n_inputs*2)(d)
        d = BatchNormalization()(d)
        d = LeakyReLU()(d)
        # è¾“å‡ºå±‚
        output = Dense(n_inputs, activation='linear')(d)
        # å®Œæ•´çš„autoencoderæ¨¡å‹
        model = Model(inputs=visible, outputs=output)
    
        # ç¼–è¯‘
        model.compile(optimizer='adam', loss='mse')
        # å›è°ƒå‡½æ•°
        callbacks = EarlyStopping(patience = 20, restore_best_weights = True)
        # è®­ç»ƒæ¨¡å‹
        model.fit(lyric_tr, lyric_tr, epochs=200,
                            batch_size=16, verbose=1, validation_split=0.2,
                 callbacks = callbacks)
        
        # åœ¨é™ç»´é˜¶æ®µï¼Œæˆ‘ä»¬åªç”¨encoderéƒ¨åˆ†å°±å¯ä»¥(å¯¹æ•°æ®è¿›è¡Œå‹ç¼©)
        encoder = Model(inputs=visible, outputs=bottleneck)
    
        return encoder
    
    # æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼Œä¸»è¦æ˜¯å¯¹ç‰¹å¾åˆ—è¿›è¡Œé™ç»´ï¼Œæ ‡ç­¾åˆ—è¿›è¡Œç¼–ç 
    def pre_process(train = pd.DataFrame,
                    test = pd.DataFrame,
                    reduction_method = "pca",
                    n_components = 400):
        # åˆ‡åˆ†Xå’Œy
        y_train = train.playlist_genre
        y_test = test.playlist_genre
        X_train = train.drop(columns = "playlist_genre")
        X_test = test.drop(columns = "playlist_genre")
        
        # æ ‡ç­¾ç¼–ç ä¸ºæ•°å­—
        label_encoder = LabelEncoder()
        label_train = label_encoder.fit_transform(y_train)
        label_test = label_encoder.transform(y_test)
    
        # å¯¹æ•°æ®è¿›è¡Œå¹…åº¦ç¼©æ”¾å¤„ç†
        scaler = MinMaxScaler()
        X_norm_tr = scaler.fit_transform(X_train)
        X_norm_te = scaler.transform(X_test)
    
        # é‡å»ºæ•°æ®
        X_norm_tr = pd.DataFrame(X_norm_tr, columns = X_train.columns)
        X_norm_te = pd.DataFrame(X_norm_te, columns = X_test.columns)
    
        # modeå’Œkeyéƒ½è®¾å®šä¸ºç±»åˆ«å‹
        X_norm_tr["audio_mode"] = X_train["audio_mode"].astype("category").reset_index(drop = True)
        X_norm_tr["audio_key"] = X_train["audio_key"].astype("category").reset_index(drop = True)
        X_norm_te["audio_mode"] = X_test["audio_mode"].astype("category").reset_index(drop = True)
        X_norm_te["audio_key"] = X_test["audio_key"].astype("category").reset_index(drop = True)
        
        # æ­Œè¯ç‰¹å¾
        lyric_tr = X_norm_tr.loc[:, "lyrics_aah":]
        lyric_te = X_norm_te.loc[:, "lyrics_aah":]
    
        # å¦‚æœä½¿ç”¨PCAé™ç»´
        if reduction_method == "pca":
            pca = PCA(n_components)
            # æ‹Ÿåˆè®­ç»ƒé›†
            reduced_tr = pd.DataFrame(pca.fit_transform(lyric_tr)).add_prefix("lyrics_pca_")
            # å¯¹æµ‹è¯•é›†å˜æ¢ï¼ˆé™ç»´ï¼‰
            reduced_te = pd.DataFrame(pca.transform(lyric_te)).add_prefix("lyrics_pca_")
        
        # å¦‚æœä½¿ç”¨SVDé™ç»´
        if reduction_method == "svd":
            svd = TruncatedSVD(n_components)
            # æ‹Ÿåˆè®­ç»ƒé›†
            reduced_tr = pd.DataFrame(svd.fit_transform(lyric_tr)).add_prefix("lyrics_svd_")
            # å¯¹æµ‹è¯•é›†å˜æ¢ï¼ˆé™ç»´ï¼‰
            reduced_te = pd.DataFrame(svd.transform(lyric_te)).add_prefix("lyrics_svd_")
        
        # å¦‚æœä½¿ç”¨è‡ªç¼–ç å™¨é™ç»´ï¼ˆæ³¨æ„ï¼Œç¥ç»ç½‘ç»œçš„è®­ç»ƒæ—¶é—´ä¼šé•¿ä¸€ç‚¹ï¼Œè¦è€å¿ƒç­‰å¾…ï¼‰
        if reduction_method == "keras":
            # æ„å»ºè‡ªç¼–ç å™¨
            encoder = autoencode(lyric_tr, n_components)
            
            # é€šè¿‡ç¼–ç å™¨éƒ¨åˆ†è¿›è¡Œæ•°æ®é™ç»´
            reduced_tr = pd.DataFrame(encoder.predict(lyric_tr)).add_prefix("lyrics_keras_")
            reduced_te = pd.DataFrame(encoder.predict(lyric_te)).add_prefix("lyrics_keras_")
    
            
            
        # åˆå¹¶é™ç»´åçš„æ­Œè¯ç‰¹å¾ä¸éŸ³é¢‘ç‰¹å¾
        X_norm_tr = pd.concat([X_norm_tr.loc[:, :"audio_duration_ms"],
                              reduced_tr
                              ], axis = 1)
    
        X_norm_te = pd.concat([X_norm_te.loc[:, :"audio_duration_ms"],
                               reduced_te
                               ], axis = 1)
    
    
        return X_norm_tr, label_train, X_norm_te, label_test, label_encoder
    
    
    # åˆ†å±‚åˆ‡åˆ†æ•°æ®
    train_raw, test_raw = train_test_split(data, test_size = 0.2,
                                           shuffle = True, random_state = 42, # random, reproducible split
                                           stratify = data.playlist_genre)
    # è®¾å®šé™ç»´æœ€ç»ˆç»´åº¦
    n_components = 500
    # é€‰æ‹©é™ç»´æ–¹æ³•ï¼Œå€™é€‰: "pca", "svd", "keras"
    reduction_method = "pca"
    
    # å®Œæ•´çš„æ•°æ®é¢„å¤„ç†
    X_train, y_train, X_test, y_test, label_encoder = pre_process(train_raw, test_raw,
                                                          reduction_method = reduction_method,
                                                         n_components = n_components)
    

ä¸Šè¿°è¿‡ç¨‹ä¹‹åæˆ‘ä»¬å·²ç»å®Œæˆå¯¹æ•°æ®çš„æ ‡å‡†åŒ–ã€ç¼–ç è½¬æ¢å’Œé™ç»´ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨å®ƒè¿›è¡Œå»ºæ¨¡ã€‚

ğŸ’¡ å»ºæ¨¡å’Œè¶…å‚æ•°ä¼˜åŒ–
===========

ğŸ“Œ æ„å»ºæ¨¡å‹
-------

åœ¨å®é™…å»ºæ¨¡ä¹‹å‰ï¼Œæˆ‘ä»¬è¦å…ˆ**é€‰å®šä¸€ä¸ªè¯„ä¼°æŒ‡æ ‡**æ¥è¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ï¼Œä¹Ÿæ–¹ä¾¿æŒ‡å¯¼è¿›ä¸€æ­¥çš„ä¼˜åŒ–ã€‚ç”±äºæˆ‘ä»¬æ•°æ®æœ€ç»ˆçš„æ ‡ç­¾ã€æµæ´¾/ç±»åˆ«ã€ç•¥æœ‰ä¸å¹³è¡¡ï¼Œ**å®è§‚ F1 åˆ†æ•°ï¼ˆmacro f1-scoreï¼‰** å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¹³ç­‰åœ°è¯„ä¼°äº†ç±»åˆ«çš„è´¡çŒ®ã€‚æˆ‘ä»¬åœ¨ä¸‹é¢å¯¹è¿™ä¸ªè¯„ä¼°å‡†åˆ™è¿›è¡Œå®šä¹‰ï¼Œä¹Ÿæ•²å®š LightGBM æ¨¡å‹çš„éƒ¨åˆ†è¶…å‚æ•°ã€‚

    from sklearn.metrics import f1_score
    
    # å®šä¹‰è¯„ä¼°å‡†åˆ™(Macro F1)
    def lgb_f1_score(preds, data):
        labels = data.get_label()
        preds = preds.reshape(5, -1).T
        preds = preds.argmax(axis = 1)
        f_score = f1_score(labels , preds,  average = 'macro')
        return 'f1_score', f_score, True
    
    # ç”¨äºç¼–è¯‘çš„å‚æ•°
    fixed_params = {
            'objective': 'multiclass',
            'metric': "None",   # æˆ‘ä»¬è‡ªå®šä¹‰çš„f1-scoreå¯ä»¥åº”ç”¨
            'num_class': 5,
            'verbosity': -1,
    }
    

LightGBM å¸¦æœ‰å¤§é‡å¯è°ƒè¶…å‚æ•°ï¼Œè¿™äº›è¶…å‚æ•°å¯¹äºæœ€ç»ˆæ•ˆæœå½±å“å¾ˆå¤§ã€‚

> å…³äº LightGBM çš„è¶…å‚æ•°ç»†èŠ‚è¯¦ç»†è®²è§£ï¼Œæ¬¢è¿å¤§å®¶æŸ¥é˜… [ShowMeAI](https://www.showmeai.tech/) çš„æ–‡ç« ï¼š
> 
> ğŸ“˜[**æœºå™¨å­¦ä¹ å®æˆ˜(5) | LightGBMå»ºæ¨¡åº”ç”¨è¯¦è§£**](https://www.showmeai.tech/article-detail/205)

ä¸‹é¢æˆ‘ä»¬ä¼šåŸºäºOptunaè¿™ä¸ªå·¥å…·åº“å¯¹ LightGBM çš„è¶…å‚æ•°è¿›è¡Œè°ƒä¼˜ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ `param` **å®šä¹‰è¶…å‚æ•°çš„æœç´¢ç©ºé—´**ï¼Œåœ¨æ­¤åŸºç¡€ä¸Š Optuna ä¼šè¿›è¡Œä¼˜åŒ–å’Œè¶…å‚æ•°çš„é€‰æ‹©ã€‚

    
    # å»ºæ¨¡
    from sklearn.model_selection import StratifiedKFold
    import lightgbm as lgb
    from optuna.integration import LightGBMPruningCallback
    
    # å®šä¹‰ç›®æ ‡å‡½æ•°
    def objective(trial, X, y):    
        # å€™é€‰è¶…å‚æ•°
        param = {**fixed_params,
            'boosting_type': 'gbdt',
            'num_leaves': trial.suggest_int('num_leaves', 2, 3000, step = 20),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.99, step = 0.05),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.99, step = 0.05),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            "n_estimators": trial.suggest_int("n_estimators", 200, 5000),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 5, 2000, step=5),
            "lambda_l1": trial.suggest_float("lambda_l1", 1e-8, 10.0, log=True),
            "lambda_l2": trial.suggest_float("lambda_l2", 1e-8, 10.0, log=True),
            "min_gain_to_split": trial.suggest_float("min_gain_to_split", 0, 10),
            "max_bin": trial.suggest_int("max_bin", 200, 300),
        }
        
        # æ„å»ºåˆ†å±‚äº¤å‰éªŒè¯
        cv = StratifiedKFold(n_splits = 5, shuffle = True)
        # 5ç»„å¾—åˆ†
        cv_scores = np.empty(5)
        
        # åˆ‡åˆ†ä¸ºKä¸ªæ•°æ®ç»„ï¼Œè½®ç•ªä½œä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡Œå®éªŒ
        for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):
            # æ•°æ®åˆ‡åˆ†
            X_train_cv, X_test_cv = X.iloc[train_idx], X.iloc[test_idx]
            y_train_cv, y_test_cv = y[train_idx], y[test_idx]
    
            # è½¬ä¸ºlightgbmçš„Datasetæ ¼å¼
            train_data = lgb.Dataset(X_train_cv, label = y_train_cv, categorical_feature="auto")
            val_data = lgb.Dataset(X_test_cv, label = y_test_cv,  categorical_feature="auto",
                                  reference = train_data)
            
            # å›è°ƒå‡½æ•°
            callbacks = [
                LightGBMPruningCallback(trial, metric = "f1_score"),
                         # é—´æ­‡è¾“å‡ºä¿¡æ¯
                        lgb.log_evaluation(period = 100),
                         # æ—©åœæ­¢ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
                        lgb.early_stopping(50)]
    
            # è®­ç»ƒæ¨¡å‹
            model = lgb.train(params = param,  train_set = train_data,
                              valid_sets = val_data,   
                              callbacks = callbacks,
                              feval = lgb_f1_score # è‡ªå®šä¹‰è¯„ä¼°å‡†åˆ™
                             )
            
            # é¢„ä¼°
            preds = np.argmax(model.predict(X_test_cv), axis = 1)
            # è®¡ç®—f1-score
            cv_scores[idx] = f1_score(y_test_cv, preds, average = "macro")
    
        return np.mean(cv_scores)
    

ğŸ“Œ è¶…å‚æ•°ä¼˜åŒ–
--------

æˆ‘ä»¬åœ¨ä¸Šé¢å®šä¹‰å®Œäº†ç›®æ ‡å‡½æ•°ï¼Œç°åœ¨å¯ä»¥ä½¿ç”¨ Optuna æ¥è°ƒä¼˜æ¨¡å‹çš„è¶…å‚æ•°äº†ã€‚

    # è¶…å‚æ•°ä¼˜åŒ–
    import optuna
    
    # å®šä¹‰Optunaçš„å®éªŒæ¬¡æ•°
    n_trials = 200
    # æ„å»ºOptuna studyå»è¿›è¡Œè¶…å‚æ•°æ£€ç´¢ä¸è°ƒä¼˜
    study = optuna.create_study(direction = "maximize", # æœ€å¤§åŒ–äº¤å‰éªŒè¯çš„F1å¾—åˆ†
                                study_name = "LGBM Classifier",
                               pruner=optuna.pruners.HyperbandPruner())
    func = lambda trial: objective(trial, X_train, y_train)
    study.optimize(func, n_trials = n_trials)
    

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ ğŸ“˜[**Optuna çš„å¯è§†åŒ–æ¨¡å—**](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html) å¯¹**ä¸åŒè¶…å‚æ•°ç»„åˆçš„æ€§èƒ½**è¿›è¡Œå¯è§†åŒ–æŸ¥çœ‹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `plot_param_importances(study)` æŸ¥çœ‹å“ªäº›è¶…å‚æ•°å¯¹æ¨¡å‹æ€§èƒ½/å½±å“ä¼˜åŒ–æœ€é‡è¦ã€‚

    plot_param_importances(study)
    

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/40eb398ac1294712a1b8f4da13992745~tplv-k3u1fbpfcp-zoom-1.image)

æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ `plot_parallel_coordinate(study)`æŸ¥çœ‹å°è¯•äº†å“ªäº›è¶…å‚æ•°ç»„åˆ/èŒƒå›´å¯ä»¥å¸¦æ¥é«˜è¯„ä¼°ç»“æœå€¼ï¼ˆå¥½çš„æ•ˆæœæ€§èƒ½ï¼‰ã€‚

    plot_parallel_coordinate(study)
    

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/32ec93455db54f6299903931dc7bd6a6~tplv-k3u1fbpfcp-zoom-1.image)

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `plot_optimization_history` æŸ¥çœ‹å†å²æƒ…å†µã€‚

    plot_optimization_history(study)
    

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c301fb35f0e94857a64f2dd2cb1539d7~tplv-k3u1fbpfcp-zoom-1.image)

**åœ¨Optunaå®Œæˆè°ƒä¼˜ä¹‹åï¼š**

*   æœ€å¥½çš„è¶…å‚æ•°å­˜å‚¨åœ¨ `study.best_params` å±æ€§ä¸­ã€‚æˆ‘ä»¬æŠŠæ¨¡å‹çš„æœ€ç»ˆå‚æ•° `params` å®šä¹‰ä¸º `params = {**fixed_params, **study.best_params}` å³å¯ï¼Œå¦‚åç»­çš„ä»£ç æ‰€ç¤ºã€‚
*   å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ç¼©å°æœç´¢ç©ºé—´/è¶…å‚æ•°èŒƒå›´ï¼Œè¿›ä¸€æ­¥åšç²¾ç¡®çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚

    # æœ€ä½³æ¨¡å‹å®éªŒ
    cv = StratifiedKFold(n_splits = 5, shuffle = True)
    # 5ç»„å¾—åˆ†
    cv_scores = np.empty(5)
    
    # åˆ‡åˆ†ä¸ºKä¸ªæ•°æ®ç»„ï¼Œè½®ç•ªä½œä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡Œå®éªŒ
    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):
        # æ•°æ®åˆ‡åˆ†
        X_train_cv, X_test_cv = X.iloc[train_idx], X.iloc[test_idx]
        y_train_cv, y_test_cv = y[train_idx], y[test_idx]
    
        # è½¬ä¸ºlightgbmçš„Datasetæ ¼å¼
        train_data = lgb.Dataset(X_train_cv, label = y_train_cv, categorical_feature="auto")
        val_data = lgb.Dataset(X_test_cv, label = y_test_cv,  categorical_feature="auto",
                              reference = train_data)
        
        # å›è°ƒå‡½æ•°
        callbacks = [
            LightGBMPruningCallback(trial, metric = "f1_score"),
                     # é—´æ­‡è¾“å‡ºä¿¡æ¯
                    lgb.log_evaluation(period = 100),
                     # æ—©åœæ­¢ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
                    lgb.early_stopping(50)]
    
        # è®­ç»ƒæ¨¡å‹
        model = lgb.train(params = {**fixed_params, **study.best_params},  train_set = train_data,
                          valid_sets = val_data,   
                          callbacks = callbacks,
                          feval = lgb_f1_score # è‡ªå®šä¹‰è¯„ä¼°å‡†åˆ™
                         )
        
        # é¢„ä¼°
        preds = np.argmax(model.predict(X_test_cv), axis = 1)
        # è®¡ç®—f1-score
        cv_scores[idx] = f1_score(y_test_cv, preds, average = "macro")
    

ğŸ’¡ æœ€ç»ˆè¯„ä¼°
=======

é€šè¿‡ä¸Šè¿°è¿‡ç¨‹æˆ‘ä»¬å°±è·å¾—äº†æœ€ç»ˆæ¨¡å‹ï¼Œè®©æˆ‘ä»¬æ¥è¯„ä¼°ä¸€ä¸‹å§ï¼

    
    # é¢„ä¼°ä¸è¯„ä¼°è®­ç»ƒé›†
    train_preds = model.predict(X_train)
    train_predictions = np.argmax(train_preds, axis = 1)
    train_error = f1_score(y_train, train_predictions, average = "macro")
    
    # äº¤å‰éªŒè¯ç»“æœ
    cv_error = np.mean(cv_scores)
    
    # è¯„ä¼°æµ‹è¯•é›†
    test_preds = model.predict(X_test)
    test_predictions = np.argmax(test_preds, axis = 1)
    test_error = f1_score(y_test, test_predictions, average = "macro")
    
    # å­˜å‚¨è¯„ä¼°ç»“æœ
    results = pd.DataFrame({"n_components": n_components,
                            "reduction_method": reduction_method,
                            "train_error": train_error,
                            "cv_error": cv_error,
                            "test_error": test_error,
                            "n_trials": n_trials
                           }, index = [0])
    

æˆ‘ä»¬å¯ä»¥å®éªŒå’Œæ¯”è¾ƒä¸åŒçš„é™ç»´æ–¹æ³•ã€é™ç»´ç»´åº¦ï¼Œå†è°ƒå‚æŸ¥çœ‹æ¨¡å‹æ•ˆæœã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨æˆ‘ä»¬å½“å‰çš„å°è¯•ä¸­ï¼Œ**PCAé™ç»´åˆ° 400 ç»´**äº§å‡ºæœ€å¥½çš„æ¨¡å‹ â€”â€”macro f1-score ä¸º66.48%ã€‚

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d810fac243e94b188b93a2ebb93b59b4~tplv-k3u1fbpfcp-zoom-1.image)

ğŸ’¡ æ€»ç»“
=====

åœ¨æœ¬ç¯‡å†…å®¹ä¸­ï¼Œ [ShowMeAI](https://www.showmeai.tech/) å±•ç¤ºäº†åŸºäºæ­Œæ›²ä¿¡æ¯ä¸æ–‡æœ¬å¯¹å…¶è¿›è¡Œã€æµæ´¾ã€åˆ†ç±»çš„è¿‡ç¨‹ï¼ŒåŒ…å«å¯¹æ–‡æœ¬æ•°æ®çš„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹å»ºæ¨¡å’Œè¶…å‚æ•°ä¼˜åŒ–ç­‰ã€‚å¤§å®¶å¯ä»¥æŠŠæ•´ä¸ªpipelineä½œä¸ºä¸€ä¸ªæ¨¡æ¿æ¥åº”ç”¨åœ¨å…¶ä»–ä»»åŠ¡å½“ä¸­ã€‚

å‚è€ƒèµ„æ–™
====

*   ğŸ“˜ **å›¾è§£æ•°æ®åˆ†æï¼šä»å…¥é—¨åˆ°ç²¾é€šç³»åˆ—æ•™ç¨‹**ï¼š[https://www.showmeai.tech/tutorials/3](https://www.showmeai.tech/tutorials/33)
*   ğŸ“˜ **æ•°æ®ç§‘å­¦å·¥å…·åº“é€ŸæŸ¥è¡¨ | Pandas é€ŸæŸ¥è¡¨**ï¼š[https://www.showmeai.tech/article-detail/101](https://www.showmeai.tech/article-detail/101)
*   ğŸ“˜ **å›¾è§£æœºå™¨å­¦ä¹ ç®—æ³• | é™ç»´ç®—æ³•è¯¦è§£**ï¼š[https://www.showmeai.tech/article-detail/198](https://www.showmeai.tech/article-detail/198)
*   ğŸ“˜ **å›¾è§£æœºå™¨å­¦ä¹ ç®—æ³• | LightGBMæ¨¡å‹è¯¦è§£**ï¼š[https://www.showmeai.tech/article-detail/195](https://www.showmeai.tech/article-detail/195)
*   ğŸ“˜ **æœºå™¨å­¦ä¹ å®æˆ˜ | LightGBMå»ºæ¨¡åº”ç”¨è¯¦è§£**ï¼š[https://www.showmeai.tech/article-detail/205](https://www.showmeai.tech/article-detail/205)
*   ğŸ“˜ [**Optuna çš„å¯è§†åŒ–æ¨¡å—**](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html)
*   ğŸ“˜ Akiba,T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019, July). Optuna: A next-generation hyperparameter optimization framework. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_ (pp. 2623â€“2631).
*   ğŸ“˜ [**Autoencoder Feature Extractions**](https://machinelearningmastery.com/autoencoder-for-classification/)
*   ğŸ“˜ [**Kagglerâ€™s Guide to LightGBM Hyperparameter Tuning with Optuna in 2021**](https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)
*   ğŸ“˜ [**You Are Missing Out on LightGBM. It Crushes XGBoost in Every Aspect**](https://towardsdatascience.com/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997)

[![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e9190f41b8de4af38c8a1a0c96f0513b~tplv-k3u1fbpfcp-zoom-1.image)](https://www.showmeai.tech/)