---
layout: post
title: "论文解读（CDTrans）《CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation》"
date: "2022-12-05T05:15:45.770Z"
---
论文解读（CDTrans）《CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation》
===================================================================================

论文信息
====

> 论文标题：CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation  
> 论文作者：Tongkun Xu, Weihua Chen, Pichao Wang, Fan Wang, Hao Li, Rong Jin  
> 论文来源：ICLR 2022  
> 论文地址：[download](https://arxiv.org/abs/2109.06165)   
> 论文代码：[download](https://github.com/CDTrans/CDTrans)

1 Introduction
==============

　　无监督域自适应（Unsupervised domain adaptation，UDA）的目的是将从标记源域学习到的知识转移到不同的未标记目标域。

　　UDA 方法：  
　　① Domain-level UDA ，通过将源域和目标域在不同尺度水平上进入相同的分布来缓解源域之间的分布差异；  
　　② fine-grained category-level UDA，通过将目标样本推向每个类别中的源样本的分布，对源域数据和目标域数据之间的每个类别分布进行对齐；（仍然存在标签噪声问题）

2 Method
========

2.1 The cross attention in Transformer
--------------------------------------

　　传统的方法给目标域打伪标签的过程中存在噪声，由于噪声的存在，需要对齐的源域和目标域的图片可能不属于同一类，强行对其可能产生很大的负面影响。而本文经过实验发现 Transformer 中的 CrossAttention 可以有效的避免噪声给对其造成的影响，CrossAttention 更多的关注源域和目标域中图片中的相似信息。换句话说，即使图片对不属于同一类，被拉近的也只会是两者相似的部分。因此，CDTrans 具有一定的抗噪能力。

　　由于在 UDA 任务中，目标域是没有标签的。因此只能借鉴伪标签的思路，来生成潜在的可能属于同一个 ID 的样本对。但是，伪标签生成的样本对中不可避免的会存在噪声。这时，本文发现 Cross Attention 对样本对中的噪声有着很强的鲁棒性。本文分析这主要是因为 Attention 机制所决定的，Attention 的 weight 更多的会关注两张图片相似的部分，而忽略其不相似的部分。如果源域图片和目标域图片不属于同一个类别的话，比如Figure 1.a“Car vs. Truck”的例子，Attention 的 weight 主要集中于两个图片中相似部分的对齐（比如轮胎），而对其他部位的对齐会给很小的 weight。

　　![](https://img2023.cnblogs.com/blog/1664108/202212/1664108-20221205012054636-1425953594.png)

　　换句话说，Cross Attention 没有在使劲拉近对齐小轿车和卡车，而更多的是在努力对齐两个图片中的轮胎。一方面，Cross Attention 避免了强行拉近小轿车和卡车，减弱了噪声样本对 UDA 训练的影响；另一方面，拉近不同域的轮胎，在一定程度上可能帮助到目标域轮胎的识别。

　　自注意力（self-attention）：

　　　　$\\operatorname{Attn}\_{\\text {self }}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V})=\\operatorname{softmax}\\left(\\frac{\\boldsymbol{Q} \\boldsymbol{K}^{T}}{\\sqrt{d\_{k}}}\\right) \\boldsymbol{V}\\quad\\quad(1)$

 　　交叉注意力（cross-attention）：

　　　　$\\operatorname{Attn}\_{\\text {cross }}\\left(\\boldsymbol{Q}\_{s}, \\boldsymbol{K}\_{t}, \\boldsymbol{V}\_{t}\\right)=\\operatorname{softmax}\\left(\\frac{\\boldsymbol{Q}\_{s} \\boldsymbol{K}\_{t}^{T}}{\\sqrt{d\_{k}}}\\right) \\boldsymbol{V}\_{t}\\quad\\quad(2)$

2.2 Two way center-aware pseudo labeling
----------------------------------------

### 2.2.1 Two way labeling

　　为了构建交叉注意模块的训练对，一种直观的方法是，对源域中的每一幅图像，我们设法从目标域找到最相似的图像。所选数据对的设置 $\\mathbb{P}\_{S}$ 为：

　　　　$\\mathbb{P}\_{S}=\\left\\{(s, t) \\mid t=\\underset{k}{\\text{min}} \\quad d\\left(\\boldsymbol{f}\_{s}, \\boldsymbol{f}\_{k}\\right), \\forall k \\in T, \\forall s \\in S\\right\\}\\quad\\quad\\quad(3)$

　　其中，$S$、$T$ 分别为源数据和目标数据。$d\\left(\\boldsymbol{f}\_{i}, \\boldsymbol{f}\_{j}\\right)$ 表示图像 $i$ 和图像 $j$ 的特征之间的距离。

　　这种策略的优点是充分利用源数据，而其弱点显然是只涉及到目标数据的一部分。为了消除目标数据的这种训练偏差，我们从相反的方式引入了更多的对 $\\mathbb{P}$，包括所有目标数据及其在源域中最相似的图像。

　　　　$\\mathbb{P}\_{T}=\\left\\{(s, t) \\mid s=\\underset{k}{\\text{min}} \\quad d\\left(\\boldsymbol{f}\_{t}, \\boldsymbol{f}\_{k}\\right), \\forall t \\in T, \\forall k \\in S\\right\\}\\quad\\quad\\quad(4)$

　　因此，最终的 $\\mathbb{P}$ 是两个集的并集，即 $\\mathbb{P}=\\left\\{\\mathbb{P}\_{S} \\cup \\mathbb{P}\_{T}\\right\\}$，使训练对包括所有的源数据和目标数据。

### 2.2.2 Center-Aware filtering

　　$\\mathbb{P}$ 中的 pair 是基于两个域图像的特征相似性构建的，因此 pair 的伪标签的准确性高度依赖于特征相似性。

　　本文发现，源数据的预训练模型也有助于进一步提高精度。首先，我们通过将所有**目标数据**送到通过预先训练的模型，从分类器得到它们在源类别上的概率分布 $\\delta$。这些分布可以通过加权 k-means 聚类来计算目标域内每个类别的初始中心：

　　　　${\\large \\boldsymbol{c}\_{k}=\\frac{\\sum\_{t \\in T} \\delta\_{t}^{k} \\boldsymbol{f}\_{t}}{\\sum\_{t \\in T} \\delta\_{t}^{k}}}\\quad\\quad(5) $  
　　其中，$\\delta\_{t}^{k}$ 表示图像 $t$ 在类别 $k$ 上的概率。目标数据的伪标签可以通过最近邻分类器产生：

　　　　$y\_{t}=\\arg   \\underset{k}{\\text{min}} \\; d\\left(\\boldsymbol{c}\_{k}, \\boldsymbol{f}\_{t}\\right)  \\quad\\quad(6) $

　　其中，$t \\in T$ 和 $d(i, j)$ 是特征 $i$ 和 $j$ 的距离。基于伪标签，我们可以计算出新的中心：

　　　　${\\normalsize \\boldsymbol{c}\_{k}^{\\prime}=\\frac{\\sum\_{t \\in T} \\mathbb{1}\\left(y\_{t}=k\\right) \\boldsymbol{f}\_{t}}{\\sum\_{t \\in T} \\mathbb{1}\\left(y\_{t}=k\\right)}}  \\quad\\quad(7) $

2.3 CDTrans:Cross-Domain Transformer
------------------------------------

     框架如下：

　　![](https://img2023.cnblogs.com/blog/1664108/202212/1664108-20221205120357172-2096688849.png)

　　上述 CDTrans 框架包括三个权重共享的 transformer ，分别是 source branch, source-target branch, target branch 。

　　输入对中的源图像和目标图像分别被发送到 source branch 和 target branch 。在这两个分支中，self-attention 涉及到学习特定领域的表示。并利用 softmax cross-entropy loss 进行分类训练。值得注意的是，由于两个图像的相同标签，所有三个分支共享相同的分类器。交叉注意力模块被导入到 source-target branch 中。source-target branch 的输入来自其他两个分支。在第 $N$ 层中，交叉注意模块的 query 来自于source branch 的第 $N$ 层中的查询，而 key 和 value 来自于  target branch 的查询。source-target branch 的特征不仅对齐了两个域的分布，而且由于交叉注意模块，对输入对中的噪声具有鲁棒性。因此，本文使用 source-target branch 的输出来指导目标分支的训练。具体来说，source-target branch 和目标分支分别表示为 teacher 和 student 。本文将分类器在 source-target branch 中的概率分布作为一个软标签，可以通过蒸馏损失来进一步监督目标分支

　　　　${\\large L\_{d t l}=\\sum\\limits\_{k} q\_{k} \\log p\_{k}}  \\quad\\quad(8) $

 　　其中，$q\_{k}$ 和 $p\_{k}$ 分别是 source-target branch 和 target branch 的概率。

　　在推理期间，只使用目标分支。输入值为来自测试数据的图像，只触发目标数据流，即 Fig.2 中的蓝线。利用其分类器的输出作为最终的预测标签。 

因上求缘，果上努力~~~~ 作者：[加微信X466550探讨](https://www.cnblogs.com/BlairGrowing/)，转载请注明原文链接：[https://www.cnblogs.com/BlairGrowing/p/16951176.html](https://www.cnblogs.com/BlairGrowing/p/16951176.html)