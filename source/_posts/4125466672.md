---
layout: post
title: "支持向量机分类算法"
date: "2022-09-11T19:16:20.815Z"
---
支持向量机分类算法
=========

支持向量机SVM

支持向量机原理

　　1.寻求最有分类边界

　　　　正确：对大部分样本可以正确的划分类别

　　　　泛化：最大化支持向量间距

　　　　公平：与支持向量等距

　　　　简单：线性、直线或平面，分割超平面

　　2.基于核函数的生维变换

　　　　通过名为核函数的特征变换，增加新的特征，使得低维度的线性不可分问题变为高维度空间中线性可分问题。

一、引论

　　使用[SVM](https://so.csdn.net/so/search?q=SVM&spm=1001.2101.3001.7020)支持向量机一般用于分类，得到低错误率的结果。SVM能够对训练集意外的数据点做出很好的分类决策。那么首先我们应该从数据层面上去看SVM到底是如何做决策的，这里来看这样一串数据集集合在二维平面坐标系上描绘的图：

![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911185555445-1811538668.png)

　　现在我们需要考虑，是否能够画出一条直线将圆形点和星星点分开。像first第一张图片来看，圆点和星点就分的很开，很容易就可以在图中画出一条直线将两组数据分开。而看第二张图片，圆点和星点几乎都聚合在一起，要区分的话十分困难。

　　我们要划线将他们区分开来的话，有有无数条可以画，但是我们难以找到一条最好区分度最高的线条将它们几乎完全区分。那么在此我们需要了解两个关于数据集的基本概念：

二、理论铺垫

**线性可分性（linear separability）**

![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911185555646-217923615.png)

　　而对[机器学习](https://so.csdn.net/so/search?q=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&spm=1001.2101.3001.7020)来说，涉及的多是高维空间（多维度）的数据分类，高维空间的SVM，即为超平面。机器学习的最终目的就是要找到最合适的（也即最优的）一个分类超平面（Hyper plane），从而应用这个最优分类超平面将特征数据很好地区分为两类。

![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911185555651-1857525993.png)

**决策边界**

　　SVM是一种优化的[分类算法](https://so.csdn.net/so/search?q=%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95&spm=1001.2101.3001.7020)，其动机是寻找一个最佳的决策边界，使得从决策边界与各组数据之间存在margin，并且需要使各侧的margin最大化。那么这个决策边界就是不同类之间的界限。

![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911185555170-1231932878.png)

　　总而言之：在具有两个类的统计分类问题中，决策边界或决策表面是超平面，其将基础向量空间划分为两个集合，一个集合。 [分类器](https://so.csdn.net/so/search?q=%E5%88%86%E7%B1%BB%E5%99%A8&spm=1001.2101.3001.7020)将决策边界一侧的所有点分类为属于一个类，而将另一侧的所有点分类为属于另一个类。

**支持向量（support [vector](https://so.csdn.net/so/search?q=vector&spm=1001.2101.3001.7020)）**

　　在了解了超平面和决策边界我们发现SVM的核心任务是找到一个超平面作为决策边界。那么满足该条件的决策边界实际上构造了2个平行的超平面作为间隔边界以判别样本的分类：

　　![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911183454061-918293742.png)

 ![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911185555284-438718964.png)

**核方法**

![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911185555541-114479855.png)

　　![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911183527312-224772319.png)

　　以回避内积的显式计算。

常见的核函数：

![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911185555247-1134553078.png)

kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'

　　当多项式核的阶为1时，其被称为线性核，对应的非线性分类器退化为线性分类器。RBF核也被称为高斯核（Gaussian kernel），其对应的映射函数将样本空间映射至无限维空间。

**SMO序列最小优化算法**

![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911185555092-896213051.png)

                                                                              ![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911183635118-1706597528.png)

![](https://img2022.cnblogs.com/blog/2958730/202209/2958730-20220911185555577-1050649473.png) 

**三、****Python sklearn****代码实现：**

**sklearn.svm.SVC****语法格式为：**

class sklearn.svm.SVC(  \*, 
                        C\=1.0, 
                        kernel\='rbf',
                        degree\=3, 
                        gamma\='scale', 
                        coef0\=0.0, 
                        shrinking\=True, 
                        probability\=False, 
                        tol\=0.001, 
                        cache\_size\=200, 
                        class\_weight\=None, 
                        verbose\=False, 
                        max\_iter\=- 1, 
                        decision\_function\_shape\='ovr', 
                        break\_ties\=False, 
                        random\_state\=None)

**基于鸢尾花数据的实现及解释**

**代码如下：**

 1 # 导入模块
 2 import numpy as np 3 import matplotlib.pyplot as plt 4 from sklearn import svm, datasets 5 from sklearn.model\_selection import train\_test\_split 6 
 7 # 鸢尾花数据
 8 iris = datasets.load\_iris()         #原始数据
 9 feature = iris.data\[:, :2\] # 为便于绘图仅选择2个特征（根据前两列数据和结果进行分类）
10 target = iris.target
11 
12 #数组分组训练数据和测试数据
13 x\_train,x\_test,y\_train,y\_test=train\_test\_split(feature,target,test\_size=0.2,random\_state=2020)
14 
15 # 测试样本（绘制分类区域）,我们数据选了两列即就是两个特征，所以这里有xlist1，xlist2
16 xlist1 = np.linspace(x\_train\[:, 0\].min(), x\_train\[:, 0\].max(), 200)
17 xlist2 = np.linspace(x\_train\[:, 1\].min(), x\_train\[:, 1\].max(), 200)
18 XGrid1, XGrid2 = np.meshgrid(xlist1, xlist2)
19 # 实例化一个svm模型，非线性SVM：RBF核，超参数为0.5，正则化系数为1，SMO迭代精度1e-5, 内存占用1000MB
20 svc = svm.SVC(kernel='rbf', C=1, gamma=0.5, tol=1e-5, cache\_size=1000)
21 drill=svc.fit(x\_train,y\_train)
22 
23 #得到测试分数和测试分类
24 print(drill.score(x\_test,y\_test))      #测试分数
25 print(drill.predict(x\_test\[3\].reshape(1,-1)))   #预测测试数据第三组样本的分类或预测结果
26 
27 # 预测并绘制结果(以下都为绘图)
28 Z = drill.predict(np.vstack(\[XGrid1.ravel(), XGrid2.ravel()\]).T)
29 Z = Z.reshape(XGrid1.shape)
30 plt.contourf(XGrid1, XGrid2, Z, cmap=plt.cm.hsv)
31 plt.contour(XGrid1, XGrid2, Z, colors=('k',))
32 plt.scatter(x\_train\[:, 0\], x\_train\[:, 1\], c=y\_train, edgecolors='k', linewidth=1.5, cmap=plt.cm.hsv)
33 plt.show()