---
layout: post
title: "知识图谱实体对齐3：无监督和自监督的方法"
date: "2022-10-21T12:48:21.683Z"
---
知识图谱实体对齐3：无监督和自监督的方法
====================

![知识图谱实体对齐3：无监督和自监督的方法](https://img2022.cnblogs.com/blog/1784958/202210/1784958-20221021193643954-1061282003.png) 我们在前面介绍的都是有监督的知识图谱对齐方法，它们都需要需要已经对齐好的实体做为种子（锚点），但是在实际场景下可能并没有那么多种子给我们使用。为了解决这个问题，有许多无监督/自监督的知识图谱对齐方法被提出。其中包括基于GAN的方法，基于对比学习的方法等。他们在不需要事先给定锚点的情况下将来自不同知识图谱实体embeddings映射到一个统一的空间。

1 导引
====

我们在博客[《知识图谱实体对齐1：基于平移(translation)嵌入的方法》](https://www.cnblogs.com/orion-orion/p/16743610.html)和博客[《知识图谱实体对齐2：基于GNN嵌入的方法》](https://www.cnblogs.com/orion-orion/p/16743610.html)中介绍的都是有监督的知识图谱对齐方法，它们都需要需要已经对齐好的实体做为种子（锚点），但是在实际场景下可能并没有那么多种子给我们使用。为了解决这个问题，有许多无监督/自监督的知识图谱对齐方法被提出。

2 一些常见无监督和自监督方法
===============

2.1 基于GAN的方法
------------

首先我们来看一个基于GAN的方法\[1\]，虽然该方法是用于解决NLP中无监督跨语言词向量对齐操作的，但是我觉得在知识图谱领域也很有借鉴意义。

在最原始的有监督跨语言词向量的对齐任务中，给定已经对齐好的字典（锚点）\\(\\left\\{x\_i, y\_i\\right\\}\_{i=1}^n\\)，我们需要找到一个线性变换\\(W\\)来将一个语言的embedding投影到另一个语言的embedding空间中：

\\\[W^{\\star}=\\underset{W \\in M\_d(\\mathbb{R})}{\\operatorname{argmin}}\\|W X-Y\\|\_{\\mathrm{F}} \\\]

其中\\(d\\)为embeddings维度，\\(X, Y\\in \\mathbb{R}^{d\\times n}\\)为字典embeddings矩阵，\\(M\_d(\\mathbb{R})\\)为\\(d\\times d\\)的实矩阵空间。源单词\\(s\\)的对应翻译单词定义为\\(t=\\operatorname{argmax}\_t \\cos \\left(W x\_s, y\_t\\right)\\)。

这个优化问题在对\\(W\\)施以正交约束的情况下，可通过对\\(YX^T\\)进行奇异值分解来获得解析解：

\\\[W^{\\star}=\\underset{W \\in O\_d(\\mathbb{R})}{\\operatorname{argmin}}\\|W X-Y\\|\_{\\mathrm{F}}=U V^T, \\text { with } U \\Sigma V^T=\\operatorname{SVD}\\left(Y X^T\\right) \\\]

事实上，若两个语言embedding空间的维度不相同，即\\(x\_i\\in\\mathbb{R}^{d\_1}\\)、\\(y\_i\\in \\mathbb{R}^{d\_2}\\)时，即\\(W\\in \\mathbb{R^{d\_2\\times d\_1}}\\)不可逆时，亦可通过SGD来求数值解\[2\]。

以上是有对齐的字典的情况，对于没有字典的情况呢？我们可以先用GAN来学到一个\\(W\\)使得两个单词分布粗略地对齐，然后通过目前的\\(W\\)找一些高频单词在另一个向量空间中的最近邻，作为锚点，进行优化以获得更好的\\(W\\)。测试时，再通过最近邻搜索来得到单词在另一个向量空间中的翻译结果。文中的最近邻搜索采用CSLS（cross-domain similarity local scaling）作为距离度量。

整体算法流程如下图所示：

![](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2075236/o_40988dc4.png)

如上图所示，(A) 为两个不同的词向量分布，红色的英语单词由\\(X\\)表示，蓝色的意大利单词由\\(Y\\)表示，我们想要进行翻译/对齐（在意大利语里面，gatto意为“cat”，profondo意为“deep”，felino意为“feline”，“auto”意为“car”）。每一个点代表词向量空间中的一个单词，点的大小和单词在训练语料中出现的频率成正比。 (B) 意为通过对抗学习机制学习一个旋转矩阵\\(W\\)将两个分布大致地对齐。 (C) 使用一些高频单词及其映射后的最近邻做为锚点，来对映射\\(W\\)进一步调整。(D) 寻找单词在目标向量空间中的最近邻以完成翻译。

首先我们来看GAN是如何训练的。设\\(\\mathcal{X}=\\{x\_1,\\cdots, x\_n\\}\\)和\\(\\mathcal{Y}=\\{y\_1,\\cdots, y\_m\\}\\)分别为源语言和目标语言embeddings的集合。GAN的判别器需要区分从\\(W\\mathcal{X}=\\{Wx\_1,\\cdots, Wx\_n\\}\\)和\\(\\mathcal{Y}\\)中随机采样的元素，而生成器（参数为\\(W\\)）要尽可能去阻止判别器做出正确的判断：

\\( \\mathcal{L}\_D\\left(\\theta\_D \\mid W\\right)=-\\frac{1}{n} \\sum\_{i=1}^n \\log P\_{\\theta\_D}\\left(\\right.\\text{source} \\left.=1 \\mid W x\_i\\right)-\\frac{1}{m} \\sum\_{i=1}^m \\log P\_{\\theta\_D}\\left(\\right. \\text{source} \\left.=0 \\mid y\_i\\right)\\).

\\( \\mathcal{L}\_W\\left(W \\mid \\theta\_D\\right)=-\\frac{1}{n} \\sum\_{i=1}^n \\log P\_{\\theta\_D}\\left(\\right.\\text{source }\\left.=0 \\mid W x\_i\\right)-\\frac{1}{m} \\sum\_{i=1}^m \\log P\_{\\theta\_D}\\left(\\right.\\text{source}\\left.=1 \\mid y\_i\\right) \\)

之后，我们从GAN初步训练得到的\\(W\\)来找到一些高频单词在另一个语言中的最近邻，把他们作为锚点，然后优化目标函数来获得更好的\\(W\\)。

注意，在GAN的优化过程中对\\(W\\)进行调整时，采用一种特殊的更新方法来使其有正交性（正交变换在欧氏空间中保范数，且使得训练过程更加稳定）：

\\\[W \\leftarrow(1+\\beta) W-\\beta\\left(W W^T\\right) W \\\]

其中经验表明\\(\\beta=0.01\\)表现良好。

在\\(W\\)训练完毕后，对每个单词映射在其目标向量空间中做最近邻搜索。如果两个语言中的两个单词互为最近邻，则我们把他们加入字典，认为是高质量的翻译。

接下来我们看文中的最近邻搜索采用的距离度量方式。文中认为单词在配对过程中要尽量满足“双向奔赴”，防止某个单词是其它语言中很多单词最近邻的“海王”情况。文中将源单词\\(x\_s\\)和目标单词\\(y\_t\\)间的距离定义为:

\\\[\\text{C S L S}(Wx\_s, x\_t)=2 \\cos \\left(W x\_s, y\_t\\right)-r\_T(Wx\_s)-r\_S(y\_t) \\\]

这里\\(r\_T(Wx\_s)\\)为\\(x\_s\\)和其目标向量空间中的\\(K\\)个邻居间的平均距离：

\\\[r\_T(W x\_s)=\\frac{1}{K} \\sum\_{y\_t \\in \\mathcal{N}\_T(Wx\_s)} \\cos \\left(W x\_s, y\_t\\right) \\\]

同理定义\\(r\_S(y\_t)\\)为\\(y\_t\\)和其\\(K\\)个邻居间的平均距离。

如果一个单词和另一语言中的很多单词都很接近，那么\\(r\\)值就会很高。\\(r\\)可以视为一种惩罚，用于抑制了某些单词是很多单词最近邻的情况。

2.2 基于对比学习的方法
-------------

本文介绍了通过一种基于对比学习的方法\[3\]将来自不同知识图谱实体embeddings映射到一个统一的空间。首先，用对比学习的视角来审视知识图谱\\(G\_x\\)和\\(G\_y\\)的对齐，可以看做是将\\(G\_x\\)中的实体\\(x\\)和其在\\(G\_y\\)中的对齐实体\\(y\\)的距离拉近（先假设已获得对齐实体），而将\\(x\\)和\\(\\mathcal{G}\_y\\)中其它实体的距离推远，如下图中左半部分：

![](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2075236/o_221013063309_%E8%87%AA%E7%9B%91%E7%9D%A3%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%AF%B9%E9%BD%90.png)

这里采用NCE损失来做实体对齐。令\\(p\_x\\)，\\(p\_y\\)为两个知识图谱\\(G\_x\\)和\\(G\_y\\)的表征分布，\\(p\_{\\text{pos}}\\)表示正实体对\\((x,y)\\in \\mathbb{R}^n \\times \\mathbb{R}^n\\)的表征分布。给定对齐的实体对\\((x,y)\\sim p\_{\\text{pos}}\\)，负样本集合\\(\\left\\{y\_i^{-} \\in \\mathbb{R}^n\\right \\}\_{i=1}^M \\stackrel{\\text { i.i.d. }}{\\sim} p\_y\\)，温度\\(\\tau\\)，满足\\(\\lVert f(\\cdot)\\rVert=1\\)的编码器\\(f\\)，我们有NCE损失为：

\\\[\\begin{aligned} \\mathcal{L}\_{\\mathrm{NCE}} & \\triangleq-\\log \\frac{e^{f(x)^{\\top} f(y) / \\tau}}{e^{f(x)^{\\top} f(y) / \\tau}+\\sum\_i e^{f(x)^{\\top} f\\left(y\_i^{-}\\right) / \\tau}} \\\\ &=\\underbrace{-\\frac{1}{\\tau} f(x)^{\\top} f(y)}\_{\\text {alignment }}+\\underbrace{\\log \\left(e^{f(x)^{\\top} f(y) / \\tau}+\\sum\_i e^{f(x)^{\\top} f\\left(y\_i^{-}\\right) / \\tau}\\right)}\_{\\text {uniformity }} . \\end{aligned} \\\]

然而，上面的NCE损失还是需要实现知道已对齐的实体，称不上完全的无监督对齐。作者在文中证明了，对于固定的\\(\\tau\\)和满足\\(\\lVert f(\\cdot)\\rVert=1\\)的编码器\\(f\\)，我们可以为原始的优化目标函数\\(\\mathcal{L}\_{ASM}\\)（即NCE）找一个代理上界做为替代：

\\\[\\begin{aligned} \\mathcal{L}\_{\\mathrm{RSM}} &=-\\frac{1}{\\tau}+\\underset{\\left\\{y\_i^{-}\\right\\}\_{i=1}^M { \\stackrel{\\text{i. i.d .}}{\\sim}} p\_y}{ \\mathbb{E}}\\left\[\\log \\left(e^{1 / \\tau}+\\sum\_i e^{f(x)^{\\top} f\\left(y\_i^{-}\\right) / \\tau}\\right)\\right\] \\\\ & \\leq \\mathcal{L}\_{\\mathrm{ASM}} \\leq \\mathcal{L}\_{\\mathrm{RSM}}+\\frac{1}{\\tau}\\left\[1-\\min \_{(x, y) \\sim p\_{\\mathrm{pos}}}\\left(f(x)^{\\top} f(y)\\right)\\right\] . \\end{aligned} \\\]

这里上界等于\\(\\mathcal{L}\_{\\text{RSM}}\\)加一个常数（\\(f(x)^Tf(y)\\approx 1\\)），因此可以直接优化\\(\\mathcal{L}\_{\\text{RSM}}\\)。这样我们就可以不用去拉近正样本间的距离，只需要推远负样本间的距离就行了。

在具体的负样本采样上，作者采用了self-negative sampling方式。传统的label-aware counterpart negative sampling（上图的左半部分）给定\\(x\\in\\text{KG}\_x\\)，需要从\\(KG\_y\\)中采负样本\\(y\_i^-\\)来将其距离推远。而这里的Self-negative Sampling（上图的右半部分）只需要从\\(KG\_x\\)从采负样本\\(x\_i^-\\)来将其距离推远即可。接下来我们看为什么可以这么做。

设\\({\\{x\_i^-\\in \\mathbb{R}^n\\}}\_{i=1}^M\\)与\\({\\{y\_i^-\\in \\mathbb{R}^n\\}}\_{i=1}^M\\)分别为从分布\\(p\_x\\)和\\(p\_y\\)中独立同分布采样的随机样本，\\(S^{d-1}\\)为\\(\\mathbb{R}^n\\)中的球面，如果存在映射\\(f:\\mathbb{R}^n\\rightarrow S^{d-1}\\)能够将\\(\\mathbb{R}^N\\)中的样本映射到球面上，使得\\(f(x\_i^-)\\)和\\(f(y\_i^-)\\)在\\(S^{d-1}\\)上满足相同的分布，那么我们有：

\\\[\\lim \_{M \\rightarrow \\infty}\\left|\\mathcal{L}\_{\\mathrm{RSM} \\mid \\lambda, \\mathrm{x}}\\left(f ; \\tau, M, p\_{\\mathrm{x}}\\right)-\\mathcal{L}\_{\\mathrm{RSM} \\mid \\lambda, \\mathrm{x}}\\left(f ; \\tau, M, p\_{\\mathrm{y}}\\right)\\right|=0 \\\]

这就启发我们在两个知识图谱共享相似的分布、且负样本数量\\(M\\)充分大的情况下，self-negative sampling 可以看做是 Lable-aware sampling的近似，也即用\\(\\mathcal{L}\_{\\mathrm{RSM} \\mid \\lambda, \\mathrm{x}}\\left(f ; \\tau, M, p\_{\\mathrm{x}}\\right)\\)来代替\\(\\mathcal{L}\_{\\mathrm{RSM} \\mid \\lambda, \\mathrm{x}}\\left(f ; \\tau, M, p\_{\\mathrm{y}}\\right)\\)。

最后，我们可以联合优化\\(G\_x\\)和\\(G\_y\\)的损失函数，如下所示：

\\\[\\mathcal{L}=\\mathcal{L}\_{\\mathrm{RSM} \\mid \\lambda, \\mathrm{x}}\\left(f ; \\tau, M, p\_{\\mathrm{x}}\\right)+\\mathcal{L}\_{\\mathrm{RSM} \\mid \\lambda, \\mathrm{y}}\\left(f ; \\tau, M, p\_{\\mathrm{y}}\\right) \\\]

在优化该目标函数的过程中，需要不断对负样本对进行采样，这里为知识图谱\\(G\_x\\)和知识图谱\\(G\_y\\)分别维护了一个负样本队列。整个训练过程如下图所示：

![](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2075236/o_221013064627_%E8%87%AA%E7%9B%91%E7%9D%A3%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B.png)

3 参考
====

*   \[1\] Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018. Word Translation Without Parallel Data. Proceedings of ICLR.
*   \[2\] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for ma-chine translation. arXiv preprint arXiv:1309.4168, 2013b.
*   \[3\] Liu X, Hong H, Wang X, et al. SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs\[C\]//Proceedings of the ACM Web Conference 2022. 2022: 860-870.

数学是符号的艺术，音乐是上界的语言。