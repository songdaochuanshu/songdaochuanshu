---
layout: post
title: "æ·±åº¦å­¦ä¹ ä¹‹Transformerç½‘ç»œ"
date: "2022-12-28T06:20:15.937Z"
---
æ·±åº¦å­¦ä¹ ä¹‹Transformerç½‘ç»œ
==================

ã€åšä¸»ä½¿ç”¨çš„pythonç‰ˆæœ¬ï¼š3.6.8ã€‘

* * *

æœ¬æ¬¡æ²¡æœ‰é¢å¤–çš„èµ„æ–™ä¸‹è½½

Packages
--------

ort tensorflow as tf
import pandas as pd
import time
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization
from transformers import DistilBertTokenizerFast #, TFDistilBertModel
from transformers import TFDistilBertForTokenClassification
from tqdm import tqdm\_notebook as tqdm

1 - ä½ç½®ç¼–ç 
--------

åœ¨é¡ºåºåˆ°åºåˆ—ä»»åŠ¡ä¸­ï¼Œæ•°æ®çš„ç›¸å¯¹é¡ºåºå¯¹å…¶å«ä¹‰éå¸¸é‡è¦ã€‚å½“ä½ è®­ç»ƒé¡ºåºç¥ç»ç½‘ç»œï¼ˆå¦‚RNNï¼‰æ—¶ï¼Œä½ æŒ‰é¡ºåºå°†è¾“å…¥è¾“å…¥åˆ°ç½‘ç»œä¸­ã€‚æœ‰å…³æ•°æ®é¡ºåºçš„ä¿¡æ¯ä¼šè‡ªåŠ¨è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚ä½†æ˜¯ï¼Œåœ¨è®­ç»ƒè½¬æ¢å™¨ç½‘ç»œæ—¶ï¼Œä¼šä¸€æ¬¡æ€§å°†æ•°æ®å…¨éƒ¨è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚è™½ç„¶è¿™å¤§å¤§å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼Œä½†æ²¡æœ‰å…³äºæ•°æ®é¡ºåºçš„ä¿¡æ¯ã€‚è¿™å°±æ˜¯ä½ç½®ç¼–ç æœ‰ç”¨çš„åœ°æ–¹ - æ‚¨å¯ä»¥ä¸“é—¨ç¼–ç è¾“å…¥çš„ä½ç½®ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹æ­£å¼¦å’Œä½™å¼¦å…¬å¼å°†å®ƒä»¬ä¼ é€’åˆ°ç½‘ç»œä¸­ï¼š![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227190108143-617349462.png)

*   dæ˜¯è¯åµŒå…¥å’Œä½ç½®ç¼–ç çš„ç»´åº¦
*   posæ˜¯å•è¯çš„ä½ç½®ã€‚
*   iæŒ‡ä½ç½®ç¼–ç çš„æ¯ä¸ªä¸åŒç»´åº¦ã€‚

æ­£å¼¦å’Œä½™å¼¦æ–¹ç¨‹çš„å€¼è¶³å¤Ÿå°ï¼ˆä»‹äº -1 å’Œ 1 ä¹‹é—´ï¼‰ï¼Œå› æ­¤å½“æ‚¨å°†ä½ç½®ç¼–ç æ·»åŠ åˆ°å•è¯åµŒå…¥æ—¶ï¼Œå•è¯åµŒå…¥ä¸ä¼šæ˜æ˜¾å¤±çœŸã€‚ä½ç½®ç¼–ç å’Œå•è¯åµŒå…¥çš„æ€»å’Œæœ€ç»ˆæ˜¯è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„å†…å®¹ã€‚ç»“åˆä½¿ç”¨è¿™ä¸¤ä¸ªæ–¹ç¨‹æœ‰åŠ©äºå˜å‹å™¨ç½‘ç»œå…³æ³¨è¾“å…¥æ•°æ®çš„ç›¸å¯¹ä½ç½®ã€‚è¯·æ³¨æ„ï¼Œè™½ç„¶åœ¨è®²åº§ä¸­ï¼ŒAndrew ä½¿ç”¨å‚ç›´å‘é‡ï¼Œä½†åœ¨æ­¤ä½œä¸šä¸­ï¼Œæ‰€æœ‰å‘é‡éƒ½æ˜¯æ°´å¹³çš„ã€‚æ‰€æœ‰çŸ©é˜µä¹˜æ³•éƒ½åº”ç›¸åº”è°ƒæ•´ã€‚

### 1.1 - æ­£å¼¦è§’å’Œä½™å¼¦è§’

é€šè¿‡è®¡ç®—æ­£å¼¦å’Œä½™å¼¦æ–¹ç¨‹çš„å†…é¡¹ï¼Œè·å–ç”¨äºè®¡ç®—ä½ç½®ç¼–ç çš„å¯èƒ½è§’åº¦ï¼š

Â ![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227190421231-809154801.png)

### ç»ƒä¹  1 - get\_angles

å®ç°å‡½æ•° get\_anglesï¼ˆï¼‰ æ¥è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç çš„å¯èƒ½è§’åº¦

def get\_angles(pos, i, d):
    """
    è·å–ä½ç½®ç¼–ç çš„è§’åº¦
    
    Arguments:
        pos -- åŒ…å«ä½ç½®çš„åˆ—å‘é‡\[\[0\], \[1\], ...,\[N-1\]\]
        i --   åŒ…å«ç»´åº¦è·¨åº¦çš„è¡Œå‘é‡ \[\[0, 1, 2, ..., M-1\]\]
        d(integer) -- ç¼–ç å¤§å°
    
    Returns:
        angles -- (pos, d) æ•°ç»„
    """
    
    angles \= pos/ (np.power(10000, (2 \* (i//2)) / np.float32(d)))
    
    
    return angles

æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ï¼š

def get\_angles\_test(target):
    position \= 4
    d\_model \= 16
    pos\_m \= np.arange(position)\[:, np.newaxis\]
    dims \= np.arange(d\_model)\[np.newaxis, :\]

    result \= target(pos\_m, dims, d\_model)

    assert type(result) == np.ndarray, "ä½ å¿…é¡»è¿”å›ä¸€ç³»åˆ—æ•°ç»„é›†åˆ"
    assert result.shape == (position, d\_model), f"é˜²æ­¢é”™è¯¯æˆ‘ä»¬å¸Œæœ›: ({position}, {d\_model})"
    assert np.sum(result\[0, :\]) == 0
    assert np.isclose(np.sum(result\[:, 0\]), position \* (position - 1) / 2)
    even\_cols \=  result\[:, 0::2\]
    odd\_cols \= result\[:,  1::2\]
    assert np.all(even\_cols == odd\_cols), "å¥‡æ•°åˆ—å’Œå¶æ•°åˆ—çš„å­çŸ©é˜µå¿…é¡»ç›¸ç­‰"
    limit \= (position - 1) / np.power(10000,14.0/16.0)
    assert np.isclose(result\[position - 1, d\_model -1\], limit ), f"ç»„åçš„å€¼å¿…é¡»æ˜¯ {limit}"

    print("\\033\[92mAll tests passed")

get\_angles\_test(get\_angles)

# ä¾‹å¦‚
position = 4
d\_model \= 8
pos\_m \= np.arange(position)\[:, np.newaxis\]
dims \= np.arange(d\_model)\[np.newaxis, :\]
get\_angles(pos\_m, dims, d\_model)

All tests passed

Out\[9\]:

array(\[\[0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00\],
       \[1.e+00, 1.e+00, 1.e-01, 1.e-01, 1.e-02, 1.e-02, 1.e-03, 1.e-03\],
       \[2.e+00, 2.e+00, 2.e-01, 2.e-01, 2.e-02, 2.e-02, 2.e-03, 2.e-03\],
       \[3.e+00, 3.e+00, 3.e-01, 3.e-01, 3.e-02, 3.e-02, 3.e-03, 3.e-03\]\])

### 1.2 - æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç 

ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è®¡ç®—çš„è§’åº¦æ¥è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç ã€‚

Â ![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227191130207-1057417217.png)

### ç»ƒä¹  2 - ä½ç½®ç¼–ç 

å®ç°å‡½æ•° positional\_encodingï¼ˆï¼‰ æ¥è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç 

*   np.newaxis æœ‰ç”¨ï¼Œå…·ä½“å–å†³äºæ‚¨é€‰æ‹©çš„å®ç°ã€‚å°±æ˜¯å°†çŸ©é˜µå‡ç»´

def positional\_encoding(positions, d):
    """
    é¢„å…ˆè®¡ç®—åŒ…å«æ‰€æœ‰ä½ç½®ç¼–ç çš„çŸ©é˜µ
    
    Arguments:
        positions (int) -- è¦ç¼–ç çš„æœ€å¤§ä½ç½®æ•°
        d (int) --ç¼–ç å¤§å° 
    
    Returns:
        pos\_encoding -- (1, position, d\_model)å…·æœ‰ä½ç½®ç¼–ç çš„çŸ©é˜µ
    """
    # åˆå§‹åŒ–æ‰€æœ‰è§’åº¦angle\_radsçŸ©é˜µ
    angle\_rads = get\_angles(np.arange(positions)\[:, np.newaxis\],
                            np.arange(d)\[ np.newaxis,:\],
                            d)
  
    # -> angle\_rads has dim (positions,d)
    # å°† sin åº”ç”¨äºæ•°ç»„ä¸­çš„å¶æ•°ç´¢å¼•;2i
    angle\_rads\[:, 0::2\] = np.sin(angle\_rads\[:, 0::2\])
  
    # aå°† cos åº”ç”¨äºæ•°ç»„ä¸­çš„å¶æ•°ç´¢å¼•;2i; 2i+1
    angle\_rads\[:, 1::2\] = np.cos(angle\_rads\[:, 1::2\])
    # END CODE HERE
    
    pos\_encoding \= angle\_rads\[np.newaxis, ...\]
    
    return tf.cast(pos\_encoding, dtype=tf.float32)

æˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹ï¼š

def positional\_encoding\_test(target):
    position \= 8
    d\_model \= 16

    pos\_encoding \= target(position, d\_model)
    sin\_part \= pos\_encoding\[:, :, 0::2\]
    cos\_part \= pos\_encoding\[:, :, 1::2\]

    assert tf.is\_tensor(pos\_encoding), "è¾“å‡ºä¸æ˜¯ä¸€ä¸ªå¼ é‡"
    assert pos\_encoding.shape == (1, position, d\_model), f"é˜²æ­¢é”™è¯¯ï¼Œæˆ‘ä»¬å¸Œæœ›: (1, {position}, {d\_model})"

    ones \= sin\_part \*\* 2  +  cos\_part \*\* 2
    assert np.allclose(ones, np.ones((1, position, d\_model // 2))), "å¹³æ–¹å’Œä¸€å®šç­‰äº1 = sin(a)\*\*2 + cos(a)\*\*2"
    
    angs \= np.arctan(sin\_part / cos\_part)
    angs\[angs < 0\] += np.pi
    angs\[sin\_part.numpy() < 0\] += np.pi
    angs \= angs % (2 \* np.pi)
    
    pos\_m \= np.arange(position)\[:, np.newaxis\]
    dims \= np.arange(d\_model)\[np.newaxis, :\]

    trueAngs \= get\_angles(pos\_m, dims, d\_model)\[:, 0::2\] % (2 \* np.pi)
    
    assert np.allclose(angs\[0\], trueAngs), "æ‚¨æ˜¯å¦åˆ†åˆ«å°† sin å’Œ cos åº”ç”¨äºå¶æ•°å’Œå¥‡æ•°éƒ¨åˆ†ï¼Ÿ"
 
    print("\\033\[92mAll tests passed")

    
positional\_encoding\_test(positional\_encoding)

All tests passed  
è®¡ç®—ä½ç½®ç¼–ç çš„å·¥ä½œå¾ˆå¥½ï¼ç°åœ¨ï¼Œæ‚¨å¯ä»¥å¯è§†åŒ–å®ƒä»¬ã€‚  

pos\_encoding = positional\_encoding(50, 512)

print (pos\_encoding.shape)

plt.pcolormesh(pos\_encoding\[0\], cmap\='RdBu')
plt.xlabel('d')
plt.xlim((0, 512))
plt.ylabel('Position')
plt.colorbar()
plt.show()

(1, 50, 512)

![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227192304860-247577888.png)

Â æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªä½ç½®ç¼–ç  - è¯·æ³¨æ„ï¼Œæ²¡æœ‰ä¸€è¡Œæ˜¯ç›¸åŒçš„ï¼æ‚¨å·²ä¸ºæ¯ä¸ªå•è¯åˆ›å»ºäº†å”¯ä¸€çš„ä½ç½®ç¼–ç ã€‚

2 - æ©ç 
------

æ„å»ºtransformerç½‘ç»œæ—¶ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„æ©ç å¾ˆæœ‰ç”¨ï¼šå¡«å……æ©ç å’Œå‰ç»æ©ç ã€‚ä¸¤è€…éƒ½æœ‰åŠ©äºsoftmaxè®¡ç®—ä¸ºè¾“å…¥å¥å­ä¸­çš„å•è¯æä¾›é€‚å½“çš„æƒé‡ã€‚

### 2.1 - å¡«å……æ©ç 

é€šå¸¸ï¼Œè¾“å…¥åºåˆ—ä¼šè¶…è¿‡ç½‘ç»œå¯ä»¥å¤„ç†çš„åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚å‡è®¾æ¨¡å‹çš„æœ€å¤§é•¿åº¦ä¸º 5ï¼Œåˆ™æŒ‰ä»¥ä¸‹åºåˆ—é¦ˆé€ï¼š

    [["Do", "you", "know", "when", "Jane", "is", "going", "to", "visit", "Africa"], 
     ["Jane", "visits", "Africa", "in", "September" ],
     ["Exciting", "!"]
    ]

    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],
     [ 56, 1285, 15, 181, 545],
     [ 87, 600]
    ]å°†åºåˆ—ä¼ é€’åˆ°è½¬æ¢å™¨æ¨¡å‹ä¸­æ—¶ï¼Œå®ƒä»¬å¿…é¡»å…·æœ‰ç»Ÿä¸€çš„é•¿åº¦ã€‚æ‚¨å¯ä»¥é€šè¿‡ç”¨é›¶å¡«å……åºåˆ—å¹¶æˆªæ–­è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦çš„å¥å­æ¥å®ç°æ­¤ç›®çš„ï¼š

    [[ 71, 121, 4, 56, 99],
     [ 2344, 345, 1284, 15, 0],
     [ 56, 1285, 15, 181, 545],
     [ 87, 600, 0, 0, 0],
    ]é•¿åº¦è¶…è¿‡æœ€å¤§é•¿åº¦ 5 çš„åºåˆ—å°†è¢«æˆªæ–­ï¼Œé›¶å°†è¢«æ·»åŠ åˆ°æˆªæ–­çš„åºåˆ—ä¸­ä»¥å®ç°ä¸€è‡´çš„é•¿åº¦ã€‚åŒæ ·ï¼Œå¯¹äºçŸ­äºæœ€å¤§é•¿åº¦çš„åºåˆ—ï¼Œå®ƒä»¬ä¹Ÿå°†æ·»åŠ é›¶ä»¥è¿›è¡Œå¡«å……ã€‚ä½†æ˜¯ï¼Œè¿™äº›é›¶ä¼šå½±å“softmaxè®¡ç®— - è¿™æ˜¯å¡«å……æ©ç æ´¾ä¸Šç”¨åœºçš„æ—¶å€™ï¼é€šè¿‡å°†å¡«å……æ©ç ä¹˜ä»¥ -1e9 å¹¶å°†å…¶æ·»åŠ åˆ°åºåˆ—ä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†é›¶è®¾ç½®ä¸ºæ¥è¿‘è´Ÿæ— ç©·å¤§æ¥å±è”½é›¶ã€‚æˆ‘ä»¬å°†ä¸ºæ‚¨å®ç°è¿™ä¸€ç‚¹ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è·å¾—æ„å»ºtransformerç½‘ç»œçš„ä¹è¶£ï¼ğŸ˜‡ åªéœ€ç¡®ä¿å®Œæˆä»£ç ï¼Œä»¥ä¾¿åœ¨æ„å»ºæ¨¡å‹æ—¶æ­£ç¡®å®ç°å¡«å……ã€‚

    å±è”½åï¼Œæ‚¨çš„è¾“å…¥åº”ä» [87ï¼Œ 600ï¼Œ 0ï¼Œ 0ï¼Œ 0] å˜ä¸º [87ï¼Œ 600ï¼Œ -1e9ï¼Œ -1e9ï¼Œ -1e9]ï¼Œè¿™æ ·å½“æ‚¨é‡‡ç”¨ softmax æ—¶ï¼Œé›¶ä¸ä¼šå½±å“åˆ†æ•°ã€‚

def create\_padding\_mask(seq):
    """
   ä¸ºå¡«å……å•å…ƒæ ¼åˆ›å»ºçŸ©é˜µæ©ç 
    
    Arguments:
        seq -- (n, m) çŸ©é˜µ
    
    Returns:
        mask -- (n, 1, 1, m)äºŒå…ƒå¼ é‡
    """
    #tf.math.equal(a,b) è¡¨ç¤ºa,bæ˜¯å¦ç›¸ç­‰
    #tf.cast(a,tf.float32) æ˜¯å°†aè½¬åŒ–ä¸ºtf.float32ç±»å‹
    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
  
    # æ·»åŠ é¢å¤–å°ºå¯¸ä»¥æ·»åŠ å¡«å……
    # to the attention logits.
    return seq\[:, tf.newaxis, tf.newaxis, :\] 

æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ï¼š

x = tf.constant(\[\[7., 6., 0., 0., 1.\], \[1., 2., 3., 0., 0.\], \[0., 0., 0., 4., 5.\]\])
print(create\_padding\_mask(x))

tf.Tensor(
\[\[\[\[0. 0. 1. 1. 0.\]\]\]


 \[\[\[0. 0. 0. 1. 1.\]\]\]


 \[\[\[1. 1. 1. 0. 0.\]\]\]\], shape=(3, 1, 1, 5), dtype=float32)  
å¦‚æœæˆ‘ä»¬å°†è¿™ä¸ªæ©ç ä¹˜ä»¥ -1e9 å¹¶å°†å…¶æ·»åŠ åˆ°æ ·æœ¬è¾“å…¥åºåˆ—ä¸­ï¼Œåˆ™é›¶åŸºæœ¬ä¸Šè®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§ã€‚è¯·æ³¨æ„é‡‡ç”¨åŸå§‹åºåˆ—å’Œæ©ç åºåˆ—çš„softmaxæ—¶çš„å·®å¼‚ï¼š

print(tf.keras.activations.softmax(x))
print(tf.keras.activations.softmax(x + create\_padding\_mask(x) \* -1.0e9))

tf.Tensor(
\[\[7.2876632e-01 2.6809818e-01 6.6454883e-04 6.6454883e-04 1.8064311e-03\]
 \[8.4437370e-02 2.2952460e-01 6.2391245e-01 3.1062772e-02 3.1062772e-02\]
 \[4.8541022e-03 4.8541022e-03 4.8541022e-03 2.6502502e-01 7.2041267e-01\]\], shape=(3, 5), dtype=float32)
tf.Tensor(
\[\[\[\[7.2973621e-01 2.6845497e-01 0.0000000e+00 0.0000000e+00
    1.8088353e-03\]
   \[2.4472848e-01 6.6524088e-01 0.0000000e+00 0.0000000e+00
    9.0030566e-02\]
   \[6.6483547e-03 6.6483547e-03 0.0000000e+00 0.0000000e+00
    9.8670328e-01\]\]\]


 \[\[\[7.3057157e-01 2.6876229e-01 6.6619500e-04 0.0000000e+00
    0.0000000e+00\]
   \[9.0030566e-02 2.4472848e-01 6.6524088e-01 0.0000000e+00
    0.0000000e+00\]
   \[3.3333334e-01 3.3333334e-01 3.3333334e-01 0.0000000e+00
    0.0000000e+00\]\]\]


 \[\[\[0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01
    7.3105854e-01\]
   \[0.0000000e+00 0.0000000e+00 0.0000000e+00 5.0000000e-01
    5.0000000e-01\]
   \[0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01
    7.3105854e-01\]\]\]\], shape=(3, 1, 3, 5), dtype=float32)

### 2.2 - å‰ç»æ©ç 

å‰ç»é¢å…·éµå¾ªç±»ä¼¼çš„ç›´è§‰ã€‚åœ¨è®­ç»ƒä¸­ï¼Œæ‚¨å°†å¯ä»¥è®¿é—®è®­ç»ƒç¤ºä¾‹çš„å®Œæ•´æ­£ç¡®è¾“å‡ºã€‚å‰ç»æ©ç å¯å¸®åŠ©æ¨¡å‹å‡è£…å®ƒæ­£ç¡®é¢„æµ‹äº†éƒ¨åˆ†è¾“å‡ºï¼Œå¹¶æŸ¥çœ‹å®ƒæ˜¯å¦å¯ä»¥åœ¨ä¸å‘å‰çœ‹çš„æƒ…å†µä¸‹æ­£ç¡®é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºã€‚

ä¾‹å¦‚ï¼Œå¦‚æœé¢„æœŸçš„æ­£ç¡®è¾“å‡ºæ˜¯ \[1ï¼Œ 2ï¼Œ 3\]ï¼Œå¹¶ä¸”æ‚¨å¸Œæœ›æŸ¥çœ‹ç»™å®šæ¨¡å‹æ˜¯å¦æ­£ç¡®é¢„æµ‹äº†ç¬¬ä¸€ä¸ªå€¼ï¼Œå®ƒæ˜¯å¦å¯ä»¥é¢„æµ‹ç¬¬äºŒä¸ªå€¼ï¼Œåˆ™å¯ä»¥å±è”½ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªå€¼ã€‚å› æ­¤ï¼Œæ‚¨å°†è¾“å…¥å±è”½åºåˆ— \[1ï¼Œ -1e9ï¼Œ -1e9\]ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦å¯ä»¥ç”Ÿæˆ \[1ï¼Œ 2ï¼Œ -1e9\]ã€‚

ä»…ä»…å› ä¸ºä½ è¿™ä¹ˆåŠªåŠ›ï¼Œæˆ‘ä»¬ä¹Ÿä¼šä¸ºä½ ğŸ˜‡ğŸ˜‡å®ç°è¿™ä¸ªæ©ç ã€‚åŒæ ·ï¼Œè¯·ä»”ç»†æŸ¥çœ‹ä»£ç ï¼Œä»¥ä¾¿ä»¥åå¯ä»¥æœ‰æ•ˆåœ°å®ç°å®ƒã€‚

def create\_look\_ahead\_mask(size):
    """
    è¿”å›ä¸€ä¸ªå¡«å……æœ‰ 1 çš„ä¸Šä¸‰è§’çŸ©é˜µ
    
    Arguments:
        size -- çŸ©é˜µå¤§å°
    
    Returns:
        mask -- (size, size) å¼ é‡
    """
    #tf.linalg.band\_part ä»¥å¯¹è§’çº¿ä¸ºä¸­å¿ƒï¼Œå–å®ƒçš„å‰¯å¯¹è§’çº¿éƒ¨åˆ†ï¼Œå…¶ä»–éƒ¨åˆ†ç”¨0å¡«å……
    mask = tf.linalg.band\_part(tf.ones((size, size)), -1, 0)
    return mask

æˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹ï¼š

x = tf.random.uniform((1, 3))
temp \= create\_look\_ahead\_mask(x.shape\[1\])

<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array(\[\[1., 0., 0.\],
       \[1., 1., 0.\],
       \[1., 1., 1.\]\], dtype=float32)>

3 - è‡ªæ³¨æ„åŠ›
--------

æ­£å¦‚å˜å½¢é‡‘åˆšè®ºæ–‡çš„ä½œè€…æ‰€è¯´ï¼Œâ€œæ³¨æ„åŠ›å°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡â€ã€‚

Â ![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227194937762-1660694506.png)

Â ä½¿ç”¨ä¸ä¼ ç»Ÿå·ç§¯ç½‘ç»œé…å¯¹çš„è‡ªæˆ‘æ³¨æ„å…è®¸å¹³è¡ŒåŒ–ï¼Œä»è€ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æ‚¨å°†å®ç°ç¼©æ”¾çš„ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œå®ƒå°†æŸ¥è¯¢ã€é”®ã€å€¼å’Œæ©ç ä½œä¸ºè¾“å…¥ï¼Œä»¥è¿”å›åºåˆ—ä¸­å•è¯çš„ä¸°å¯Œçš„ã€åŸºäºæ³¨æ„åŠ›çš„çŸ¢é‡è¡¨ç¤ºã€‚è¿™ç§ç±»å‹çš„è‡ªæˆ‘æ³¨æ„å¯ä»¥åœ¨æ•°å­¦ä¸Šè¡¨ç¤ºä¸ºï¼š

![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227195025507-1780387974.png)

*   Qæ˜¯æŸ¥è¯¢çŸ©é˜µ
*   Kæ˜¯é”®çš„çŸ©é˜µ
*   Væ˜¯å€¼çš„çŸ©é˜µ
*   Mæ˜¯æ‚¨é€‰æ‹©åº”ç”¨çš„å¯é€‰è’™ç‰ˆ
*   dkæ˜¯æŒ‰é”®çš„å°ºå¯¸ï¼Œç”¨äºç¼©å°æ‰€æœ‰å†…å®¹ï¼Œä»¥ä¾¿ softmax ä¸ä¼šçˆ†ç‚¸

### ç»ƒä¹  3 - scaled\_dot\_product\_attention

Â å®ç°å‡½æ•° 'scaled\_dot\_product\_attentionï¼ˆï¼‰' æ¥åˆ›å»ºåŸºäºæ³¨æ„åŠ›çš„è¡¨ç¤º

def scaled\_dot\_product\_attention(q, k, v, mask):
    """
    è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚
      Qã€Kã€V å¿…é¡»å…·æœ‰åŒ¹é…çš„å‰å¯¼å°ºå¯¸ã€‚
      kï¼Œ v å¿…é¡»å…·æœ‰åŒ¹é…çš„å€’æ•°ç¬¬äºŒä¸ªç»´åº¦ï¼Œå³ï¼šseq\_len\_k = seq\_len\_vã€‚
      é¢å…·æ ¹æ®å…¶ç±»å‹æœ‰ä¸åŒçš„å½¢çŠ¶ï¼ˆå¡«å……æˆ–å‘å‰çœ‹ï¼‰
      ä½†å®ƒå¿…é¡»æ˜¯å¯å¹¿æ’­çš„æ·»åŠ ã€‚

    Arguments:
        q -- query shape == (..., seq\_len\_q, depth)
        k -- key shape == (..., seq\_len\_k, depth)
        v -- value shape == (..., seq\_len\_v, depth\_v)
        æ©ç ï¼šå½¢çŠ¶å¯å¹¿æ’­çš„æµ®ç‚¹å¼ é‡
              è‡ª(..., seq\_len\_q, seq\_len\_k). Defaults to None.

    Returns:
        output -- attention\_weights
    """
    # START CODE HERE
    
    # Q\*K' å†…ç§¯
    matmul\_qk = tf.matmul(q, k, transpose\_b=True)

    #  matmul\_qk çš„è§„æ¨¡
    dk = tf.cast(tf.shape(k)\[-1\], tf.float32)
    scaled\_attention\_logits \= matmul\_qk / tf.math.sqrt(dk)

    # å°†æ©ç æ·»åŠ åˆ°ç¼©æ”¾å¼ é‡ä¸­ã€‚
    if mask is not None:
        scaled\_attention\_logits += (mask \* -1e9)

    # softmax åœ¨æœ€åä¸€ä¸ªè½´ ï¼ˆseq\_len\_kï¼‰ ä¸Šå½’ä¸€åŒ–ï¼Œä»¥ä¾¿åˆ†æ•°
    # ç›¸åŠ ç­‰äº1
    attention\_weights = tf.nn.softmax(scaled\_attention\_logits, axis=-1) 
    # æ³¨æ„åŠ›æƒé‡ \* V
    output = tf.matmul(attention\_weights, v)   # (..., seq\_len\_q, depth\_v)
    
    # END CODE HERE

    return output, attention\_weights

æˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹ï¼š

def scaled\_dot\_product\_attention\_test(target):
    q \= np.array(\[\[1, 0, 1, 1\], \[0, 1, 1, 1\], \[1, 0, 0, 1\]\]).astype(np.float32)
    k \= np.array(\[\[1, 1, 0, 1\], \[1, 0, 1, 1 \], \[0, 1, 1, 0\], \[0, 0, 0, 1\]\]).astype(np.float32)
    v \= np.array(\[\[0, 0\], \[1, 0\], \[1, 0\], \[1, 1\]\]).astype(np.float32)

    attention, weights \= target(q, k, v, None)
    assert tf.is\_tensor(weights), "Weights must be a tensor"
    assert tuple(tf.shape(weights).numpy()) == (q.shape\[0\], k.shape\[1\]), f"Wrong shape. We expected ({q.shape\[0\]}, {k.shape\[1\]})"
    assert np.allclose(weights, \[\[0.2589478,  0.42693272, 0.15705977, 0.15705977\],
                                   \[0.2772748,  0.2772748,  0.2772748,  0.16817567\],
                                   \[0.33620113, 0.33620113, 0.12368149, 0.2039163 \]\])

    assert tf.is\_tensor(attention), "Output must be a tensor"
    assert tuple(tf.shape(attention).numpy()) == (q.shape\[0\], v.shape\[1\]), f"Wrong shape. We expected ({q.shape\[0\]}, {v.shape\[1\]})"
    assert np.allclose(attention, \[\[0.74105227, 0.15705977\],
                                   \[0.7227253,  0.16817567\],
                                   \[0.6637989,  0.2039163 \]\])

    mask \= np.array(\[\[0, 0, 1, 0\], \[0, 0, 1, 0\], \[0, 0, 1, 0\]\])
    attention, weights \= target(q, k, v, mask)

    assert np.allclose(weights, \[\[0.30719590187072754, 0.5064803957939148, 0.0, 0.18632373213768005\],
                                 \[0.3836517333984375, 0.3836517333984375, 0.0, 0.2326965481042862\],
                                 \[0.3836517333984375, 0.3836517333984375, 0.0, 0.2326965481042862\]\]), "Wrong masked weights"
    assert np.allclose(attention, \[\[0.6928040981292725, 0.18632373213768005\],
                                   \[0.6163482666015625, 0.2326965481042862\], 
                                   \[0.6163482666015625, 0.2326965481042862\]\]), "Wrong masked attention"
    
    print("\\033\[92mAll tests passed")
    
scaled\_dot\_product\_attention\_test(scaled\_dot\_product\_attention)

å‡ºè‰²çš„å·¥ä½œï¼æ‚¨ç°åœ¨å¯ä»¥å®ç°è‡ªæˆ‘å…³æ³¨ã€‚æœ‰äº†å®ƒï¼Œæ‚¨å°±å¯ä»¥å¼€å§‹æ„å»ºç¼–ç å™¨å—äº†ï¼

4 - ç¼–ç å¿«
-------

è½¬æ¢å™¨ç¼–ç å™¨å±‚å°†è‡ªæˆ‘æ³¨æ„å’Œå·ç§¯ç¥ç»ç½‘ç»œé£æ ¼çš„å¤„ç†é…å¯¹ï¼Œä»¥æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶å°† K å’Œ V çŸ©é˜µä¼ é€’ç»™è§£ç å™¨ï¼Œç¨åå°†åœ¨ä½œä¸šä¸­æ„å»ºè§£ç å™¨ã€‚åœ¨ä½œä¸šçš„è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæ‚¨å°†é€šè¿‡é…å¯¹å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œæ¥å®ç°ç¼–ç å™¨ï¼ˆå›¾ 2aï¼‰ã€‚

![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227195835273-1067141677.png)

*   å¤šå¤´æ³¨æ„åŠ›å¯ä»¥è®¤ä¸ºæ˜¯å¤šæ¬¡è®¡ç®—è‡ªæˆ‘æ³¨æ„åŠ›ä»¥æ£€æµ‹ä¸åŒçš„ç‰¹å¾ã€‚
*   å‰é¦ˆç¥ç»ç½‘ç»œåŒ…å«ä¸¤ä¸ªå¯†é›†å±‚ï¼Œæˆ‘ä»¬å°†å®ç°ä¸ºå‡½æ•°å…¨è¿æ¥

æ‚¨çš„è¾“å…¥å¥å­é¦–å…ˆé€šè¿‡å¤šå¤´æ³¨æ„åŠ›å±‚ï¼Œç¼–ç å™¨åœ¨å¯¹ç‰¹å®šå•è¯è¿›è¡Œç¼–ç æ—¶ä¼šæŸ¥çœ‹è¾“å…¥å¥å­ä¸­çš„å…¶ä»–å•è¯ã€‚ç„¶åå°†å¤šå¤´æ³¨æ„åŠ›å±‚çš„è¾“å‡ºé¦ˆé€åˆ°å‰é¦ˆç¥ç»ç½‘ç»œã€‚å®Œå…¨ç›¸åŒçš„å‰é¦ˆç½‘ç»œç‹¬ç«‹åº”ç”¨äºæ¯ä¸ªä½ç½®ã€‚

*   å¯¹äºMultiHeadAttentionå±‚ï¼Œæ‚¨å°†ä½¿ç”¨Keraså®ç°ã€‚å¦‚æœæ‚¨å¯¹å¦‚ä½•å°†æŸ¥è¯¢çŸ©é˜µ Qã€é”®çŸ©é˜µ K å’Œå€¼çŸ©é˜µ V æ‹†åˆ†ä¸ºä¸åŒçš„å¤´æ„Ÿåˆ°å¥½å¥‡ï¼Œå¯ä»¥æŸ¥çœ‹å®ç°ã€‚
*   æ‚¨è¿˜å°†ä½¿ç”¨å…·æœ‰ä¸¤ä¸ªå¯†é›†å±‚çš„é¡ºåº API æ¥æ„å»ºå‰é¦ˆç¥ç»ç½‘ç»œå±‚ã€‚

def FullyConnected(embedding\_dim, fully\_connected\_dim):
    return tf.keras.Sequential(\[
        tf.keras.layers.Dense(fully\_connected\_dim, activation\='relu'),  # (batch\_size, seq\_len, dff)
        tf.keras.layers.Dense(embedding\_dim)  # (batch\_size, seq\_len, d\_model)
    \])

### 4.1-ç¼–ç å±‚

ç°åœ¨ï¼Œæ‚¨å¯ä»¥åœ¨ç¼–ç å™¨å±‚ä¸­å°†å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œé…å¯¹åœ¨ä¸€èµ·ï¼æ‚¨è¿˜å°†ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–æ¥å¸®åŠ©åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆå›¾ 2aï¼‰ã€‚

### ç»ƒä¹ 4 - EncoderLayer

ä½¿ç”¨ callï¼ˆï¼‰ æ–¹æ³•å®ç° EncoderLayerï¼ˆï¼‰

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ callï¼ˆï¼‰ æ–¹æ³•å®ç°ä¸€ä¸ªç¼–ç å™¨å—ï¼ˆå›¾ 2ï¼‰ã€‚è¯¥å‡½æ•°åº”æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1.  æ‚¨å°† Qã€Vã€K çŸ©é˜µå’Œå¸ƒå°”æ©ç ä¼ é€’ç»™å¤šå¤´æ³¨æ„åŠ›å±‚ã€‚è¯·è®°ä½ï¼Œè¦è®¡ç®—è‡ªæ³¨æ„Qï¼ŒVå’ŒKåº”è¯¥æ˜¯ç›¸åŒçš„ã€‚
2.  æ¥ä¸‹æ¥ï¼Œæ‚¨å°†å¤šå¤´æ³¨æ„åŠ›å±‚çš„è¾“å‡ºä¼ é€’ç»™è¾å­¦å±‚ã€‚ä¸è¦å¿˜è®°ä½¿ç”¨è®­ç»ƒå‚æ•°æ¥è®¾ç½®æ¨¡å‹çš„æ¨¡å¼ã€‚
3.  ç°åœ¨ï¼Œé€šè¿‡æ·»åŠ åŸå§‹è¾“å…¥ x å’Œ dropout å›¾å±‚çš„è¾“å‡ºæ¥æ·»åŠ è·³è¿‡è¿æ¥ã€‚
4.  æ·»åŠ è·³è¿‡è¿æ¥åï¼Œé€šè¿‡ç¬¬ä¸€å±‚è§„èŒƒåŒ–ä¼ é€’è¾“å‡ºã€‚
5.  æœ€åï¼Œé‡å¤æ­¥éª¤ 1-4ï¼Œä½†ä½¿ç”¨å‰é¦ˆç¥ç»ç½‘ç»œè€Œä¸æ˜¯å¤šå¤´æ³¨æ„åŠ›å±‚ã€‚

**å…¶ä»–æç¤ºï¼š**

*   \_\_init\_\_ æ–¹æ³•åˆ›å»ºå°†ç”±è°ƒç”¨æ–¹æ³•è®¿é—®çš„æ‰€æœ‰å±‚ã€‚æ— è®ºæƒ³åœ¨å“ªé‡Œä½¿ç”¨åœ¨ \_\_init\_\_ æ–¹æ³•ä¸­å®šä¹‰çš„å±‚ï¼Œéƒ½å¿…é¡»ä½¿ç”¨è¯­æ³• selfã€‚\[æ’å…¥å›¾å±‚åç§°\]ã€‚
*   æ‚¨ä¼šå‘ç°MultiHeadAttentionçš„æ–‡æ¡£å¾ˆæœ‰å¸®åŠ©ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæŸ¥è¯¢ã€é”®å’Œå€¼ç›¸åŒï¼Œåˆ™æ­¤å‡½æ•°æ‰§è¡Œè‡ªæˆ‘æ³¨æ„ã€‚

class EncoderLayer(tf.keras.layers.Layer):
    """
    ç¼–ç å™¨å±‚ç”±å¤šå¤´è‡ªæ³¨æ„åŠ›æœºæ„ç»„æˆï¼Œ
    ç„¶åæ˜¯ä¸€ä¸ªç®€å•çš„ã€æŒ‰ä½ç½®çš„å…¨è¿æ¥å‰é¦ˆç½‘ç»œã€‚
    è¿™ä¸ªæ‹±é—¨åŒ…æ‹¬å›´ç»•ä¸¤è€…çš„æ®‹ä½™è¿æ¥
    å­å±‚ï¼Œç„¶åæ˜¯å±‚å½’ä¸€åŒ–ã€‚
    """
    def \_\_init\_\_(self, embedding\_dim, num\_heads, fully\_connected\_dim, dropout\_rate=0.1, layernorm\_eps=1e-6):
        super(EncoderLayer, self).\_\_init\_\_()

        self.mha \= MultiHeadAttention(num\_heads=num\_heads,
                                      key\_dim\=embedding\_dim)

        self.ffn \= FullyConnected(embedding\_dim=embedding\_dim,
                                  fully\_connected\_dim\=fully\_connected\_dim)

        self.layernorm1 \= LayerNormalization(epsilon=layernorm\_eps)
        self.layernorm2 \= LayerNormalization(epsilon=layernorm\_eps)

        self.dropout1 \= Dropout(dropout\_rate)
        self.dropout2 \= Dropout(dropout\_rate)
    
    def call(self, x, training, mask):
        """
        ç¼–ç å™¨å±‚çš„æ­£å‘ä¼ é€’
        
        Arguments:
           x -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€input\_seq\_lenã€embedding\_dimï¼‰
            è®­ç»ƒ -- å¸ƒå°”å€¼ï¼Œè®¾ç½®ä¸º true ä»¥æ¿€æ´»
                        å¤±æ´»å±‚çš„è®­ç»ƒæ¨¡å¼
            æ©ç  -- å¸ƒå°”æ©ç ï¼Œä»¥ç¡®ä¿å¡«å……ä¸æ˜¯
                    è¢«è§†ä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†
        Returns:
            out2 -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€input\_seq\_lenã€embedding\_dimï¼‰
        """
        # START CODE HERE
        # è®¡ç®—è‡ªæ³¨æ„åŠ›ä½¿ç”¨ mha(~1 line)
        #\-> è¦è®¡ç®—è‡ªæˆ‘æ³¨æ„Qï¼ŒVå’ŒKåº”è¯¥ç›¸åŒï¼ˆxï¼‰
        self\_attn\_output = self.mha(x, x, x, mask) # Self attention (batch\_size, input\_seq\_len, embedding\_dim)
        
        # å°†å¤±æ´»å±‚åº”ç”¨äºè‡ªæˆ‘æ³¨æ„è¾“å‡º(~1 line)
        self\_attn\_output = self.dropout1(self\_attn\_output, training=training)
        
        # å¯¹è¾“å…¥å’Œæ³¨æ„åŠ›è¾“å‡ºçš„æ€»å’Œåº”ç”¨å±‚å½’ä¸€åŒ–ï¼Œä»¥è·å¾—
        # å¤šå¤´æ³¨æ„åŠ›å±‚è¾“å‡º (~1 line)
        mult\_attn\_out = self.layernorm1(x + self\_attn\_output)  # (batch\_size, input\_seq\_len, embedding\_dim)

        # é€šè¿‡FFNä¼ é€’å¤šå¤´æ³¨æ„åŠ›å±‚çš„è¾“å‡º(~1 line)
        ffn\_output = self.ffn(mult\_attn\_out)  # (batch\_size, input\_seq\_len, embedding\_dim)
        
        # å°†å¤±æ´»å±‚åº”ç”¨äº FFN è¾“å‡º (~1 line)
        ffn\_output = self.dropout2(ffn\_output, training=training)
        
        # å¯¹å¤šå¤´æ³¨æ„åŠ›å’Œ FFN è¾“å‡ºçš„è¾“å‡ºä¹‹å’Œåº”ç”¨å±‚å½’ä¸€åŒ–ï¼Œä»¥è·å¾—
        # ç¼–ç å™¨å±‚è¾“å‡ºï¼ˆ~1 è¡Œï¼‰
        encoder\_layer\_out = self.layernorm2(ffn\_output + mult\_attn\_out)  # (batch\_size, input\_seq\_len, embedding\_dim)
        # END CODE HERE
        
        return encoder\_layer\_out

æµ‹è¯•ä¸€ä¸‹å§ï¼š

def EncoderLayer\_test(target):
    q \= np.array(\[\[\[1, 0, 1, 1\], \[0, 1, 1, 1\], \[1, 0, 0, 1\]\]\]).astype(np.float32)
    encoder\_layer1 \= EncoderLayer(4, 2, 8)
    tf.random.set\_seed(10)
    encoded \= encoder\_layer1(q, True, np.array(\[\[1, 0, 1\]\]))
    
    assert tf.is\_tensor(encoded), "Wrong type. Output must be a tensor"
    assert tuple(tf.shape(encoded).numpy()) == (1, q.shape\[1\], q.shape\[2\]), f"Wrong shape. We expected ((1, {q.shape\[1\]}, {q.shape\[2\]}))"

    assert np.allclose(encoded.numpy(), 
                       \[\[\-0.5214877 , -1.001476  , -0.12321664,  1.6461804 \],
                       \[\-1.3114998 ,  1.2167752 , -0.5830886 ,  0.6778133 \],
                       \[ 0.25485858,  0.3776546 , -1.6564771 ,  1.023964  \]\],), "Wrong values"
    
    print("\\033\[92mAll tests passed")
    

EncoderLayer\_test(EncoderLayer)

All tests passed

### 4.2 - å…¨ç¼–ç å™¨

å¹²å¾—çœŸæ£’ï¼æ‚¨ç°åœ¨å·²ç»æˆåŠŸå®ç°äº†ä½ç½®ç¼–ç ã€è‡ªæˆ‘æ³¨æ„å’Œç¼–ç å™¨å±‚ - æ‹æ‹è‡ªå·±çš„èƒŒã€‚ç°åœ¨ï¼Œæ‚¨å·²å‡†å¤‡å¥½æ„å»ºå®Œæ•´çš„å˜å‹å™¨ç¼–ç å™¨ï¼ˆå›¾ 2bï¼‰ï¼Œæ‚¨å°†åœ¨å…¶ä¸­åµŒå…¥è¾“å…¥å¹¶æ·»åŠ è®¡ç®—çš„ä½ç½®ç¼–ç ã€‚ç„¶åï¼Œæ‚¨å°†ç¼–ç çš„åµŒå…¥é¦ˆé€åˆ°ç¼–ç å™¨å±‚å †æ ˆã€‚

![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227203010262-1730429517.png)

### ç»ƒä¹  5 - Encoder

ä½¿ç”¨ callï¼ˆï¼‰ æ–¹æ³•å®Œæˆ Encoderï¼ˆï¼‰ å‡½æ•°ï¼Œä»¥åµŒå…¥è¾“å…¥ã€æ·»åŠ ä½ç½®ç¼–ç å¹¶å®ç°å¤šä¸ªç¼–ç å™¨å±‚

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨åµŒå…¥å±‚ã€ä½ç½®ç¼–ç å’Œå¤šä¸ªç¼–ç å™¨å±‚åˆå§‹åŒ–ç¼–ç å™¨ã€‚æ‚¨çš„ callï¼ˆï¼‰ æ–¹æ³•å°†æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1.  é€šè¿‡åµŒå…¥å±‚ä¼ é€’è¾“å…¥ã€‚
2.  é€šè¿‡å°†åµŒå…¥ä¹˜ä»¥åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹æ¥ç¼©æ”¾åµŒå…¥ã€‚è¯·è®°ä½åœ¨è®¡ç®—å¹³æ–¹æ ¹ä¹‹å‰å°†åµŒå…¥ç»´åº¦è½¬æ¢ä¸ºæ•°æ®ç±»å‹ tf.float32ã€‚
3.  å°†ä½ç½®ç¼–ç ï¼šself.pos\_encoding \[ï¼šï¼Œ ï¼šseq\_lenï¼Œ ï¼š\] æ·»åŠ åˆ°åµŒå…¥ä¸­ã€‚
4.  å°†ç¼–ç åµŒå…¥ä¼ é€’åˆ°ä¸€ä¸ª dropout å±‚ï¼Œè®°ä½ä½¿ç”¨è®­ç»ƒå‚æ•°æ¥è®¾ç½®æ¨¡å‹è®­ç»ƒæ¨¡å¼ã€‚
5.  ä½¿ç”¨ for å¾ªç¯å°† dropout å±‚çš„è¾“å‡ºä¼ é€’åˆ°ç¼–ç å±‚å †æ ˆã€‚

class Encoder(tf.keras.layers.Layer):
    """
    æ•´ä¸ªç¼–ç å™¨é¦–å…ˆå°†è¾“å…¥ä¼ é€’åˆ°åµŒå…¥å±‚
    å¹¶ä½¿ç”¨ä½ç½®ç¼–ç å°†è¾“å‡ºä¼ é€’åˆ°å †æ ˆ
    ç¼–ç å™¨å±‚
        
    """   
    def \_\_init\_\_(self, num\_layers, embedding\_dim, num\_heads, fully\_connected\_dim, input\_vocab\_size,
               maximum\_position\_encoding, dropout\_rate\=0.1, layernorm\_eps=1e-6):
        super(Encoder, self).\_\_init\_\_()

        self.embedding\_dim \= embedding\_dim
        self.num\_layers \= num\_layers

        self.embedding \= Embedding(input\_vocab\_size, self.embedding\_dim)
        self.pos\_encoding \= positional\_encoding(maximum\_position\_encoding, 
                                                self.embedding\_dim)


        self.enc\_layers \= \[EncoderLayer(embedding\_dim=self.embedding\_dim,
                                        num\_heads\=num\_heads,
                                        fully\_connected\_dim\=fully\_connected\_dim,
                                        dropout\_rate\=dropout\_rate,
                                        layernorm\_eps\=layernorm\_eps) 
                           for \_ in range(self.num\_layers)\]

        self.dropout \= Dropout(dropout\_rate)
        
    def call(self, x, training, mask):
        """
       ç¼–ç å™¨çš„æ­£å‘ä¼ é€’
        
        Arguments:
           x -- å½¢çŠ¶å¼ é‡ ï¼ˆbatch\_sizeï¼Œ input\_seq\_lenï¼‰
            è®­ç»ƒ -- å¸ƒå°”å€¼ï¼Œè®¾ç½®ä¸º true ä»¥æ¿€æ´»
                        è¾å­¦å±‚çš„è®­ç»ƒæ¨¡å¼
            æ©ç  -- å¸ƒå°”æ©ç ï¼Œä»¥ç¡®ä¿å¡«å……ä¸æ˜¯
                    è¢«è§†ä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†
        Returns:
            out2 -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€input\_seq\_lenã€embedding\_dimï¼‰
        """

        seq\_len \= tf.shape(x)\[1\]
        
        # START CODE HERE
        # é€šè¿‡åµŒå…¥å±‚ä¼ é€’è¾“å…¥
        x = self.embedding(x)  # (batch\_size, input\_seq\_len, embedding\_dim)
        # é€šè¿‡å°†åµŒå…¥ä¹˜ä»¥åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹æ¥ç¼©æ”¾åµŒå…¥
        x \*= tf.math.sqrt(tf.cast(self.embedding\_dim,tf.float32))
        # å°†ä½ç½®ç¼–ç æ·»åŠ åˆ°åµŒå…¥
        x += self.pos\_encoding\[:, :seq\_len, :\]
        # é€šè¿‡å¤±æ´»å±‚ä¼ é€’ç¼–ç åµŒå…¥
        x = self.dropout(x, training=training)
        # é€šè¿‡ç¼–ç å±‚å †æ ˆä¼ é€’è¾“å‡º
        for i in range(self.num\_layers):
            x \= self.enc\_layers\[i\](x,training, mask)
        # END CODE HERE

        return x  # (batch\_size, input\_seq\_len, embedding\_dim)

æµ‹è¯•ä¸€ä¸‹å§ï¼š

def Encoder\_test(target):
    tf.random.set\_seed(10)
    
    embedding\_dim\=4
    
    encoderq \= target(num\_layers=2,
                      embedding\_dim\=embedding\_dim,
                      num\_heads\=2,
                      fully\_connected\_dim\=8,
                      input\_vocab\_size\=32,
                      maximum\_position\_encoding\=5)
    
    x \= np.array(\[\[2, 1, 3\], \[1, 2, 0\]\])
    
    encoderq\_output \= encoderq(x, True, None)
    
    assert tf.is\_tensor(encoderq\_output), "Wrong type. Output must be a tensor"
    assert tuple(tf.shape(encoderq\_output).numpy()) == (x.shape\[0\], x.shape\[1\], embedding\_dim), f"Wrong shape. We expected ({eshape\[0\]}, {eshape\[1\]}, {embedding\_dim})"
    assert np.allclose(encoderq\_output.numpy(), 
                       \[\[\[\-0.40172306,  0.11519244, -1.2322885,   1.5188192 \],
                         \[ 0.4017268,   0.33922842, -1.6836855,   0.9427304 \],
                         \[ 0.4685002,  -1.6252842,   0.09368491,  1.063099  \]\],
                        \[\[\-0.3489219,   0.31335592, -1.3568854,   1.3924513 \],
                         \[\-0.08761203, -0.1680029,  -1.2742313,   1.5298463 \],
                         \[ 0.2627198,  -1.6140151,   0.2212624 ,  1.130033  \]\]\]), "Wrong values"
    
    print("\\033\[92mAll tests passed")
    
Encoder\_test(Encoder)

All tests passed

5 - è¯‘ç å™¨
-------

è§£ç å™¨å±‚é‡‡ç”¨ç¼–ç å™¨ç”Ÿæˆçš„ K å’Œ V çŸ©é˜µï¼Œå¹¶ä½¿ç”¨è¾“å‡ºä¸­çš„ Q çŸ©é˜µè®¡ç®—ç¬¬äºŒä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼ˆå›¾ 3aï¼‰ã€‚

![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227203948255-2017454349.png)

### 5.1 - è¯‘ç å™¨å±‚

åŒæ ·ï¼Œæ‚¨å°†å¤šå¤´æ³¨æ„åŠ›ä¸å‰é¦ˆç¥ç»ç½‘ç»œé…å¯¹ï¼Œä½†è¿™æ¬¡æ‚¨å°†å®ç°ä¸¤ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ã€‚æ‚¨è¿˜å°†ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–æ¥å¸®åŠ©åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆå›¾ 3aï¼‰ã€‚

### ç»ƒä¹  6 - DecoderLayer

ä½¿ç”¨ callï¼ˆï¼‰ æ–¹æ³•å®ç°è§£ç å™¨å±‚ï¼ˆï¼‰

*   å— 1 æ˜¯ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼Œå…·æœ‰æ®‹å·®è¿æ¥ã€è¾å­¦å±‚å’Œå‰ç»æ©ç ã€‚
*   æ¨¡å— 2 å°†è€ƒè™‘ç¼–ç å™¨çš„è¾“å‡ºï¼Œå› æ­¤å¤šå¤´æ³¨æ„å±‚å°†ä»ç¼–ç å™¨æ¥æ”¶ K å’Œ Vï¼Œä»æ¨¡å— 1 æ¥æ”¶ Qã€‚ç„¶åï¼Œæ‚¨å°†åº”ç”¨è¾å­¦å±‚ã€å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥ï¼Œå°±åƒæ‚¨ä¹‹å‰æ‰€åšçš„é‚£æ ·ã€‚
*   æœ€åï¼ŒBlock 3 æ˜¯ä¸€ä¸ªå…·æœ‰ dropout å’Œå½’ä¸€åŒ–å±‚ä»¥åŠæ®‹å·®è¿æ¥çš„å‰é¦ˆç¥ç»ç½‘ç»œã€‚
*   å‰ä¸¤ä¸ªå—ä¸ EncoderLayer éå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯åœ¨è®¡ç®—è‡ªæˆ‘æ³¨æ„æ—¶ä¼šè¿”å›attention\_scores

class DecoderLayer(tf.keras.layers.Layer):
    """
   è§£ç å™¨å±‚ç”±ä¸¤ä¸ªå¤šå¤´æ³¨æ„åŠ›å—ç»„æˆï¼Œ
    ä¸€ä¸ªæ¥å—æ–°çš„è¾“å…¥å¹¶ä½¿ç”¨è‡ªæˆ‘æ³¨æ„ï¼Œå¦ä¸€ä¸ª
    ä¸€ä¸ªå°†å…¶ä¸ç¼–ç å™¨çš„è¾“å‡ºç›¸ç»“åˆï¼Œç„¶åæ˜¯
    å®Œå…¨è¿æ¥çš„å—ã€‚
    """
    def \_\_init\_\_(self, embedding\_dim, num\_heads, fully\_connected\_dim, dropout\_rate=0.1, layernorm\_eps=1e-6):
        super(DecoderLayer, self).\_\_init\_\_()

        self.mha1 \= MultiHeadAttention(num\_heads=num\_heads,
                                      key\_dim\=embedding\_dim)

        self.mha2 \= MultiHeadAttention(num\_heads=num\_heads,
                                      key\_dim\=embedding\_dim)

        self.ffn \= FullyConnected(embedding\_dim=embedding\_dim,
                                  fully\_connected\_dim\=fully\_connected\_dim)

        self.layernorm1 \= LayerNormalization(epsilon=layernorm\_eps)
        self.layernorm2 \= LayerNormalization(epsilon=layernorm\_eps)
        self.layernorm3 \= LayerNormalization(epsilon=layernorm\_eps)

        self.dropout1 \= Dropout(dropout\_rate)
        self.dropout2 \= Dropout(dropout\_rate)
        self.dropout3 \= Dropout(dropout\_rate)
    
    def call(self, x, enc\_output, training, look\_ahead\_mask, padding\_mask):
        """
        è§£ç å™¨å±‚çš„æ­£å‘ä¼ é€’
        
        å‚æ•°ï¼š
            x -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€target\_seq\_lenã€embedding\_dimï¼‰
            enc\_output -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€input\_seq\_lenã€embedding\_dimï¼‰
            è®­ç»ƒ -- å¸ƒå°”å€¼ï¼Œè®¾ç½®ä¸º true ä»¥æ¿€æ´»
                        è¾å­¦å±‚çš„è®­ç»ƒæ¨¡å¼
            look\_ahead\_mask -- target\_inputçš„å¸ƒå°”æ©ç 
            padding\_mask -- ç¬¬äºŒä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚çš„å¸ƒå°”æ©ç 
        è¿”å›ï¼š
            out3 -- å½¢çŠ¶å¼ é‡ ï¼ˆbatch\_sizeï¼Œ target\_seq\_lenï¼Œ embedding\_dimï¼‰
            attn\_weights\_block1 -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€num\_headsã€target\_seq\_lenã€input\_seq\_lenï¼‰
            attn\_weights\_block2 -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€num\_headsã€target\_seq\_lenã€input\_seq\_lenï¼‰
        """
        
        # START CODE HERE
        # enc\_output.shape == (batch\_size, input\_seq\_len, embedding\_dim)
        
        # BLOCK 1
        # è®¡ç®—è‡ªæˆ‘æ³¨æ„å’Œè¿”å›æ³¨æ„åŠ›åˆ†æ•°ä¸º attn\_weights\_block1 ï¼ˆ~1 è¡Œï¼‰
        attn1, attn\_weights\_block1 = self.mha1(x, x, x,look\_ahead\_mask, return\_attention\_scores=True)  # (batch\_size, target\_seq\_len, d\_model)
        
        # åœ¨æ³¨æ„åŠ›è¾“å‡ºä¸Šåº”ç”¨å¤±æ´»å±‚ï¼ˆ~1 è¡Œï¼‰
        attn1 = self.dropout1(attn1, training = training)
        
        # å¯¹æ³¨æ„åŠ›è¾“å‡ºå’Œè¾“å…¥çš„æ€»å’Œåº”ç”¨å±‚å½’ä¸€åŒ–ï¼ˆ~1 è¡Œï¼‰
        out1 = self.layernorm1(attn1 + x)

        # BLOCK 2
        # ä½¿ç”¨æ¥è‡ªç¬¬ä¸€ä¸ªå—çš„ Q å’Œæ¥è‡ªç¼–ç å™¨è¾“å‡ºçš„ K å’Œ V è®¡ç®—è‡ªæˆ‘æ³¨æ„ã€‚
        # å¤šå¤´æ³¨æ„åŠ›çš„è°ƒç”¨æ¥å—è¾“å…¥ï¼ˆæŸ¥è¯¢ã€å€¼ã€é”®ã€attention\_maskã€return\_attention\_scoresã€è®­ç»ƒï¼‰
        # å°†æ³¨æ„åŠ›åˆ†æ•°ä½œä¸ºattn\_weights\_block2è¿”å›ï¼ˆ~1 è¡Œï¼‰
        attn2, attn\_weights\_block2 = self.mha2( out1,enc\_output, enc\_output, padding\_mask, return\_attention\_scores=True)  # (batch\_size, target\_seq\_len, d\_model)
        
        # åœ¨æ³¨æ„åŠ›è¾“å‡ºä¸Šåº”ç”¨å¤±æ´»å±‚ï¼ˆ~1 è¡Œï¼‰
        attn2 = self.dropout2(attn2, training=training)
        
        # å¯¹æ³¨æ„åŠ›è¾“å‡ºå’Œç¬¬ä¸€ä¸ªå—çš„è¾“å‡ºä¹‹å’Œåº”ç”¨å±‚å½’ä¸€åŒ–ï¼ˆ~1 è¡Œï¼‰
        out2 = self.layernorm2(attn2 + out1)  # (batch\_size, target\_seq\_len, embedding\_dim)
        
        #BLOCK 3
        # é€šè¿‡ FFN ä¼ é€’ç¬¬äºŒä¸ªå—çš„è¾“å‡º
        ffn\_output = self.ffn(out2) # (batch\_size, target\_seq\_len, embedding\_dim)
        
        # å°†è¾å­¦å›¾å±‚åº”ç”¨äº FFN è¾“å‡º
        ffn\_output = self.dropout3(ffn\_output, training=training)
        
        # å°†å±‚å½’ä¸€åŒ–åº”ç”¨äº FFN è¾“å‡ºå’Œç¬¬äºŒä¸ªå—çš„è¾“å‡ºä¹‹å’Œ
        out3 =  self.layernorm3(ffn\_output + out2) # (batch\_size, target\_seq\_len, embedding\_dim)
        # END CODE HERE

        return out3, attn\_weights\_block1, attn\_weights\_block2

æµ‹è¯•ä¸€ä¸‹ï¼š

def DecoderLayer\_test(target):
    
    num\_heads\=8
    tf.random.set\_seed(10)
    
    decoderLayerq \= target(
        embedding\_dim\=4, 
        num\_heads\=num\_heads,
        fully\_connected\_dim\=32, 
        dropout\_rate\=0.1, 
        layernorm\_eps\=1e-6)
    
    encoderq\_output \= tf.constant(\[\[\[-0.40172306,  0.11519244, -1.2322885,   1.5188192 \],
                                   \[ 0.4017268,   0.33922842, -1.6836855,   0.9427304 \],
                                   \[ 0.4685002,  -1.6252842,   0.09368491,  1.063099  \]\]\])
    
    q \= np.array(\[\[\[1, 0, 1, 1\], \[0, 1, 1, 1\], \[1, 0, 0, 1\]\]\]).astype(np.float32)
    
    look\_ahead\_mask \= tf.constant(\[\[1., 0., 0.\],
                       \[1., 1., 0.\],
                       \[1., 1., 1.\]\])
    
    padding\_mask \= None
    out, attn\_w\_b1, attn\_w\_b2 \= decoderLayerq(q, encoderq\_output, True, look\_ahead\_mask, padding\_mask)
    
    assert tf.is\_tensor(attn\_w\_b1), "Wrong type for attn\_w\_b1. Output must be a tensor"
    assert tf.is\_tensor(attn\_w\_b2), "Wrong type for attn\_w\_b2. Output must be a tensor"
    assert tf.is\_tensor(out), "Wrong type for out. Output must be a tensor"
    
    shape1 \= (q.shape\[0\], num\_heads, q.shape\[1\], q.shape\[1\])
    assert tuple(tf.shape(attn\_w\_b1).numpy()) == shape1, f"Wrong shape. We expected {shape1}"
    assert tuple(tf.shape(attn\_w\_b2).numpy()) == shape1, f"Wrong shape. We expected {shape1}"
    assert tuple(tf.shape(out).numpy()) == q.shape, f"Wrong shape. We expected {q.shape}"

    assert np.allclose(attn\_w\_b1\[0, 0, 1\], \[0.5271505,  0.47284946, 0.\], atol=1e-2), "Wrong values in attn\_w\_b1. Check the call to self.mha1"
    assert np.allclose(attn\_w\_b2\[0, 0, 1\], \[0.33365652, 0.32598493, 0.34035856\]),  "Wrong values in attn\_w\_b2. Check the call to self.mha2"
    assert np.allclose(out\[0, 0\], \[0.04726627, -1.6235218, 1.0327158, 0.54353976\]), "Wrong values in out"
    

    # Now let's try a example with padding mask
    padding\_mask = np.array(\[\[0, 0, 1\]\])
    out, attn\_w\_b1, attn\_w\_b2 \= decoderLayerq(q, encoderq\_output, True, look\_ahead\_mask, padding\_mask)

    assert np.allclose(out\[0, 0\], \[-0.34323323, -1.4689083, 1.1092525, 0.7028891\]), "Wrong values in out when we mask the last word. Are you passing the padding\_mask to the inner functions?"

    print("\\033\[92mAll tests passed")
    
DecoderLayer\_test(DecoderLayer)

All tests passed

### 5.2 - å…¨è¯‘ç å™¨

ä½ å¿«åˆ°äº†ï¼æ˜¯æ—¶å€™ä½¿ç”¨è§£ç å™¨å±‚æ„å»ºå®Œæ•´çš„è½¬æ¢å™¨è§£ç å™¨äº†ï¼ˆå›¾ 3bï¼‰ã€‚æ‚¨å°†åµŒå…¥è¾“å‡ºå¹¶æ·»åŠ ä½ç½®ç¼–ç ã€‚ç„¶åï¼Œæ‚¨å°†ç¼–ç çš„åµŒå…¥é¦ˆé€åˆ°è§£ç å™¨å±‚å †æ ˆã€‚

![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227210649954-685795503.png)

### ç»ƒä¹ 7 - Decoder

mplement Decoderï¼ˆï¼‰ ä½¿ç”¨ callï¼ˆï¼‰ æ–¹æ³•åµŒå…¥è¾“å‡ºã€æ·»åŠ ä½ç½®ç¼–ç å’Œå®ç°å¤šä¸ªè§£ç å™¨å±‚

åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨åµŒå…¥å±‚ã€ä½ç½®ç¼–ç å’Œå¤šä¸ªè§£ç å™¨å±‚åˆå§‹åŒ–è§£ç å™¨ã€‚æ‚¨çš„ callï¼ˆï¼‰ æ–¹æ³•å°†æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1.  é€šè¿‡åµŒå…¥å±‚ä¼ é€’ç”Ÿæˆçš„è¾“å‡ºã€‚
2.  é€šè¿‡å°†åµŒå…¥ä¹˜ä»¥åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹æ¥ç¼©æ”¾åµŒå…¥ã€‚è¯·è®°ä½åœ¨è®¡ç®—å¹³æ–¹æ ¹ä¹‹å‰å°†åµŒå…¥ç»´åº¦è½¬æ¢ä¸ºæ•°æ®ç±»å‹ tf.float32ã€‚
3.  å°†ä½ç½®ç¼–ç ï¼šself.pos\_encoding \[ï¼šï¼Œ ï¼šseq\_lenï¼Œ ï¼š\] æ·»åŠ åˆ°åµŒå…¥ä¸­ã€‚
4.  å°†ç¼–ç åµŒå…¥ä¼ é€’åˆ°ä¸€ä¸ª dropout å±‚ï¼Œè®°ä½ä½¿ç”¨è®­ç»ƒå‚æ•°æ¥è®¾ç½®æ¨¡å‹è®­ç»ƒæ¨¡å¼ã€‚
5.  ä½¿ç”¨ for å¾ªç¯é€šè¿‡è§£ç å±‚å †æ ˆä¼ é€’ dropout å±‚çš„è¾“å‡ºã€‚

class Decoder(tf.keras.layers.Layer):
    """
   æ•´ä¸ªç¼–ç å™¨é¦–å…ˆå°†ç›®æ ‡è¾“å…¥ä¼ é€’åˆ°åµŒå…¥å±‚
    å¹¶ä½¿ç”¨ä½ç½®ç¼–ç å°†è¾“å‡ºä¼ é€’åˆ°å †æ ˆ
    è§£ç å™¨å±‚
        
    """ 
    def \_\_init\_\_(self, num\_layers, embedding\_dim, num\_heads, fully\_connected\_dim, target\_vocab\_size,
               maximum\_position\_encoding, dropout\_rate\=0.1, layernorm\_eps=1e-6):
        super(Decoder, self).\_\_init\_\_()

        self.embedding\_dim \= embedding\_dim
        self.num\_layers \= num\_layers

        self.embedding \= Embedding(target\_vocab\_size, self.embedding\_dim)
        self.pos\_encoding \= positional\_encoding(maximum\_position\_encoding, self.embedding\_dim)

        self.dec\_layers \= \[DecoderLayer(embedding\_dim=self.embedding\_dim,
                                        num\_heads\=num\_heads,
                                        fully\_connected\_dim\=fully\_connected\_dim,
                                        dropout\_rate\=dropout\_rate,
                                        layernorm\_eps\=layernorm\_eps) 
                           for \_ in range(self.num\_layers)\]
        self.dropout \= Dropout(dropout\_rate)
    
    def call(self, x, enc\_output, training, 
           look\_ahead\_mask, padding\_mask):
        """
       è§£ç å™¨çš„æ­£å‘ä¼ é€’
        
å‚æ•°ï¼š
            x -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€target\_seq\_lenã€embedding\_dimï¼‰
            enc\_output -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€input\_seq\_lenã€embedding\_dimï¼‰
            è®­ç»ƒ -- å¸ƒå°”å€¼ï¼Œè®¾ç½®ä¸º true ä»¥æ¿€æ´»
                        è¾å­¦å±‚çš„è®­ç»ƒæ¨¡å¼
            look\_ahead\_mask -- target\_inputçš„å¸ƒå°”æ©ç 
            padding\_mask -- ç¬¬äºŒä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚çš„å¸ƒå°”æ©ç 
        è¿”å›ï¼š
            x -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€target\_seq\_lenã€embedding\_dimï¼‰
            attention\_weights - åŒ…å«æ‰€æœ‰æ³¨æ„åŠ›æƒé‡çš„å¼ é‡å­—å…¸
                                æ¯ä¸ªå½¢çŠ¶ å½¢çŠ¶çš„å¼ é‡ï¼ˆbatch\_sizeã€num\_headsã€target\_seq\_lenã€input\_seq\_lenï¼‰
        """

        seq\_len \= tf.shape(x)\[1\]
        attention\_weights \= {}
        
        # START CODE HERE
        # åˆ›å»ºå•è¯åµŒå…¥
        x = self.embedding(x)  # (batch\_size, target\_seq\_len, embedding\_dim)
        
        # é€šè¿‡ä¹˜ä»¥ç»´åº¦çš„å¹³æ–¹æ ¹æ¥ç¼©æ”¾åµŒå…¥
        x \*= tf.math.sqrt(tf.cast(self.embedding\_dim, tf.float32))
        
        # è®¡ç®—ä½ç½®ç¼–ç å¹¶æ·»åŠ åˆ°å•è¯åµŒå…¥
        x += self.pos\_encoding\[:, :seq\_len, :\]
        
        # å°†å¤±æ´»å›¾å±‚åº”ç”¨äº X
        x = self.dropout(x, training=training)

        # ä½¿ç”¨ for å¾ªç¯é€šè¿‡è§£ç å™¨å±‚å †æ ˆä¼ é€’ x å¹¶æ›´æ–°attention\_weightsï¼ˆæ€»å…± ~4 è¡Œï¼‰
        for i in range(self.num\_layers):
            # pASS Xå’Œç¼–ç å™¨é€šè¿‡ä¸€å †è§£ç å™¨å±‚è¾“å‡ºï¼ŒèŠ‚çœæ³¨æ„åŠ›æƒé‡
            #å— 1 å’Œå— 2 çš„ # ï¼ˆ~1 è¡Œï¼‰
            x, block1, block2 = self.dec\_layers\[i\](x, enc\_output, training, look\_ahead\_mask, padding\_mask)

            #update attention\_weights å­—å…¸ï¼Œå…·æœ‰å— 1 å’Œå— 2 çš„æ³¨æ„æƒé‡
            attention\_weights\['decoder\_layer{}\_block1\_self\_att'.format(i+1)\] = block1
            attention\_weights\['decoder\_layer{}\_block2\_decenc\_att'.format(i+1)\] = block2
        # END CODE HERE
        
        # x.shape == (batch\_size, target\_seq\_len, embedding\_dim)
        return x, attention\_weights

æµ‹è¯•ä¸€ä¸‹ï¼š

def Decoder\_test(target):
    
    tf.random.set\_seed(10)
        
    num\_layers\=7
    embedding\_dim\=4 
    num\_heads\=3
    fully\_connected\_dim\=8
    target\_vocab\_size\=33
    maximum\_position\_encoding\=6
    
    x \= np.array(\[\[3, 2, 1\], \[2, 1, 0\]\])

    
    encoderq\_output \= tf.constant(\[\[\[-0.40172306,  0.11519244, -1.2322885,   1.5188192 \],
                         \[ 0.4017268,   0.33922842, -1.6836855,   0.9427304 \],
                         \[ 0.4685002,  -1.6252842,   0.09368491,  1.063099  \]\],
                        \[\[\-0.3489219,   0.31335592, -1.3568854,   1.3924513 \],
                         \[\-0.08761203, -0.1680029,  -1.2742313,   1.5298463 \],
                         \[ 0.2627198,  -1.6140151,   0.2212624 ,  1.130033  \]\]\])
    
    look\_ahead\_mask \= tf.constant(\[\[1., 0., 0.\],
                       \[1., 1., 0.\],
                       \[1., 1., 1.\]\])
    
    decoderk \= Decoder(num\_layers,
                    embedding\_dim, 
                    num\_heads, 
                    fully\_connected\_dim,
                    target\_vocab\_size,
                    maximum\_position\_encoding)
    outd, att\_weights \= decoderk(x, encoderq\_output, False, look\_ahead\_mask, None)
    
    assert tf.is\_tensor(outd), "Wrong type for outd. It must be a dict"
    assert np.allclose(tf.shape(outd), tf.shape(encoderq\_output)), f"Wrong shape. We expected { tf.shape(encoderq\_output)}"
    print(outd\[1, 1\])
    assert np.allclose(outd\[1, 1\], \[-0.2715261, -0.5606001, -0.861783, 1.69390933\]), "Wrong values in outd"
    
    keys \= list(att\_weights.keys())
    assert type(att\_weights) == dict, "Wrong type for att\_weights\[0\]. Output must be a tensor"
    assert len(keys) == 2 \* num\_layers, f"Wrong length for attention weights. It must be 2 x num\_layers = {2\*num\_layers}"
    assert tf.is\_tensor(att\_weights\[keys\[0\]\]), f"Wrong type for att\_weights\[{keys\[0\]}\]. Output must be a tensor"
    shape1 \= (x.shape\[0\], num\_heads, x.shape\[1\], x.shape\[1\])
    assert tuple(tf.shape(att\_weights\[keys\[1\]\]).numpy()) == shape1, f"Wrong shape. We expected {shape1}" 
    assert np.allclose(att\_weights\[keys\[0\]\]\[0, 0, 1\], \[0.52145624, 0.47854376, 0.\]), f"Wrong values in att\_weights\[{keys\[0\]}\]"
    
    print("\\033\[92mAll tests passed")
    
Decoder\_test(Decoder)

tf.Tensor(\[-0.2715261 -0.5606004 -0.8617829  1.6939092\], shape=(4,), dtype=float32)
All tests passed

6 - Transformer
---------------

å”·ï¼è¿™æ˜¯ç›¸å½“è‰°å·¨çš„ä»»åŠ¡ï¼Œç°åœ¨ä½ å·²ç»å®Œæˆäº†æ·±åº¦å­¦ä¹ ä¸“ä¸šåŒ–çš„æœ€åä¸€æ¬¡ç»ƒä¹ ã€‚ç¥è´ºï¼ä½ å·²ç»å®Œæˆäº†æ‰€æœ‰è‰°è‹¦çš„å·¥ä½œï¼Œç°åœ¨æ˜¯æ—¶å€™æŠŠå®ƒä»¬æ”¾åœ¨ä¸€èµ·äº†ã€‚

![](https://img2023.cnblogs.com/blog/2434201/202212/2434201-20221227211148809-539727813.png)

é€šè¿‡è½¬æ¢å™¨ä½“ç³»ç»“æ„çš„æ•°æ®æµå¦‚ä¸‹æ‰€ç¤ºï¼š

1.  é¦–å…ˆï¼Œè¾“å…¥é€šè¿‡ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨åªæ˜¯æ‚¨å®ç°çš„é‡å¤ç¼–ç å™¨å±‚ï¼š

*   è¾“å…¥çš„åµŒå…¥å’Œä½ç½®ç¼–ç 
*   å¤šå¤´å…³æ³¨æ‚¨çš„è¾“å…¥
*   å‰é¦ˆç¥ç»ç½‘ç»œä»¥å¸®åŠ©æ£€æµ‹ç‰¹å¾

1.  ç„¶åï¼Œé¢„æµ‹çš„è¾“å‡ºé€šè¿‡è§£ç å™¨ï¼Œè§£ç å™¨ç”±ä½ å®ç°çš„è§£ç å™¨å±‚ç»„æˆï¼š

*   è¾“å‡ºçš„åµŒå…¥å’Œä½ç½®ç¼–ç 
*   å¯¹ç”Ÿæˆçš„è¾“å‡ºè¿›è¡Œå¤šå¤´å…³æ³¨
*   å¤šå¤´æ³¨æ„åŠ›ï¼ŒQæ¥è‡ªç¬¬ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼ŒKå’ŒVæ¥è‡ªç¼–ç å™¨
*   å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå¸®åŠ©æ£€æµ‹ç‰¹å¾

1.  æœ€åï¼Œåœ¨ç¬¬ N ä¸ªè§£ç å™¨å±‚ä¹‹åï¼Œåº”ç”¨ä¸¤ä¸ªå¯†é›†å±‚å’Œä¸€ä¸ª softmax æ¥ç”Ÿæˆåºåˆ—ä¸­ä¸‹ä¸€ä¸ªè¾“å‡ºçš„é¢„æµ‹ã€‚

### ç»ƒä¹ 8 - Transformer

ä½¿ç”¨ callï¼ˆï¼‰ æ–¹æ³•å®ç° Transformerï¼ˆï¼‰

1.  ä½¿ç”¨é€‚å½“çš„æ©ç å°†è¾“å…¥ä¼ é€’åˆ°ç¼–ç å™¨ã€‚
2.  ä½¿ç”¨é€‚å½“çš„æ©ç é€šè¿‡è§£ç å™¨ä¼ é€’ç¼–ç å™¨è¾“å‡ºå’Œç›®æ ‡ã€‚
3.  åº”ç”¨çº¿æ€§å˜æ¢å’Œè½¯æœ€å¤§å€¼æ¥è·å¾—é¢„æµ‹ã€‚

class Transformer(tf.keras.Model):
    """
    å¸¦ç¼–ç å™¨å’Œè§£ç å™¨çš„å®Œæ•´transformer
    """
    def \_\_init\_\_(self, num\_layers, embedding\_dim, num\_heads, fully\_connected\_dim, input\_vocab\_size, 
               target\_vocab\_size, max\_positional\_encoding\_input,
               max\_positional\_encoding\_target, dropout\_rate\=0.1, layernorm\_eps=1e-6):
        super(Transformer, self).\_\_init\_\_()

        self.encoder \= Encoder(num\_layers=num\_layers,
                               embedding\_dim\=embedding\_dim,
                               num\_heads\=num\_heads,
                               fully\_connected\_dim\=fully\_connected\_dim,
                               input\_vocab\_size\=input\_vocab\_size,
                               maximum\_position\_encoding\=max\_positional\_encoding\_input,
                               dropout\_rate\=dropout\_rate,
                               layernorm\_eps\=layernorm\_eps)

        self.decoder \= Decoder(num\_layers=num\_layers, 
                               embedding\_dim\=embedding\_dim,
                               num\_heads\=num\_heads,
                               fully\_connected\_dim\=fully\_connected\_dim,
                               target\_vocab\_size\=target\_vocab\_size, 
                               maximum\_position\_encoding\=max\_positional\_encoding\_target,
                               dropout\_rate\=dropout\_rate,
                               layernorm\_eps\=layernorm\_eps)

        self.final\_layer \= Dense(target\_vocab\_size, activation='softmax')
    
    def call(self, inp, tar, training, enc\_padding\_mask, look\_ahead\_mask, dec\_padding\_mask):
        """
        æ•´ä¸ªå˜å‹å™¨çš„æ­£å‘ä¼ é€’
        å‚æ•°ï¼š
            inp -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€input\_seq\_lenã€fully\_connected\_dimï¼‰
            tar -- å½¢çŠ¶å¼ é‡ï¼ˆbatch\_sizeã€target\_seq\_lenã€fully\_connected\_dimï¼‰
            è®­ç»ƒ -- å¸ƒå°”å€¼ï¼Œè®¾ç½®ä¸º true ä»¥æ¿€æ´»
                        è¾å­¦å±‚çš„è®­ç»ƒæ¨¡å¼
            enc\_padding\_mask -- å¸ƒå°”æ©ç ï¼Œä»¥ç¡®ä¿å¡«å……ä¸æ˜¯
                    è¢«è§†ä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†
            look\_ahead\_mask -- target\_inputçš„å¸ƒå°”æ©ç 
            padding\_mask -- ç¬¬äºŒä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚çš„å¸ƒå°”æ©ç 
        è¿”å›ï¼š
            final\_output -- æè¿°æˆ‘
            attention\_weights - åŒ…å«è§£ç å™¨æ‰€æœ‰æ³¨æ„åŠ›æƒé‡çš„å¼ é‡å­—å…¸
                                æ¯ä¸ªå½¢çŠ¶ å½¢çŠ¶çš„å¼ é‡ï¼ˆbatch\_sizeã€num\_headsã€target\_seq\_lenã€input\_seq\_lenï¼‰
        
        """
        # START CODE HERE
        # ä½¿ç”¨é€‚å½“çš„å‚æ•°è°ƒç”¨ self.encoder ä»¥è·å–ç¼–ç å™¨è¾“å‡º
        enc\_output = self.encoder(inp,training,enc\_padding\_mask) # (batch\_size, inp\_seq\_len, fully\_connected\_dim)
        
        # ä½¿ç”¨é€‚å½“çš„å‚æ•°è°ƒç”¨ self.decoder ä»¥è·å–è§£ç å™¨è¾“å‡º
        # dec\_output.shape == ï¼ˆbatch\_sizeï¼Œ tar\_seq\_lenï¼Œ fully\_connected\_dimï¼‰
        dec\_output, attention\_weights = self.decoder(tar, enc\_output, training, look\_ahead\_mask, dec\_padding\_mask)
        
        # é€šè¿‡çº¿æ€§å±‚å’Œsoftmaxï¼ˆ~2è¡Œï¼‰ä¼ é€’è§£ç å™¨è¾“å‡º
        final\_output = self.final\_layer(dec\_output)  # (batch\_size, tar\_seq\_len, target\_vocab\_size)
        # START CODE HERE

        return final\_output, attention\_weights

æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ï¼š

def Transformer\_test(target):
    
    tf.random.set\_seed(10)


    num\_layers \= 6
    embedding\_dim \= 4
    num\_heads \= 4
    fully\_connected\_dim \= 8
    input\_vocab\_size \= 30
    target\_vocab\_size \= 35
    max\_positional\_encoding\_input \= 5
    max\_positional\_encoding\_target \= 6

    trans \= Transformer(num\_layers, 
                        embedding\_dim, 
                        num\_heads, 
                        fully\_connected\_dim, 
                        input\_vocab\_size, 
                        target\_vocab\_size, 
                        max\_positional\_encoding\_input,
                        max\_positional\_encoding\_target)
    # 0 is the padding value
    sentence\_lang\_a = np.array(\[\[2, 1, 4, 3, 0\]\])
    sentence\_lang\_b \= np.array(\[\[3, 2, 1, 0, 0\]\])

    enc\_padding\_mask \= np.array(\[\[0, 0, 0, 0, 1\]\])
    dec\_padding\_mask \= np.array(\[\[0, 0, 0, 1, 1\]\])

    look\_ahead\_mask \= create\_look\_ahead\_mask(sentence\_lang\_a.shape\[1\])

    translation, weights \= trans(
        sentence\_lang\_a,
        sentence\_lang\_b,
        True,
        enc\_padding\_mask,
        look\_ahead\_mask,
        dec\_padding\_mask
    )
    
    
    assert tf.is\_tensor(translation), "Wrong type for translation. Output must be a tensor"
    shape1 \= (sentence\_lang\_a.shape\[0\], max\_positional\_encoding\_input, target\_vocab\_size)
    assert tuple(tf.shape(translation).numpy()) == shape1, f"Wrong shape. We expected {shape1}"
        
    print(translation\[0, 0, 0:8\])
    assert np.allclose(translation\[0, 0, 0:8\],
                       \[\[0.02616475, 0.02074359, 0.01675757, 
                         0.025527, 0.04473696, 0.02171909, 
                         0.01542725, 0.03658631\]\]), "Wrong values in outd"
    
    keys \= list(weights.keys())
    assert type(weights) == dict, "Wrong type for weights. It must be a dict"
    assert len(keys) == 2 \* num\_layers, f"Wrong length for attention weights. It must be 2 x num\_layers = {2\*num\_layers}"
    assert tf.is\_tensor(weights\[keys\[0\]\]), f"Wrong type for att\_weights\[{keys\[0\]}\]. Output must be a tensor"

    shape1 \= (sentence\_lang\_a.shape\[0\], num\_heads, sentence\_lang\_a.shape\[1\], sentence\_lang\_a.shape\[1\])
    assert tuple(tf.shape(weights\[keys\[1\]\]).numpy()) == shape1, f"Wrong shape. We expected {shape1}" 
    assert np.allclose(weights\[keys\[0\]\]\[0, 0, 1\], \[0.4992985, 0.5007015, 0., 0., 0.\]), f"Wrong values in weights\[{keys\[0\]}\]"
    
    print(translation)
    
    print("\\033\[92mAll tests passed")

    
Transformer\_test(Transformer)

tf.Tensor(
\[0.02616474 0.02074358 0.01675757 0.025527   0.04473696 0.02171908
 0.01542725 0.0365863 \], shape=(8,), dtype=float32)
tf.Tensor(
\[\[\[0.02616474 0.02074358 0.01675757 0.025527   0.04473696 0.02171908
   0.01542725 0.0365863  0.02433536 0.02948791 0.01698964 0.02147778
   0.05749574 0.02669399 0.01277918 0.03276358 0.0253941  0.01698772
   0.02758245 0.02529753 0.04394253 0.06258809 0.03667333 0.03009712
   0.05011232 0.01414333 0.01601288 0.01800467 0.02506283 0.01607273
   0.06204056 0.02099288 0.03005534 0.03070701 0.01854689\]
  \[0.02490053 0.017258   0.01794802 0.02998915 0.05038004 0.01997478
   0.01526351 0.03385608 0.03138068 0.02608407 0.01852771 0.01744511
   0.05923333 0.03287777 0.01450072 0.02815487 0.02676623 0.01684978
   0.02482791 0.02307897 0.04122656 0.05552057 0.03742857 0.03390089
   0.04666695 0.016675   0.01400229 0.01981527 0.02202851 0.01818
   0.05918451 0.02173372 0.03040997 0.03337187 0.02055808\]
  \[0.01867789 0.01225462 0.02509718 0.04180383 0.06244645 0.02000666
   0.01934387 0.03032456 0.05771374 0.02616111 0.01742368 0.01100331
   0.05456048 0.04248188 0.02078062 0.02245298 0.03337654 0.02052129
   0.0239658  0.02193134 0.0406813  0.03323279 0.04556257 0.03676545
   0.04394966 0.01574801 0.01223158 0.02734469 0.01154951 0.02240609
   0.03563078 0.02169302 0.02025472 0.02886864 0.02175328\]
  \[0.02305288 0.01215192 0.0224808  0.04188109 0.05324595 0.016529
   0.01626855 0.02452859 0.05319849 0.01741914 0.02720063 0.01175193
   0.04887013 0.05262584 0.02324444 0.01787255 0.02867536 0.01768711
   0.01800393 0.01797925 0.02830287 0.03332608 0.0324963  0.04277937
   0.03038616 0.03231759 0.01166379 0.0261881  0.01842925 0.02784597
   0.0434657  0.02524558 0.0328582  0.0404315  0.02959606\]
  \[0.01859851 0.01163484 0.02560123 0.04363472 0.06270956 0.01928385
   0.01924486 0.02882556 0.06161032 0.02436098 0.01855855 0.01041807
   0.05321557 0.04556077 0.0220504  0.02093103 0.03341144 0.02041205
   0.02265851 0.02099104 0.03823084 0.03121314 0.04416507 0.03813417
   0.04104865 0.01757099 0.01183266 0.0281889  0.0114538  0.02377768
   0.03464995 0.02217591 0.02084129 0.03000083 0.02300426\]\]\], shape=(1, 5, 35), dtype=float32)
All tests passed

Conclusion
----------

æ‚¨å·²ç»ç»“æŸäº†ä½œä¸šçš„è¯„åˆ†éƒ¨åˆ†ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨å·²ç»ï¼š

*   åˆ›å»ºä½ç½®ç¼–ç ä»¥æ•è·æ•°æ®ä¸­çš„é¡ºåºå…³ç³»
*   ä½¿ç”¨è¯åµŒå…¥è®¡ç®—ç¼©æ”¾çš„ç‚¹ç§¯è‡ªæ³¨æ„
*   å®ç°å±è”½å¤šå¤´æ³¨æ„
*   ç”Ÿæˆå’Œè®­ç»ƒè½¬æ¢å™¨æ¨¡å‹

ä½ åº”è¯¥è®°ä½ä»€ä¹ˆï¼š

*   è‡ªæˆ‘æ³¨æ„å’Œå·ç§¯ç½‘ç»œå±‚çš„ç»“åˆå…è®¸è®­ç»ƒçš„å¹³è¡ŒåŒ–å’Œæ›´å¿«çš„è®­ç»ƒã€‚
*   ä½¿ç”¨ç”Ÿæˆçš„æŸ¥è¯¢ Qã€é”® K å’Œå€¼ V çŸ©é˜µè®¡ç®—è‡ªæˆ‘æ³¨æ„ã€‚
*   å°†ä½ç½®ç¼–ç æ·»åŠ åˆ°å•è¯åµŒå…¥ä¸­æ˜¯åœ¨è‡ªæˆ‘æ³¨æ„è®¡ç®—ä¸­åŒ…å«åºåˆ—ä¿¡æ¯çš„æœ‰æ•ˆæ–¹æ³•ã€‚
*   å¤šå¤´æ³¨æ„åŠ›å¯ä»¥å¸®åŠ©æ£€æµ‹å¥å­ä¸­çš„å¤šä¸ªç‰¹å¾ã€‚
*   æ©ç ä¼šé˜»æ­¢æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´â€œå‘å‰çœ‹â€ï¼Œæˆ–è€…åœ¨å¤„ç†è£å‰ªçš„å¥å­æ—¶è¿‡å¤šåœ°åŠ æƒé›¶ã€‚