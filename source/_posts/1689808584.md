---
layout: post
title: "Relational Learning with Gated and Attentive Neighbor Aggregator for Few-Shot Knowledge Graph Completion 小样本关系学习论文解读"
date: "2022-12-09T14:14:55.096Z"
---
Relational Learning with Gated and Attentive Neighbor Aggregator for Few-Shot Knowledge Graph Completion 小样本关系学习论文解读
====================================================================================================================

　　小样本知识图补全——关系学习。论文利用三元组的**邻域**信息，提升模型的关系表示学习，来实现小样本的链接预测。主要应用的思想和模型包括：GAT（图注意力神经网络）、TransH、SLTM、Model-Agnostic Meta-Learning (MAML)。

　　论文地址：[https://arxiv.org/pdf/2104.13095.pdf](https://arxiv.org/pdf/2104.13095.pdf)

引出
==

　　在WIkidata数据集中，有超大约10%的关系只被不超过10个的三元组所包含，所以要用小样本学习来为这些关系扩充实体。尽管我们可以直接学习整个知识图谱的关系，然后对整个知识图谱进行推理和扩充，但是这样计算量太大。论文用关系的邻域作为额外信息来提升关系表示的小样本学习。

方法
==

　　在这篇论文中，实体和关系的嵌入在训练和测试阶段都是使用TransE预训练好已知的，论文通过引入邻域信息来进一步发掘关系的表示。另外，论文方法使用了[模型不可知元学习（Model-Agnostic Meta Learning, MAML）](https://www.cnblogs.com/qizhou/p/16965888.html)，为了表述清晰，我们先对测试的pipeline进行介绍，然后介绍训练过程。

测试pipeline
----------

　　对于知识图谱$\\mathcal{G}$中的小样本关系$r$，其包含少量的$K$个三元组$\\{(h\_1,r,t\_1),...,(h\_K,r,t\_K)\\}$，我们称这个集合为支撑集（Support set）$S$。模型就是用支撑集作为推理信息，对查询集（Query set）$Q=\\{(h\_1,r,t\_1^?),...,(h\_m,r,t^?\_m)\\}$进行推理（已知关系和一个实体，预测另一个实体）。查询集中三元组头实体$h$或尾实体$t$未知都可。

　　如论文中Figure 2所示，对于支撑集的每个三元组$(h\_i,r,t\_i)$，都在$\\mathcal{G}$中找出它们邻域的三元组$\\{\\mathcal{N}\_{h\_i},\\mathcal{N}\_{t\_i}\\}$。然后，通过一个所谓门控注意力邻域聚合器，将$h\_i$和$t\_i$分别和它们各自的邻域$\\mathcal{N}\_{h\_i}$和$\\mathcal{N}\_{t\_i}$融合，分别得到$h'\_i$和$t'\_i$。然后拼接得到$s\_i$。$s\_i$可以看做是$r$在实体$h\_i$和$t\_i$下，同时融合了它们的邻域的表示。具体操作请看论文中式(1)-式(6)。

　　然后为了对$r$进行完整的表示，也就是将$r$的所有三元组信息都融合起来，文中用Bi-LSTM和注意力机制将所有的$s\_i$进行融合，最后得到$r$的表示$r'$。具体操作请看文中式(7)-式(13)。其实不太了解为什么要用Bi-LSTM，毕竟它们之间没有时序关系。

　　然后文中使用知识图嵌入方法（KGE）[TransH](https://zhuanlan.zhihu.com/p/156937012)来使表示$r'$和支撑集$S$中实体的嵌入相契合，从而能进行后面的推理步骤。与TransH一样，文中定义一个$P\_r$作为$r'$所在超平面的法向量，并使用TransH相关的损失对之前计算出来的$r'$和$P\_r$进行优化。也就是说，优化$r'$和$P\_r$，使得对于支撑集$S$中所有的$h\_i$和$t\_i$，有

$(h\_i-P\_r^Th\_iP\_r)+r'=(t\_i-P\_r^Tt\_iP\_r),\\,\\,i=1,...,K$

　　优化方式就是用梯度下降，如文中式(14)-式(19)所示。需要注意的是，论文中TransH映射到超平面的式子写错了，式(14)和式(20)。

　　最后就是用优化后的$P\_r$和$r'$进行在查询集$Q$上的推理。也就是对知识图谱所有实体$e$进行排序，与映射后的嵌入差异$Dif$越小排名越靠前。$Dif$计算方式如下：

$Dif=\\|(h\_i-P\_r^Th\_iP\_r)+r'-(e-P\_r^TeP\_r)\\|$

　　其中优化后的$P\_r$和$r'$在论文中表示为$P'\_r$和$r\_m$。另外，文中还出现了一个$P^\*\_r$。它是模型经过MAML预训练后得到的参数，在下面的训练过程中介绍。

训练过程
----

　　训练过程是对计算$r'$所用的模型参数（图2The Global Stage的参数）进行训练，以及对超平面法向量$P\_r$进行预训练（MAML）。之所以称之为预训练，是因为$P\_r$在测试阶段依然需要使用支撑集进行微调，而聚合器参数如Bi-LSTM在测试阶段则是固定不变的。

　　训练与测试阶段一样，同样包含支撑集与查询集。训练过程描述如下（论文Algorithm 1）：

　　初始化The Global Stage的模型参数，以及$P\_r$。

　　对于某个关系$r$，设其支撑集和查询集分别为$S\_r=\\{(h\_1,r,t\_1),...,(h\_K,r,t\_K)\\}$和$Q\_r=\\{(h\_1,r,t\_1^?),...,(h\_m,r,t^?\_m)\\}$。$S\_r$经过邻域聚合器得到表示$r'$。然后与测试阶段一样用支撑集实体优化$r'$和$P\_r$。最后计算$r'$和$P\_r$在查询集$Q\_r$上的损失，并用该损失对The Global Stage的模型参数$\\theta$（经由$r'$）以及$P\_r$进行优化：

$\\displaystyle L=\\frac{1}{m}\\sum\\limits\_{i=1}^m\\|(h\_i-P\_r^Th\_iP\_r)+r'-(t\_i^?-P\_r^Tt\_i^?P\_r)\\|$

$\\theta=\\theta-\\beta\\nabla\_\\theta L$

$P\_r=P\_r-\\beta\\nabla\_{P\_r} L$

　　值得注意的是，由于执行了二重反向传播，因此在实现时第一次对$r'$和$P\_r$的优化需要保存其反向传播的计算图。

　　使用多个关系的支撑集和查询集对模型进行训练。最终训练完成得到$\\theta^\*$和$P\_r^\*$。

实验
==

实验设置
----

　　论文实验使用NELL-One和Wiki-One数据集进行实验，选择包含三元组数量在50到500之间的关系进行训练（支撑集和查询集的size没说），之后使用支撑集大小为1/3/5的关系进行测试。使用[平均倒数秩（Mean reciprocal rank，MRR）和Hits@n](https://blog.csdn.net/zcs2632008/article/details/123258206)作为评价指标，都是越高越好。指标计算流程是这样的：

　　对于某个关系$r$，测试集的查询集是$Q=\\{(h\_1,r,t^?\_1),...,(h\_m,r,t^?\_m)\\}$。对每个待查询三元组$(h\_i,r,?)$进行推理，按照相似度得到可能的尾实体排序$\\{t^1\_i,...,t^w\_i\\}$。

　　MRR：获取$t^?\_i$在其中的排序$rank\_i$，然后将所有$rank\_i$的倒数进行平均，得到这个关系$r$的MRR：

$\\displaystyle MRR\_r=\\frac{1}{m}\\sum\\limits\_{i=1}^m\\frac{1}{rank\_i}$

　　Hits@n：查看尾实体排序的前$n$个，如果$t^?\_i$在其中，则记$Hit\_i=1$，否则为$0$，将所有$Hit\_i$进行平均，得到这个关系$r$的Hits@n：

$\\displaystyle Hits@n\_r=\\frac{1}{m}\\sum\\limits\_{i=1}^mHit\_i$

　　然后测试集可能包含对多种关系的测试，最终MRR和Hits@n的结果应该是所有$MRR\_r$和$Hits@n\_r$的平均。

实验结果
----

　　做了1/3/5-shot实验、1-N/N-1/N-N实验、消融实验，证明了模型和各个模块的有效性，如表2/3/4所示。

　　因为论文使用关系的邻域促进模型对关系表示的学习，论文还对邻域给关系表示的贡献作了可视化，如图3所示。权重越大，表示这个邻居对关系的表示贡献越大。