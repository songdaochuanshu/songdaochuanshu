---
layout: post
title: "徐童：视频人物社交关系图生成与应用"
date: "2022-05-21T08:23:50.778Z"
---
徐童：视频人物社交关系图生成与应用
=================

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521141922196-971204350.png)

* * *

**导读：** 在线社交媒体平台的发展，带来了细粒度检索、视频语义摘要等媒体智能服务的巨大需求。现有的视频理解技术缺乏深入的语义线索，结合视频中人物的社交关系才能更完整、准确地理解剧情，从而提升用户体验，支撑智能应用。这里主要介绍我们将动态分析和图机器学习相结合，围绕视频中的人物社交关系网络所开展的两个最新的工作。主要内容包括：

*   问题背景
*   关系图生成
*   关系图应用
*   未来展望

\--

01 问题背景
=======

### \*\* 1. 现有视频理解技术缺乏深入语义线索\*\*

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521141937846-1970612935.png)

现有的视频理解技术更多地着眼于描述画面里人物的行为、动作、身份，很少关注更深层的语义信息。例如上图中让子弹飞的例子，现有算法理解的是“两个人坐在一起，一个人表情愤怒，另一个人表情开心”，但由于缺乏深入的语义线索，不能解释这两个人为什么表情各异地坐在一起，对剧情的理解其实就不完整，不准确。在视频理解当中，除了浅层的“所得即所见”之外，还需要更多深层的“所得不可见”的语义挖掘。在多种多样的语义线索中间，人物社交关系是最核心的线索。

### **2\. 视频人物社交关系相关研究**

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521141949312-672627187.png)

在2015年前后就已经有了图像社交关系识别的研究，其中经典的工作包括PIPA（Zhang et al. 2015）、PISC（Li et al. 2017），主要解决的问题是在静态图片上理解图片中两个人物之间的关系。但由于静态图片本身包含的信息量较少，无法描绘完整的动作和互动行为。到2018年-2019年，社交关系识别任务逐渐推广到了视频，产生了MovieGraphs、ViSR等广为人知的数据集。相比于图片来说，视频包含时序信息、人物完整的动作等，信息更加丰富，建模工作相对来说也更加充实。接下来介绍两篇相关的工作。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521141952464-1097282894.png)

CVPR 2019的一个工作提出了MSTR框架，通过整合视频中的时间域和空间域的信息，来帮助我们理解人物之间的关系。具体来说，如模型框架图中间部分所示，MSTR采用了三个图结构，分别是针对同一个人的 Intra Graph、人物之间的Inter Graph、描述人和物之间交互的 Triple Graph。在这三张图的基础上，分别采用了TSN和GCN来描述时间和空间上的信息，最后把时间域和空间域两个向量拼接起来，作为关系分类的一个特征。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521141955304-2123960727.png)

MSTR在一些数据集上取得了很好的效果，但这个框架重点是描述人物之间的互动行为，与社交关系存在一定的差异。例如对视，微笑到拥抱这样一组互动行为，既可能发生在情侣之间，也可能发生在好朋友之间。这时互动行为会对人物关系产生一些干扰。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521141956781-946722603.png)

针对上面的问题，我们在去年有一个相关工作，尝试增加视频中的文本信息，包括台词、实时弹幕，强化对人物关系的判断。文中采用多通道特征提取网络的框架，融合某一帧画面以及对应的文本信息，得到融合后的向量表征，实现关系分类任务。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142000000-1039118608.png)

通过多模态的信息引入，相比于单模态的方法，效果有明显提升。另外，在之前MSTR的工作中，要预测两个人物之间的关系，要求这两个人必须同时出现，才能得到他们之间的互动行为。但在我们的工作中，即使两个人没有同时出现，通过人物对话中对关系的描述，可以间接得到人物关系。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142002468-1451159449.png)

上面工作的局限性在于，没有充分利用人物之间社交关系的传递性。如果站在全局视角，获得完整的社交关系图，能更充分地发挥社交关系相互佐证的作用。下面重点分享下如何生成社交关系图。

\--

02 关系图生成
========

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142004029-319490800.png)

我们提出了层次累积的图卷积网络，一方面整合了短期的视觉、文本、听觉等线索，另一方面，通过两个层次的图卷积网络，生成全局的社交关系图。其中包含三个模块：

*   \*\*帧级别图卷积网络
*   多通道时序累积
*   片段级图卷积网络\*\*

### **1\. 帧级别图卷积网络**

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142005165-353350717.png)

帧级别图卷积网络模块的核心目的是生成帧级别的关系子图，描述当前帧中人物社交关系。为了解决单帧信息量较少的问题，除了融合多模态信息之外，这里还加入了特殊类型的节点。例如上图红框中标出来的部分，每一张graph对应一个帧级别的子图。C开头的是单个人物的节点，基于人物检测或人物重识别的技术来识别。P节点是表示人物pair对的节点，G节点描述背景信息，T节点表示当前帧前后几十秒的文本信息。利用图卷积网络信息传递的特点整合这些信息，强化人物节点的表征。

### \*\* 2. 多通道时序累积\*\*

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142007145-1048393769.png)

多通道时序累积模块的目的是表达帧和帧之间人物关系的动态变化。这里使用了两个LSTM，第一个LSTM用在C类型节点（人物外观姿态的变化），第二个LSTM用于P类型节点（人物之间交互行为的变化），用于捕捉单个人物以及人物之间交互的变化。

### **3\. 片段级图卷积网络**

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142008385-362125920.png)

片段集图卷积网络模块的目的是整合帧级别的子图，得到片段级的人物关系图。片段级别中包含的信息量比较丰富。有人物、人物之间完整的动作行为、一个完整的小情节，有相对完整的对话信息，不需要太多的辅助信息。因此在上图红框中标出来的子图里，只包含了C和P两个类型的节点。另外，这里额外把片段中的对话音频信息也加入进来，对应图中最右的蓝色框，此时特征向量中已经包含了视频、音频、文本的信息。

### **4\. 模型训练方式**

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142010458-399613760.png)

整个模型的训练基于弱监督学习的方式来实现。对标注者而言，逐帧标注人物之间的关系是几乎不可能完成的任务。这里能够获得的label只有片段级的人物关系，直接用于训练帧级别的网络存在一定的噪声，这里通过设计弱监督的损失函数来解决这个问题，只围绕片段级的图卷积网络来做训练。

### **5\. 实验结果**

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142011926-37630915.png)

我们在两个数据集上进行了验证：公开数据集ViSR，还有自己构造的bilibili数据集。在两个数据集上都取得了不错的效果，由于bilibili数据包含了弹幕，文本信息更加丰富，效果也更加优越。模型中有两个有趣的发现：

*   敌对关系比友好关系更难识别。由于敌对关系之间的互动较少，能够捕捉到的线索较少。
*   部分友好关系存在混淆。例如亲属、朋友之间体现的互动和传递作用中较为类似。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142019318-304744830.png)

第一个片段例子表明，片段中涉及的人物越多，我们的模型优势越大。因为此时片段子图规模更大，更稠密，社交关系相互印证的作用更明显。

第三个片段例子中，从视觉上看是敌对关系，但在故事后期两人发展成了情侣。也就是说随着剧情发展，人物之间的关系是不断变化的，这也启发了我们对于后续工作的灵感。

\--

03 关系图应用
========

### **1\. 视频人物社交关系图应用概览**

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142026550-647172911.png)

人物社交关系图可以有效提升用户体验，支撑语义的智能应用。

*   社交关系图本身可以帮助观众更好地理解剧情。
*   智能应用：剧情片段描述、剧情因果串联。例如在哈利波特第一部，斯内普对哈利看似十分不满，但在关键时刻又总是帮助他，令人费解。有了完整的人物关系图之后，就能更好地解释这些剧情。

### **2\. 基于社交关系的视频人物检索**

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142029172-1029467044.png)

基于社交关系图的视频人物检索，即把视频中某一个人物出现的片段全部挖掘出来。区别于传统的人物检索任务，视频人物出现的场景没有特定场景限制，角度、衣着、行为一直在变化，传统检索任务中常用的重识别类方法很难获得好的效果。此时可以利用社交关系对候选集进行筛选，实现更准确的人物识别。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142032431-968261878.png)

基于上述思路，我们提出了一种基于社交感知的多模态人物检索方法。模型的主要目的是为了对社交关系的作用进行一个初步验证，因此没有用复杂的结构，只用了基础的矩阵运算和SVM，后续也会考虑如何把GCN融入进来。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142033947-2123954079.png)

视频片段中间的人物框视作节点，节点之间的关系通过视觉信息+概率校准的SVM来做分类，得到类别标签及概率。

人物关系图作为先验知识融合到网络中，完善视觉相似度的局限性。

*   当两个人物没有正脸时，靠视觉信息很难分辨。通过社交关系，这两个人产生交互的对象完全不同，这时可以认为这两个人不是同一个人。
*   有时由于姿态、光线的变化，同一个人的两张图片视觉相似度较低，这时也可以通过社交关系加以强化。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142035128-1803666786.png)

实验结果表明，基于社交关系，通过简单的预训练+SVM-based关系判别就可以超过当下SOTA的纯视觉人物识别效果，证明了这个思路的可行性。尤其是在一些有大量遮挡的极端情况下，纯视觉的方法失效了，但通过社交好友关系可以帮助我们做判断。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142036254-346826884.png)

\--

04 未来展望
=======

“万物皆可图”，多模态内容概莫能外。动态化、语义化，是多模态+图的未来方向。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142037724-826086805.png)

多模态与图相关技术的融合，例如视觉的分割与关联，把左边的图片转成右边这样的场景图结构。但这里主要描述的还是“所得及所见”的直接视觉关系，例如物体的位置关系、包含关系，以及人物的衣着行为等。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142039195-1982270759.png)

在上述描述关系的场景图的基础之上，目前已经可以实现对实体的关联，支撑更细粒度的理解和任务。如图所示，输入查询query “某一个人在打篮球的地方”。如果只使用实体匹配的技术，这里认为需要找的是人和篮球，会把“人抱着篮球在场边谈话”的场景也检索出来，如图中最下面的一个场景所示。如果用关系图来描述，前三个场景与最后一个场景得到的关系图会有很大差异，两个实体间的边类型不同。此时可以把检索问题转换成子图相似度匹配的任务，从而获得更加准确的检索结果。

![file](https://img2022.cnblogs.com/other/1701474/202205/1701474-20220521142040235-858208791.png)

目前的场景图中包含的主要是物体的位置关系、包含关系，以及人物的衣着行为等在画面中显而易见的关系。在未来，可以对场景图增加更多动态化、语义化的线索，支撑更丰富的下游应用。  
本文首发于微信公众号“DataFunTalk”。