---
layout: post
title: "2.数据及其预处理"
date: "2022-06-19T13:24:43.910Z"
---
2.数据及其预处理
=========

1\. 数据样本矩阵
==========

一般数据集的构造形式：**一行一样本，一列一特征**，以下为一个示例

姓名

年龄

性别

工作经验

月薪

A

22

男

2

5000

B

23

女

3

6000

C

25

男

3

7000

在数学推导中，常用\\(x=\\{x\_1,x\_2,\\cdots,x\_p\\}^T\\)来表示一个样本，用\\(X=\\{{x^{(1)}}^T,{x^{(2)}}^T,\\cdots,{x^{(n)}}^T\\}^T\\)来表示数据集。

2.数据预处理及相关`API`
===============

`sklearn.preprocessing`包中提供了一系列的关于数据预处理的工具。

    import numpy as np
    import sklearn.preprocessing as sp
    

2.1 标准化
-------

    针对于数据的每一列，都减去均值并除以标准差，来抵消掉由于量纲不同而代来的影响。
    

### 2.1.1 `scale`

    raw_sample=np.array([
        [17,100,4000],
        [20,80,5000],
        [23,75,5000]
    ])
    std_sample=sp.scale(raw_sample)
    print(std_sample)
    

    [[-1.22474487  1.38873015 -1.41421356]
     [ 0.         -0.46291005  0.70710678]
     [ 1.22474487 -0.9258201   0.70710678]]
    

### 2.1.2 `StandardScaler`

和`scale`相比，使用`StandardScaler`类类的好处在于：

> 1.  可以保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据,
> 2.  它可以当作一个学习器放在`Pipline`管道中。

    ss=sp.StandardScaler()
    ss.fit(raw_sample)# 按原始训练集生成规则，即训练的均值和标准差
    std_sample=ss.transform(raw_sample)  # 讲规则应用到样本中
    test_sample=np.array([
        [20,80,3000],
        [21,88.2500]
    ])
    std_test_sample=ss.transform(test_sample)#将规则应用到测试集中
    

    #也可以将按原始训练集生成规则和将规则应用到原始数据的过程合并为一步
    std_sample=ss.fit_transform(raw_sample)
    

对于其他形式的数据预处理工具，一般的使用方法(`fit`,`transform`,`fit_transform`)都相同。

2.2 范围缩放
--------

将样本矩阵中每一列的的等比例缩放到相同的区间，一般是\[0,1\]区间。  
对于每列数据\\(x=\\{x\_1,x\_2,x\_3,x\_4,\\cdots,x\_n\\}\\),

\\\[ x\_i^{\*}=\\frac{x\_i-\\min\\{x\\}}{\\max\\{x\\}-\\min\\{x\\}} \\\]

    mms=sp.MinMaxScaler(feature_range=(0,1))
    mms.fit_transform(raw_sample)
    

    array([[0. , 1. , 0. ],
           [0.5, 0.2, 1. ],
           [1. , 0. , 1. ]])
    

2.3 归一化
-------

有时候每个样本的每个特征具体的值不重要，但是每个样本特征值的占比很重要。

姓名

动画片

剧情片

动作片

喜剧片

A

20

4

50

1

B

10

1000

200

6000

C

80

20

20000

50

假设一个视频网站，A是新注册的会员，BC是老会员，因此在衡量样本相似度(爱好相似)时不能单纯考虑数量，而是考虑每种类中所占的比例。

归一化就是将每个样本缩放到单位范数(每个样本的范数为 1)。

其思想原理是： 对每个样本计算其**p-范数**，然后对该样本中每个元素**除以该范数**，这样处理的结果是是的每个处理后样本的 p-范数（L1-norm, L2-norm）等于 1。

    raw_sample=np.array([
        [20,4,50,1],
        [10,1000,200,6000],
        [80,20,20000,50]
    ])
    normalized_sample = sp.normalize(raw_sample, norm='l1')
    print(normalized_sample)
    

    [[0.26666667 0.05333333 0.66666667 0.01333333]
     [0.00138696 0.13869626 0.02773925 0.83217753]
     [0.00397022 0.00099256 0.99255583 0.00248139]]
    

2.4 二值化
-------

特征二值化是将数值特征用阈值过滤得到布尔值的过程。即，给定阈值，将特征转换为 0/1，一般用于图像中。

    import matplotlib.pyplot as plt
    raw_image=np.random.uniform(low=0,high=255,size=(28,28))
    plt.imshow(raw_image,cmap="gray")
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/20dd9973606e45709b23af31f3064083.png)

    bin=sp.Binarizer(threshold=100)
    plt.imshow(bin.fit_transform(raw_image),cmap="gray")
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/c5b171ae7d0a4118841c5267af8cf01a.png)

2.5 标签编码和独热编码
-------------

真实世界的数据集通常都含有非数值型的变量，例如人的性别，在分类任务中，标签通常也是非数值型的变量。

许多机器学习库要求类别是整型数值。虽然sklearn中大部分学习器都能自动将类别转为整型，但还是建议手动将类别进行转换。对类别进行编码。

    label=np.array(
        ["男","女","女","男"]
    )
    le=sp.LabelEncoder()
    label_encoder=le.fit_transform(label)
    label_encoder
    

    array([1, 0, 0, 1])
    

    #也可以将新的标签编码安原来的规则转换为原始文本
    le.inverse_transform([0,1,1,0,1])
    

    array(['女', '男', '男', '女', '男'], dtype='<U1')
    

假设一个四分类分类任务，其标签分别为1,2,3,4,现在将其每个标签转换为概率值的形式。

    ohe=sp.OneHotEncoder()
    ohe.fit_transform(label_encoder.reshape(-1,1)).toarray()
    

    array([[0., 1.],
           [1., 0.],
           [1., 0.],
           [0., 1.]])
    

3 `sklearn`中的数据集
================

SKLearn里面有很多自带数据集供用户使用，`sklearn.datasets`专门用来获取数据集的模块。

根据数据集的形式，获取方式分为以下几种：

*   自带的小数据集（packaged dataset）：`sklearn.datasets.load_*`
*   可在线下载的数据集（Downloaded Dataset）：`sklearn.datasets.fetch_*`
*   计算机生成的数据集（Generated Dataset）：`sklearn.datasets.make_*`
*   svmlight/libsvm格式的数据集:sklearn.datasets.load\_svmlight\_file(...)
*   从买了data.org在线下载获取的数据集:sklearn.datasets.fetch\_mldata(...)

    sklearn.datasets.load_*
    

*   `*`代表数据集的名称,可以通过`Tab`键列出可支持的数据集
*   数据集明细

数据集

介绍

load\_wine

葡萄酒数据集

load\_iris

鸢尾花数据集

load\_boston

波士顿房屋数据集

load\_breast\_cancer

乳腺癌数据集

load\_diabetes

糖尿病数据集

load\_linnerud

体能训练数据集

load\_digits

手写体数据集

**下面以load\_iris()为例：**  
`load_iris()`:

*   参数：

> *   `as_frame`:`bool,default=false`,决定返回的数据类型是`pandas.Dataframe`或`Series`还是`numpy.ndarray`(默认)
> *   `return_X_y`:`bool,default=false`,默认返回的是`sklearn.utils.Bunch`对象，它是对字典对象的继承，通过`.keys()`方法可以查看有那些键，通过对应的键查看关键信息。如果不需要这些信息，可以设为`true`,以元组的形式返回数据和标签。

    from sklearn import datasets
    
    *#导入数据集*
    
    iris=datasets.load_iris()
    
    *#可以把数据集看作一个字典，查看数据集中的键*
    
    iris.keys()
    

dict\_keys(\['data', 'target', 'frame', 'target\_names', 'DESCR', 'feature\_names', 'filename', 'data\_module'\])

属性

介绍

data

数据，一行一样本，一列一特征，根据 as\_frame决定什么类型

target

数据标签，根据 as\_frame决定什么类型

frame

当as\_frame为True是才返回内容

target\_names

标签类别名

DESCR

对数据集的描述

feature\_names

每个特征对应的名称

filename

数据集所在的文件名

data\_module

数据集所在的模块

4.划分训练集和测试集
===========

`sklearn.model_selection`中提供的`train_test_split()`方法来切分训练集和测试集

*   参数：

> *   X:数据
> *   y:标签
> *   test\_size:float or int, default=None,可以设置0到1之间的数,做为测试集的比例，也可以是整数，作为测试集的样本数，若果设置为default。则为0.25。
> *   train\_size:训练集样本数的设置，同上，一般只设置`test_size`即可，不需要设置概参数
> *   random\_state:int, RandomState instance or None, default=None,随机种子
> *   shufflebool, default=True,是否打乱

*   **具体使用：**
    
*         from sklearn.model_selection import train_test_split
          X=iris.data
          y=iris.target
          X_train, X_test, y_train, y_test =train_test_split(X, y, random_state=2022,test_size=0.2, shuffle=True)