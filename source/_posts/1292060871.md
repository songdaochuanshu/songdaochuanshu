---
layout: post
title: "爬虫基础 - 爬虫学的好，牢饭吃得饱系列"
date: "2022-11-13T11:16:10.250Z"
---
### 安装

pip install requests

### get请求

    import requests
    ​
    res = requests.get(url="https://baidu.com")
    print(res) # Response对象
    print(res.content) # b"..." 二进制文本流
    print(res.content.decode("utf-8")) # 二进制文本流转为字符串
    print(res.text) # 直接获取字符串，效果和上面的一样
    print(res.request.headers) # 请求头
    print(res.headers) # 响应头
    print(res.status_code) # 请求状态码 200是成功
    print(res.url) # 请求地址
    if res.status_code == 200:
        with open("./baidu.html", 'w') as fp:
            fp.write(res.text)
    

### get请求反反爬

上面我们把baidu首页成功下载下来了，如果我们换成具有简单反爬的网站就会失败了，如下

    res = requests.get(url="https://www.ifunmac.com/")
    if res.status_code == 200:
      with open("./baidu.html", 'w') as fp:
        fp.write(res.text)
      else:
        print("获取失败")
    

解决方案：定义请求头信息(这里的请求头可以随便找一个浏览器里的请求，直接复制过来即可)

    headers = {
        "user-agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36"
    }
    res = requests.get(url="https://www.ifunmac.com/", headers=headers)
    

### post请求

    data = { "kw":"hello" } # post请求发送的数据
    res = requests.post(url="https://fanyi.baidu.com/sug", headers=headers, data=data)
    if res.status_code == 200:
        print(res.json())
    

post请求的时候要带上headers头信息和data要发送的数据

### 什么是cookie和session

举个例子，如果你已经登录了某网站，为什么打开该网站其他页面的时候不需登录了？也就是说服务端如何知道你已经是登录状态的？客户端在哪记录的状态？那就是cookie，你使用用户名和密码登录之后，服务端生成一个cookie给客户端，下次客户端打开其他页面请求数据的时候，带着这个cookie，服务端就知道是已登录状态。http请求是无状态的，所以使用cookie来记录状态。

session是在服务端存储数据，并且会给每个用户生成一个sessionID也就是cookie

### cookie应用实例

我们登录简书之后能看到自己账号的粉丝个数，我们如何用python爬虫拿到粉丝数呢？那就是先登录简书，获取到cookie之后再抓取

    headers = {
        "cookie": "复制登录后的cookie"
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36"
    }
    res = requests.get(url="https://www.jianshu.com/users/xxxxxx/following?page=2", headers=headers)
    if res.status_code == 200:
        print(res.text)
    

### 如何自动化获取cookie并获取登录后的页面信息

这里和我平时的认知有点不一样，一般都是模拟登录，然后从登录请求里获取cookie来进行下一步的使用。这里需要大家注意，有点不一样。

流程是：先获取cookie对象，然后使用cookie对象发起模拟登录，再直接使用cookie对象发起获取数据请求即可！

代码实例结构如下：

    req = requests.session()
    res = req.post(url="https://2", headers=headers, data=data)
    if res.status_code == 200:
        req.get(url="https://....")
    

扫码关注公众号”macosdev“，分享更多技术和资源，回复”电子书“可以获得海量免费电子书资源  
![](https://img2022.cnblogs.com/blog/324817/202211/324817-20221113191248032-1404131991.jpg)

爬虫的灵魂，解析数据
==========

爬虫学的再牛逼，不会解析数据一切都是白扯

### 牛逼方式一 xpath

xpath解析html非常方便，安装`pip3 install lxml` ，我们使用lxml库可以实现xpath方式解析html元素内容，只需要从lxml引入etree就可以使用xpath语法来解析html数据，关于xpath语法也有w3cschool教程，非常简单，这里就不细说了

    from lxml import etree
    html = etree.parse("html文件路径", etree.HTMLParser())
    r = html.xpath("/html/body/ul/li/text()")
    print(r)
    

### 牛逼方式二 bs4

Beautifulsoup4简称bs4，是爬虫解析网页的神器，安装`pip3 install Beautifulsoup4`

bs4功能强大，有一定的学习成本，但是使用起来非常简单，要想爬虫学的好，bs4必不可少，具体学习内容这里就不说了，去官网看看文档就能用起来

    from bs4 import BeautifulSoup
    bs = BeautifulSoup(html_doc, 'lxml')
    

### 牛逼方式三 正则表达式re

python自带正则表达式库re，正则表达式如果学的好，解析数据会事半功倍，如需安装第三方，直接`import re`即可使用

这块内容比较多，所以也是很多初学者觉得困难的地方。不要觉得无从下手，re里面有俩函数学会基本能应付大部分场景，match和search函数

### 反反爬方式之一：代理服务器

写过服务器接口的人都知道，为了防止恶意刷接口对服务器造成压力，我们一般会记录访问者的ip并且对访问次数做限制。如果我就是想爬数据还不想被限制怎么办？

使用代理可以解决这个问题。

代理分为透明代理、匿名代理、高匿代理

透明代理是虽然代理了，但是服务端能获取到你的真实ip，没办法解决限制问题

匿名代理是虽然服务端不知道你的真实ip，但是知道你使用代理了

高匿代理是服务端不知道你的真实ip，也不知道你使用代理了，隐蔽度最高的一种方式

代理地址可以去百度搜索一下，很多免费的地址可以用，比如西祠代理

使用方式如下：

    proxies = {
        "http": "192.19.19.9:9999",
        "https": "192.19.19.9:9999"
    }
    res = req.post(url="https://2", headers=headers,proxies=proxies, data=data)
    

个人网站地址：https://hehuoya.com 专 业：计算机科学与技术 先后入职 360、新氧、自如

发表于 2022-11-13 19:13  [李长鸿](https://www.cnblogs.com/huntaiji/)  阅读(0)  评论(0)  [编辑](https://i.cnblogs.com/EditPosts.aspx?postid=16886666)  [收藏](javascript:void(0))  [举报](javascript:void(0))