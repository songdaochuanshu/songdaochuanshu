---
layout: post
title: "ClickHouse 对付单表上亿条记录分组查询秒出, OLAP应用秒杀其他数据库"
date: "2022-05-03T06:23:34.029Z"
---
ClickHouse 对付单表上亿条记录分组查询秒出, OLAP应用秒杀其他数据库
=========================================

1.  启动并下载一个clickhouse-server, By default, starting above server instance will be run as default user without password.

docker run -d --name ch-server --ulimit nofile=262144:262144 -p 8123:8123 -p 9000:9000 -p 9009:9009 yandex/clickhouse-server  
或者加一个Mount  
docker run -d --name ch-server --ulimit nofile=262144:262144 -p 8123:8123 -p 9000:9000 -p 9009:9009  -v e:\\\\taobao:/usr/src/app/  yandex/clickhouse-server

2\. 打开浏览器, 输入, 查询结果就下载下来

http://localhost:8123/?query=SELECT%20%27Hello,%20ClickHouse!%27

3\. 进入docker 的CLI, server端安装在/etc/clickhouse-server 目录      用clickhouse-client来执行客户端命令

![](https://img2022.cnblogs.com/blog/873/202205/873-20220501235015446-1872362081.png)

 4. 通过csv导入数据, 把本地目录Mount 到/usr/src/app目录

clickhouse-client --query='INSERT INTO tblSale FORMAT CSV' < /usr/src/app/tblsale\_in.csv

csv必须是逗号分隔,而且DateTime格式必须是不带毫秒的, 不然会有下面的错误

clickhouse-client --query='INSERT INTO tblSale FORMAT CSV' < /usr/src/app/tblsale\_in.csv#
Code: 117. DB::Exception: Expected end of line: (at row 1)

Row 1:
Column 0,   name: id,          type: UInt32,   parsed text: "1"
Column 1,   name: prod\_id,     type: UInt32,   parsed text: "1"
Column 2,   name: user\_id,     type: UInt32,   parsed text: "1"
Column 3,   name: cnt,         type: UInt32,   parsed text: "1"
Column 4,   name: total\_price, type: Float32,  parsed text: "1.00"
Column 5,   name: date,        type: DateTime, parsed text: "2022-04-22 17:56:00"
ERROR: garbage after DateTime: ".783<CARRIAGE RETURN><LINE FEED>2,2,"
ERROR: **DateTime must be** **in YYYY-MM-DD hh:mm:ss or NNNNNNNNNN (unix timestamp, exactly 10** **digits) format.**

: While executing CSVRowInputFormat: data for INSERT was parsed from stdin: (in query: INSERT INTO tblSale FORMAT CSV). (INCORRECT\_DATA)

我的数据是从SQLServer导出的, 可以用BCP导出指定逗号分隔

bcp "SELECT  \[id\],\[prod\_id\],\[user\_id\] ,\[cnt\],\[total\_price\],Convert(varchar(20) , date, 120) as date FROM \[taobao\].\[dbo\].\[tblSale\_in\]" queryout tblSale\_in.csv -c -t , -T -S .\\SQLExpress

当我导入的csv数据量很少时, 很快就成功了, 我再用有200w条数据时的csv, 执行这条命令是感觉就没有反应了.新打开一个CLI,进入 clickhouse-client 也没有反应了,不知道是否死掉了

等待5分种后,出现这个错误, 

Code: 209. DB::NetException: Timeout exceeded while reading from socket (127.0.0.1:9000, 300000 ms). (SOCKET\_TIMEOUT)

 但是第2天再导入的时候就算导入2000w数据也很快, 在1分钟以内, 不知道什么原因

\=========================以下是ClickHouse和SQLServer 2012的性能对比=====================================

硬件: THINKPAD470, I5CPU,8G内存,机械硬盘

\--统计每日销量，2000w数据 SQLServer 17秒


SELECT Convert(varchar(8) , date, 112) as date,count(\*) as dayCnt
  FROM \[taobao\].\[dbo\].\[tblSale\] group by Convert(varchar(8) , date, 112) 
  order by  Convert(varchar(8) , date, 112) 

\--按日统计数量, ClickHouse 2500w数据第一次用时8.35秒,第二次用时0.3秒
SELECT toYYYYMMDD(date), count(\*) as dayCnt  FROM tblSale 
group by toYYYYMMDD(date) order by toYYYYMMDD(date)

继续导入2000w数据, 再用上面的SQL, clickhouse 4500w数据用时0.58秒, 6000w数据用时0.5秒 (第一次查询用1.24秒), 8000w数据用时0.68秒,1亿数据用时0.95秒

\=================== 结论==================================

对于一般OLAP应用, 比如查用户的历史记录, 不要去搞分库分表, 那是条歪路,还是转到列数据库, 搞个大宽表, 尽量不要用Join, 单机都能达到这么高的性能.