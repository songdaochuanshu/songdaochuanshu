---
layout: post
title: "什么是机器学习回归算法？【线性回归、正规方程、梯度下降、正则化、欠拟合和过拟合、岭回归】"
date: "2022-04-05T20:19:26.574Z"
---
什么是机器学习回归算法？【线性回归、正规方程、梯度下降、正则化、欠拟合和过拟合、岭回归】
============================================

1 、线性回归
=======

1.1 线性回归应用场景
------------

*   房价预测
*   销售额度预测
*   金融：贷款额度预测、利用线性回归以及系数分析因子

1.2 什么是线性回归
-----------

### 1.2.1定义与公式

线性回归(Linear regression)是利用**回归方程(函数)**对一个或**多个自变量(特征值)和因变量(目标值)之间**关系进行建模的一种分析方式。

*   特点：只有一个自变量的情况称为单变量回归，大于一个自变量情况的叫做多元回归

![](https://img2022.cnblogs.com/blog/2090080/202204/2090080-20220405083244414-1694560486.png)

那么怎么理解呢？我们来看几个例子

*   **期末成绩：0.7×考试成绩+0.3×平时成绩**
*   **房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率**

上面两个例子，**我们看到特征值与目标值之间建立的一个关系，这个可以理解为回归方程**。

1.3 线性回归的损失和优化原理
----------------

**假设刚才的房子例子，真实的数据之间存在这样的关系**

    真实关系：真实房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率
    

那么现在呢，我们随意指定一个关系（猜测）

    随机指定关系：预测房子价格 = 0.25×中心区域的距离 + 0.14×城市一氧化氮浓度 + 0.42×自住房平均房价 + 0.34×城镇犯罪率
    

这两个关系肯定是存在误差的，那么我们怎么表示这个误差并且衡量优化呢？

### 1.3.1 损失函数

**最小二乘法**

![](https://img2022.cnblogs.com/blog/2090080/202204/2090080-20220405083254671-427596596.png)

*   y\_i为第i个训练样本的真实值
*   h(x\_i)为第i个训练样本特征值组合预测函数

**如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！**

### 1.3.2 优化算法---正规方程

**如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）**

![](https://img2022.cnblogs.com/blog/2090080/202204/2090080-20220405083304655-1781876660.png)

理解：**X为特征值矩阵，y为目标值矩阵**。直接求到最好的结果

缺点：当特征过多过复杂时，求解速度太慢并且得不到结果

### 1.3.2 优化算法---梯度下降

![](https://img2022.cnblogs.com/blog/2090080/202204/2090080-20220405083313263-1069949301.png)

理解：**α为学习速率，需要手动指定（超参数），α旁边的整体表示方向**

沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新W值

使用：面对训练数据规模十分庞大的任务 ，能够找到较好的结果

![](https://img2022.cnblogs.com/blog/2090080/202204/2090080-20220405083325835-143149359.png)

1.4 线性回归API
-----------

*   sklearn.linear\_model.LinearRegression(fit\_intercept=True)
    *   通过正规方程优化
    *   fit\_intercept：是否计算偏置
    *   LinearRegression.coef\_：回归系数
    *   LinearRegression.intercept\_：偏置
*   sklearn.linear\_model.SGDRegressor(loss="squared\_loss", fit\_intercept=True, learning\_rate ='invscaling', eta0=0.01)
    *   SGDRegressor类实现了**随机梯度**下降学习，它支持不同的**loss函数和正则化惩罚项**来拟合线性回归模型。
    *   loss:损失类型
        *   **loss=”squared\_loss”: 普通最小二乘法**
    *   fit\_intercept：是否计算偏置
    *   learning\_rate : string, optional
        *   学习率填充
        *   **'constant': eta = eta0**
        *   **'optimal': eta = 1.0 / (alpha \* (t + t0)) \[default\]**
        *   'invscaling': eta = eta0 / pow(t, power\_t)
            *   **power\_t=0.25:存在父类当中**
        *   **对于一个常数值的学习率来说，可以使用learning\_rate=’constant’ ，并使用eta0来指定学习率。**
    *   SGDRegressor.coef\_：回归系数
    *   SGDRegressor.intercept\_：偏置

1.5 回归性能评估
----------

**均方误差(Mean Squared Error)MSE)评价机制：**

![](https://img2022.cnblogs.com/blog/2090080/202204/2090080-20220405083336770-2053844321.png)

**注：y^i为预测值，¯y为真实值**

*   sklearn.metrics.mean\_squared\_error(y\_true, y\_pred)
    *   均方误差回归损失
    *   y\_true:真实值
    *   y\_pred:预测值
    *   return:浮点数结果

1.6 案例（正规方程的优化方法对波士顿房价进行预测）
---------------------------

    def linear1():
        """
        正规方程的优化方法对波士顿房价进行预测
        :return:
        """
        # 1）获取数据
        boston = load_boston()
    
        # 2）划分数据集
        x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)
    
        # 3）标准化
        transfer = StandardScaler()
        x_train = transfer.fit_transform(x_train)
        x_test = transfer.transform(x_test)
    
        # 4）预估器
        """
        通过正规方程优化
        fit_intercept：是否计算偏置
        LinearRegression.coef_：回归系数
        LinearRegression.intercept_：偏置
        """
        estimator = LinearRegression()
        estimator.fit(x_train, y_train)
    
        # 5）得出模型
        print("正规方程-权重系数为：\n", estimator.coef_)
        print("正规方程-偏置为：\n", estimator.intercept_)
    
        # 6）模型评估
        y_predict = estimator.predict(x_test)
        print("预测房价：\n", y_predict)
        error = mean_squared_error(y_test, y_predict)
        print("正规方程-均方误差为：\n", error)
    
        return None
    

1.7 案例（梯度下降的优化方法对波士顿房价进行预测）
---------------------------

    def linear2():
        """
        梯度下降的优化方法对波士顿房价进行预测
        :return:
        """
        # 1）获取数据
        boston = load_boston()
        print("特征数量：\n", boston.data.shape)
    
        # 2）划分数据集
        x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)
    
        # 3）标准化
        transfer = StandardScaler()
        x_train = transfer.fit_transform(x_train)
        x_test = transfer.transform(x_test)
    
        # 4）预估器
        """
        sklearn.linear_model.SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)
        学习率填充
        'constant': eta = eta0
        'optimal': eta = 1.0 / (alpha * (t + t0)) [default]
        'invscaling': eta = eta0 / pow(t, power_t)
        power_t=0.25:存在父类当中
        对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。
        """
        estimator = SGDRegressor(learning_rate="constant", eta0=0.01, max_iter=10000, penalty="l1")
        estimator.fit(x_train, y_train)
    
        # 5）得出模型
        print("梯度下降-权重系数为：\n", estimator.coef_)
        print("梯度下降-偏置为：\n", estimator.intercept_)
    
        # 6）模型评估
        y_predict = estimator.predict(x_test)
        print("预测房价：\n", y_predict)
        error = mean_squared_error(y_test, y_predict)
        print("梯度下降-均方误差为：\n", error)
    
        return None
    

2、欠拟合与过拟合
=========

2.1 什么是过拟合与欠拟合
--------------

![](https://img2022.cnblogs.com/blog/2090080/202204/2090080-20220405083354180-2046962801.png)

![](https://img2022.cnblogs.com/blog/2090080/202204/2090080-20220405083402768-742714237.png)

*   分析
    *   第一种情况：因为机器学习到的天鹅特征太少了，导致区分标准太粗糙，不能准确识别出天鹅。
    *   第二种情况：机器已经基本能区别天鹅和其他动物了。然后，很不巧已有的天鹅图片全是白天鹅的，于是机器经过学习后，会认为天鹅的羽毛都是白的，以后看到羽毛是黑的天鹅就会认为那不是天鹅。

### 2.1.1 定义

*   过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂)
*   欠拟合：一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。(模型过于简单)

### 2.1.2 原因和解决办法

*   欠拟合原因以及解决办法
    *   原因：学习到数据的特征过少
    *   解决办法：增加数据的特征数量
*   **过拟合原因以及解决办法**
    *   原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点
    *   解决办法：
        *   **正则化**

2.2 正则化类别
---------

*   **L2正则化**
    *   作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响
    *   优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象
    *   Ridge回归
*   **L1正则化**
    *   作用：可以使得其中一些W的值直接为0，删除这个特征的影响
    *   LASSO回归

3、带有L2正则化的线性回归-岭回归
==================

3.1 岭回归API
----------

*   sklearn.linear\_model.Ridge(alpha=1.0, fit\_intercept=True,solver="auto", normalize=False)
    *   具有L2正则化的线性回归
    *   alpha:**正则化力度**，也叫 λ
        *   **λ取值：0~1 1~10**
    *   solver:会根据数据自动选择优化方法
        *   **sag:如果数据集、特征都比较大，选择该随机梯度下降优化**
    *   normalize:数据是否进行标准化
        *   normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据
    *   Ridge.coef\_:回归权重
    *   Ridge.intercept\_:回归偏置

Ridge方法相当于**SGDRegressor(penalty='l2', loss="squared\_loss")**

只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG随机梯度下降)

*   sklearn.linear\_model.**RidgeCV**(\_BaseRidgeCV, RegressorMixin)
    *   具有l2正则化的线性回归，可以进行交叉验证
    *   coef\_:回归系数

![](https://img2022.cnblogs.com/blog/2090080/202204/2090080-20220405083435281-1149039808.png)

*   正则化力度越大，权重系数会越小
*   正则化力度越小，权重系数会越大

* * *

注：参考了黑马程序员相关资料。