---
layout: post
title: "【机器学习】李宏毅——Adversarial Attack（对抗攻击）"
date: "2022-12-23T02:36:55.625Z"
---
【机器学习】李宏毅——Adversarial Attack（对抗攻击）
===================================

![【机器学习】李宏毅——Adversarial Attack（对抗攻击）](https://img2023.cnblogs.com/blog/2966067/202212/2966067-20221223100210535-886259556.png) 本文主要介绍了Adversarial Attack（对抗攻击）当前的研究现状，包括如何攻击、攻击的类别，以及原始模型如何进行防御等相关知识点。

研究这个方向的动机，是因为在将神经网络模型应用于实际场景时，它仅仅拥有较高的正确率是不够的，例如在异常检测中、垃圾邮件分类等等场景，那些负类样本也会想尽办法来“欺骗”模型，使模型无法辨别出它为负类。因此我们希望我们的模型能够拥有应对这种攻击的能力。

### How to Attack

通过影像辨识的例子来解释如何进行攻击。

假设我们已经训练了一个图像的分类器，对于我们下图中输入的图片它能够分辨出来是一只猫；那么我们现在对原始的输入进行一定的扰动，加入干扰项再输入到模型中看看它是否会辨别成其中的东西（图中这种扰动太大了，一般加入的扰动项是人眼无法辨别的）：

![在这里插入图片描述](https://img-blog.csdnimg.cn/aad7bcceebb04ecca2e5e29a9138cfab.png#pic_center)

那么这种攻击又划分为两类：

*   Non-targeted：这一类的攻击只要求能够让模型无法辨认出来是猫就行
*   targeted：这一类的模型是有目的性的，除了让模型无法辨别出来是猫之外，还希望让模型辨别出来是特定的物品

这里有一个很神奇的现象，假设我们加入的杂讯比较大，我们人眼能够直接观察得到：

![在这里插入图片描述](https://img-blog.csdnimg.cn/21eaa8be008841e0b7990e30212807a7.png#pic_center)

可以发现机器还是大部分能够发现这是一只猫，只不过可能品种不同而已。但如果加入的杂讯是我们特别准备的，并且肉眼看不出来的：

![在这里插入图片描述](https://img-blog.csdnimg.cn/92053516ee18402595a3852cf66f4de0.png#pic_center)

可以看到**不仅分类错误了，连信心分数都激增**，并且事实上**我们可以调整我们的杂讯让机器把这张看起来像一只猫的图片分辨成任何东西**，因此这也是我们需要机器能够对抗攻击的原因之一。

那么接下来我们就来认识一下是怎么做到这种攻击的。

对于我们当前拥有的分类器，输入一张猫的图片\\(x^0\\)它输出为一个向量，是每一个类别的信心分数，其中最高的为猫。

*   对于无目标的攻击来说，**我们在原始图像更换成一张图片\\(x\\)，它也经过分类器的处理后输出一个向量，那希望是这个向量能够和猫这个类别对应的One-hat-vector之间的距离越远越好**
*   对于有目标的攻击来说，**我们在原始图像更换成一张图片\\(x\\)，它也经过分类器的处理后输出一个向量，那不仅希望是这个向量能够和猫这个类别对应的One-hat-vector之间的距离越远越好，还希望这个向量与目标类别的One-hat-vector之间的差距越小越好**

而对于向量之间的差距我们可以用交叉熵来表示，因此得到：  
![在这里插入图片描述](https://img-blog.csdnimg.cn/5a08fa32784d458ebd3a69c6c1afd79d.png#pic_center)

那么对于无目标的攻击，可以设定其损失函数为：

\\\[L(x)=-e(y,\\hat{y}) \\\]

对于有目标的攻击可以设定其损失函数为：

\\\[L(x)=-e(y,\\hat{y})+e(y,y^{target}) \\\]

因此优化目标为：

\\\[x^\*=argmin\_{d(x^0,x)<\\varepsilon}~ L(x) \\\]

其中\\(d(x^0,x)<\\varepsilon\\)代表我们希望加入的图像和原始的图像比较接近，这样肉眼才看不出来。而这个距离的计算方式下面举两个例子：

![在这里插入图片描述](https://img-blog.csdnimg.cn/ef131efa2fb54c648701304b11c7fc37.png#pic_center)

*   L2-norm：\\(d(x^0,x)=\\lVert \\Delta \\vec{x}\\rVert =\\sum\_{i=1}(\\Delta x\_i)^2\\)
*   L-infinity：\\(d(x^0,x)=\\lVert \\Delta \\vec{x} \\rVert=max\\{\\Delta x\_i\\}\\)

那么这两种距离计算方法的区别**在于人眼的观感程度**，这也许听起来很抽象但可以通过下面的例子解释：

![在这里插入图片描述](https://img-blog.csdnimg.cn/a7b262c85cda48b88dd03a5d9ea3f9e5.png#pic_center)

上方和下方的图它们与原始的图的L2-norm距离相同，只不过上方的图距离都分散的每一个像素，下方的图集中在右下角的像素，因此下方的图我们能够明显感受出差别。  
而这两张图的L-infinity差距是不同的，第一张图显然比第二张图小。**那么为了要让我们人眼无法辨认，我们需要对这个L-infinity进行限制，才可以让我们无法看出来**，因此一般是选择L-infinity。

那么下面的问题就是**我们如何求解这个优化问题**：

\\\[x^\*=argmin\_{d(x^0,x)<\\varepsilon}~ L(x) \\\]

对于此问题，**跟之前我们训练模型时调整参数是一样的，只不过调整的参数变成了输入而已，同样也可以采用梯度下降来求解**，只不过要加上一定的限制而已：

![在这里插入图片描述](https://img-blog.csdnimg.cn/140c199d616848de98aaa605c2383b0e.png#pic_center)

**进行梯度更新的时候，我们要时刻检查更新之后其距离是否会超过限制的范围，如果超过了就要及时地将其修正回来**。

有一个应用上述思想的简单算法为FGSM，其特点在于：

*   它只迭代一次
*   它的学习率设为我们限制的距离大小
*   它的梯度计算出来后会对每一个分量加上一个Sign函数，使其成为正负1

![在这里插入图片描述](https://img-blog.csdnimg.cn/99dd570049e848a6a0d3ff29534454de.png#pic_center)

那么可以看到，**这样更新一次是绝对不会超过范围的**，因此这样找到的或许是可行的。

### White Box v.s. Black Bos

在前面介绍的攻击方法中我们需要计算梯度，**而计算梯度则要求我们知道该模型内部的参数**，因此这一种攻击称为White Box（白箱攻击），这一种攻击对于一些未知模型来说可能是无法进行的。但这不代表着不让别人知道模型参数就是安全的，因为还有Black Bos（黑箱攻击），这一类攻击不需要知道模型内部的参数就可以发动攻击。

对于黑箱攻击来说，一种简单的情况是**我们知道这个未知的模型是由哪些训练资料训练出来的，那么我们就可以用一个具有类似网络结构的模型也对这些训练资料进行训练，得到我们自己模型的参数，那么再在这个模型上计算如何攻击，最终将得到的攻击应用到目标模型中就可能会成功**，如下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/27d3815eb8a649afa53dbe248c74e861.png#pic_center)

那么复杂一点的情况就是我们也不知道训练集，那么可能可行的做法为**用一大堆我们自己的资料放进去这个模型，相应会得到一大堆输出，那么将这些输入和输出来作为训练集，就可以类比上面的做法进行训练**。

在实际上，黑箱攻击和白箱攻击都是很容易成功的，**并且在一个模型上攻击成功的x，用在另外的模型上也非常容易攻击成功**，那么这就让人有了研究的空间，但目前仍然没有明确的答案，值得让人信服的解释是**实际上攻击是否成功主要取决于你的训练资料而不是取决于你的模型，相同的训练资料所训练出来的不同模型在被攻击时很可能呈现相同的结果，也就是说攻击可以认为是“具有特征性的”，也许你得到的这个攻击向量看起来真的很像是杂讯， 但这可能就是机器从训练资料中学习到的特征**。

这里补充一个小知识点，因为我们之前说到的攻击都说**客制化的**，即对于每一张图片单独计算它的攻击向量，那么假设影响场景为某个摄像头，我们想要让这个摄像头对于输入都辨认错误的话则要对每张图片都计算，那么运算量很大；那么有没有可能**能够有一个通用的攻击向量，如果将它加入摄像头摄取的每一个图片的时候，都能够使该图片被辨认错误，那么这样的攻击称为Universal Adversarial Attack**。这种是可能可以做到的。

### 其他攻击类型综述

#### Adversarial Reprogramming

这一个类别的攻击在于**直接攻击模型**，它像是寄生虫一样寄生于别的模型之上，然后让其他已经训练好的模型来做我们想做的任务

#### "Backdoor" in Model

这种开后门的方法是**在训练阶段就已经攻击了**，例如在训练阶段加入特定的图片，让训练完成后的模型看到某一张特定图片的鱼是会分辨成狗。但是这种训练要保证我们加进去的特殊图片它是人眼无法检查出来的，不能说加入一大堆鱼的图片然后标准改成狗，这是不行的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/180bb94fa0d946cab10e2779d8462531.png#pic_center)

### Defense

前面都是在讲如何进行攻击，那么接下来进行介绍我们如何进行防御

#### Passive Defense

这类防御，训练完模型之后就不改变模型，**而是在将样本输入到模型之前，增加一个filter环节**，如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/dfd7a04e268b4ec9ae717259e4c9b4e6.png#pic_center)

那么这个Filter的作用可以认为**是削减这个攻击信号的威力，使我们的模型仍然能够正常的进行辨认**。那么这个Filter也不一定特别复杂，有时候例如进行模糊化就可以达到我们想要的效果，但要注意模糊化也有负作用，就是让机器的信心分数降低：

![在这里插入图片描述](https://img-blog.csdnimg.cn/5d72afce020a4477a61940bd663cda9a.png#pic_center)

类似于这种方法还有很多，例如**将影像进行压缩和解压缩，可能就可以让攻击信号失去威力，或者通过AE自编码器来重新生成，也可能可以过滤掉杂讯**等等。而这种被动防御一般来说如果被攻击方明确你采用的防御方法，那么就非常容易被破解。  
那么可能的改进是**加入随机化，即在对图像处理处理的时候随机选择可选的处理策略**，不过还是得保护住你随机的分布才可以保持防御的有效性。

#### Proactive Defense

这种思想是**在训练时就训练一个不容易被攻击的模型**。具体的做法是**我们自己创造攻击类型的向量来进行攻击训练**，即对原始的样本修改为攻击的样本，不过我们要加上它原来正确的标签，将这些作为训练资料来对模型进行训练。

![在这里插入图片描述](https://img-blog.csdnimg.cn/8a1148ebd29949b59169c0781e000922.png#pic_center)

但这个问题主要是能够抵挡你训练过的那些攻击的方法，对于未见过的攻击算法很可能挡不住。