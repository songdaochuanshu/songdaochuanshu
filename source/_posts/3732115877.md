---
layout: post
title: "在有限 computational budget 下，借助 low-fidelity 模型提高精度"
date: "2023-04-16T01:09:53.657Z"
---
在有限 computational budget 下，借助 low-fidelity 模型提高精度
=================================================

motivation：一些预算用于训 low-fidelity model，剩下预算用于 Monte Carlo 模拟，以得到结果。数学证明：近似 + 递推或迭代或归纳法。总结：目前看来，对我的工作意义不大。

  

*   论文名称：context-aware learning of hierarchies of low-fidelity models for multi-fidelity uncertainty quantification
*   链接：[https://www.sciencedirect.com/science/article/pii/S0045782523000312](https://www.sciencedirect.com/science/article/pii/S0045782523000312)
*   国际计算力学领域的顶级期刊《Computer Methods in Applied Mechanics and Engineering》（中科院一区 TOP，IF：6.756）

### 0 abstract

*   背景：
    
    *   multi-fidelity Monte Carlo 方法利用 low-fidelity and surrogate models 来减少方差（variance），使不确定性量化变得可行，尽管物理系统的 high-fidelity 数值模拟计算成本很高。
*   工作简述：
    
    *   我们提出了一种 context-aware 的 multi-fidelity Monte Carlo 方法，实现了训练 low-fidelity 模型的成本和 Monte Carlo 采样的成本之间的最佳平衡。
        
    *   当训练 low-fidelity 模型时，我们考虑到了所学的 low-fidelity 模型将被使用的背景，即在 Monte Carlo 估计中减少方差，这使得它能够在训练和抽样之间找到最佳的权衡，以最小化给定计算预算（computational budget）下估计器的均方误差（mean-squared error）上限。
        
*   继承了之前的工作：
    
    *   它将以前开发的 context-aware bi-fidelity Monte Carlo 方法，推广到多个模型的层次结构 和 更普遍的 low-fidelity 模型类型，如 sparse-grid（比如说 PDE 仿真的网格粒度粗一点）和 deep-network。
*   文献树上的位置：
    
    *   我们与传统的 surrogate modeling 和 model reduction 技术不一样，后者构建 low-fidelity 模型的主要目的是为了很好地接近 high-fidelity 模型的输出，通常忽略了所学模型在 upstream tasks 中的 context。
*   实验结果：
    
    *   用陀螺动力学模拟代码 Gene 进行的数值实验表明，在做一个不确定性量化时，与 single-fidelity Monte Carlo 和 standard multi-fidelity estimators 相比，速度提高了两个数量级：相当于在德州高级计算中心 Lonestar6 超级计算机的一个节点上，运行时间从 72 天减少到 4 小时。

### 1 intro & related method

*   literature：\[1\] 是一个 Multi-Fidelity 的 survey。其他 literature 懒得整理了。
*   motivation：如果没有现成的 low-fidelity model，那么就需要首先训练得到它们，这可能会产生额外的计算成本，并且需要对 high-fidelity model 进行额外的评估，以产生训练数据。
*   main idea：该方法将 ① 训练多个 low-fidelity 模型的层次的成本 ② 蒙特卡洛采样以获得多保真估计器的成本进行 trade-off，在给定的 computational budget 下，使均方误差（mean-squared error）的上限最小（context-aware：最大限度地减少蒙特卡罗估计的方差），而不是尽可能接近 high-fidelity model。
*   structure：
    *   2：preliminaries，介绍符号定义，传统的 multi-fidelity Monte Carlo 算法，他们之前做的一个 bi-fidelity context-aware 算法。
    *   3：method。
    *   4：两个 experiment，1 具有九个不确定参数的二维空间域上的热传导问题，2 具有不确定输入的现实等离子体微扰动情况。数值结果的代码：[https://github.com/ionutfarcas/context-aware-mfmc](https://github.com/ionutfarcas/context-aware-mfmc)

### 2 背景 & 前情提要

#### 2.1 背景：static multi-fidelity Monte Carlo estimation

*   \\(f^{(0)}:X→Y\\) 是一个输入-输出响应（input-output response），expensive to evaluate。输入为 d 维，输出为 1 维。
    *   对一个随机变量 Θ=\[Θ1,Θ2,...,Θd\]^T，我们想估计 f^(0)(Θ) 的期望值 μ0。
*   MFMC（multi-fidelity Monte Carlo）estimator 包含 k+1 个模型，f^(0) high-fidelity，f^(1) ... f^(k) low-fidelity。
    *   low-fidelity model 的精度 ρ：用 f^(j) 对 f^(0) 的 Pearson correlation coefficient 来定义：\\(\\rho\_j = Cov\[f^{(0)}, f^{(j)}\]/σ\_0σ\_j\\)，其中 σ 是方差（variance）。设定 ρ\_k+1 = 0。
    *   models 的评估成本：w1, w2, ..., wk＞0。归一化 high-fidelity f^(0) 的评估成本 w0 = 1。
    *   假设模型们满足排序：精度：1 = |ρ0|＞|ρ1|＞…＞|ρk|；评估成本：\\(w\_{j-1}/w\_{j}\\gt\[ρ^2\_{j-1}-ρ^2\_j\]/\[ρ^2\_{j}-ρ^2\_{j+1}\]\\)。
*   设 m\_j 为 model f^(j) 的评估次数，0 ≤ m0 ≤ m1 ≤ … ≤ m\_k。每一次评估都从独立同分布（iid）的分布 \\(\\pi\\) 里抽样。
*   于是 MFMC estimator 形式：\\(\\hat E^{MFMC} = \\hat E\_{m\_0}^{(0)}+\\sum\_{j=1}^k\\alpha\_j(\\hat E\_{m\_j}^{(j)}-\\hat E\_{m\_{j-1}}^{(j)})\\)，其中 $\\hat E\_{m\_j}^{(j)}=\\frac 1 {m\_0}f^{(0)}(\\boldsymbol\\theta\_i) $ 即 f(θ) 的均值。
*   总 computational cost： \\(p=\\sum\_{j=0}^km\_jw\_j\\)。
*   我们把 p 固定（budget），去找最优的 \\(m\_0^\*, \\cdots, m\_k^\*\\) 以及 \\(\\alpha\_0^\*, \\cdots, \\alpha\_k^\*\\)，来让 \\(\\hat E^{MFMC}\\) 的方差最小。
    *   \\(\\hat E^{MFMC}\\) 的 MSE = \\(\\frac {\\sigma\_0^2}p\\bigg(\\sum\_{j=0}^k\\sqrt{w\_j(\\rho\_j^2-\\rho\_{j+1}^2)}\\bigg)^2\\)。
    *   其实是有闭式解的，见 \[14\]。

#### 2.2 前情提要：context-aware bi-fidelity Monte Carlo estimator

*   他们之前做的 context-aware bi-fidelity MC estimator 的工作是 \[2\]。
    
    *   改了一下 notation： low-fidelity model \\(f\_n^{(1)}\\) 表示训 f^(1) 需要用 high-fidelity f^(0) 的 n 个样本。
    *   假设所有 low-fidelity model 都是用相同的 NN 来训，唯一不同的是训练样本数量，那么 Pearson 系数 ρ1 和评估成本 w1 都取决于 n。
    *   【这是假设 assumption】Pearson 系数的 bound：\\(1-\\rho\_1^2(n)\\le c\_1n^{-\\alpha}\\)；评估成本的 bound：\\(w\_1(n)\\le c\_2n^\\beta\\)；其中 c1 c2 α＞0 β＞0 都是常数。
*   我们的 budget 是 p。如果用 n 个样本训练 f^(1)，那么还有 p-n 的预算用于 f^(1) 的评估。
    
*   context-aware bi-fidelity MC estimator： \\(\\hat E\_n^{CA-MFMC}=\\hat E\_{m\_0^\*}^{(0)}+\\alpha\_1^\*(E\_{m\_1^\*}^{(1)}-E\_{m\_0^\*}^{(1)})\\) ，决策变量为 \\(m\_0^\*, m\_1^\*, α\_1^\*\\) ，目标函数为最小化 \\(\\hat E\_n^{CA-MFMC}\\) 的 MSE。
    
    *   \\(\\hat E\_n^{\\rm CA-MFMC}\\) 的 MSE = \\(\\frac{\\sigma\_0^2}{p-n}\\bigg(\\sqrt{1-\\rho\_1^2(n)}+\\sqrt{w\_1(n)\\rho\_1^2(n)}\\bigg)^2\\) （公式 2.6）。
*   如果预算 p 是固定的，n 可以通过最小化 MSE 的上界来选择。
    
    *   上界： \\(\\rm {MSE}(\\hat E\_n^{CA-MFMC})\\le\\frac{2\\sigma\_0^2}{p-n}(c\_1n^{-\\alpha}+c\_2n^\\beta)\\) 。
    *   工作 \[2\] 表明，在某些假设下，给定一个 p，存在一个唯一的 n∗，最小化(2.6)；然而，n∗ 没有闭式解，只能数值寻找。
    *   最佳的 n∗ 是独立于预算 p 的。

### 3 method

#### 3.1 一些关于 multi-fidelity models 的假设

*   假设 1：存在 \\(c\_{a,j}\\ge0\\)，函数 \\(r\_{a,j}(n\_j)\\) 值为正数、对 n\_j 单调递减、二次可微。限制精度（Pearson 系数）： \\(1-ρ\_j^2(n\_j)\\le c\_{a,j}r\_{a,j}(n\_j)\\)。
*   假设 2：存在 \\(c\_{c,j}\\ge0\\)，函数 \\(r\_{c,j}(n\_j)\\) 值为正数、对 n\_j 单调递增、二次可微。限制评估成本： \\(w\_j(n\_j)\\le c\_{c,j}r\_{c,j}(n\_j)\\)。
*   貌似，假设两个 r 函数为： \\(r\_{a,j}=n^{-\\alpha},r\_{c,j}=n^\\alpha,\\alpha\\gt0\\) 。
*   一个备注：事实上，如果一组数据拿去训 f^(i)，那么也有可能可以拿去训 f^(j)；不过，更有可能的一种情况是，两个模型结构不一样，需要的训练数据结构也不一样，所以不能重用，所以，下文都不考虑样本的重用。

#### 3.2 只用一个 low-fidelity 模型：\[2\] 基础上的改进

*   首先，放缩 \\(\\rm MSE(\\hat E\_n^{CA-MFMC})\\le\\frac{2\\sigma\_0^2}{p-n}(c\_{a,1}r\_{a,1}(n\_1)+c\_{c,1}r\_{c,1}(n\_1))\\)，将它记为 u1。接下来，我们关心这个 upper bound 何时存在唯一的全局最小值。
    *   PS：证明直接看原文吧，本科高数难度。
*   命题 1 ：u1 何时存在唯一的全局最小值：
    *   假设满足 \\(c\_{a,1}r''\_{a,1}(n\_1)+c\_{c,1}r''\_{c,1}(n\_1)\\gt0\\)【公式 (3.6)】。那么，u1 具有唯一的全局最小值 \\(n\_1^\*\\in\[1,p-1\]\\)。
*   命题 2 ：假设对于所有 \\(n\_1\\in(0,\\infty)\\) 满足 公式 (3.6)，
    *   并且存在一个 \\(\\bar n\_1\\in(0,\\infty)\\) 使得 \\(c\_{a,1}r\_{a,1}(\\bar n\_1)+c\_{c,1}r'\_{c,1}(\\bar n\_1)=0\\)。那么 \\(\\bar n\_1\\) 是唯一的，并且 \\(n\_1^\*\\le\\max\\{1,\\bar n\_1\\}\\)。

### 3.3 context-aware multi-fidelity MC sampling

一种 sequential 训练方法，来为 CA-MFMC estimator 拟合 hierarchies of low-fidelity models，其中每一步都实现了 training 和 sampling 之间的 optimal trade-off。

我主要关心 context-aware 是什么东西。

*   引理 1：在假设 1 假设 2 下，CA-MFMC estimator 的 MSE 的 upper bound：
    *   \\(\\rm MSE(\\hat E\_{n\_1,\\cdots,n\_k}^{CA-MFMC}) \\le \\frac{(k+1)\\sigma\_0^2}{p\_{k-1}-n\_k}(\\kappa\_{k-1}+\\hat c\_{a,k}r\_{a,k}(n\_k)+c\_{c,k}r\_{c,k}(n\_k))\\) 。
    *   其中 \\(p\_{k-1}=p-\\sum\_{j=1}^{k-1}n\_j,~~p\_0=p\\) ，
    *   \\(\\kappa\_{k-1}=c\_{a,1}r\_{a,1}(n\_1)+\\sum\_{j=1}^{k-2}c\_{c,j}r\_{c,j}(n\_j)c\_{a,j+1}r\_{a,j+1}(n\_{j+1}),~~\\kappa\_0=0\\) ，
    *   \\(\\hat c\_{a,k} = c\_{c,k-1}r\_{c,k-1}(n\_{k-1})c\_{a,k},~~\\hat c\_{a,1} =c\_{a,1}\\) 。
    *   （重申：n 是训 low-fidelity model 的样本数量）
    *   证明：直接用一个 平方和不等式 展开。
*   看这个 upper bound 括号内加和的部分，\\(\\hat c\_{a,k}\\) 和 \\(κ\_{k-1}\\) 都仅依赖于 \\(n\_1, \\cdots,n\_{k-1}\\)，而 \\(r\_{a,k}(n\_k),~r\_{ck}(n\_k)\\) 仅依赖于 n\_k。这启发了一种 sequentially 向 CA-MFMC estimator 添加 low-fidelity model 的做法。
    *   给定 \\(n\_1, \\cdots,n\_{k-1}\\)，寻找 \\(n\_k\\)，使得 \\(u\_j(n\_j;n\_1, \\cdots,n\_{k-1}):\[1,p\_{j-1}-1\]\\rightarrow(0,\\infty)\\)，\\(u\_j(n\_j;n\_1, \\cdots,n\_{k-1})=\\frac1{p\_{j-1}-n\_j}(\\kappa\_{j-1}+\\hat c\_{a,j}r\_{a,j}(n\_j)+c\_{c,k}r\_{c,k}(n\_j))\\)。
*   命题 3：使用命题 1，即 \\(n\_1^\*\\) 是 u1 的全局最小值。现在去考虑 j = 2,3,...,k。
    *   若 \\(\\hat{c}\_{a, j} r\_{a, j}^{\\prime \\prime}\\left(n\_j\\right)+c\_{c, j} r\_{c, j}^{\\prime \\prime}\\left(n\_j\\right)>0\\)，则存在 u\_j 的全局最小值 \\(n\_j^\* \\in\\left\[1, p\_{j-1}-1\\right\]\\)。
    *   证明好像跟命题 1 同理。
*   命题 4：使用命题 1，即 \\(n\_j^\*\\) 是 u\_j 的全局最小值。
    *   若存在 \\(\\bar{n}\_j \\in(0, \\infty)\\) 使得 \\(\\hat{c}\_{a, j} r\_{a, j}^{\\prime}\\left(\\bar{n}\_j\\right)+c\_{c, j} r\_{c, j}^{\\prime}\\left(\\bar{n}\_j\\right)=0\\)，则有 \\(n\_j^\* \\leq \\bar{n}\_j\\)，即 \\(n\_j^\*\\) 的一个 upper bound。
    *   继续跟命题 2 同理，归纳法。
*   一个备注：models 的 hierarchy 必须满足评估次数 m 递减（2.1）。

啊…… 这就结束了？感觉看了一肚子数学…

### 4 experiment

图挺好看的。

要赶着看 MFRL 了，不细看了。