---
layout: post
title: "Coursera 学习笔记｜Machine Learning by Standford University - 吴恩达"
date: "2022-04-04T10:20:46.857Z"
---
Coursera 学习笔记｜Machine Learning by Standford University - 吴恩达
============================================================

/ 20220404 Week 1 - 2 /

* * *

Chapter 1 - Introduction
========================

1.1 Definition
--------------

*   _Arthur Samuel_  
    The field of study that gives computers the ability to learn without being explicitly programmed.
*   _Tom Mitchell_  
    A computer program is said to learn from **experience E** with respect to some class of **tasks T** and **performance measure P**, if its performance at tasks in T, as measured by P, improves with experience E.

1.2 Concepts
------------

### 1.2.1 Classification of Machine Learning

*   **Supervised Learning 监督学习**：given a labeled data set; already know what a correct output/result should look like
    *   Regression 回归：continuous output
    *   Classification 分类：discrete output
*   **Unsupervised Learning 无监督学习**：given an unlabeled data set or an data set with the same labels; group the data by ourselves
    *   Clustering 聚类：group the data into different clusters
    *   Non-Clustering 非聚类
*   Others: Reinforcement Learning, Recommender Systems...

### 1.2.2 Model Representation

*   Training Set 训练集
    
    \\\[\\begin{matrix} x^{(1)}\_1&x^{(1)}\_2&\\cdots&x^{(1)}\_n&&y^{(1)}\\\\ x^{(2)}\_1&x^{(2)}\_2&\\cdots&x^{(2)}\_n&&y^{(2)}\\\\ \\vdots&\\vdots&\\ddots&\\vdots&&\\vdots\\\\ x^{(m)}\_1&x^{(m)}\_2&\\cdots&x^{(m)}\_n&&y^{(m)} \\end{matrix}\\\]
    
*   符号说明  
    \\(m=\\) the number of training examples 训练样本的数量 - 行数  
    \\(n=\\) the number of features 特征数量 - 列数  
    \\(x=\\) input variable/feature 输入变量/特征  
    \\(y=\\) output variable/target variable 输出变量/目标变量  
    \\((x^{(i)}\_j,y^{(i)})\\) ：第\\(j\\)个特征的第 \\(i\\) 个训练样本，其中 \\(i=1, ..., m\\)，\\(j=1, ..., n\\)
    

### 1.2.3 Cost Function 代价函数

### 1.2.4 Gradient Descent 梯度下降

Chapter 2 - Linear Regression 线性回归
==================================

\\\[\\begin{matrix} x\_0&x^{(1)}\_1&x^{(1)}\_2&\\cdots&x^{(1)}\_n&&y^{(1)}\\\\ x\_0&x^{(2)}\_1&x^{(2)}\_2&\\cdots&x^{(2)}\_n&&y^{(2)}\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\vdots&&\\vdots\\\\ x\_0&x^{(m)}\_1&x^{(m)}\_2&\\cdots&x^{(m)}\_n&&y^{(m)}\\\\ \\\\ \\theta\_0&\\theta\_1&\\theta\_2&\\cdots&\\theta\_n&& \\end{matrix}\\\]

2.1 Linear Regression with One Variable 单元线性回归
----------------------------------------------

*   Hypothesis Function
    
    \\\[h\_{\\theta}(x)=\\theta\_0+\\theta\_1x \\\]
    
*   Cost Function - **Square Error Cost Function 平方误差代价函数**
    

\\\[J(\\theta\_0,\\theta\_1)=\\frac{1}{2m}\\displaystyle\\sum\_{i=1}^m(h\_{\\theta}(x^{(i)})-y^{(i)})^2 \\\]

*   Goal
    
    \\\[\\min\_{(\\theta\_0,\\theta\_1)}J(\\theta\_0,\\theta\_1) \\\]
    

2.2 Multivariate Linear Regression 多元线性回归
-----------------------------------------

*   Hypothesis Function
    
    \\\[\\theta= \\left\[ \\begin{matrix} \\theta\_0\\\\ \\theta\_1\\\\ \\vdots\\\\ \\theta\_n \\end{matrix} \\right\],\\ x= \\left\[ \\begin{matrix} x\_0\\\\ x\_1\\\\ \\vdots\\\\ x\_n \\end{matrix} \\right\]\\\]
    
    \\\[\\begin{aligned}h\_\\theta(x)&=\\theta\_0+\\theta\_1x\_1+\\theta\_2x\_2+\\cdots+\\theta\_nx\_n\\\\ &=\\theta^Tx \\end{aligned}\\\]
    
*   Cost Function
    
    \\\[J(\\theta^T)=\\frac{1}{2m}\\displaystyle\\sum\_{i=1}^m(h\_{\\theta}(x^{(i)})-y^{(i)})^2 \\\]
    
*   Goal
    
    \\\[\\min\_{\\theta^T}J(\\theta^T) \\\]
    

2.3 Algorithm Optimization
--------------------------

### 2.3.1 Gradient Descent 梯度下降法

*   算法过程  
    Repeat until convergence(simultaneous update for each \\(j=1, ..., n\\))

\\\[\\begin{aligned} \\theta\_j &:=\\theta\_j-\\alpha{\\partial\\over\\partial\\theta\_j}J(\\theta^T)\\\\ &:=\\theta\_j-\\alpha{1\\over{m}}\\displaystyle\\sum\_{i=1}^m(h\_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}\_j \\end{aligned}\\\]

*   **Feature Scaling 特征缩放**  
    对每个特征 \\(x\_j\\) 有$$x\_j={{x\_j-\\mu\_j}\\over{s\_j}}$$  
    其中 \\(\\mu\_j\\) 为 \\(m\\) 个特征 \\(x\_j\\) 的平均值，\\(s\_j\\) 为 \\(m\\) 个特征 \\(x\_j\\) 的范围（最大值与最小值之差）或标准差。
*   **Learning Rate 学习率**

### 2.3.2 Normal Equation(s) 正规方程（组）

令

\\\[X=\\left\[ \\begin{matrix} x\_0&x^{(1)}\_1&x^{(1)}\_2&\\cdots&x^{(1)}\_n\\\\ x\_0&x^{(2)}\_1&x^{(2)}\_2&\\cdots&x^{(2)}\_n\\\\ \\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\ x\_0&x^{(m)}\_1&x^{(m)}\_2&\\cdots&x^{(m)}\_n\\\\ \\end{matrix} \\right\],\\ y=\\left\[ \\begin{matrix} y^{(1)}\\\\ y^{(2)}\\\\ \\vdots\\\\ y^{(m)}\\\\ \\end{matrix} \\right\]\\\]

其中 \\(X\\) 为 \\(m\\times(n+1)\\) 维矩阵，\\(y\\) 为 \\(m\\) 维的列向量。则

\\\[\\theta=(X^TX)^{-1}X^Ty \\\]

如果 \\(X^TX\\) 不可逆（noninvertible），可能是因为：

1.  Redundant features 冗余特征：存在线性相关的两个特征，需要删除其中一个；
2.  特征过多，如 \\(m\\leq n\\)：需要删除一些特征，或对其进行正规化（regularization）处理。

2.4 Polynomial Regression 多项式回归
-------------------------------

If a linear \\(h\_\\theta(x)\\) can't fit the data well, we can change the behavior or curve of \\(h\_\\theta(x)\\) by making it a quadratic, cubic or square root function(or any other form).  
e.g.

*   \\(h\_{\\theta}(x)=\\theta\_0+\\theta\_1x\_1+\\theta\_2x\_1^2,\\ x\_2=x\_1^2\\)
    
*   \\(h\_{\\theta}(x)=\\theta\_0+\\theta\_1x\_1+\\theta\_2x\_1^2+\\theta\_3x\_1^3,\\ x\_2=x\_1^2,\\ x\_3=x\_1^3\\)
    
*   \\(h\_{\\theta}(x)=\\theta\_0+\\theta\_1x\_1+\\theta\_2\\sqrt{x\_1},\\ x\_2=\\sqrt{x\_1}\\)
    

* * *