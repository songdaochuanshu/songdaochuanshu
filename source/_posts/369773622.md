---
layout: post
title: "跟我读CVPR 2022论文：基于场景文字知识挖掘的细粒度图像识别算法"
date: "2022-04-24T07:17:38.619Z"
---
跟我读CVPR 2022论文：基于场景文字知识挖掘的细粒度图像识别算法
===================================

> **摘要：**本文通过场景文字从人类知识库（Wikipedia）中挖掘其背后丰富的上下文语义信息，并结合视觉信息来共同推理图像内容。

本文分享自华为云社区《[\[CVPR 2022\] 基于场景文字知识挖掘的细粒度图像识别算法](https://bbs.huaweicloud.com/blogs/348533?utm_source=cnblog&utm_medium=bbs-ex&utm_campaign=ei&utm_content=content)》，作者： 谷雨润一麦。

本文简要介绍CVPR 2022录用的论文“Knowledge Mining with Scene Text for Fine-Grained Recognition”的主要工作。该论文旨在利用场景文本的线索来提升细粒度图像识别的性能。本文通过场景文字从人类知识库（Wikipedia）中挖掘其背后丰富的上下文语义信息，并结合视觉信息来共同推理图像内容。数据集和代码已开源，下载地址见文末。

![](https://pic1.zhimg.com/80/v2-8fe0c6e9685a90b03fd57ab2e73fe388_720w.jpg)

研究背景
----

文字是人类传达信息、知识和情感的重要载体，其蕴含了丰富的语义信息。利用文字的语义信息，可以更好地理解图像中的内容。和文档文本不同，场景文字具有稀疏性，通常以少许关键词的形式存在于自然环境中，通过稀疏的关键词，机器难以获取精准的语义。然而，人类能够较为充分地理解稀疏的场景文字，其原因在于，人类具有大量的外部知识库，能够通过知识库来弥补稀疏的场景文字所带来的语义损失。

如图1所示：该数据集是关于细粒度图像分类任务，旨在区分图像中的瓶子属于哪种饮品或酒类。图中3张图像均属于soda类饮品，尽管（a）（b）两案例的瓶子具有不同的视觉属性（不同材质、形状），但是关键词soda提供了极具区分力的线索来告知样本属于soda饮品。尽管案例（c）同样属于soda类饮品，但是其附属的场景文本的表面信息无法提供明显的线索。表格（d）中列出了案例（c）中的场景文字在Wikipedia中的描述，Wikipedia告知我们，场景文本leninade代表某种品牌，其属于soda类饮品。因此，挖掘场景文本背后丰富的语义信息能够进一步弥补场景文本的语义损失，从而更为准确地理解图像中的目标。

![](https://pic3.zhimg.com/80/v2-e51514072a601ea9a949adc266462ede_720w.jpg)

*   Bottle数据集中的案例，3张图像均属于soda类别

方法简述
----

**算法框架：**如图2所示，网络框架由视觉特征分支、知识提取分支和知识增强分支、视觉-知识注意力模块和分类器构成。算法输入包括3部分：图像，图像中包含的场景文本实例，外部知识库。其中场景文本实例通过已有的文字识别器从输入图像中获取，外部知识库采用了Wikipedia。知识提取分支提取场景文本实例背后的语义信息（知识特征），知识增强分支融合场景文本实例和挖掘出的知识特征。随后，视觉-知识注意力模块融合视觉和知识特征，并将其输入给分类器进行分类。

![](https://pic3.zhimg.com/80/v2-7648654362524c3b917be8d216a5967e_720w.jpg)

算法框架图，由视觉特征分支、知识提取分支和知识增强分支、视觉-知识注意力模块（VKAC）和分类器构成。

**知识提取分支：**该分支由实体候选选择器和实体编码器构成。在知识库中，同一关键词能够表示多个实体，比如apple可表示fruit apple，也可表示company apple。实体候选选择器预先在大量语料库上统计单词在所有可能实体上的概率分布，根据概率分布选取前10个候选实体，并将其输入给实体编码器进行特征编码。实体编码器在Wikipedia的数据库上进行预训练，预训练任务旨在通过Wikipedia上实体的描述来预测该页面的标题（实体名称）。通过此任务的学习，实体名称对于的特征编码了该词条的上下文信息。

**知识增强特征分支：**该分支主要由bert\[1\]构成，在bert的第10层后插入知识注意力模块（KARC），该模块融合了文本实例特征和知识特征后，接着输入给bert剩余的层。Bert第12层输出的特征给VKAC模块。KARC的网络结构如图3所示。

**视觉-知识注意力模块：**并非所有的场景文本或知识对理解图像有积极作用，为选取和图像内容相关的场景文本和知识来加强对图像的理解。该模块以图像全局特征作为访问特征，从增强的知识特征中选取相关的知识特征来加强视觉特征。其网络结构由注意力模型构成。

![](https://pic2.zhimg.com/80/v2-9777517b1149851af3e388a30e6e819d_720w.jpg)

知识注意力模块（KARC），橙色和绿色模块是模块的两种输入

实验结果
----

为研究场景文本背后的知识对图像识别的帮助，我们收集了一个关于人群活动的数据集。该数据集中的类别主要分为游行示威和日常人群密集活动两大类，细分为21类。数据集案例如图4所示。

![](https://pic2.zhimg.com/80/v2-bf141eaceecc68373a40960cc2342525_720w.jpg)

人群活动数据集样例

**和SOTA对比：**在公开数据集Con-Text、Bottles以及我们收集的Activity数据集上，在使用resnet50\[3\]和E2E-MLT\[4\]作为视觉特征提取器和文字提取器时，我们方法能在同等情况下取得最佳结果。当使用ViT和Google OCR时，其模型性能结果能进一步提升。

![](https://pic2.zhimg.com/80/v2-7939edccc0ae78fb9f7f7e7ce0e0f589_720w.jpg)

**视觉、文本、知识特征对识别的影响：**可以看出，文本的表面语义（Glove，fastText）在视觉网络为Resne50\[3\]的时候，能对识别性能有较大提升。当视觉网络为ViT\[2\]时，提升极其有限。如图5所示，Resnet50关注于主要于视觉目标具有区分力的区域，而ViT能同时关注在视觉目标和场景文字上。因此，再使用场景文字的表语含义难以对ViT有较大促进作用。而挖掘文本的背后语义后，能进一步提升ViT作为视觉backbone的模型的性能。

![](https://pic4.zhimg.com/80/v2-d92660e0b1e41bcf74d40d51d573e3b3_720w.jpg)

![](https://pic3.zhimg.com/80/v2-6d32030c0d64e1104718a6fcdf2af41a_720w.jpg)

上下两行分别为resnet50和ViT模型的注意力热图

总结与结论
-----

本文提出了一种通过挖掘场景文本背后语义来增强分类模型理解图像内容的方法，该方法的核心是利用场景文字作为关键词，到wikipedia知识库中检索出相关的知识，并获取其特征表达，和图像视觉特征进行融合理解，而并非仅仅利用场景文字的表面语义信息。得益于挖掘场景文本背后的知识，该方法能够更好地理解文字语义并不非常直观的内容。实验表明，该方法在3个数据集上均取得了最佳结果。

相关资源
----

论文地址：[https://arxiv.org/pdf/2203.14215.pdf](https://arxiv.org/pdf/2203.14215.pdf)

数据集和代码链接:[https://github.com/lanfeng4659/KnowledgeMiningWithSceneText](https://github.com/lanfeng4659/KnowledgeMiningWithSceneText)

参考文献
----

\[1\] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018).

\[2\] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale\[J\]. arXiv preprint arXiv:2010.11929, 2020.

\[3\] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition\[C\]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.

\[4\] Bušta M, Patel Y, Matas J. E2e-mlt-an unconstrained end-to-end method for multi-language scene text\[C\]//Asian Conference on Computer Vision. Springer, Cham, 2018: 127-143.

**[点击关注，第一时间了解华为云新鲜技术~](https://bbs.huaweicloud.com/blogs?utm_source=cnblog&utm_medium=bbs-ex&utm_campaign=ei&utm_content=content)**