---
layout: post
title: "B-神经网络模型复杂度分析"
date: "2022-11-28T17:16:34.891Z"
---
B-神经网络模型复杂度分析
=============

![B-神经网络模型复杂度分析](https://img2023.cnblogs.com/blog/2989634/202211/2989634-20221128152641593-2110990320.png) 终端设备上运行深度学习算法需要考虑内存和算力的需求，因此需要进行模型复杂度分析，涉及到模型计算量（时间/计算复杂度）和模型参数量（空间复杂度）分析。 为了分析模型计算复杂度，一个广泛采用的度量方式是模型推断时浮点运算的次数 （FLOPs），即模型理论计算量，但是，它是一个间接的度量，是对我们真正关心的直接度量比如速度或者时延的一种近似估计。

*   [前言](#%E5%89%8D%E8%A8%80)
*   [一，模型计算量分析](#%E4%B8%80%E6%A8%A1%E5%9E%8B%E8%AE%A1%E7%AE%97%E9%87%8F%E5%88%86%E6%9E%90)
    *   [卷积层 FLOPs 计算](#%E5%8D%B7%E7%A7%AF%E5%B1%82-flops-%E8%AE%A1%E7%AE%97)
    *   [全连接层的 FLOPs 计算](#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84-flops-%E8%AE%A1%E7%AE%97)
*   [二，模型参数量分析](#%E4%BA%8C%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E9%87%8F%E5%88%86%E6%9E%90)
    *   [卷积层参数量](#%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E9%87%8F)
    *   [BN 层参数量](#bn-%E5%B1%82%E5%8F%82%E6%95%B0%E9%87%8F)
    *   [全连接层参数量](#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%8F%82%E6%95%B0%E9%87%8F)
*   [三，模型内存访问代价计算](#%E4%B8%89%E6%A8%A1%E5%9E%8B%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E4%BB%A3%E4%BB%B7%E8%AE%A1%E7%AE%97)
    *   [卷积层 MAC 计算](#%E5%8D%B7%E7%A7%AF%E5%B1%82-mac-%E8%AE%A1%E7%AE%97)
*   [四，一些概念](#%E5%9B%9B%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5)
    *   [双精度、单精度和半精度](#%E5%8F%8C%E7%B2%BE%E5%BA%A6%E5%8D%95%E7%B2%BE%E5%BA%A6%E5%92%8C%E5%8D%8A%E7%B2%BE%E5%BA%A6)
    *   [浮点计算能力](#%E6%B5%AE%E7%82%B9%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B)
    *   [硬件利用率(Utilization)](#%E7%A1%AC%E4%BB%B6%E5%88%A9%E7%94%A8%E7%8E%87utilization)
*   [五，参考资料](#%E4%BA%94%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99)

前言
--

现阶段的轻量级模型 MobileNet/ShuffleNet 系列、CSPNet、RepVGG、VoVNet 等都必须依赖于于具体的计算平台（如 CPU/GPU/ASIC 等）才能更完美的发挥网络架构。

1，计算平台主要有两个指标：算力 $\\pi $和 带宽 $\\beta $。

*   算力指的是计算平台每秒完成的最大浮点运算次数，单位是 `FLOPS`
*   带宽指的是计算平台一次每秒最多能搬运多少数据（每秒能完成的内存交换量），单位是 `Byte/s`。

计算强度上限 \\(I\_{max}\\)，上面两个指标相除得到计算平台的**计算强度上限**。它描述了单位内存交换最多用来进行多少次计算，单位是 `FLOPs/Byte`。

\\\[I\_{max} = \\frac {\\pi }{\\beta} \\\]

> 这里所说的“内存”是广义上的内存。对于 `CPU` 而言指的就是真正的内存（`RAM`）；而对于 `GPU` 则指的是显存。

2，和计算平台的两个指标相呼应，模型也有两个主要的反馈速度的**间接指标**：计算量 `FLOPs` 和访存量 `MAC`。

*   **计算量（FLOPs）**：指的是输入单个样本（一张图像），模型完成一次前向传播所发生的浮点运算次数，即模型的时间复杂度，单位是 `FLOPs`。
*   **访存量（MAC）**：指的是输入单个样本（一张图像），模型完成一次前向传播所发生的内存交换总量，即模型的空间复杂度，单位是 `Byte`，因为数据类型通常为 `float32`，所以需要乘以 `4`。`CNN` 网络中每个网络层 `MAC` 的计算分为读输入 `feature map` 大小、权重大小（`DDR` 读）和写输出 `feature map` 大小（`DDR` 写）三部分。
*   模型的计算强度 \\(I\\) ：\\(I = \\frac{FLOPs}{MAC}\\)，即计算量除以访存量后的值，**表示此模型在计算过程中，每 `Byte` 内存交换到底用于进行多少次浮点运算**。单位是 `FLOPs/Byte`。可以看到，模计算强度越大，其内存使用效率越高。
*   模型的理论性能 \\(P\\) ：我们最关心的指标，即模型在计算平台上所能达到的每秒浮点运算次数（理论值）。单位是 `FLOPS or FLOP/s`。`Roof-line Model` 给出的就是计算这个指标的方法。

3，`Roofline` 模型讲的是程序在计算平台的算力和带宽这两个指标限制下，所能达到的理论性能上界，而不是实际达到的性能，因为实际计算过程中还有除算力和带宽之外的其他重要因素，它们也会影响模型的实际性能，这是 `Roofline Model` 未考虑到的。例如矩阵乘法，会因为 `cache` 大小的限制、`GEMM` 实现的优劣等其他限制，导致你几乎无法达到 `Roofline` 模型所定义的边界（屋顶）。

所谓 “Roof-line”，指的就是由计算平台的算力和带宽上限这两个参数所决定的“屋顶”形态，如下图所示。

*   算力决定“屋顶”的高度（绿色线段）
*   带宽决定“房檐”的斜率（红色线段）

![roof-line](https://img2023.cnblogs.com/blog/2989634/202211/2989634-20221128152458500-804825510.png)

`Roof-line` 划分出的两个瓶颈区域定义如下：

![Roof-line划分出的两个瓶颈区域](https://img2023.cnblogs.com/blog/2989634/202211/2989634-20221128152458888-357724639.png)

**个人感觉如果在给定计算平台上做模型部署工作，因为芯片的算力已定，工程师能做的主要工作应该是提升带宽。**

一，模型计算量分析
---------

> 终端设备上运行深度学习算法需要考虑内存和算力的需求，因此需要进行模型复杂度分析，涉及到模型计算量（时间/计算复杂度）和模型参数量（空间复杂度）分析。

为了分析模型计算复杂度，一个广泛采用的度量方式是模型推断时浮点运算的次数 （`FLOPs`），即模型理论计算量，但是，它是一个间接的度量，是对我们真正关心的直接度量比如速度或者时延的一种近似估计。

本文的卷积核尺寸假设为为一般情况，即正方形，长宽相等都为 `K`。

*   `FLOPs`：floating point operations 指的是浮点运算次数,**理解为计算量**，可以用来衡量算法/模型时间的复杂度。
*   `FLOPS`：（全部大写）,Floating-point Operations Per Second，每秒所执行的浮点运算次数，理解为计算速度,是一个衡量硬件性能/模型速度的指标。
*   `MACCs`：multiply-accumulate operations，乘-加操作次数，`MACCs` 大约是 FLOPs 的一半。将 \\(w\[0\]\*x\[0\] + ...\\) 视为一个乘法累加或 `1` 个 `MACC`。

注意相同 `FLOPs` 的两个模型其运行速度是会相差很多的，因为影响模型运行速度的两个重要因素只通过 `FLOPs` 是考虑不到的，比如 `MAC`（`Memory Access Cost`）和网络并行度；二是具有相同 `FLOPs` 的模型在不同的平台上可能运行速度不一样。

> 注意，网上很多文章将 MACCs 与 MACC 概念搞混，我猜测可能是机器翻译英文文章不准确的缘故，可以参考此[链接](http://machinethink.net/blog/how-fast-is-my-model/)了解更多。需要指出的是，现有很多硬件都将**乘加运算作为一个单独的指令**。

### 卷积层 FLOPs 计算

> 卷积操作本质上是个线性运算，假设卷积核大小相等且为 \\(K\\)。这里给出的公式写法是为了方便理解，大多数时候为了方便记忆，会写成比如 \\(MACCs = H \\times W \\times K^2 \\times C\_i \\times C\_o\\)。

*   \\(FLOPs=(2\\times C\_i\\times K^2-1)\\times H\\times W\\times C\_o\\)（不考虑bias）
*   \\(FLOPs=(2\\times C\_i\\times K^2)\\times H\\times W\\times C\_o\\)（考虑bias）
*   \\(MACCs=(C\_i\\times K^2)\\times H\\times W\\times C\_o\\)（考虑bias）

**\\(C\_i\\) 为输入特征图通道数，\\(K\\) 为过卷积核尺寸，\\(H,W,C\_o\\) 为输出特征图的高，宽和通道数**。`二维卷积过程`如下图所示：

> 二维卷积是一个相当简单的操作：从卷积核开始，这是一个小的权值矩阵。这个卷积核在 2 维输入数据上「滑动」，对当前输入的部分元素进行矩阵乘法，然后将结果汇为单个输出像素。

![卷积过程](https://img2023.cnblogs.com/blog/2989634/202211/2989634-20221128152459235-219421962.png)

公式解释，参考[这里](https://zhuanlan.zhihu.com/p/70306015?utm_source=wechat_session&utm_medium=social&utm_oi=571954943427219456)，如下：

**理解 `FLOPs` 的计算公式分两步**。括号内是第一步，计算出`output feature map` 的一个 `pixel`，然后再乘以 \\(H\\times W\\times C\_o\\)，从而拓展到整个 output feature map。括号内的部分又可以分为两步：\\((2\\times C\_i\\times K^2-1)=(C\_i\\times K^2) + (C\_i\\times K^2-1)\\)。第一项是乘法运算次数，第二项是加法运算次数，因为 \\(n\\) 个数相加，要加 \\(n-1\\)次，所以不考虑 `bias` 的情况下，会有一个 -1，如果考虑 `bias`，刚好中和掉，括号内变为\\((2\\times C\_i\\times K^2)\\)。

所以卷积层的 \\(FLOPs=(2\\times C\_{i}\\times K^2-1)\\times H\\times W\\times C\_o\\) (\\(C\_i\\) 为输入特征图通道数，\\(K\\) 为过滤器尺寸，\\(H, W, C\_o\\)为输出特征图的高，宽和通道数)。

### 全连接层的 FLOPs 计算

全连接层的 \\(FLOPs = (2I − 1)O\\)，\\(I\\) 是输入层的维度，\\(O\\) 是输出层的维度。

二，模型参数量分析
---------

> 模型参数数量（params）：指模型含有多少参数，直接决定模型的大小，也影响推断时对内存的占用量，单位通常为 `M`，`GPU` 端通常参数用 `float32` 表示，所以模型大小是参数数量的 `4` 倍。这里考虑的卷积核长宽是相同的一般情况，都为 `K`。

模型参数量的分析是为了了解内存占用情况，内存带宽其实比 `FLOPs` 更重要。目前的计算机结构下，单次内存访问比单次运算慢得多的多。对每一层网络，端侧设备需要：

*   从主内存中读取输入向量 / `feature map`；
*   从主内存中读取权重并计算点积；
*   将输出向量或 `feature map` 写回主内存。

`MAes`：memory accesse，内存访问次数。

### 卷积层参数量

**卷积层权重参数量** = $ C\_i\\times K^2\\times C\_o + C\_o$。

\\(C\_i\\) 为输入特征图通道数，\\(K\\) 为过滤器(卷积核)尺寸，\\(C\_o\\) 为输出的特征图的 `channel` 数(也是 `filter` 的数量)，算式第二项是偏置项的参数量 。(一般不写偏置项，偏置项对总参数量的数量级的影响可以忽略不记，这里为了准确起见，把偏置项的参数量也考虑进来。）

假设输入层矩阵维度是 96×96×3，第一层卷积层使用尺寸为 5×5、深度为 16 的过滤器（卷积核尺寸为 5×5、卷积核数量为 16），那么这层卷积层的参数个数为 ５×5×3×16+16=1216个。

### BN 层参数量

**`BN` 层参数量** = \\(2\\times C\_i\\)。

其中 \\(C\_i\\) 为输入的 `channel` 数（BN层有两个需要学习的参数，平移因子和缩放因子）

### 全连接层参数量

**全连接层参数量** = \\(T\_i\\times T\_o + T\_O\\)。

\\(T\_i\\) 为输入向量的长度， \\(T\_o\\) 为输出向量的长度，公式的第二项为偏置项参数量。(目前全连接层已经逐渐被 `Global Average Pooling` 层取代了。) 注意，全连接层的权重参数量（内存占用）远远大于卷积层。

三，模型内存访问代价计算
------------

`MAC`(`memory access cost`) 内存访问代价也叫内存使用量，指的是输入单个样本（一张图像），模型/卷积层完成一次前向传播所发生的内存交换总量，即模型的空间复杂度，单位是 `Byte`。

`CNN` 网络中每个网络层 `MAC` 的计算分为读输入 `feature map` 大小（`DDR` 读）、权重大小（`DDR` 读）和写输出 `feature map` 大小（`DDR` 写）三部分。

### 卷积层 MAC 计算

以卷积层为例计算 `MAC`，可假设某个卷积层输入 `feature map` 大小是 (`Cin, Hin, Win`)，输出 `feature map` 大小是 (`Hout, Wout, Cout`)，卷积核是 (`Cout, Cin, K, K`)，理论 MAC（理论 MAC 一般小于 实际 MAC）计算公式如下：

    # 端侧推理IN8量化后模型，单位一般为 1 byte
    input = Hin x Win x Cin  # 输入 feature map 大小
    output = Hout x Wout x Cout  # 输出 feature map 大小
    weights = K x K x Cin x Cout + bias   # bias 是卷积层偏置
    ddr_read = input +  weights
    ddr_write = output
    MAC = ddr_read + ddr_write
    

> `feature map` 大小一般表示为 （`N, C, H, W`），`MAC` 指标一般用在端侧模型推理中，端侧模型推理模式一般都是单帧图像进行推理，即 `N = 1(batch_size = 1)`，不同于模型训练时的 `batch_size` 大小一般大于 1。

四，一些概念
------

### 双精度、单精度和半精度

`CPU/GPU` 的浮点计算能力得区分不同精度的浮点数，分为双精度 `FP64`、单精度 `FP32` 和半精度 `FP16`。因为采用不同位数的浮点数的表达精度不一样，所以造成的计算误差也不一样，对于需要处理的数字范围大而且需要精确计算的科学计算来说，就要求采用双精度浮点数，而对于常见的多媒体和图形处理计算，`32` 位的单精度浮点计算已经足够了，对于要求精度更低的机器学习等一些应用来说，半精度 `16` 位浮点数就可以甚至 `8` 位浮点数就已经够用了。  
对于浮点计算来说， `CPU` 可以同时支持不同精度的浮点运算，但在 `GPU` 里针对单精度和双精度就需要各自独立的计算单元。

### 浮点计算能力

`FLOPS`：每秒浮点运算次数，每秒所执行的浮点运算次数，浮点运算包括了所有涉及小数的运算，比整数运算更费时间。下面几个是表示浮点运算能力的单位。我们一般常用 `TFLOPS(Tops)` 作为衡量 `NPU/GPU` 性能/算力的指标，比如海思 `3519AV100` 芯片的算力为 `1.7Tops` 神经网络运算性能。

*   `MFLOPS`（megaFLOPS）：等于每秒一佰万（=10^6）次的浮点运算。
*   `GFLOPS`（gigaFLOPS）：等于每秒拾亿（=10^9）次的浮点运算。
*   `TFLOPS`（teraFLOPS）：等于每秒万亿（=10^12）次的浮点运算。
*   `PFLOPS`（petaFLOPS）：等于每秒千万亿（=10^15）次的浮点运算。
*   `EFLOPS`（exaFLOPS）：等于每秒百亿亿（=10^18）次的浮点运算。

### 硬件利用率(Utilization)

在这种情况下，利用率（Utilization）是可以有效地用于实际工作负载的芯片的原始计算能力的百分比。深度学习和神经网络使用相对数量较少的计算原语（computational primitives），而这些数量很少的计算原语却占用了大部分计算时间。矩阵乘法（MM）和转置是基本操作。MM 由乘法累加（MAC）操作组成。OPs/s（每秒完成操作的数量）指标通过每秒可以完成多少个 MAC（每次乘法和累加各被认为是 1 个 operation，因此 MAC 实际上是 2 个 OP）得到。所以我们可以将利用率定义为实际使用的运算能力和原始运算能力的比值：

![算力利用率公式](https://img2023.cnblogs.com/blog/2989634/202211/2989634-20221128152459562-292340493.png)

\\\[mac\\ utilization = \\frac {used\\ Ops/s}{raw\\ OPs/s} = \\frac {FLOPs/time(s)}{Raw\\\_FLOPs}(Raw\\\_FLOPs = 1.7T\\ at\\ 3519) \\\]

五，参考资料
------

*   [PRUNING CONVOLUTIONAL NEURAL NETWORKS FOR RESOURCE EFFICIENT INFERENCE](https://arxiv.org/pdf/1611.06440.pdf)
*   [神经网络参数量的计算：以UNet为例](https://zhuanlan.zhihu.com/p/57437131)
*   [How fast is my model?](http://machinethink.net/blog/how-fast-is-my-model/)
*   [MobileNetV1 & MobileNetV2 简介](https://blog.csdn.net/mzpmzk/article/details/82976871)
*   [双精度，单精度和半精度](https://blog.csdn.net/sinat_24143931/article/details/78557852?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase)
*   [AI硬件的Computational Capacity详解](https://zhuanlan.zhihu.com/p/27836831)
*   [Roofline Model与深度学习模型的性能分析](https://zhuanlan.zhihu.com/p/34204282)