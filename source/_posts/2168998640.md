---
layout: post
title: "云原生之旅 - 4）基础设施即代码 使用 Terraform 创建 Kubernetes"
date: "2022-10-29T19:15:54.527Z"
---
云原生之旅 - 4）基础设施即代码 使用 Terraform 创建 Kubernetes
============================================

前言
--

上一篇文章我们已经简单的入门Terraform， 本篇介绍如何使用Terraform在GCP和AWS 创建Kubernetes 资源。

Kubernetes 在云原生时代的重要性不言而喻，等于这个时代的操作系统，基本上只需要建这个资源，就可以将绝大多数的应用跑在上面，包括数据库，甚至很多团队的大数据处理例如 Spark, Flink 都跑在Kubernetes上。

*   GCP Kubernetes = GKE
*   AWS Kubernetes = EKS
*   Azure Kubernetes = AKS

本篇文章主要介绍前两者的Terraform 代码实现，现在使用官方的 module 要比以前方便太多了，哪怕是新手都可以很快的将资源建起来，当然如果要更多的了解，还是需要慢慢下功夫的。

**关键词**：IaC, Infrastructure as Code, Terraform, 基础设施即代码，使用Terraform创建GKE，使用Terraform创建EKS

**环境信息:**

\* Terraform 1.2.9

\* Google Cloud SDK 397.0.0

\* aws-cli 2.7.7

使用Terraform创建GKE
----------------

准备一个GCS bucket  

\# valid LOCATION values are \`asia\`, \`eu\` or \`us\`
gsutil mb -l $LOCATION gs://$BUCKET\_NAME
gsutil versioning set on gs://$BUCKET\_NAME

准备如下tf文件

backend.tf

terraform {
  backend "gcs" {
    bucket = "sre-dev-terraform-test"
    prefix = "demo/state"
  }
}

providers.tf

terraform {
  required\_version = ">= 1.2.9"

  required\_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 4.0"
    }
    google-beta = {
      source  = "hashicorp/google-beta"
      version = "~> 4.0"
    }
  }
}

provider "google" {
  project = local.project.project\_id
  region  = local.project.region
}

provider "google-beta" {
  project = local.project.project\_id
  region  = local.project.region
}

使用 [terraform google module](https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/latest) 事半功倍，代码如下

gke-cluster.tf 

data "google\_compute\_zones" "available" {
  region = "us-central1"
  status = "UP"
}

resource "google\_compute\_network" "default" {
  project                 = local.project.project\_id
  name                    = local.project.network\_name
  auto\_create\_subnetworks = false
  routing\_mode            = "GLOBAL"
}

resource "google\_compute\_subnetwork" "wade-gke" {
  project       = local.project.project\_id
  network       = google\_compute\_network.default.name
  name          = local.wade\_cluster.subnet\_name
  ip\_cidr\_range = local.wade\_cluster.subnet\_range
  region        = local.wade\_cluster.region

  secondary\_ip\_range {
    range\_name    = format("%s-secondary1", local.wade\_cluster.cluster\_name)
    ip\_cidr\_range = local.wade\_cluster.secondary\_ip\_range\_pods
  }

  secondary\_ip\_range {
    range\_name    = format("%s-secondary2", local.wade\_cluster.cluster\_name)
    ip\_cidr\_range = local.wade\_cluster.secondary\_ip\_range\_services
  }

  private\_ip\_google\_access = true

}

resource "google\_service\_account" "sa-wade-test" {
  account\_id   = "sa-wade-test"
  display\_name = "sa-wade-test"
}


module "wade-gke" {
  source = "terraform-google-modules/kubernetes-engine/google//modules/beta-private-cluster"
  version = "23.1.0"

  project\_id = local.project.project\_id
  name       = local.wade\_cluster.cluster\_name

  kubernetes\_version     = local.wade\_cluster.cluster\_version
  region                 = local.wade\_cluster.region
  network                = google\_compute\_network.default.name
  subnetwork             = google\_compute\_subnetwork.wade-gke.name
  master\_ipv4\_cidr\_block = "10.1.0.0/28"
  ip\_range\_pods          = google\_compute\_subnetwork.wade-gke.secondary\_ip\_range.0.range\_name
  ip\_range\_services      = google\_compute\_subnetwork.wade-gke.secondary\_ip\_range.1.range\_name

  service\_account                 = google\_service\_account.sa-wade-test.email
  master\_authorized\_networks      = local.wade\_cluster.master\_authorized\_networks
  master\_global\_access\_enabled    = false
  istio                           = false
  issue\_client\_certificate        = false
  enable\_private\_endpoint         = false
  enable\_private\_nodes            = true
  remove\_default\_node\_pool        = true
  enable\_shielded\_nodes           = false
  identity\_namespace              = "enabled"
  node\_metadata                   = "GKE\_METADATA"
  horizontal\_pod\_autoscaling      = true
  enable\_vertical\_pod\_autoscaling = false

  node\_pools              = local.wade\_cluster.node\_pools
  node\_pools\_oauth\_scopes = local.wade\_cluster.oauth\_scopes
  node\_pools\_labels       = local.wade\_cluster.node\_pools\_labels
  node\_pools\_metadata     = local.wade\_cluster.node\_pools\_metadata
  node\_pools\_taints       = local.wade\_cluster.node\_pools\_taints
  node\_pools\_tags         = local.wade\_cluster.node\_pools\_tags

}

变量 locals.tf

master\_authorized\_networks 需要改为自己要放行的白名单，只有白名单的IP才能访问 cluster api endpoint。为了安全性，不要用0.0.0.0/0

locals {
  # project details
  project = {
    project\_id       = "sre-eng-cn-dev"
    region           = "us-central1"
    network\_name     = "wade-test-network"
  }

  # cluster details
  wade\_cluster = {
    cluster\_name                = "wade-gke"
    cluster\_version             = "1.22.12-gke.500"
    subnet\_name                 = "wade-gke"
    subnet\_range                = "10.254.71.0/24"
    secondary\_ip\_range\_pods     = "172.20.72.0/21"
    secondary\_ip\_range\_services = "10.127.8.0/24"
    region                      = "us-central1"

    node\_pools = \[
      {
        name               = "app-pool"
        machine\_type       = "n1-standard-2"
        node\_locations     = join(",", slice(data.google\_compute\_zones.available.names, 0, 3))
        initial\_node\_count = 1
        min\_count          = 1
        max\_count          = 10
        max\_pods\_per\_node  = 64
        disk\_size\_gb       = 100
        disk\_type          = "pd-standard"
        image\_type         = "COS"
        auto\_repair        = true
        auto\_upgrade       = false
        preemptible        = false
        max\_surge          = 1
        max\_unavailable    = 0
      }
    \]

    node\_pools\_labels = {
      all = {}
    }

    node\_pools\_tags = {
      all = \["k8s-nodes"\]
    }

    node\_pools\_metadata = {
      all = {
        disable-legacy-endpoints = "true"
      }
    }

    node\_pools\_taints = {
      all = \[\]
    }

    oauth\_scopes = {
      all = \[
        "https://www.googleapis.com/auth/monitoring",
        "https://www.googleapis.com/auth/compute",
        "https://www.googleapis.com/auth/devstorage.full\_control",
        "https://www.googleapis.com/auth/logging.write",
        "https://www.googleapis.com/auth/service.management",
        "https://www.googleapis.com/auth/servicecontrol",
      \]
    }

    master\_authorized\_networks = \[
      {
        display\_name = "Whitelist 1"
        cidr\_block   = "4.14.xxx.xx/32"
      },
      {
        display\_name = "Whitelist 2"
        cidr\_block   = "64.124.xxx.xx/32"
      },
    \]
  }
}

output.tf 

output "cluster\_id" {
  description = "GKE cluster ID"
  value       = module.wade-gke.cluster\_id
}

output "cluster\_endpoint" {
  description = "Endpoint for GKE control plane"
  value       = module.wade-gke.endpoint
  sensitive   = true
}

output "cluster\_name" {
  description = "Google Kubernetes Cluster Name"
  value       = module.wade-gke.name
}

output "region" {
  description = "GKE region"
  value       = module.wade-gke.region
}

output "project\_id" {
  description = "GCP Project ID"
  value       = local.project.project\_id
}

tf文件结构如下

![](https://img2022.cnblogs.com/blog/713188/202210/713188-20221029191525659-406507833.png)

### 部署

确保自己的GCP account已经登陆，并且有足够的权限操作GCP Project。

gcloud auth login

gcloud auth list

terraform init  
terraform plan  
terraform apply

### 配置连接GKE集群

\### Adding the cluster to your context
gcloud container clusters get-credentials $(terraform output -raw cluster\_name) \\
--region $(terraform output -raw region) \\
--project $(terraform output -raw project\_id)

### 使用

 下载安装 [kubectl](https://kubernetes.io/docs/reference/kubectl/) 来管理以及部署资源到集群。

 ### 本文首发于博客园 [https://www.cnblogs.com/wade-xu/p/16839468.html](https://www.cnblogs.com/wade-xu/p/16839468.html)

使用Terraform创建EKS
----------------

准备好S3 bucket，更新backend

providers.tf

terraform {
  backend "s3" {
    bucket = "sre-dev-terraform"
    key    = "test/eks.tfstate"
    region = "cn-north-1"
  }
  required\_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.25.0"
    }
  }
}

provider "aws" {
  region     = local.region
}

# https://github.com/terraform-aws-modules/terraform-aws-eks/issues/2009
provider "kubernetes" {
  host                   = module.wade-eks.cluster\_endpoint
  cluster\_ca\_certificate = base64decode(module.wade-eks.cluster\_certificate\_authority\_data)

  exec {
    api\_version = "client.authentication.k8s.io/v1beta1"
    command     = "aws"
    # This requires the awscli to be installed locally where Terraform is executed
    args = \["eks", "get-token", "--cluster-name", module.wade-eks.cluster\_id\]
  }
}

类似的，使用[terraform aws module](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest)， 这里有个小插曲，我建的时候提示 cn-north-1d 这个zone没有足够的资源，所以我在data available zone 里面排除了这个zone

data "aws\_availability\_zones" "available" {

  # Cannot create cluster because cn-north-1d, 
  # the targeted availability zone, does not currently have sufficient capacity to support the cluster.
  exclude\_names = \["cn-north-1d"\]
}

module "wade-eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "18.27.1"

  cluster\_name    = local.cluster\_name
  cluster\_version = local.cluster\_version

  cluster\_endpoint\_private\_access = true
  cluster\_endpoint\_public\_access  = true

  # api server authorized network list
  cluster\_endpoint\_public\_access\_cidrs = local.master\_authorized\_networks

  cluster\_addons = {
    coredns = {
      resolve\_conflicts = "OVERWRITE"
    }
    kube-proxy = {}
    vpc-cni = {
      resolve\_conflicts = "OVERWRITE"
    }
  }

  vpc\_id     = module.vpc.vpc\_id
  subnet\_ids = module.vpc.private\_subnets

  # Extend cluster security group rules
  cluster\_security\_group\_additional\_rules = local.cluster\_security\_group\_additional\_rules

  eks\_managed\_node\_group\_defaults = {
    ami\_type     = local.node\_group\_default.ami\_type
    min\_size     = local.node\_group\_default.min\_size
    max\_size     = local.node\_group\_default.max\_size
    desired\_size = local.node\_group\_default.desired\_size
  }

  eks\_managed\_node\_groups = {
    # dmz = {
    #   name = "dmz-pool"
    # }
    app = {
      name                           = "app-pool"
      instance\_types                 = local.app\_group.instance\_types
      create\_launch\_template         = false
      launch\_template\_name           = ""
      disk\_size                      = local.app\_group.disk\_size

      create\_security\_group          = true
      security\_group\_name            = "app-node-group-sg"
      security\_group\_use\_name\_prefix = false
      security\_group\_description     = "EKS managed app node group security group"
      security\_group\_rules            = local.app\_group.security\_group\_rules

      update\_config = {
        max\_unavailable\_percentage = 50
      }
    }
  }

  # aws-auth configmap
  # create\_aws\_auth\_configmap = true
  manage\_aws\_auth\_configmap = true

  aws\_auth\_roles = \[
    {
      rolearn  = "arn:aws-cn:iam::9935108xxxxx:role/CN-SRE" # replace me
      username = "sre"
      groups   = \["system:masters"\]
    },
  \]

  aws\_auth\_users = \[
    {
      userarn  = "arn:aws-cn:iam::9935108xxxxx:user/wadexu" # replace me
      username = "wadexu"
      groups   = \["system:masters"\]
    },
  \]

  tags = {
    Environment = "dev"
    Terraform   = "true"
  }

  # aws china only because https://github.com/terraform-aws-modules/terraform-aws-eks/pull/1905
  cluster\_iam\_role\_dns\_suffix = "amazonaws.com"
}

locals.tf 这里为了安全性，最好给cluster api server endpoint 加好白名单来访问，否则 0.0.0.0/0代表全开

locals {
  cluster\_name    = "test-eks-2022"
  cluster\_version = "1.22"
  region          = "cn-north-1"

  vpc = {
    cidr = "10.0.0.0/16"
    private\_subnets = \["10.0.1.0/24", "10.0.2.0/24"\]
    public\_subnets  = \["10.0.4.0/24", "10.0.5.0/24"\]
  }

  master\_authorized\_networks =   \[
    "4.14.xxx.xx/32",   # allow office 1
    "64.124.xxx.xx/32", # allow office 2
    "0.0.0.0/0"         # allow all access master node
  \]

  # Extend cluster security group rules example
  cluster\_security\_group\_additional\_rules = {
    egress\_nodes\_ephemeral\_ports\_tcp = {
      description                = "To node 1025-65535"
      protocol                   = "tcp"
      from\_port                  = 1025
      to\_port                    = 65535
      type                       = "egress"
      source\_node\_security\_group = true
    }
  }

  node\_group\_default = {
    ami\_type     = "AL2\_x86\_64"
    min\_size     = 1
    max\_size     = 5
    desired\_size = 1
  }

  dmz\_group = {
  }

  app\_group = {
    instance\_types = \["t3.small"\]
    disk\_size    = 50

    # example rules added for app node group
    security\_group\_rules = {
        egress\_1 = {
          description = "Hello CloudFlare"
          protocol    = "udp"
          from\_port   = 53
          to\_port     = 53
          type        = "egress"
          cidr\_blocks = \["1.1.1.1/32"\]
        }
      }
  }
}

vpc.tf

module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "3.14.2"

  name = "wade-test-vpc"

  cidr = local.vpc.cidr
  azs  = slice(data.aws\_availability\_zones.available.names, 0, 2)

  private\_subnets = local.vpc.private\_subnets
  public\_subnets  = local.vpc.public\_subnets

  enable\_nat\_gateway   = true
  single\_nat\_gateway   = true
  enable\_dns\_hostnames = true

  public\_subnet\_tags = {
    "kubernetes.io/cluster/${local.cluster\_name}" = "shared"
    "kubernetes.io/role/elb"                      = 1
  }

  private\_subnet\_tags = {
    "kubernetes.io/cluster/${local.cluster\_name}" = "shared"
    "kubernetes.io/role/internal-elb"             = 1
  }
}

output.tf

output "cluster\_id" {
  description = "EKS cluster ID"
  value       = module.wade-eks.cluster\_id
}

output "cluster\_endpoint" {
  description = "Endpoint for EKS control plane"
  value       = module.wade-eks.cluster\_endpoint
}

output "region" {
  description = "EKS region"
  value       = local.region
}

output "cluster\_name" {
  description = "AWS Kubernetes Cluster Name"
  value       = local.cluster\_name
}

文件结构如下

![](https://img2022.cnblogs.com/blog/713188/202210/713188-20221029192704204-106154525.png)

 ### 本文首发于博客园 [https://www.cnblogs.com/wade-xu/p/16839468.html](https://www.cnblogs.com/wade-xu/p/16839468.html)

### 部署

配置aws account key/secret

Option 1: Export AWS access and security to environment variables

export AWS\_ACCESS\_KEY\_ID=xxx

export AWS\_SECRET\_ACCESS\_KEY=xxx

Option 2: Add a profile to your AWS credentials file

aws configure
# or
vim ~/.aws/credentials

\[default\]
aws\_access\_key\_id=xxx
aws\_secret\_access\_key=xxx

可以使用如下命令来验证当前用的是哪个credentials

aws sts get-caller-identity 

部署tf资源

terraform init

terraform plan

terraform apply

成功之后有如下输出

![](https://img2022.cnblogs.com/blog/713188/202210/713188-20221029193206410-1031004726.png)

### 配置连接EKS集群

\#### Adding the cluster to your context
aws eks --region $(terraform output -raw region) update-kubeconfig \\
    --name $(terraform output -raw cluster\_name)　　

### 使用

同上面，需要下载kubectl 

Example 命令：

kubectl cluster-info

kubectl get nodes

**感谢阅读，如果您觉得本文的内容对您的学习有所帮助，您可以打赏和推荐，您的鼓励是我创作的动力**

Learning by Doing