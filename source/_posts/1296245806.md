---
layout: post
title: "详解视频中动作识别模型与代码实践"
date: "2022-12-14T21:15:24.242Z"
---
详解视频中动作识别模型与代码实践
================

> **摘要：**本案例将为大家介绍视频动作识别领域的经典模型并进行代码实践。

本文分享自华为云社区《[视频动作识别](https://bbs.huaweicloud.com/blogs/386278?utm_source=cnblog&utm_medium=bbs-ex&utm_campaign=other&utm_content=content)》，作者：HWCloudAI。实验目标

通过本案例的学习：

*   掌握 C3D 模型训练和模型推理、I3D 模型推理的方法；

注意事项
----

1.  本案例推荐使用TensorFlow-1.13.1，需使用 GPU 运行，请查看[《ModelArts JupyterLab 硬件规格使用指南》](https://marketplace.huaweicloud.com/markets/aihub/article/detail/?content_id=638c55c5-816d-421c-9b5f-90c804b1fa6b)了解切换硬件规格的方法；
    
2.  如果您是第一次使用 JupyterLab，请查看[《ModelArts JupyterLab使用指导》](https://marketplace.huaweicloud.com/markets/aihub/article/detail/?content_id=03676d0a-0630-4a3f-b62c-07fba43d2857)了解使用方法；
    
3.  如果您在使用 JupyterLab 过程中碰到报错，请参考[《ModelArts JupyterLab常见问题解决办法》](https://marketplace.huaweicloud.com/markets/aihub/article/detail/?content_id=9ad8ce7d-06f7-4394-80ef-4dbf6cfb4be1)尝试解决问题。
    

实验步骤
----

### 案例内容介绍

视频动作识别是指对一小段视频中的内容进行分析，判断视频中的人物做了哪种动作。视频动作识别与图像领域的图像识别，既有联系又有区别，图像识别是对一张静态图片进行识别，而视频动作识别不仅要考察每张图片的静态内容，还要考察不同图片静态内容之间的时空关系。比如一个人扶着一扇半开的门，仅凭这一张图片无法判断该动作是开门动作还是关门动作。

视频分析领域的研究相比较图像分析领域的研究，发展时间更短，也更有难度。视频分析模型完成的难点首先在于，需要强大的计算资源来完成视频的分析。视频要拆解成为图像进行分析，导致模型的数据量十分庞大。视频内容有很重要的考虑因素是动作的时间顺序，需要将视频转换成的图像通过时间关系联系起来，做出判断，所以模型需要考虑时序因素，加入时间维度之后参数也会大量增加。

得益于 PASCAL VOC、ImageNet、MS COCO 等数据集的公开，图像领域产生了很多的经典模型，那么在视频分析领域有没有什么经典的模型呢？答案是有的，本案例将为大家介绍视频动作识别领域的经典模型并进行代码实践。

1\. 准备源代码和数据
------------

这一步准备案例所需的源代码和数据，相关资源已经保存在 OBS 中，我们通过[ModelArts SDK](https://support.huaweicloud.com/sdkreference-modelarts/modelarts_04_0002.html)将资源下载到本地，并解压到当前目录下。解压后，当前目录包含 data、dataset\_subset 和其他目录文件，分别是预训练参数文件、数据集和代码文件等。

import os
import moxing as mox
if not os.path.exists('videos'):
 mox.file.copy("obs://ai-course-common-26-bj4-v2/video/video.tar.gz", "./video.tar.gz")
 # 使用tar命令解压资源包
 os.system("tar xf ./video.tar.gz")
 # 使用rm命令删除压缩包
 os.system("rm ./video.tar.gz")
INFO:root:Using MoXing\-v1.17.3\-
INFO:root:Using OBS\-Python-SDK-3.20.7

上一节课我们已经介绍了视频动作识别有 HMDB51、UCF-101 和 Kinetics 三个常用的数据集，本案例选用了 UCF-101 数据集的部分子集作为演示用数据集，接下来，我们播放一段 UCF-101 中的视频：

video\_name = "./data/v\_TaiChi\_g01\_c01.avi"

from IPython.display import clear\_output, Image, display, HTML
import time
import cv2
import base64
import numpy as np
def arrayShow(img):
 \_,ret \= cv2.imencode('.jpg', img) 
 return Image(data=ret) 
cap \= cv2.VideoCapture(video\_name)
while True:
 try:
 clear\_output(wait\=True)
        ret, frame \= cap.read()
 if ret:
 tmp \= cv2.cvtColor(frame, cv2.COLOR\_BGR2RGB)
 img \= arrayShow(frame)
            display(img)
 time.sleep(0.05)
 else:
 break
 except KeyboardInterrupt:
 cap.release()
cap.release()

![](https://pic3.zhimg.com/80/v2-092e202bccd7759a3d47b567fbc1daf6_720w.webp)

2\. 视频动作识别模型介绍
--------------

在图像领域中，ImageNet 作为一个大型图像识别数据集，自 2010 年开始，使用此数据集训练出的图像算法层出不穷，深度学习模型经历了从 AlexNet 到 VGG-16 再到更加复杂的结构，模型的表现也越来越好。在识别千种类别的图片时，错误率表现如下：

![](https://pic2.zhimg.com/80/v2-5cd8523fa5a5336a451231f49b1641dd_720w.webp)

在图像识别中表现很好的模型，可以在图像领域的其他任务中继续使用，通过复用模型中部分层的参数，就可以提升模型的训练效果。有了基于 ImageNet 模型的图像模型，很多模型和任务都有了更好的训练基础，比如说物体检测、实例分割、人脸检测、人脸识别等。

那么训练效果显著的图像模型是否可以用于视频模型的训练呢？答案是 yes，有研究证明，在视频领域，如果能够复用图像模型结构，甚至参数，将对视频模型的训练有很大帮助。但是怎样才能复用上图像模型的结构呢？首先需要知道视频分类与图像分类的不同，如果将视频视作是图像的集合，每一个帧将作为一个图像，视频分类任务除了要考虑到图像中的表现，也要考虑图像间的时空关系，才可以对视频动作进行分类。

为了捕获图像间的时空关系，论文 I3D 介绍了三种旧的视频分类模型，并提出了一种更有效的 Two-Stream Inflated 3D ConvNets（简称 I3D）的模型，下面将逐一简介这四种模型，更多细节信息请查看原论文。

### 旧模型一：卷积网络 + LSTM

模型使用了训练成熟的图像模型，通过卷积网络，对每一帧图像进行特征提取、池化和预测，最后在模型的末端加一个 LSTM 层（长短期记忆网络），如下图所示，这样就可以使模型能够考虑时间性结构，将上下文特征联系起来，做出动作判断。这种模型的缺点是只能捕获较大的工作，对小动作的识别效果较差，而且由于视频中的每一帧图像都要经过网络的计算，所以训练时间很长。

![](https://pic2.zhimg.com/80/v2-6586e450603364dc18094e994a0be5dd_720w.webp)

### 旧模型二：3D 卷积网络

3D 卷积类似于 2D 卷积，将时序信息加入卷积操作。虽然这是一种看起来更加自然的视频处理方式，但是由于卷积核维度增加，参数的数量也增加了，模型的训练变得更加困难。这种模型没有对图像模型进行复用，而是直接将视频数据传入 3D 卷积网络进行训练。

![](https://pic2.zhimg.com/80/v2-6c691145bd3e57282dba07da9f91cb0d_720w.webp)

### 旧模型三：Two-Stream 网络

Two-Stream 网络的两个流分别为 1 张 RGB 快照和 10 张计算之后的光流帧画面组成的栈。两个流都通过 ImageNet 预训练好的图像卷积网络，光流部分可以分为竖直和水平两个通道，所以是普通图片输入的 2 倍，模型在训练和测试中表现都十分出色。

![](https://pic2.zhimg.com/80/v2-4ec24ee239793a2430e41a07260c6dd9_720w.webp)

### 光流视频 optical flow video

上面讲到了光流，在此对光流做一下介绍。光流是什么呢？名字很专业，感觉很陌生，但实际上这种视觉现象我们每天都在经历，我们坐高铁的时候，可以看到窗外的景物都在快速往后退，开得越快，就感受到外面的景物就是 “刷” 地一个残影，这种视觉上目标的运动方向和速度就是光流。光流从概念上讲，是对物体运动的观察，通过找到相邻帧之间的相关性来判断帧之间的对应关系，计算出相邻帧画面中物体的运动信息，获取像素运动的瞬时速度。在原始视频中，有运动部分和静止的背景部分，我们通常需要判断的只是视频中运动部分的状态，而光流就是通过计算得到了视频中运动部分的运动信息。

下面是一个经过计算后的原视频及光流视频。

原视频

![See videos/v_CricketShot_g04_c01_rgb.gif](https://modelarts-labs-bj4-v2.obs.cn-north-4.myhuaweicloud.com/course/ai_in_action/2021/video/action_recognition/img/v_CricketShot_g04_c01_rgb.gif)

光流视频

![See videos/v_CricketShot_g04_c01_flow.gif](https://modelarts-labs-bj4-v2.obs.cn-north-4.myhuaweicloud.com/course/ai_in_action/2021/video/action_recognition/img/v_CricketShot_g04_c01_flow.gif)

### 新模型：Two-Stream Inflated 3D ConvNets

新模型采取了以下几点结构改进：

*   拓展 2D 卷积为 3D。直接利用成熟的图像分类模型，只不过将网络中二维 $ N × N 的 filters 和 pooling kernels 直接变成的 _filters_ 和 _poolingkernels_ 直接变成 N × N × N $；
*   用 2D filter 的预训练参数来初始化 3D filter 的参数。上一步已经利用了图像分类模型的网络，这一步的目的是能利用上网络的预训练参数，直接将 2D filter 的参数直接沿着第三个时间维度进行复制 N 次，最后将所有参数值再除以 N；
*   调整感受野的形状和大小。新模型改造了图像分类模型 Inception-v1 的结构，前两个 max-pooling 层改成使用 $ 1 × 3 × 3 kernels and stride 1 in time，其他所有 max-pooling 层都仍然使用对此的 kernel 和 stride，最后一个 average pooling 层使用 _kernelsandstride_1_intime_，其他所有 _max_−_pooling_ 层都仍然使用对此的 _kernel_ 和 _stride_，最后一个 _averagepooling_ 层使用 2 × 7 × 7 $ 的 kernel。
*   延续了 Two-Stream 的基本方法。用双流结构来捕获图片之间的时空关系仍然是有效的。

最后新模型的整体结构如下图所示：

![](https://pic4.zhimg.com/80/v2-69ebc11ec33cbcc4b9f24d542453aecb_720w.webp)

好，到目前为止，我们已经讲解了视频动作识别的经典数据集和经典模型，下面我们通过代码来实践地跑一跑其中的两个模型：C3D 模型（ 3D 卷积网络）以及 I3D 模型（Two-Stream Inflated 3D ConvNets）。

### C3D 模型结构

我们已经在前面的 “旧模型二：3D 卷积网络” 中讲解到 3D 卷积网络是一种看起来比较自然的处理视频的网络，虽然它有效果不够好，计算量也大的特点，但它的结构很简单，可以构造一个很简单的网络就可以实现视频动作识别，如下图所示是 3D 卷积的示意图：

![](https://pic2.zhimg.com/80/v2-5daa166a1066e6bc3a38d7c636a961cd_720w.webp)

a) 中，一张图片进行了 2D 卷积， b) 中，对视频进行 2D 卷积，将多个帧视作多个通道， c) 中，对视频进行 3D 卷积，将时序信息加入输入信号中。

ab 中，output 都是一张二维特征图，所以无论是输入是否有时间信息，输出都是一张二维的特征图，2D 卷积失去了时序信息。只有 3D 卷积在输出时，保留了时序信息。2D 和 3D 池化操作同样有这样的问题。

如下图所示是一种 C3D 网络的变种：（如需阅读原文描述，请查看 I3D 论文 2.2 节）

![](https://pic3.zhimg.com/80/v2-7cbedf23987c779f7c0fbb5837a2d882_720w.webp)

C3D 结构，包括 8 个卷积层，5 个最大池化层以及 2 个全连接层，最后是 softmax 输出层。

所有的 3D 卷积核为 $ 3 × 3 × 3$ 步长为 1，使用 SGD，初始学习率为 0.003，每 150k 个迭代，除以 2。优化在 1.9M 个迭代的时候结束，大约 13epoch。

数据处理时，视频抽帧定义大小为：$ c × l × h × w，，c 为通道数量，为通道数量，l 为帧的数量，为帧的数量，h 为帧画面的高度，为帧画面的高度，w 为帧画面的宽度。3D 卷积核和池化核的大小为为帧画面的宽度。3_D_ 卷积核和池化核的大小为 d × k × k，，d 是核的时间深度，是核的时间深度，k 是核的空间大小。网络的输入为视频的抽帧，预测出的是类别标签。所有的视频帧画面都调整大小为是核的空间大小。网络的输入为视频的抽帧，预测出的是类别标签。所有的视频帧画面都调整大小为 128 × 171 $，几乎将 UCF-101 数据集中的帧调整为一半大小。视频被分为不重复的 16 帧画面，这些画面将作为模型网络的输入。最后对帧画面的大小进行裁剪，输入的数据为 $16 × 112 × 112 $

3.C3D 模型训练
----------

接下来，我们将对 C3D 模型进行训练，训练过程分为：数据预处理以及模型训练。在此次训练中，我们使用的数据集为 UCF-101，由于 C3D 模型的输入是视频的每帧图片，因此我们需要对数据集的视频进行抽帧，也就是将视频转换为图片，然后将图片数据传入模型之中，进行训练。

在本案例中，我们随机抽取了 UCF-101 数据集的一部分进行训练的演示，感兴趣的同学可以下载完整的 UCF-101 数据集进行训练。

UCF-101 下载

数据集存储在目录 dataset\_subset 下

如下代码是使用 cv2 库进行视频文件到图片文件的转换

import cv2
import os
# 视频数据集存储位置
video\_path \= './dataset\_subset/'
# 生成的图像数据集存储位置
save\_path \= './dataset/'
# 如果文件路径不存在则创建路径
if not os.path.exists(save\_path):
 os.mkdir(save\_path)
# 获取动作列表
action\_list \= os.listdir(video\_path)
# 遍历所有动作
for action in action\_list:
 if action.startswith(".")==False:
 if not os.path.exists(save\_path+action):
 os.mkdir(save\_path+action)
 video\_list \= os.listdir(video\_path+action)
 # 遍历所有视频
 for video in video\_list:
            prefix \= video.split('.')\[0\]
 if not os.path.exists(os.path.join(save\_path, action, prefix)):
 os.mkdir(os.path.join(save\_path, action, prefix))
 save\_name \= os.path.join(save\_path, action, prefix) + '/'
 video\_name \= video\_path+action+'/'+video
 # 读取视频文件
 # cap为视频的帧
            cap \= cv2.VideoCapture(video\_name)
 # fps为帧率
            fps \= int(cap.get(cv2.CAP\_PROP\_FRAME\_COUNT))
 fps\_count \= 0
 for i in range(fps):
                ret, frame \= cap.read()
 if ret:
 # 将帧画面写入图片文件中
                    cv2.imwrite(save\_name+str(10000+fps\_count)+'.jpg',frame)
 fps\_count += 1

此时，视频逐帧转换成的图片数据已经存储起来，为模型训练做准备。

4\. 模型训练
--------

首先，我们构建模型结构。

C3D 模型结构我们之前已经介绍过，这里我们通过 keras 提供的 Conv3D，MaxPool3D，ZeroPadding3D 等函数进行模型的搭建。

from keras.layers import Dense,Dropout,Conv3D,Input,MaxPool3D,Flatten,Activation, ZeroPadding3D
from keras.regularizers import l2
from keras.models import Model, Sequential
# 输入数据为 112×112 的图片，16帧， 3通道
input\_shape \= (112,112,16,3)
# 权重衰减率
weight\_decay \= 0.005
# 类型数量，我们使用UCF\-101 为数据集，所以为101
nb\_classes \= 101
# 构建模型结构
inputs \= Input(input\_shape)
x \= Conv3D(64,(3,3,3),strides=(1,1,1),padding='same',
           activation\='relu',kernel\_regularizer=l2(weight\_decay))(inputs)
x \= MaxPool3D((2,2,1),strides=(2,2,1),padding='same')(x)
x \= Conv3D(128,(3,3,3),strides=(1,1,1),padding='same',
           activation\='relu',kernel\_regularizer=l2(weight\_decay))(x)
x \= MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)
x \= Conv3D(128,(3,3,3),strides=(1,1,1),padding='same',
           activation\='relu',kernel\_regularizer=l2(weight\_decay))(x)
x \= MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)
x \= Conv3D(256,(3,3,3),strides=(1,1,1),padding='same',
           activation\='relu',kernel\_regularizer=l2(weight\_decay))(x)
x \= MaxPool3D((2,2,2),strides=(2,2,2),padding='same')(x)
x \= Conv3D(256, (3, 3, 3), strides=(1, 1, 1), padding='same',
           activation\='relu',kernel\_regularizer=l2(weight\_decay))(x)
x \= MaxPool3D((2, 2, 2), strides=(2, 2, 2), padding='same')(x)
x \= Flatten()(x)
x \= Dense(2048,activation='relu',kernel\_regularizer=l2(weight\_decay))(x)
x \= Dropout(0.5)(x)
x \= Dense(2048,activation='relu',kernel\_regularizer=l2(weight\_decay))(x)
x \= Dropout(0.5)(x)
x \= Dense(nb\_classes,kernel\_regularizer=l2(weight\_decay))(x)
x \= Activation('softmax')(x)
model \= Model(inputs, x)
Using TensorFlow backend.
/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_qint8 \= np.dtype(\[("qint8", np.int8, 1)\])
/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_quint8 \= np.dtype(\[("quint8", np.uint8, 1)\])
/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_qint16 \= np.dtype(\[("qint16", np.int16, 1)\])
/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_quint16 \= np.dtype(\[("quint16", np.uint16, 1)\])
/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_qint32 \= np.dtype(\[("qint32", np.int32, 1)\])
/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
 np\_resource \= np.dtype(\[("resource", np.ubyte, 1)\])
WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/op\_def\_library.py:263: colocate\_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/keras/backend/tensorflow\_backend.py:3445: calling dropout (from tensorflow.python.ops.nn\_ops) with keep\_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use \`rate\` instead of \`keep\_prob\`. Rate should be set to \`rate = 1 - keep\_prob\`.

通过 keras 提供的 summary () 方法，打印模型结构。可以看到模型的层构建以及各层的输入输出情况。

model.summary()

此处输出较长，省略

通过 keras 的 input 方法可以查看模型的输入形状，shape 分别为 (batch size, width, height, frames, channels) 。

model.input
<tf.Tensor 'input\_1:0' shape=(?, 112, 112, 16, 3) dtype=float32>

可以看到模型的数据处理的维度与图像处理模型有一些差别，多了 frames 维度，体现出时序关系在视频分析中的影响。

接下来，我们开始将图片文件转为训练需要的数据形式。

\# 引用必要的库
from keras.optimizers import SGD,Adam
from keras.utils import np\_utils
import numpy as np
import random
import cv2
import matplotlib.pyplot as plt
# 自定义callbacks
from schedules import onetenth\_4\_8\_12
INFO:matplotlib.font\_manager:font search path \['/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf', '/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/afm', '/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts'\]
INFO:matplotlib.font\_manager:generated new fontManager

参数定义

img\_path = save\_path # 图片文件存储位置
results\_path \= './results' # 训练结果保存位置
if not os.path.exists(results\_path):
 os.mkdir(results\_path)

数据集划分，随机抽取 4/5 作为训练集，其余为验证集。将文件信息分别存储在 train\_list 和 test\_list 中，为训练做准备。

cates = os.listdir(img\_path)
train\_list \= \[\]
test\_list \= \[\]
# 遍历所有的动作类型
for cate in cates:
    videos \= os.listdir(os.path.join(img\_path, cate))
    length \= len(videos)//5
 # 训练集大小，随机取视频文件加入训练集
    train\= random.sample(videos, length\*4)
 train\_list.extend(train)
 # 将余下的视频加入测试集
 for video in videos:
 if video not in train:
 test\_list.append(video)
print("训练集为：") 
print( train\_list)
print("共%d 个视频\\n"%(len(train\_list)))
print("验证集为：") 
print(test\_list)
print("共%d 个视频"%(len(test\_list)))

此处输出较长，省略

接下来开始进行模型的训练。

首先定义数据读取方法。方法 process\_data 中读取一个 batch 的数据，包含 16 帧的图片信息的数据，以及数据的标注信息。在读取图片数据时，对图片进行随机裁剪和翻转操作以完成数据增广。

def process\_data(img\_path, file\_list,batch\_size=16,train=True):
    batch \= np.zeros((batch\_size,16,112,112,3),dtype='float32')
    labels \= np.zeros(batch\_size,dtype='int')
 cate\_list \= os.listdir(img\_path)
 def read\_classes():
        path \= "./classInd.txt"
 with open(path, "r+") as f:
            lines \= f.readlines()
        classes \= {}
 for line in lines:
 c\_id \= line.split()\[0\]
 c\_name \= line.split()\[1\]
            classes\[c\_name\] \=c\_id 
 return classes
 classes\_dict \= read\_classes()
 for file in file\_list:
        cate \= file.split("\_")\[1\]
 img\_list \= os.listdir(os.path.join(img\_path, cate, file))
 img\_list.sort()
 batch\_img \= \[\]
 for i in range(batch\_size):
            path \= os.path.join(img\_path, cate, file)
            label \= int(classes\_dict\[cate\])-1
            symbol \= len(img\_list)//16
 if train:
 # 随机进行裁剪
 crop\_x \= random.randint(0, 15)
 crop\_y \= random.randint(0, 58)
 # 随机进行翻转
 is\_flip \= random.randint(0, 1)
 # 以16 帧为单位
 for j in range(16):
 img \= img\_list\[symbol + j\]
                    image \= cv2.imread( path + '/' + img)
                    image \= cv2.cvtColor(image, cv2.COLOR\_BGR2RGB)
                    image \= cv2.resize(image, (171, 128))
 if is\_flip == 1:
                        image \= cv2.flip(image, 1)
                    batch\[i\]\[j\]\[:\]\[:\]\[:\] \= image\[crop\_x:crop\_x + 112, crop\_y:crop\_y + 112, :\]
                    symbol\-=1
 if symbol<0:
 break
                labels\[i\] \= label
 else:
 for j in range(16):
 img \= img\_list\[symbol + j\]
                    image \= cv2.imread( path + '/' + img)
                    image \= cv2.cvtColor(image, cv2.COLOR\_BGR2RGB)
                    image \= cv2.resize(image, (171, 128))
                    batch\[i\]\[j\]\[:\]\[:\]\[:\] \= image\[8:120, 30:142, :\]
                    symbol\-=1
 if symbol<0:
 break
                labels\[i\] \= label
 return batch, labels
batch, labels \= process\_data(img\_path, train\_list)
print("每个batch的形状为：%s"%(str(batch.shape)))
print("每个label的形状为：%s"%(str(labels.shape)))
每个batch的形状为：(16, 16, 112, 112, 3)
每个label的形状为：(16,)

定义 data generator， 将数据批次传入训练函数中。

def generator\_train\_batch(train\_list, batch\_size, num\_classes, img\_path):
 while True:
 # 读取一个batch的数据
 x\_train, x\_labels \= process\_data(img\_path, train\_list, batch\_size=16,train=True)
        x \= preprocess(x\_train)
 # 形成input要求的数据格式
        y \= np\_utils.to\_categorical(np.array(x\_labels), num\_classes)
        x \= np.transpose(x, (0,2,3,1,4))
 yield x, y
def generator\_val\_batch(test\_list, batch\_size, num\_classes, img\_path):
 while True:
 # 读取一个batch的数据
 y\_test,y\_labels \= process\_data(img\_path, train\_list, batch\_size=16,train=False)
        x \= preprocess(y\_test)
 # 形成input要求的数据格式
        x \= np.transpose(x,(0,2,3,1,4))
        y \= np\_utils.to\_categorical(np.array(y\_labels), num\_classes)
 yield x, y

定义方法 preprocess， 对函数的输入数据进行图像的标准化处理。

def preprocess(inputs):
 inputs\[..., 0\] -= 99.9
 inputs\[..., 1\] -= 92.1
 inputs\[..., 2\] -= 82.6
 inputs\[..., 0\] /= 65.8
 inputs\[..., 1\] /= 62.3
 inputs\[..., 2\] /= 60.3
 return inputs
# 训练一个epoch大约需4分钟
# 类别数量
num\_classes \= 101
# batch大小
batch\_size \= 4
# epoch数量
epochs \= 1
# 学习率大小
lr \= 0.005
# 优化器定义
sgd \= SGD(lr=lr, momentum=0.9, nesterov=True)
model.compile(loss\='categorical\_crossentropy', optimizer=sgd, metrics=\['accuracy'\])
# 开始训练
history \= model.fit\_generator(generator\_train\_batch(train\_list, batch\_size, num\_classes,img\_path),
 steps\_per\_epoch\= len(train\_list) // batch\_size,
                              epochs=epochs,
                              callbacks\=\[onetenth\_4\_8\_12(lr)\],
 validation\_data\=generator\_val\_batch(test\_list, batch\_size,num\_classes,img\_path),
 validation\_steps\= len(test\_list) // batch\_size,
                              verbose=1)
# 对训练结果进行保存
model.save\_weights(os.path.join(results\_path, 'weights\_c3d.h5'))
WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math\_ops.py:3066: to\_int32 (from tensorflow.python.ops.math\_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Epoch 1/1
20/20 \[==============================\] - 442s 22s/step - loss: 28.7099 - acc: 0.9344 - val\_loss: 27.7600 - val\_acc: 1.0000

5\. 模型测试
--------

接下来我们将训练之后得到的模型进行测试。随机在 UCF-101 中选择一个视频文件作为测试数据，然后对视频进行取帧，每 16 帧画面传入模型进行一次动作预测，并且将动作预测以及预测百分比打印在画面中并进行视频播放。

首先，引入相关的库。

from IPython.display import clear\_output, Image, display, HTML
import time
import cv2
import base64
import numpy as np

构建模型结构并且加载权重。

from models import c3d\_model
model \= c3d\_model()
model.load\_weights(os.path.join(results\_path, 'weights\_c3d.h5'), by\_name=True) # 加载刚训练的模型

定义函数 arrayshow，进行图片变量的编码格式转换。

def arrayShow(img):
 \_,ret \= cv2.imencode('.jpg', img) 
 return Image(data=ret) 

进行视频的预处理以及预测，将预测结果打印到画面中，最后进行播放。

\# 加载所有的类别和编号
with open('./ucfTrainTestlist/classInd.txt', 'r') as f:
 class\_names \= f.readlines()
 f.close()
# 读取视频文件
video \= './videos/v\_Punch\_g03\_c01.avi'
cap \= cv2.VideoCapture(video)
clip \= \[\]
# 将视频画面传入模型
while True:
 try:
 clear\_output(wait\=True)
        ret, frame \= cap.read()
 if ret:
 tmp \= cv2.cvtColor(frame, cv2.COLOR\_BGR2RGB)
 clip.append(cv2.resize(tmp, (171, 128)))
 # 每16帧进行一次预测
 if len(clip) == 16:
                inputs \= np.array(clip).astype(np.float32)
                inputs \= np.expand\_dims(inputs, axis=0)
 inputs\[..., 0\] -= 99.9
 inputs\[..., 1\] -= 92.1
 inputs\[..., 2\] -= 82.6
 inputs\[..., 0\] /= 65.8
 inputs\[..., 1\] /= 62.3
 inputs\[..., 2\] /= 60.3
                inputs \= inputs\[:,:,8:120,30:142,:\]
                inputs \= np.transpose(inputs, (0, 2, 3, 1, 4))
 # 获得预测结果
 pred \= model.predict(inputs)
                label \= np.argmax(pred\[0\])
 # 将预测结果绘制到画面中
                cv2.putText(frame, class\_names\[label\].split(' ')\[-1\].strip(), (20, 20),
                            cv2.FONT\_HERSHEY\_SIMPLEX, 0.6,
 (0, 0, 255), 1)
                cv2.putText(frame, "prob: %.4f" % pred\[0\]\[label\], (20, 40),
                            cv2.FONT\_HERSHEY\_SIMPLEX, 0.6,
 (0, 0, 255), 1)
 clip.pop(0)
 # 播放预测后的视频 
            lines, columns, \_ \= frame.shape
            frame \= cv2.resize(frame, (int(columns), int(lines)))
 img \= arrayShow(frame)
            display(img)
 time.sleep(0.02)
 else:
 break
 except:
 print(0)
cap.release()

![](https://pic4.zhimg.com/80/v2-d3c74ca679673266eccc79f60a9d8c5f_720w.webp)

6.I3D 模型
--------

在之前我们简单介绍了 I3D 模型，I3D 官方 github 库提供了在 Kinetics 上预训练的模型和预测代码，接下来我们将体验 I3D 模型如何对视频进行预测。

首先，引入相关的包

import numpy as np
import tensorflow as tf
import i3d
WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
 \* https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
 \* https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

进行参数的定义

\# 输入图片大小
\_IMAGE\_SIZE \= 224
#  视频的帧数
\_SAMPLE\_VIDEO\_FRAMES \= 79
# 输入数据包括两部分：RGB和光流
# RGB和光流数据已经经过提前计算
\_SAMPLE\_PATHS \= {
 'rgb': 'data/v\_CricketShot\_g04\_c01\_rgb.npy',
 'flow': 'data/v\_CricketShot\_g04\_c01\_flow.npy',
}
# 提供了多种可以选择的预训练权重
# 其中，imagenet系列模型从ImageNet的2D权重中拓展而来，其余为视频数据下的预训练权重
\_CHECKPOINT\_PATHS \= {
 'rgb': 'data/checkpoints/rgb\_scratch/model.ckpt',
 'flow': 'data/checkpoints/flow\_scratch/model.ckpt',
 'rgb\_imagenet': 'data/checkpoints/rgb\_imagenet/model.ckpt',
 'flow\_imagenet': 'data/checkpoints/flow\_imagenet/model.ckpt',
}
# 记录类别文件
\_LABEL\_MAP\_PATH \= 'data/label\_map.txt'
# 类别数量为400
NUM\_CLASSES \= 400

定义参数：

*   imagenet\_pretrained ：如果为 True，则调用预训练权重，如果为 False，则调用 ImageNet 转成的权重

imagenet\_pretrained = True
# 加载动作类型
kinetics\_classes \= \[x.strip() for x in open(\_LABEL\_MAP\_PATH)\]
tf.logging.set\_verbosity(tf.logging.INFO)

构建 RGB 部分模型

rgb\_input = tf.placeholder(tf.float32, shape=(1, \_SAMPLE\_VIDEO\_FRAMES, \_IMAGE\_SIZE, \_IMAGE\_SIZE, 3))
with tf.variable\_scope('RGB', reuse=tf.AUTO\_REUSE):
 rgb\_model \= i3d.InceptionI3d(NUM\_CLASSES, spatial\_squeeze=True, final\_endpoint='Logits')
 rgb\_logits, \_ \= rgb\_model(rgb\_input, is\_training=False, dropout\_keep\_prob=1.0)
rgb\_variable\_map \= {}
for variable in tf.global\_variables():
 if variable.name.split('/')\[0\] == 'RGB':
 rgb\_variable\_map\[variable.name.replace(':0', '')\] = variable
rgb\_saver \= tf.train.Saver(var\_list=rgb\_variable\_map, reshape=True)

构建光流部分模型

flow\_input = tf.placeholder(tf.float32,shape=(1, \_SAMPLE\_VIDEO\_FRAMES, \_IMAGE\_SIZE, \_IMAGE\_SIZE, 2))
with tf.variable\_scope('Flow', reuse=tf.AUTO\_REUSE):
 flow\_model \= i3d.InceptionI3d(NUM\_CLASSES, spatial\_squeeze=True, final\_endpoint='Logits')
 flow\_logits, \_ \= flow\_model(flow\_input, is\_training=False, dropout\_keep\_prob=1.0)
flow\_variable\_map \= {}
for variable in tf.global\_variables():
 if variable.name.split('/')\[0\] == 'Flow':
 flow\_variable\_map\[variable.name.replace(':0', '')\] = variable
flow\_saver \= tf.train.Saver(var\_list=flow\_variable\_map, reshape=True) 

将模型联合，成为完整的 I3D 模型

model\_logits = rgb\_logits + flow\_logits
model\_predictions \= tf.nn.softmax(model\_logits)

开始模型预测，获得视频动作预测结果。

预测数据为开篇提供的 RGB 和光流数据：

![See videos/v_CricketShot_g04_c01_rgb.gif](https://modelarts-labs-bj4-v2.obs.cn-north-4.myhuaweicloud.com/course/ai_in_action/2021/video/action_recognition/img/v_CricketShot_g04_c01_rgb.gif)

![See videos/v_CricketShot_g04_c01_flow.gif](https://modelarts-labs-bj4-v2.obs.cn-north-4.myhuaweicloud.com/course/ai_in_action/2021/video/action_recognition/img/v_CricketShot_g04_c01_flow.gif)

with tf.Session() as sess:
 feed\_dict \= {}
 if imagenet\_pretrained:
 rgb\_saver.restore(sess, \_CHECKPOINT\_PATHS\['rgb\_imagenet'\]) # 加载rgb流的模型
 else:
 rgb\_saver.restore(sess, \_CHECKPOINT\_PATHS\['rgb'\])
 tf.logging.info('RGB checkpoint restored')
 if imagenet\_pretrained:
 flow\_saver.restore(sess, \_CHECKPOINT\_PATHS\['flow\_imagenet'\]) # 加载flow流的模型
 else:
 flow\_saver.restore(sess, \_CHECKPOINT\_PATHS\['flow'\])
 tf.logging.info('Flow checkpoint restored') 
 start\_time \= time.time()
 rgb\_sample \= np.load(\_SAMPLE\_PATHS\['rgb'\]) # 加载rgb流的输入数据
 tf.logging.info('RGB data loaded, shape=%s', str(rgb\_sample.shape))
 feed\_dict\[rgb\_input\] \= rgb\_sample
 flow\_sample \= np.load(\_SAMPLE\_PATHS\['flow'\]) # 加载flow流的输入数据
 tf.logging.info('Flow data loaded, shape=%s', str(flow\_sample.shape))
 feed\_dict\[flow\_input\] \= flow\_sample
 out\_logits, out\_predictions \= sess.run(
 \[model\_logits, model\_predictions\],
 feed\_dict\=feed\_dict)
 out\_logits \= out\_logits\[0\]
 out\_predictions \= out\_predictions\[0\]
 sorted\_indices \= np.argsort(out\_predictions)\[::-1\]
 print('Inference time in sec: %.3f' % float(time.time() - start\_time))
 print('Norm of logits: %f' % np.linalg.norm(out\_logits))
 print('\\nTop classes and probabilities')
 for index in sorted\_indices\[:20\]:
 print(out\_predictions\[index\], out\_logits\[index\], kinetics\_classes\[index\])
WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint\_exists (from tensorflow.python.training.checkpoint\_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from data/checkpoints/rgb\_imagenet/model.ckpt
INFO:tensorflow:RGB checkpoint restored
INFO:tensorflow:Restoring parameters from data/checkpoints/flow\_imagenet/model.ckpt
INFO:tensorflow:Flow checkpoint restored
INFO:tensorflow:RGB data loaded, shape\=(1, 79, 224, 224, 3)
INFO:tensorflow:Flow data loaded, shape\=(1, 79, 224, 224, 2)
Inference time in sec: 1.511
Norm of logits: 138.468643
Top classes and probabilities
1.0 41.813675 playing cricket
1.497162e-09 21.49398 hurling (sport)
3.8431236e-10 20.13411 catching or throwing baseball
1.549242e-10 19.22559 catching or throwing softball
1.1360187e-10 18.915354 hitting baseball
8.801105e-11 18.660116 playing tennis
2.4415466e-11 17.37787 playing kickball
1.153184e-11 16.627766 playing squash or racquetball
6.1318893e-12 15.996157 shooting goal (soccer)
4.391727e-12 15.662376 hammer throw
2.2134352e-12 14.9772005 golf putting
1.6307096e-12 14.67167 throwing discus
1.5456218e-12 14.618079 javelin throw
7.6690325e-13 13.917259 pumping fist
5.1929587e-13 13.527372 shot put
4.2681337e-13 13.331245 celebrating
2.7205462e-13 12.880901 applauding
1.8357015e-13 12.487494 throwing ball
1.6134511e-13 12.358444 dodgeball
1.1388395e-13 12.010078 tap dancing

**[点击关注，第一时间了解华为云新鲜技术～](https://bbs.huaweicloud.com/blogs?utm_source=cnblog&utm_medium=bbs-ex&utm_campaign=other&utm_content=content)**