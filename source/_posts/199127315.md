---
layout: post
title: "迁移学习（ADDA）《Adversarial Discriminative Domain Adaptation》【已复现迁移】"
date: "2023-01-29T02:46:08.928Z"
---
迁移学习（ADDA）《Adversarial Discriminative Domain Adaptation》【已复现迁移】
===============================================================

论文信息
====

> 论文标题：Adversarial Discriminative Domain Adaptation  
> 论文作者：Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell  
> 论文来源：CVPR 2017  
> 论文地址：[download](https://arxiv.org/abs/1901.00976)   
> 论文代码：[download](https://github.com/corenel/pytorch-adda)  
> 引用次数：3257

1 简介
====

　　本文主要探讨的是：源域和目标域特征提取器共享参数的必要性。

　　源域和目标域特征提取器共享参数的代表——DANN。

2 对抗域适应
=======

　　标准监督损失训练源数据：

　　　　$\\underset{M\_{s}, C}{\\text{min}} \\quad \\mathcal{L}\_{\\mathrm{cls}}\\left(\\mathbf{X}\_{s}, Y\_{t}\\right)=  \\mathbb{E}\_{\\left(\\mathbf{x}\_{s}, y\_{s}\\right) \\sim\\left(\\mathbf{X}\_{s}, Y\_{t}\\right)}-\\sum\\limits \_{k=1}^{K} \\mathbb{1}\_{\\left\[k=y\_{s}\\right\]} \\log C\\left(M\_{s}\\left(\\mathbf{x}\_{s}\\right)\\right)\\quad\\quad(1)$

　　域对抗：首先使得域鉴别器分类准确，即最小化交叉熵损失 $\\mathcal{L}\_{\\operatorname{adv}\_{D}}\\left(\\mathbf{X}\_{s}, \\mathbf{X}\_{t}, M\_{s}, M\_{t}\\right)$：

　　　　$\\begin{array}{l}\\mathcal{L}\_{\\text {adv }\_{D}}\\left(\\mathbf{X}\_{s}, \\mathbf{X}\_{t}, M\_{s}, M\_{t}\\right)= -\\mathbb{E}\_{\\mathbf{x}\_{s} \\sim \\mathbf{X}\_{s}}\\left\[\\log D\\left(M\_{s}\\left(\\mathbf{x}\_{s}\\right)\\right)\\right\] -\\mathbb{E}\_{\\mathbf{x}\_{t} \\sim \\mathbf{X}\_{t}}\\left\[\\log \\left(1-D\\left(M\_{t}\\left(\\mathbf{x}\_{t}\\right)\\right)\\right)\\right\]\\end{array} \\quad\\quad(2)$

　　其次，源映射和目标映射根据一个受约束的对抗性目标进行优化（使得域鉴别器损失最大）。

　　域对抗技术的通用公式如下：

　　　　$\\begin{array}{l}\\underset{D}{\\text{min}}  & \\mathcal{L}\_{\\mathrm{adv}\_{D}}\\left(\\mathbf{X}\_{s}, \\mathbf{X}\_{t}, M\_{s}, M\_{t}\\right) \\\\\\underset{M\_{s}, M\_{t}}{\\text{min}}  & \\mathcal{L}\_{\\mathrm{adv}\_{M}}\\left(\\mathbf{X}\_{s}, \\mathbf{X}\_{t}, D\\right) \\\\\\text { s.t. } & \\psi\\left(M\_{s}, M\_{t}\\right)\\end{array}\\quad\\quad(3)$

2.1 源域和目标域映射
------------

　　![](https://img2023.cnblogs.com/blog/1664108/202301/1664108-20230128212822431-929236295.png)

　　归结为三个问题：

1.  *   选择生成式模型还是判别式模型？
    *   针对源域与目标域的映射是否共享参数？
    *   损失函数如何定义？

2.2 Adversarial losses
----------------------

　　回顾DANN 的训练方式：DANN 的梯度反转层优化映射，使鉴别器损失最大化

　　　　$\\mathcal{L}\_{\\text {adv }\_{M}}=-\\mathcal{L}\_{\\mathrm{adv}\_{D}}\\quad\\quad(6)$

　　这个目标可能有问题，因为在训练的早期，鉴别器快速收敛，导致梯度消失。

　　当训练 GANs 时，而不是直接使用 minimax，通常是用带有倒置标签\[10\]的标准损失函数来训

　　回顾 GAN ：GAN将优化分为两个独立的目标，一个用于生成器，另一个用于鉴别器。训练生成器的时候，其中 $\\mathcal{L}\_{\\mathrm{adv}\_{D}}$ 保持不变，但 $\\mathcal{L}\_{\\mathrm{adv}\_{M}}$ 变成：

　　　　$\\mathcal{L}\_{\\mathrm{adv}\_{M}}\\left(\\mathbf{X}\_{s}, \\mathbf{X}\_{t}, D\\right)=-\\mathbb{E}\_{\\mathbf{x}\_{t} \\sim \\mathbf{X}\_{t}}\\left\[\\log D\\left(M\_{t}\\left(\\mathbf{x}\_{t}\\right)\\right)\\right\] \\quad\\quad(7)$

　　Note：$\\mathbf{x}\_{t}$ 代表噪声数据，这里是使得噪声数据尽可能迷惑鉴别器。

![](https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif)![](https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif)

adversarial\_loss = torch.nn.BCELoss()  # 损失函数（二分类交叉熵损失）
generator = Generator()           #生成器
discriminator = Discriminator()   #鉴别器
optimizer\_G \= torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))  # 生成器优化器
optimizer\_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))   # 鉴别器优化器

for epoch in range(opt.n\_epochs):
    for i, (imgs, \_) in enumerate(dataloader):
        # Adversarial ground truths
        valid = Variable(Tensor(imgs.size(0), 1).fill\_(1.0), requires\_grad=False)  #torch.Size(\[64, 1\])
        fake = Variable(Tensor(imgs.size(0), 1).fill\_(0.0), requires\_grad=False)   #torch.Size(\[64, 1\])
        real\_imgs = Variable(imgs.type(Tensor))     #torch.Size(\[64, 1, 28, 28\])   真实数据

        # ----------------------> 训练生成器  \[生成器使用噪声数据，使得其尽可能为真，迷惑鉴别器\]
        optimizer\_G.zero\_grad()
        z \= Variable(Tensor(np.random.normal(0, 1, (imgs.shape\[0\], opt.latent\_dim))))    #torch.Size(\[64, 100\])
        gen\_imgs = generator(z)        #torch.Size(\[64, 1, 28, 28\])
        g\_loss = adversarial\_loss(discriminator(gen\_imgs), valid)
        g\_loss.backward()
        optimizer\_G.step()

        # ----------------------> 训练鉴别器  \[ 尽可能将真实数据和噪声数据区分开\]
        optimizer\_D.zero\_grad()
        real\_loss \= adversarial\_loss(discriminator(real\_imgs), valid)
        fake\_loss \= adversarial\_loss(discriminator(gen\_imgs.detach()), fake)
        d\_loss \= (real\_loss + fake\_loss) / 2
        d\_loss.backward()
        optimizer\_D.step()

GAN code

　　本文采用的方法类似于  GAN 。

3 对抗性域适应  

===========

　　与之前方法不同： 

　　![](https://img2023.cnblogs.com/blog/1664108/202301/1664108-20230104193707281-1848619714.png)

　　本文方法：

　　![](https://img2023.cnblogs.com/blog/1664108/202301/1664108-20230104193737566-712070250.png)

　　首先：Pretrain ，使用源域训练一个分类器；\[ 公式 9 第一个子公式\]

　　其次：Adversarial Adaption 

1.  *   使用源域和目标域数据，训练一个域鉴别器 Discriminator ，是的鉴别器尽可能区分源域和目标域数据 ；\[ 公式 9 第二个子公式\]　　
    *   使用目标域数据，训练目标域特征提取器，尽可能使得域鉴别器区分不出目标域样本；\[ 公式 9 第三个子公式\]　　

　　最后：Testing，在目标域上做 Eval；

　　ADDA对应于以下无约束优化：

　　　　$\\begin{array}{l}\\underset{M\_{s}, C}{\\text{min}} \\quad \\mathcal{L}\_{\\mathrm{cls}}\\left(\\mathbf{X}\_{s}, Y\_{s}\\right) &=&-\\mathbb{E}\_{\\left(\\mathbf{x}\_{s}, y\_{s}\\right) \\sim\\left(\\mathbf{X}\_{s}, Y\_{s}\\right)} \\sum\_{k=1}^{K} \\mathbb{1}\_{\\left\[k=y\_{s}\\right\]} \\log C\\left(M\_{s}\\left(\\mathbf{x}\_{s}\\right)\\right) \\\\\\underset{D}{\\text{min}}  \\quad\\mathcal{L}\_{\\text {adv }\_{D}}\\left(\\mathbf{X}\_{s}, \\mathbf{X}\_{t}, M\_{s}, M\_{t}\\right)&=& -\\mathbb{E}\_{\\mathbf{x}\_{s} \\sim \\mathbf{X}\_{s}}\\left\[\\log D\\left(M\_{s}\\left(\\mathbf{x}\_{s}\\right)\\right)\\right\] \\text { - } \\mathbb{E}\_{\\mathbf{x}\_{t} \\sim \\mathbf{X}\_{t}}\\left\[\\log \\left(1-D\\left(M\_{t}\\left(\\mathbf{x}\_{t}\\right)\\right)\\right)\\right\] \\\\\\underset{M\_{t}}{\\text{min}}  \\quad \\mathcal{L}\_{\\operatorname{adv}\_{M}}\\left(\\mathbf{X}\_{s}, \\mathbf{X}\_{t}, D\\right)&=& -\\mathbb{E}\_{\\mathbf{x}\_{t} \\sim \\mathbf{X}\_{t}}\\left\[\\log D\\left(M\_{t}\\left(\\mathbf{x}\_{t}\\right)\\right)\\right\] \\\\\\end{array} \\quad\\quad(9)$

![](https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif)![](https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif)

    tgt\_encoder.train()
    discriminator.train()

    # setup criterion and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer\_tgt \= optim.Adam(tgt\_encoder.parameters(),lr=params.c\_learning\_rate,betas=(params.beta1, params.beta2))
    optimizer\_discriminator \= optim.Adam(discriminator.parameters(),lr=params.d\_learning\_rate,betas=(params.beta1, params.beta2))
    len\_data\_loader \= min(len(src\_data\_loader), len(tgt\_data\_loader))  #149

    for epoch in range(params.num\_epochs):
        # zip source and target data pair
        data\_zip = enumerate(zip(src\_data\_loader, tgt\_data\_loader))
        for step, ((images\_src, \_), (images\_tgt, \_)) in data\_zip:
            # 2.1 训练域鉴别器，使得域鉴别器尽可能的准确
            images\_src = make\_variable(images\_src)
            images\_tgt \= make\_variable(images\_tgt)
            discriminator.zero\_grad()
            feat\_src,feat\_tgt \= src\_encoder(images\_src) ,tgt\_encoder(images\_tgt)   # 源域特征提取  # 目标域特征提取
            feat\_concat = torch.cat((feat\_src, feat\_tgt), 0)
            pred\_concat \= discriminator(feat\_concat.detach())    # 域分类结果
            label\_src \= make\_variable(torch.ones(feat\_src.size(0)).long())   #假设源域的标签为 1
            label\_tgt = make\_variable(torch.zeros(feat\_tgt.size(0)).long())  #假设目标域域的标签为 0
            label\_concat = torch.cat((label\_src, label\_tgt), 0)

            loss\_critic \= criterion(pred\_concat, label\_concat)
            loss\_critic.backward()
            optimizer\_discriminator.step()     # 域鉴别器优化
            pred\_cls \= torch.squeeze(pred\_concat.max(1)\[1\])
            acc \= (pred\_cls == label\_concat).float().mean()

            # 2.2 train target encoder # 使得目标域特征生成器，尽可能使得域鉴别器区分不出源域和目标域样本
            optimizer\_discriminator.zero\_grad()
            optimizer\_tgt.zero\_grad()
            feat\_tgt \= tgt\_encoder(images\_tgt)
            pred\_tgt \= discriminator(feat\_tgt)
            label\_tgt \= make\_variable(torch.ones(feat\_tgt.size(0)).long())   #假设目标域域的标签为 1（错误标签），使得域鉴别器鉴别错误
            loss\_tgt = criterion(pred\_tgt, label\_tgt)
            loss\_tgt.backward()
            optimizer\_tgt.step()  # 目标域 encoder 优化

ADDA Code

因上求缘，果上努力~~~~ 作者：[加微信X466550探讨](https://www.cnblogs.com/BlairGrowing/)，转载请注明原文链接：[https://www.cnblogs.com/BlairGrowing/p/17020378.html](https://www.cnblogs.com/BlairGrowing/p/17020378.html)