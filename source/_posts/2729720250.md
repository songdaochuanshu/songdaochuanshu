---
layout: post
title: "体验SRCNN和FSRCNN两种图像超分网络应用"
date: "2022-07-11T03:39:18.059Z"
---
体验SRCNN和FSRCNN两种图像超分网络应用
========================

> **摘要：**图像超分即超分辨率，将图像从模糊的状态变清晰。

本文分享自华为云社区《[图像超分实验：SRCNN/FSRCNN](https://bbs.huaweicloud.com/blogs/363764?utm_source=cnblog&utm_medium=bbs-ex&utm_campaign=other&utm_content=content)》，作者：zstar。

图像超分即超分辨率，将图像从模糊的状态变清晰。本文对BSDS500数据集进行超分实验。

1.实验目标
------

输入大小为h×w的图像X，输出为一个sh×sw的图像 Y，s为放大倍数。

2.数据集简介
-------

本次实验采用的是 BSDS500 数据集，其中训练集包含 200 张图像，验证集包含 100 张图像，测试集包含 200 张图像。

数据集来源：https://download.csdn.net/download/weixin\_42028424/11045313

3.数据预处理
-------

数据预处理包含两个步骤：

### (1)将图片转换成YCbCr模式

由于RGB颜色模式色调、色度、饱和度三者混在一起难以分开，因此将其转换成 YcbCr 颜色模式，Y是指亮度分量，Cb表示 RGB输入信号蓝色部分与 RGB 信号亮度值之间的差异，Cr 表示 RGB 输入信号红色部分与 RGB 信号亮度值之间的差异。

### (2)将图片裁剪成 300×300 的正方形

由于后面采用的神经网路输入图片要求长宽一致，而 BSDS500 数据集中的图片长宽并不一致，因此需要对其进行裁剪。这里采用的方式是先定位到每个图片中心，然后以图片中心为基准，向四个方向拓展 150 个像素，从而将图片裁剪成 300×300 的正方形。

相关代码：

def is\_image\_file(filename):
    return any(filename.endswith(extension) for extension in \[".png", ".jpg", ".jpeg"\])
def load\_img(filepath):
    img \= Image.open(filepath).convert('YCbCr')
    y, \_, \_ \= img.split()
    return y
CROP\_SIZE \= 300
class DatasetFromFolder(Dataset):
    def \_\_init\_\_(self, image\_dir, zoom\_factor):
        super(DatasetFromFolder, self).\_\_init\_\_()
        self.image\_filenames \= \[join(image\_dir, x)
                                for x in listdir(image\_dir) if is\_image\_file(x)\]
        crop\_size \= CROP\_SIZE - (CROP\_SIZE % zoom\_factor)
        # 从图片中心裁剪成300\*300
        self.input\_transform \= transforms.Compose(\[transforms.CenterCrop(crop\_size),
                                                   transforms.Resize(
                                                       crop\_size // zoom\_factor),
                                                   transforms.Resize(
                                                       crop\_size, interpolation\=Image.BICUBIC),
                                                   # BICUBIC 双三次插值
                                                   transforms.ToTensor()\])
        self.target\_transform \= transforms.Compose(
            \[transforms.CenterCrop(crop\_size), transforms.ToTensor()\])

    def \_\_getitem\_\_(self, index):
        input \= load\_img(self.image\_filenames\[index\])
        target \= input.copy()
        input \= self.input\_transform(input)
        target \= self.target\_transform(target)
        return input, target

    def \_\_len\_\_(self):
        return len(self.image\_filenames)

4.网络结构
------

本次实验尝试了SRCNN和FSRCNN两个网络。

### 4.1 SRCNN

SRCNN 由 2014 年 Chao Dong 等人提出，是深度学习在图像超分领域的开篇之作。其网络结构如下图所示：

![](https://pic4.zhimg.com/80/v2-62996aa0d4e5e4ef016ca6e8a7888e03_720w.jpg)

该网络对于一个低分辨率图像，先使用双三次插值将其放大到目标大小，再通过三层卷积网络做非线性映射，得到的结果作为高分辨率图像输出。

作者对于这三层卷积层的解释：

(1)特征块提取和表示：此操作从低分辨率图像Y中提取重叠特征块，并将每个特征块表示为一个高维向量。这些向量包括一组特征图，其数量等于向量的维数。

(2)非线性映射：该操作将每个高维向量非线性映射到另一个高维向量。每个映射向量在概念上都是高分辨率特征块的表示。这些向量同样包括另一组特征图。

(3)重建：该操作聚合上述高分辨率patch-wise（介于像素级别和图像级别的区域）表示，生成最终的高分辨率图像。

各层结构：

*   输入：处理后的低分辨率图像
*   卷积层 1：采用 9×9 的卷积核
*   卷积层 2：采用 1×1 的卷积核
*   卷积层 3：采用 5×5 的卷积核
*   输出：高分辨率图像

模型结构代码：

class SRCNN(nn.Module):
    def \_\_init\_\_(self, upscale\_factor):
        super(SRCNN, self).\_\_init\_\_()

        self.relu \= nn.ReLU()
        self.conv1 \= nn.Conv2d(1, 64, kernel\_size=5, stride=1, padding=2)
        self.conv2 \= nn.Conv2d(64, 64, kernel\_size=3, stride=1, padding=1)
        self.conv3 \= nn.Conv2d(64, 32, kernel\_size=3, stride=1, padding=1)
        self.conv4 \= nn.Conv2d(32, upscale\_factor \*\* 2,
                               kernel\_size\=3, stride=1, padding=1)
        self.pixel\_shuffle \= nn.PixelShuffle(upscale\_factor)

        self.\_initialize\_weights()

    def \_initialize\_weights(self):
        init.orthogonal\_(self.conv1.weight, init.calculate\_gain('relu'))
        init.orthogonal\_(self.conv2.weight, init.calculate\_gain('relu'))
        init.orthogonal\_(self.conv3.weight, init.calculate\_gain('relu'))
        init.orthogonal\_(self.conv4.weight)

    def forward(self, x):
        x \= self.conv1(x)
        x \= self.relu(x)
        x \= self.conv2(x)
        x \= self.relu(x)
        x \= self.conv3(x)
        x \= self.relu(x)
        x \= self.conv4(x)
        x \= self.pixel\_shuffle(x)
        return x

### 4.2 FSRCNN

FSRCNN 由 2016 年 Chao Dong 等人提出，与 SRCNN 是相同作者。其网络结构如下图所示：

![](https://pic2.zhimg.com/80/v2-ea0141beaf24d79932a7597614b0b475_720w.jpg)

FSRCNN在SRCNN基础上做了如下改变：

1.FSRCNN直接采用低分辨的图像作为输入，不同于SRCNN需要先对低分辨率的图像进行双三次插值然后作为输入；  
2.FSRCNN在网络的最后采用反卷积层实现上采样；  
3.FSRCNN中没有非线性映射，相应地出现了收缩、映射和扩展；  
4.FSRCNN选择更小尺寸的滤波器和更深的网络结构。

各层结构：

*   输入层：FSRCNN不使用bicubic插值来对输入图像做上采样，它直接进入特征提取层
*   特征提取层：采用1 × d × ( 5 × 5 )的卷积层提取
*   收缩层：采用d × s × ( 1 × 1 ) 的卷积层去减少通道数，来减少模型复杂度
*   映射层：采用s × s × ( 3 × 3 ) 卷积层去增加模型非线性度来实现LR → SR 的映射
*   扩张层：该层和收缩层是对称的，采用s × d × ( 1 × 1 ) 卷积层去增加重建的表现力
*   反卷积层：s × 1 × ( 9 × 9 )
*   输出层：输出HR图像

模型结构代码：

class FSRCNN(nn.Module):
    def \_\_init\_\_(self, scale\_factor, num\_channels\=1, d=56, s=12, m=4):
        super(FSRCNN, self).\_\_init\_\_()
        self.first\_part \= nn.Sequential(
            nn.Conv2d(num\_channels, d, kernel\_size\=5, padding=5//2),
            nn.PReLU(d)
        )
        self.mid\_part \= \[nn.Conv2d(d, s, kernel\_size=1), nn.PReLU(s)\]
        for \_ in range(m):
            self.mid\_part.extend(\[nn.Conv2d(s, s, kernel\_size\=3, padding=3//2), nn.PReLU(s)\])
        self.mid\_part.extend(\[nn.Conv2d(s, d, kernel\_size=1), nn.PReLU(d)\])
        self.mid\_part \= nn.Sequential(\*self.mid\_part)
        self.last\_part \= nn.ConvTranspose2d(d, num\_channels, kernel\_size=9, stride=scale\_factor, padding=9//2,
                                            output\_padding=scale\_factor-1)

        self.\_initialize\_weights()

    def \_initialize\_weights(self):
        for m in self.first\_part:
            if isinstance(m, nn.Conv2d):
                nn.init.normal\_(m.weight.data, mean\=0.0, std=math.sqrt(2/(m.out\_channels\*m.weight.data\[0\]\[0\].numel())))
                nn.init.zeros\_(m.bias.data)
        for m in self.mid\_part:
            if isinstance(m, nn.Conv2d):
                nn.init.normal\_(m.weight.data, mean\=0.0, std=math.sqrt(2/(m.out\_channels\*m.weight.data\[0\]\[0\].numel())))
                nn.init.zeros\_(m.bias.data)
        nn.init.normal\_(self.last\_part.weight.data, mean\=0.0, std=0.001)
        nn.init.zeros\_(self.last\_part.bias.data)

    def forward(self, x):
        x \= self.first\_part(x)
        x \= self.mid\_part(x)
        x \= self.last\_part(x)
        return x

5.评估指标
------

本次实验尝试了 PSNR 和 SSIM 两个指标。

### 5.1 PSNR

PSNR(Peak Signal to Noise Ratio)为峰值信噪比，计算公式如下：

![](https://pic1.zhimg.com/80/v2-0bb667bf5da6d60181a54b3b285113b8_720w.jpg)

其中，n为每像素的比特数。

PSNR 的单位是dB，数值越大表示失真越小，一般认为 PSNR 在 38 以上的时候，人眼就无法区分两幅图片了。

相关代码：

def psnr(loss):
    return 10 \* log10(1 / loss.item())

### 5.2 SSIM

SSIM(Structural Similarity)为结构相似性，由三个对比模块组成：亮度、对比度、结构。

![](https://pic3.zhimg.com/80/v2-bcca3f22b4798abbe239dd940a631e8e_720w.jpg)

### 亮度对比函数

图像的平均灰度计算公式：

![](https://pic1.zhimg.com/80/v2-e18b69a730dd0dfa69cecf6e42cf88a0_720w.jpg)

亮度对比函数计算公式：

![](https://pic4.zhimg.com/80/v2-ac6c1f411275a69dc21149333147dc4f_720w.jpg)

### 对比度对比函数

图像的标准差计算公式：

![](https://pic1.zhimg.com/80/v2-0f74ca070f1e1903d96d2e184d248700_720w.jpg)

对比度对比函数计算公式：

![](https://pic3.zhimg.com/80/v2-9c3ff54b2bf09076e53ffca5f1fd9d3a_720w.jpg)

### 结构对比函数

结构对比函数计算公式：

![](https://pic3.zhimg.com/80/v2-318e77e7fadde4fc4789254d5b30986e_720w.jpg)

综合上述三个部分，得到 SSIM 计算公式：

![](https://pic4.zhimg.com/80/v2-315f55bcde3fd3d82604bb18c5694893_720w.jpg)

其中，\\alpha_α_,\\beta_β_,\\gamma_γ_ > 0，用来调整这三个模块的重要性。

SSIM 函数的值域为\[0, 1\], 值越大说明图像失真越小，两幅图像越相似。

相关代码：

由于pytorch没有类似tensorflow类似tf.image.ssim这样计算SSIM的接口，因此根据公式进行自定义函数用来计算

"""
计算ssim函数
"""
\# 计算一维的高斯分布向量
def gaussian(window\_size, sigma):
    gauss \= torch.Tensor(
        \[exp(\-(x - window\_size//2)\*\*2/float(2\*sigma\*\*2)) for x in range(window\_size)\])
    return gauss/gauss.sum()
# 创建高斯核，通过两个一维高斯分布向量进行矩阵乘法得到
# 可以设定channel参数拓展为3通道
def create\_window(window\_size, channel\=1):
    \_1D\_window \= gaussian(window\_size, 1.5).unsqueeze(1)
    \_2D\_window \= \_1D\_window.mm(
        \_1D\_window.t()).float().unsqueeze(0).unsqueeze(0)
    window \= \_2D\_window.expand(
        channel, 1, window\_size, window\_size).contiguous()
    return window
# 计算SSIM
# 直接使用SSIM的公式，但是在计算均值时，不是直接求像素平均值，而是采用归一化的高斯核卷积来代替。
# 在计算方差和协方差时用到了公式Var(X)\=E\[X^2\]-E\[X\]^2, cov(X,Y)=E\[XY\]-E\[X\]E\[Y\].
def ssim(img1, img2, window\_size\=11, window=None, size\_average=True, full=False, val\_range=None):
    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).
    if val\_range is None:
        if torch.max(img1) > 128:
            max\_val \= 255
        else:
            max\_val \= 1

        if torch.min(img1) < -0.5:
            min\_val \= -1
        else:
            min\_val \= 0
        L \= max\_val - min\_val
    else:
        L \= val\_range

    padd \= 0
    (\_, channel, height, width) \= img1.size()
    if window is None:
        real\_size \= min(window\_size, height, width)
        window \= create\_window(real\_size, channel=channel).to(img1.device)

    mu1 \= F.conv2d(img1, window, padding=padd, groups=channel)
    mu2 \= F.conv2d(img2, window, padding=padd, groups=channel)

    mu1\_sq \= mu1.pow(2)
    mu2\_sq \= mu2.pow(2)
    mu1\_mu2 \= mu1 \* mu2

    sigma1\_sq \= F.conv2d(img1 \* img1, window, padding=padd,
                         groups\=channel) - mu1\_sq
    sigma2\_sq \= F.conv2d(img2 \* img2, window, padding=padd,
                         groups\=channel) - mu2\_sq
    sigma12 \= F.conv2d(img1 \* img2, window, padding=padd,
                       groups\=channel) - mu1\_mu2

    C1 \= (0.01 \* L) \*\* 2
    C2 \= (0.03 \* L) \*\* 2

    v1 \= 2.0 \* sigma12 + C2
    v2 \= sigma1\_sq + sigma2\_sq + C2
    cs \= torch.mean(v1 / v2)  # contrast sensitivity

    ssim\_map \= ((2 \* mu1\_mu2 + C1) \* v1) / ((mu1\_sq + mu2\_sq + C1) \* v2)

    if size\_average:
        ret \= ssim\_map.mean()
    else:
        ret \= ssim\_map.mean(1).mean(1).mean(1)

    if full:
        return ret, cs
    return ret
class SSIM(torch.nn.Module):
    def \_\_init\_\_(self, window\_size\=11, size\_average=True, val\_range=None):
        super(SSIM, self).\_\_init\_\_()
        self.window\_size \= window\_size
        self.size\_average \= size\_average
        self.val\_range \= val\_range

        # Assume 1 channel for SSIM
        self.channel \= 1
        self.window \= create\_window(window\_size)

    def forward(self, img1, img2):
        (\_, channel, \_, \_) \= img1.size()

        if channel == self.channel and self.window.dtype == img1.dtype:
            window \= self.window
        else:
            window \= create\_window(self.window\_size, channel).to(
                img1.device).type(img1.dtype)
            self.window \= window
            self.channel \= channel

        return ssim(img1, img2, window=window, window\_size=self.window\_size, size\_average=self.size\_average)

6.模型训练/测试
---------

设定 epoch 为 500 次，保存验证集上 PSNR 最高的模型。两个模型在测试集上的表现如下表所示：

![](https://pic3.zhimg.com/80/v2-f18c31704579c775e63da3c1281c7182_720w.jpg)

从结果可以发现，FSRCNN 的 PSNR 比 SRCNN 低，但 FSRCNN 的 SSIM 比 SRCNN 高，说明 PSNR 和 SSIM 并不存在完全正相关的关系。

训练/验证代码：

model = FSRCNN(1).to(device)
criterion \= nn.MSELoss()
optimizer \= optim.Adam(model.parameters(), lr=1e-2)
scheduler \= MultiStepLR(optimizer, milestones=\[50, 75, 100\], gamma=0.1)
best\_psnr \= 0.0
for epoch in range(nb\_epochs):
    # Train
    epoch\_loss \= 0
    for iteration, batch in enumerate(trainloader):
        input, target \= batch\[0\].to(device), batch\[1\].to(device)
        optimizer.zero\_grad()
        out = model(input)
        loss \= criterion(out, target)
        loss.backward()
        optimizer.step()
        epoch\_loss += loss.item()
    print(f"Epoch {epoch}. Training loss: {epoch\_loss / len(trainloader)}")
    # Val
    sum\_psnr \= 0.0
    sum\_ssim \= 0.0
    with torch.no\_grad():
        for batch in valloader:
            input, target \= batch\[0\].to(device), batch\[1\].to(device)
            out = model(input)
            loss \= criterion(out, target)
            pr \= psnr(loss)
            sm \= ssim(input, out)
            sum\_psnr += pr
            sum\_ssim += sm
    print(f"Average PSNR: {sum\_psnr / len(valloader)} dB.")
    print(f"Average SSIM: {sum\_ssim / len(valloader)} ")
    avg\_psnr \= sum\_psnr / len(valloader)
    if avg\_psnr >= best\_psnr:
        best\_psnr \= avg\_psnr
        torch.save(model, r"best\_model\_FSRCNN.pth")
    scheduler.step()

测试代码：

BATCH\_SIZE = 4
model\_path \= "best\_model\_FSRCNN.pth"
testset \= DatasetFromFolder(r"./data/images/test", zoom\_factor)
testloader \= DataLoader(dataset=testset, batch\_size=BATCH\_SIZE,
                        shuffle\=False, num\_workers=NUM\_WORKERS)
sum\_psnr \= 0.0
sum\_ssim \= 0.0
model \= torch.load(model\_path).to(device)
criterion \= nn.MSELoss()
with torch.no\_grad():
    for batch in testloader:
        input, target \= batch\[0\].to(device), batch\[1\].to(device)
        out = model(input)
        loss \= criterion(out, target)
        pr \= psnr(loss)
        sm \= ssim(input, out)
        sum\_psnr += pr
        sum\_ssim += sm
print(f"Test Average PSNR: {sum\_psnr / len(testloader)} dB")
print(f"Test Average SSIM: {sum\_ssim / len(testloader)} ")

7.实图测试
------

为了直观感受两个模型的效果，我用自己拍摄的图进行实图测试，效果如下：

s=1(放大倍数=1)

![](https://pic4.zhimg.com/80/v2-84114fc8f0875b82cc824e6400bcb06f_720w.jpg)

当放大倍数=1时，SRCNN的超分结果比FSRCNN的超分效果要更好一些，这和两个模型平均 PSNR 的数值相吻合。

s=2(放大倍数=2)

![](https://pic1.zhimg.com/80/v2-643b70b0a78d9a5f6f6375e29379b3bc_720w.jpg)

当放大倍数=2时，SRCNN 的超分结果和 FSRCNN 的超分效果相差不大。

相关代码：

\# 参数设置
zoom\_factor \= 1
model \= "best\_model\_SRCNN.pth"
model2 \= "best\_model\_FSRCNN.pth"
image \= "tree.png"
cuda \= 'store\_true'
device \= torch.device("cuda:0" if torch.cuda.is\_available() else "cpu")

# 读取图片
img \= Image.open(image).convert('YCbCr')
img \= img.resize((int(img.size\[0\] \* zoom\_factor), int(img.size\[1\] \* zoom\_factor)), Image.BICUBIC)
y, cb, cr \= img.split()
img\_to\_tensor \= transforms.ToTensor()
input \= img\_to\_tensor(y).view(1, -1, y.size\[1\], y.size\[0\]).to(device)

# 输出图片
model \= torch.load(model).to(device)
out = model(input).cpu()
out\_img\_y \= out\[0\].detach().numpy()
out\_img\_y \*= 255.0
out\_img\_y \= out\_img\_y.clip(0, 255)
out\_img\_y \= Image.fromarray(np.uint8(out\_img\_y\[0\]), mode='L')
out\_img \= Image.merge('YCbCr', \[out\_img\_y, cb, cr\]).convert('RGB')

model2 \= torch.load(model2).to(device)
out2 \= model2(input).cpu()
out\_img\_y2 \= out2\[0\].detach().numpy()
out\_img\_y2 \*= 255.0
out\_img\_y2 \= out\_img\_y2.clip(0, 255)
out\_img\_y2 \= Image.fromarray(np.uint8(out\_img\_y2\[0\]), mode='L')
out\_img2 \= Image.merge('YCbCr', \[out\_img\_y2, cb, cr\]).convert('RGB')

# 绘图显示
fig, ax \= plt.subplots(1, 3, figsize=(20, 20))
ax\[0\].imshow(img)
ax\[0\].set\_title("原图")
ax\[1\].imshow(out\_img)
ax\[1\].set\_title("SRCNN恢复结果")
ax\[2\].imshow(out\_img2)
ax\[2\].set\_title("FSRCNN恢复结果")
plt.show()
fig.savefig(r"tree2.png")

**[点击关注，第一时间了解华为云新鲜技术~](https://bbs.huaweicloud.com/blogs?utm_source=cnblog&utm_medium=bbs-ex&utm_campaign=other&utm_content=content)**