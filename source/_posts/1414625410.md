---
layout: post
title: "一个博客园文章信息统计的小程序"
date: "2023-02-14T13:26:48.854Z"
---
一个博客园文章信息统计的小程序
===============

前言
--

博客园在个人首页有一个简单的博客数据统计，以博客园官方的首页为例：

![image](https://img2023.cnblogs.com/blog/2918335/202302/2918335-20230214125014246-1675187872.png)

但是这些数据不足以分析更为细节的东西

起初我是想把博客园作为个人学习的云笔记，但在一点点的记录中，我逐渐把博客园视为**知识创作**和**知识分享**的平台

所以从年后开始，就想着做一个类似 CSDN 里统计文章数据的工具

![](https://img2023.cnblogs.com/blog/2918335/202302/2918335-20230214125028187-508078314.png)

这样的统计功能可以更好的去分析读者对于内容的需求，了解文章内容的价值，以及从侧面认识自己在知识创作方面的能力

程序
--

这个程序是我昨天晚上一时兴起，看到了一位博主的文章 [Python爬虫实战-统计博客园阅读量问题](https://www.cnblogs.com/andrew3/p/12969703.html) ，正好检验自己对python的掌握，于是补充和修改了他的代码。因为想着要更为直观的展示文章数据，所以分了几个模块去写，以方便后续增加和修改功能

程序目前只有三个 .py 文件，爬取数据后解析并写入到 txt 中（后续会使用更规范的方法做持久化处理）

### 主程序 main.py

    from spider import spider
    from store import write_data
    
    
    # 设置博客名，例如我的博客地址为：https://www.cnblogs.com/KoiC，此处则填入KoiC
    blog_name = 'KoiC'
    
    
    
    if __name__ == '__main__':
        post_info = spider(blog_name)
        # print(post_info)
        write_data(post_info, blog_name)
        print('执行完毕！')
    

### 爬虫模块 spider.py

    import time
    import requests
    import re
    from lxml import etree
    
    
    def spider(blog_name):
        """
            爬取相关数据
        """
        
        # 设置UA和目标博客url
        headers = {
            "User-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.41"
        }
        url = "https://www.cnblogs.com/" + blog_name + "/default.html?page=%d"
        
        # 测试访问
        req = requests.get(url, headers)
        print('测试访问状态：%d'%req.status_code)
        
        
        print('开始爬取数据...')
        
        post_info = [] # 全部博文信息
        
        #分页爬取数据
        for page_num in range(1, 999):
            
            # 指向目标url
            new_url = format(url%page_num)
            
            # 获取页面
            req = requests.get(url=new_url, headers=headers)
            # print(req.status_code)
            
            tree = etree.HTML(req.text)
            
            # 获取目标数据（各博文名称和阅读量）
            count_list = tree.xpath('//div[@class="forFlow"]/div/div[@class="postDesc"]/span[1]/text()')        
            title_list = tree.xpath('//div[@class="postTitle"]/a/span/text()')
            
            # 获取该页博文数量
            post_count = len(count_list)
            # 如果该页没有博文，跳出循环
            if post_count == 0:
                break
            
            # 解析目标数据
            
            for i in range(post_count):
                # 对数据进行处理
                post_title = title_list[i].strip() # 处理前后多余的空格、换行等
                post_view_count = re.findall('\d+', count_list[i]) # 正则表达式获取阅读量数据
                
                single_post_info = [post_title, post_view_count[0]] # 单篇博文数据
                
                post_info.append(single_post_info)
            
            time.sleep(0.8)
            
        return post_info  
    

### 持久化模块 store.py

    import os
    import time
    
    
    def write_data(post_info, blog_name):
        """
            对数据进行持久化
        """
        
        print('开始写入数据...')
        
        # 获取时间
        now_time = time.localtime(time.time())
        select_date = time.strftime('%Y-%m-%d', now_time)
        select_time = time.strftime('%Y-%m-%d %H:%M:%S ', now_time)
        
        # 按日期创建文件路径
        file_path = './{:s}/{:s}'.format(str(now_time.tm_year), str(now_time.tm_mon))
        
        try: 
            os.makedirs(file_path) # 该方法创建路径时，若路径存在会报异常，使用 try catch 跳过异常
        except OSError:
            pass
        
        # 写入数据  
        try:
            fp = open('{:s}/{:s}.txt'.format(file_path, select_date), 'a+', encoding = 'utf-8')
    
            fp.write('阅读量\t\t 博文题目\n')
    
            view_count = 0 # 总阅读量
            for single_post_info in post_info:
                view_count += int(single_post_info[1])
                fp.write('{:<12s}{:s}\n'.format(single_post_info[1], single_post_info[0]))
            
            fp.write('------博客名:{:s} 博文数量:{:d} 总阅读量:{:d} 统计时间:{:s}\n\n'.format(blog_name, len(post_info), view_count, select_time))
            
            # 关闭资源
            fp.close()
        except FileNotFoundError:
            print('无法打开指定的文件')
        except LookupError:
            print('指定编码错误')
        except UnicodeDecodeError:
            print('读取文件时解码错误')
    

执行结果
----

程序会在目录下按日期创建文件夹

![image](https://img2023.cnblogs.com/blog/2918335/202302/2918335-20230214125145678-1058019168.png)

进入后可找到以日期命名的 txt 文件，以我自己的博客为例，得到以下统计信息：

![image](https://img2023.cnblogs.com/blog/2918335/202302/2918335-20230214125156792-1834017512.png)

可以将程序挂在服务器上，定时统计数据，观察阅读量的涨幅。

后续我会逐渐完善功能，形成一个自动化的小工具，感兴趣的可以点个关注，谢谢阅读！

参考
--

[Python爬虫实战-统计博客园阅读量问题](https://www.cnblogs.com/andrew3/p/12969703.html)

[XPath 教程](https://www.runoob.com/xpath/xpath-tutorial.html)

[Python 正则表达式](https://www.runoob.com/python/python-reg-expressions.html)

[python正则表达式从字符串中提取数字](https://blog.csdn.net/u010412858/article/details/83062200)

[Python os.makedirs() 方法](https://www.runoob.com/python/os-makedirs.html)

[Python File(文件) 方法](https://www.runoob.com/python/file-methods.html)

[Python异常捕获与处理](https://blog.csdn.net/zong596568821xp/article/details/78180229)