---
layout: post
title: "机器学习基础05DAY"
date: "2023-03-27T01:07:14.927Z"
---
机器学习基础05DAY
===========

分类算法之k-近邻
=========

k-近邻算法采用测量不同特征值之间的距离来进行分类

> 优点：精度高、对异常值不敏感、无数据输入假定
> 
> 缺点：计算复杂度高、空间复杂度高
> 
> 使用数据范围：数值型和标称型

一个例子弄懂k-近邻
----------

电影可以按照题材分类，每个题材又是如何定义的呢？那么假如两种类型的电影，动作片和爱情片。动作片有哪些公共的特征？那么爱情片又存在哪些明显的差别呢？我们发现动作片中打斗镜头的次数较多，而爱情片中接吻镜头相对更多。当然动作片中也有一些接吻镜头，爱情片中也会有一些打斗镜头。所以不能单纯通过是否存在打斗镜头或者接吻镜头来判断影片的类别。那么现在我们有6部影片已经明确了类别，也有打斗镜头和接吻镜头的次数，还有一部电影类型未知。

电影名称

打斗镜头

接吻镜头

电影类型

California Man

3

104

爱情片

He's not Really into dues

2

100

爱情片

Beautiful Woman

1

81

爱情片

Kevin Longblade

101

10

动作片

Robo Slayer 3000

99

5

动作片

Amped II

98

2

动作片

?

18

90

未知

那么我们使用K-近邻算法来分类爱情片和动作片：存在一个样本数据集合，也叫训练样本集，样本个数M个，知道每一个数据特征与类别对应关系，然后存在未知类型数据集合1个，那么我们要选择一个测试样本数据中与训练样本中M个的距离，排序过后选出最近的K个，这个取值一般不大于20个。选择K个最相近数据中次数最多的分类。那么我们根据这个原则去判断未知电影的分类

电影名称

与未知电影的距离

California Man

20.5

He's not Really into dues

18.7

Beautiful Woman

19.2

Kevin Longblade

115.3

Robo Slayer 3000

117.4

Amped II

118.9

我们假设K为3，那么排名前三个电影的类型都是爱情片，所以我们判定这个未知电影也是一个爱情片。那么计算距离是怎样计算的呢？

**欧氏距离** 那么对于两个向量点_a_1和_a_2之间的距离,可以通过该公式表示：

![](https://img2023.cnblogs.com/blog/2922318/202303/2922318-20230326234601940-54173073.png)

如果说输入变量有四个特征，例如（1，3，5，2）和（7，6，9，4）之间的距离计算为：

![](https://img2023.cnblogs.com/blog/2922318/202303/2922318-20230326234552489-938968818.png)

sklearn.neighbors
-----------------

sklearn.neighbors提供监督的基于邻居的学习方法的功能，sklearn.neighbors.KNeighborsClassifier是一个最近邻居分类器。那么KNeighborsClassifier是一个类，我们看一下实例化时候的参数

    class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)**
      """
      :param n_neighbors：int，可选（默认= 5），k_neighbors查询默认使用的邻居数
    
      :param algorithm：{'auto'，'ball_tree'，'kd_tree'，'brute'}，可选用于计算最近邻居的算法：'ball_tree'将会使用 BallTree，'kd_tree'将使用 KDTree，“野兽”将使用强力搜索。'auto'将尝试根据传递给fit方法的值来决定最合适的算法。
    
      :param n_jobs：int，可选（默认= 1),用于邻居搜索的并行作业数。如果-1，则将作业数设置为CPU内核数。不影响fit方法。
    
      """
    import numpy as np
    from sklearn.neighbors import KNeighborsClassifier
    
    neigh = KNeighborsClassifier(n_neighbors=3)
    

### Method

**fit(X, y)**

使用X作为训练数据拟合模型，y作为X的类别值。X，y为数组或者矩阵

    X = np.array([[1,1],[1,1.1],[0,0],[0,0.1]])
    y = np.array([1,1,0,0])
    neigh.fit(X,y)
    

**kneighbors(X=None, n\_neighbors=None, return\_distance=True)**

找到指定点集X的n\_neighbors个邻居，return\_distance为False的话，不返回距离

    neigh.kneighbors(np.array([[1.1,1.1]]),return_distance= False)
    
    neigh.kneighbors(np.array([[1.1,1.1]]),return_distance= False,an_neighbors=2)
    

**predict(X)**

预测提供的数据的类标签

    neigh.predict(np.array([[0.1,0.1],[1.1,1.1]]))
    

**predict\_proba(X)**

返回测试数据X属于某一类别的概率估计

    neigh.predict_proba(np.array([[1.1,1.1]]))
    

K-邻近算法鸢尾花数据集案例
==============

In \[ \]:

    from sklearn.datasets import load_iris  # 导入鸢尾花数据集
    from sklearn.preprocessing import StandardScaler # 导入特征工程标准化
    from sklearn.neighbors import KNeighborsClassifier # 导入K邻近模型
    from sklearn.model_selection import train_test_split# 导入划分训练集测试集方法
    import pandas as pd
    

In \[ \]:

    iris = load_iris()  # 实例化鸢尾花数据集
    df = pd.concat([pd.DataFrame(iris.data), pd.DataFrame(iris.target)],axis=1)
    df
    

Out\[ \]:

0

1

2

3

0

0

5.1

3.5

1.4

0.2

0

1

4.9

3.0

1.4

0.2

0

2

4.7

3.2

1.3

0.2

0

3

4.6

3.1

1.5

0.2

0

4

5.0

3.6

1.4

0.2

0

...

...

...

...

...

...

145

6.7

3.0

5.2

2.3

2

146

6.3

2.5

5.0

1.9

2

147

6.5

3.0

5.2

2.0

2

148

6.2

3.4

5.4

2.3

2

149

5.9

3.0

5.1

1.8

2

150 rows × 5 columns

In \[ \]:

    # 划分测试集和训练集
    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25)
    

In \[ \]:

    # 数据标准化
    # 实例化标准化类，传入训练集数据计方差标准
    std = StandardScaler().fit(x_train)
    #实例化训练集特征值
    x_train = std.transform(x_train)
    #实例化测试集特征值
    x_test = std.transform(x_test)
    

In \[ \]:

    # K邻近预测，K值为10
    knn = KNeighborsClassifier(n_neighbors=10)
    # 拟合k-nearest neighbors分类器
    knn.fit(x_train,y_train)
    # 查看预测样本的目标值
    y_predict = knn.predict(x_test)
    y_predict
    

Out\[ \]:

    array([0, 1, 0, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 0, 0, 2, 0, 2, 1, 0, 2, 2,
           0, 2, 2, 2, 1, 2, 1, 0, 1, 0, 0, 0, 1, 2, 1, 1])
    

In \[ \]:

    #查看预测准确度
    score = knn.score(x_test,y_test)
    score
    

Out\[ \]:

    0.9473684210526315