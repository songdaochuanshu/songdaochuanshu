---
layout: post
title: "智能语音之远场关键词识别实践（二）"
date: "2022-12-19T02:33:42.869Z"
---
智能语音之远场关键词识别实践（二）

上篇（[智能语音之远场关键词识别实践（一）](https://www.cnblogs.com/talkaudiodev/p/15956449.html)）讲了“远场关键词识别”项目中后端上的实践。本篇将讲在前端上的一些实践以及将前端和后端连起来形成一个完整的方案。下图是其框图：（麦克风阵列为圆阵且有四个麦克风，即有四个语音通道）

![](https://img2023.cnblogs.com/blog/1181527/202212/1181527-20221218115117908-837207189.png)

从上图可以看出，前端主要包括去混响、声源定位和波速形成（beamforming）、单通道降噪四大功能模块。每个模块的作用在上篇中已简单描述过，这里就不讲了。每个模块由一个人负责，我负责做单声道降噪。对于每个功能模块来说，通常都会有多种不同的实现算法，不同的算法在性能和运算复杂度上有优劣，因此先要去评估，选择最适合我们项目的算法。评估主要从性能以及运算复杂度两方面去做。评估下来后去混响选择了WPE（Weighted prediction error，加权预测误差）算法，声源定位选择了GCC-PHAT（Generalized Cross Correlation-Phase Transform，广义互相关-相位变换）算法，波速形成选择了MVDR（Minimum Variance Distortionless Response，最小方差无失真响应）算法。单声道降噪不像其他模块只有几种主流的算法，去评估后选择一到两种就可以了。单声道降噪好多年前就研究了，有多种效果不错的算法，且不断有新的算法（比如基于深度学习的算法）提出来。它以前主要用在语音通话中，现在我们要把它用在远场麦克风阵列的语音识别中。我深入的调研了一番，没有文章公开说哪种算法在语音识别中效果较好，市面上主流的产品（比如小度音箱、天猫精灵等）也没说用了哪种单声道降噪算法，只能摸着石头过河了。先前有同事找到了一篇传统方法和深度学习相结合的单声道降噪的文章，大家讨论了一下，觉得有点符合潮流（深度学习），就决定先研究这个。经过一段时间的学习和实践，有了一些输出，也写了关于这个算法理论和实践的几篇文章，具体见《[语音降噪论文“A Hybrid Approach for Speech Enhancement Using MoG Model and Neural Network Phoneme Classifier”的研读](https://www.cnblogs.com/talkaudiodev/p/14770731.html) 》，《[基于混合模型的语音降噪实践](https://www.cnblogs.com/talkaudiodev/p/14877539.html) 》，《[基于混合模型的语音降噪效果提升](https://www.cnblogs.com/talkaudiodev/p/15024732.html) 》。这个算法的降噪效果还可以，但是里面有神经网络模型，且参数不少，要求一定的算力，在嵌入式上部署load有点吃紧。讨论后这种算法留作备份，再去研究有没有更适合的算法。考虑到基于深度学习的方法在嵌入式上都不太能部署，就在传统方法中寻找。调研后发现基于MCRA-OMLSA的降噪算法效果很好且运算复杂度不高，讨论后决定试试这种方法。依旧是先学习理论，然后用python实现和tuning看效果。实验做下来效果还是挺好的，就决定用这个算法了。关于MCRA-OMLSA的降噪算法，我也写了三篇文章。具体见《[基于MCRA-OMLSA的语音降噪(一)：原理](https://www.cnblogs.com/talkaudiodev/p/15703937.html) 》，《[基于MCRA-OMLSA的语音降噪(二)：实现](https://www.cnblogs.com/talkaudiodev/p/15737338.html) 》，《[基于MCRA-OMLSA的语音降噪(三)：实现(续)](https://www.cnblogs.com/talkaudiodev/p/15765294.html) 》。

当前端的各个模块的算法都python实现完成后就开始把前端和后端串起来看效果，即把前端的输出作为后端的输入看识别率。不过模型是基于先前录的单声道的数据训练的。测试下来发现识别率比先前的降很多。出问题就要找原因和解决方法。大家先分头调查和思考，然后一起讨论。讨论后觉得原因很可能是这个：模型是基于先前单声道的语料训练的，而现在识别时的语音是多声道语音经过前端各算法处理后得到的单声道语音，两者不匹配。要想得到好的识别率，应该基于第二次录得的多声道的语料做完前端各算法处理后得到的单声道的语料来重新训练模型。简而言之，就是要让模型学习一下前端中的各个算法处理。于是基于第二次录得的多声道的语料重新训练模型。先把多声道数据经过前端各算法处理得到单声道数据，再做各种augmentation来增强语料库，最后拿这些处理后的数据去训练得到新的模型。新的模型用上后识别率有了很大的提升。经过实践，我们的经验是要想有好的识别率，模型一定要把pipeline中的各种算法都学习到。

在python下有了一个好的识别率，接下来就要看怎么在嵌入式上部署了，即用C语言来做实现。后端的C语言实现已经做好，要做的就是前端各算法的C语言实现。先前负责前端各算法的同学负责实现同样的算法，先用浮点实现。一段时间后各个算法的浮点实现都做好了，再与后端串起来看识别率，结果与python下的基本一致。后面要做的是算法的定点实现。由于team有了更高优先级的任务，只得暂停这方面的工作。只差最后一步就能真正部署了，有点可惜。后面有时间再把它完成吧。

远场关键词识别的项目由于参与人少做了近两年。正是因为人少，每个人做的东西就多一些，也就学到的东西多一些。就我自己而言，不仅学到了后端深度学习相关的（模型训练、量化等），也学到了前端信号处理算法相关的，同时通过实践还累积了不少关键词识别相关的经验。

posted on 2022-12-19 07:52  [davidtym](https://www.cnblogs.com/talkaudiodev/)  阅读(4)  评论(0)  [编辑](https://i.cnblogs.com/EditPosts.aspx?postid=16990164)  [收藏](javascript:void(0))  [举报](javascript:void(0))