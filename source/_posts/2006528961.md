---
layout: post
title: "kube-schedulerçš„è°ƒåº¦ä¸Šä¸‹æ–‡"
date: "2022-07-21T23:20:05.894Z"
---
kube-schedulerçš„è°ƒåº¦ä¸Šä¸‹æ–‡
====================

Schedulerç»“æ„
-----------

[Scheduler](https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L64-L102) æ˜¯æ•´ä¸ª `kube-scheduler` çš„ä¸€ä¸ª structureï¼Œæä¾›äº† `kube-scheduler` è¿è¡Œæ‰€éœ€çš„ç»„ä»¶ã€‚

    type Scheduler struct {
    	// Cacheæ˜¯ä¸€ä¸ªæŠ½è±¡ï¼Œä¼šç¼“å­˜podçš„ä¿¡æ¯ï¼Œä½œä¸ºschedulerè¿›è¡ŒæŸ¥æ‰¾ï¼Œæ“ä½œæ˜¯åŸºäºPodè¿›è¡Œå¢åŠ 
    	Cache internalcache.Cache
    	// Extenders ç®—æ˜¯è°ƒåº¦æ¡†æ¶ä¸­æä¾›çš„è°ƒåº¦æ’ä»¶ï¼Œä¼šå½±å“kubernetesä¸­çš„è°ƒåº¦ç­–ç•¥
    	Extenders []framework.Extender
    
    	// NextPod ä½œä¸ºä¸€ä¸ªå‡½æ•°æä¾›ï¼Œä¼šé˜»å¡è·å–ä¸‹ä¸€ä¸ªke'diao'du
    	NextPod func() *framework.QueuedPodInfo
    
    	// Error is called if there is an error. It is passed the pod in
    	// question, and the error
    	Error func(*framework.QueuedPodInfo, error)
    
    	// SchedulePod å°è¯•å°†ç»™å‡ºçš„podè°ƒåº¦åˆ°Nodeã€‚
    	SchedulePod func(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (ScheduleResult, error)
    
    	// å…³é—­schedulerçš„ä¿¡å·
    	StopEverything <-chan struct{}
    
    	// SchedulingQueueä¿å­˜è¦è°ƒåº¦çš„Pod
    	SchedulingQueue internalqueue.SchedulingQueue
    
    	// Profilesä¸­æ˜¯å¤šä¸ªè°ƒåº¦æ¡†æ¶
    	Profiles profile.Map
    	client clientset.Interface
    	nodeInfoSnapshot *internalcache.Snapshot
    	percentageOfNodesToScore int32
    	nextStartNodeIndex int
    }
    

ä½œä¸ºå®é™…æ‰§è¡Œçš„ä¸¤ä¸ªæ ¸å¿ƒï¼Œ`SchedulingQueue` ï¼Œä¸ `scheduleOne` å°†ä¼šåˆ†æåˆ°è¿™ä¸¤ä¸ª

SchedulingQueue
---------------

åœ¨çŸ¥é“ `kube-scheduler` åˆå§‹åŒ–è¿‡ç¨‹åï¼Œéœ€è¦å¯¹ `kube-scheduler` çš„æ•´ä¸ª _structure_ å’Œ _workflow_ è¿›è¡Œåˆ†æ

åœ¨ [Run](https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L336-L340) ä¸­ï¼Œè¿è¡Œçš„æ˜¯ ä¸€ä¸ª `SchedulingQueue` ä¸ ä¸€ä¸ª `scheduleOne` ï¼Œä»ç»“æ„ä¸Šçœ‹æ˜¯å±äº _Scheduler_

    func (sched *Scheduler) Run(ctx context.Context) {
    	sched.SchedulingQueue.Run()
    
    	// We need to start scheduleOne loop in a dedicated goroutine,
    	// because scheduleOne function hangs on getting the next item
    	// from the SchedulingQueue.
    	// If there are no new pods to schedule, it will be hanging there
    	// and if done in this goroutine it will be blocking closing
    	// SchedulingQueue, in effect causing a deadlock on shutdown.
    	go wait.UntilWithContext(ctx, sched.scheduleOne, 0)
    
    	<-ctx.Done()
    	sched.SchedulingQueue.Close()
    }
    
    

[SchedulingQueue](https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L81-L110) æ˜¯ä¸€ä¸ªé˜Ÿåˆ—çš„æŠ½è±¡ï¼Œç”¨äºå­˜å‚¨ç­‰å¾…è°ƒåº¦çš„Podã€‚è¯¥æ¥å£éµå¾ªç±»ä¼¼äº cache.FIFO å’Œ cache.Heap çš„æ¨¡å¼ã€‚

    type SchedulingQueue interface {
    	framework.PodNominator
    	Add(pod *v1.Pod) error
    	// Activate moves the given pods to activeQ iff they're in unschedulablePods or backoffQ.
    	// The passed-in pods are originally compiled from plugins that want to activate Pods,
    	// by injecting the pods through a reserved CycleState struct (PodsToActivate).
    	Activate(pods map[string]*v1.Pod)
    	// å°†ä¸å¯è°ƒåº¦çš„Podé‡å…¥åˆ°é˜Ÿåˆ—ä¸­
    	AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error
    	// SchedulingCycle returns the current number of scheduling cycle which is
    	// cached by scheduling queue. Normally, incrementing this number whenever
    	// a pod is popped (e.g. called Pop()) is enough.
    	SchedulingCycle() int64
    	// Popä¼šå¼¹å‡ºä¸€ä¸ªpodï¼Œå¹¶ä»headä¼˜å…ˆçº§é˜Ÿåˆ—ä¸­åˆ é™¤
    	Pop() (*framework.QueuedPodInfo, error)
    	Update(oldPod, newPod *v1.Pod) error
    	Delete(pod *v1.Pod) error
    	MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck)
    	AssignedPodAdded(pod *v1.Pod)
    	AssignedPodUpdated(pod *v1.Pod)
    	PendingPods() []*v1.Pod
    	// Close closes the SchedulingQueue so that the goroutine which is
    	// waiting to pop items can exit gracefully.
    	Close()
    	// Run starts the goroutines managing the queue.
    	Run()
    }
    

è€Œ [PriorityQueue](https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L134-L175) æ˜¯ `SchedulingQueue` çš„å®ç°ï¼Œè¯¥éƒ¨åˆ†çš„æ ¸å¿ƒæ„æˆæ˜¯ä¸¤ä¸ªå­é˜Ÿåˆ—ä¸ä¸€ä¸ªæ•°æ®ç»“æ„ï¼Œå³ `activeQ`ã€`backoffQ` å’Œ `unschedulablePods`

*   `activeQ`ï¼šæ˜¯ä¸€ä¸ª _heap_ ç±»å‹çš„ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œæ˜¯ _sheduler_ ä»ä¸­è·å¾—ä¼˜å…ˆçº§æœ€é«˜çš„Podè¿›è¡Œè°ƒåº¦
*   `backoffQ`ï¼šä¹Ÿæ˜¯ä¸€ä¸ª _heap_ ç±»å‹çš„ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œå­˜æ”¾çš„æ˜¯ä¸å¯è°ƒåº¦çš„Pod
*   `unschedulablePods` ï¼šä¿å­˜ç¡®å®šä¸å¯è¢«è°ƒåº¦çš„Pod

    type SchedulingQueue interface {
    	framework.PodNominator
    	Add(pod *v1.Pod) error
    	// Activate moves the given pods to activeQ iff they're in unschedulablePods or backoffQ.
    	// The passed-in pods are originally compiled from plugins that want to activate Pods,
    	// by injecting the pods through a reserved CycleState struct (PodsToActivate).
    	Activate(pods map[string]*v1.Pod)
    	// AddUnschedulableIfNotPresent adds an unschedulable pod back to scheduling queue.
    	// The podSchedulingCycle represents the current scheduling cycle number which can be
    	// returned by calling SchedulingCycle().
    	AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error
    	// SchedulingCycle returns the current number of scheduling cycle which is
    	// cached by scheduling queue. Normally, incrementing this number whenever
    	// a pod is popped (e.g. called Pop()) is enough.
    	SchedulingCycle() int64
    	// Pop removes the head of the queue and returns it. It blocks if the
    	// queue is empty and waits until a new item is added to the queue.
    	Pop() (*framework.QueuedPodInfo, error)
    	Update(oldPod, newPod *v1.Pod) error
    	Delete(pod *v1.Pod) error
    	MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck)
    	AssignedPodAdded(pod *v1.Pod)
    	AssignedPodUpdated(pod *v1.Pod)
    	PendingPods() []*v1.Pod
    	// Close closes the SchedulingQueue so that the goroutine which is
    	// waiting to pop items can exit gracefully.
    	Close()
    	// Run starts the goroutines managing the queue.
    	Run()
    }
    

åœ¨New _scheduler_ æ—¶å¯ä»¥çœ‹åˆ°ä¼šåˆå§‹åŒ–è¿™ä¸ªqueue

    podQueue := internalqueue.NewSchedulingQueue(
        // å®ç°podå¯¹æ¯”çš„ä¸€ä¸ªå‡½æ•°å³less
        profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
        informerFactory,
        internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
        internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
        internalqueue.WithPodNominator(nominator),
        internalqueue.WithClusterEventMap(clusterEventMap),
        internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
    )
    

è€Œ [NewSchedulingQueue](https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L252-L289) åˆ™æ˜¯åˆå§‹åŒ–è¿™ä¸ª PriorityQueue

    // NewSchedulingQueue initializes a priority queue as a new scheduling queue.
    func NewSchedulingQueue(
    	lessFn framework.LessFunc,
    	informerFactory informers.SharedInformerFactory,
    	opts ...Option) SchedulingQueue {
    	return NewPriorityQueue(lessFn, informerFactory, opts...)
    }
    
    // NewPriorityQueue creates a PriorityQueue object.
    func NewPriorityQueue(
    	lessFn framework.LessFunc,
    	informerFactory informers.SharedInformerFactory,
    	opts ...Option,
    ) *PriorityQueue {
    	options := defaultPriorityQueueOptions
    	for _, opt := range opts {
    		opt(&options)
    	}
    	// è¿™ä¸ªå°±æ˜¯ lesså‡½æ•°ï¼Œä½œä¸ºæ‰“åˆ†çš„ä¸€éƒ¨åˆ†
    	comp := func(podInfo1, podInfo2 interface{}) bool {
    		pInfo1 := podInfo1.(*framework.QueuedPodInfo)
    		pInfo2 := podInfo2.(*framework.QueuedPodInfo)
    		return lessFn(pInfo1, pInfo2)
    	}
    
    	if options.podNominator == nil {
    		options.podNominator = NewPodNominator(informerFactory.Core().V1().Pods().Lister())
    	}
    
    	pq := &PriorityQueue{
    		PodNominator:                      options.podNominator,
    		clock:                             options.clock,
    		stop:                              make(chan struct{}),
    		podInitialBackoffDuration:         options.podInitialBackoffDuration,
    		podMaxBackoffDuration:             options.podMaxBackoffDuration,
    		podMaxInUnschedulablePodsDuration: options.podMaxInUnschedulablePodsDuration,
    		activeQ:                           heap.NewWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()),
    		unschedulablePods:                 newUnschedulablePods(metrics.NewUnschedulablePodsRecorder()),
    		moveRequestCycle:                  -1,
    		clusterEventMap:                   options.clusterEventMap,
    	}
    	pq.cond.L = &pq.lock
    	pq.podBackoffQ = heap.NewWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder())
    	pq.nsLister = informerFactory.Core().V1().Namespaces().Lister()
    
    	return pq
    }
    

äº†è§£äº†Queueçš„ç»“æ„ï¼Œå°±éœ€è¦çŸ¥é“ å…¥é˜Ÿåˆ—ä¸å‡ºé˜Ÿåˆ—æ˜¯åœ¨å“ªé‡Œæ“ä½œçš„ã€‚åœ¨åˆå§‹åŒ–æ—¶ï¼Œéœ€è¦æ³¨å†Œä¸€ä¸ª `addEventHandlerFuncs` è¿™ä¸ªæ—¶å€™ï¼Œä¼šæ³¨å…¥ä¸‰ä¸ªåŠ¨ä½œå‡½æ•°ï¼Œä¹Ÿå°±æ˜¯controllerä¸­çš„æ¦‚å¿µï¼›è€Œåœ¨AddFuncä¸­å¯ä»¥çœ‹åˆ°ä¼šå…¥é˜Ÿåˆ—ã€‚

æ³¨å…¥æ˜¯å¯¹ Pod çš„[informer](https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/eventhandlers.go#L302-L305)æ³¨å…¥çš„ï¼Œæ³¨å…¥çš„å‡½æ•° [addPodToSchedulingQueue](https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/eventhandlers.go#L114-L120) å°±æ˜¯å…¥æ ˆ

    Handler: cache.ResourceEventHandlerFuncs{
        AddFunc:    sched.addPodToSchedulingQueue,
        UpdateFunc: sched.updatePodInSchedulingQueue,
        DeleteFunc: sched.deletePodFromSchedulingQueue,
    },
    
    func (sched *Scheduler) addPodToSchedulingQueue(obj interface{}) {
    	pod := obj.(*v1.Pod)
    	klog.V(3).InfoS("Add event for unscheduled pod", "pod", klog.KObj(pod))
    	if err := sched.SchedulingQueue.Add(pod); err != nil {
    		utilruntime.HandleError(fmt.Errorf("unable to queue %T: %v", obj, err))
    	}
    }
    

è€Œè¿™ä¸ª `SchedulingQueue` çš„å®ç°å°±æ˜¯ `PriorityQueue` ï¼Œè€ŒAddä¸­åˆ™å¯¹ activeQè¿›è¡Œçš„æ“ä½œ

    func (p *PriorityQueue) Add(pod *v1.Pod) error {
    	p.lock.Lock()
    	defer p.lock.Unlock()
        // æ ¼å¼åŒ–å…¥æ ˆæ•°æ®ï¼ŒåŒ…å«podinfoï¼Œé‡Œä¼šåŒ…å«v1.Pod
        // åˆå§‹åŒ–çš„æ—¶é—´ï¼Œåˆ›å»ºçš„æ—¶é—´ï¼Œä»¥åŠä¸èƒ½è¢«è°ƒåº¦æ—¶çš„è®°å½•å…¶pluginçš„åç§°
    	pInfo := p.newQueuedPodInfo(pod)
        // å…¥æ ˆ
    	if err := p.activeQ.Add(pInfo); err != nil {
    		klog.ErrorS(err, "Error adding pod to the active queue", "pod", klog.KObj(pod))
    		return err
    	}
    	if p.unschedulablePods.get(pod) != nil {
    		klog.ErrorS(nil, "Error: pod is already in the unschedulable queue", "pod", klog.KObj(pod))
    		p.unschedulablePods.delete(pod)
    	}
    	// Delete pod from backoffQ if it is backing off
    	if err := p.podBackoffQ.Delete(pInfo); err == nil {
    		klog.ErrorS(nil, "Error: pod is already in the podBackoff queue", "pod", klog.KObj(pod))
    	}
    	metrics.SchedulerQueueIncomingPods.WithLabelValues("active", PodAdd).Inc()
    	p.PodNominator.AddNominatedPod(pInfo.PodInfo, nil)
    	p.cond.Broadcast()
    
    	return nil
    }
    

åœ¨ä¸Šé¢çœ‹ _scheduler_ ç»“æ„æ—¶ï¼Œå¯ä»¥çœ‹åˆ°æœ‰ä¸€ä¸ª nextPodçš„ï¼ŒnextPodå°±æ˜¯ä»é˜Ÿåˆ—ä¸­å¼¹å‡ºä¸€ä¸ªpodï¼Œè¿™ä¸ªåœ¨_scheduler_ æ—¶ä¼šä¼ å…¥ [MakeNextPodFunc](https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L952-L965) å°±æ˜¯è¿™ä¸ª nextpod

    func MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo {
    	return func() *framework.QueuedPodInfo {
    		podInfo, err := queue.Pop()
    		if err == nil {
    			klog.V(4).InfoS("About to try and schedule pod", "pod", klog.KObj(podInfo.Pod))
    			for plugin := range podInfo.UnschedulablePlugins {
    				metrics.UnschedulableReason(plugin, podInfo.Pod.Spec.SchedulerName).Dec()
    			}
    			return podInfo
    		}
    		klog.ErrorS(err, "Error while retrieving next pod from scheduling queue")
    		return nil
    	}
    }
    

è€Œè¿™ä¸ª `queue.Pop()` å¯¹åº”çš„å°±æ˜¯ `PriorityQueue` çš„ [Pop()](https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L483-L503) ï¼Œåœ¨è¿™é‡Œä¼šå°†ä½œä¸º activeQ çš„æ¶ˆè´¹ç«¯

    func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
       p.lock.Lock()
       defer p.lock.Unlock()
       for p.activeQ.Len() == 0 {
          // When the queue is empty, invocation of Pop() is blocked until new item is enqueued.
          // When Close() is called, the p.closed is set and the condition is broadcast,
          // which causes this loop to continue and return from the Pop().
          if p.closed {
             return nil, fmt.Errorf(queueClosed)
          }
          p.cond.Wait()
       }
       obj, err := p.activeQ.Pop()
       if err != nil {
          return nil, err
       }
       pInfo := obj.(*framework.QueuedPodInfo)
       pInfo.Attempts++
       p.schedulingCycle++
       return pInfo, nil
    }
    

åœ¨ä¸Šé¢å…¥å£éƒ¨åˆ†ä¹Ÿçœ‹åˆ°äº†ï¼ŒscheduleOne å’Œ schedulerï¼ŒscheduleOne å°±æ˜¯å»æ¶ˆè´¹ä¸€ä¸ªPodï¼Œä»–ä¼šè°ƒç”¨ NextPodï¼ŒNextPodå°±æ˜¯åœ¨åˆå§‹åŒ–ä¼ å…¥çš„ `MakeNextPodFunc` ï¼Œè‡³æ­¤å›åˆ°å¯¹åº”çš„ Popæ¥åšæ¶ˆè´¹ã€‚

schedulerOneæ˜¯ä¸ºä¸€ä¸ªPodåšè°ƒåº¦çš„æµç¨‹ã€‚

    func (sched *Scheduler) scheduleOne(ctx context.Context) {
    	podInfo := sched.NextPod()
    	// pod could be nil when schedulerQueue is closed
    	if podInfo == nil || podInfo.Pod == nil {
    		return
    	}
    	pod := podInfo.Pod
    	fwk, err := sched.frameworkForPod(pod)
    	if err != nil {
    		// This shouldn't happen, because we only accept for scheduling the pods
    		// which specify a scheduler name that matches one of the profiles.
    		klog.ErrorS(err, "Error occurred")
    		return
    	}
    	if sched.skipPodSchedule(fwk, pod) {
    		return
    	}
    ...
    

è°ƒåº¦ä¸Šä¸‹æ–‡
-----

![image](https://img2022.cnblogs.com/blog/1380340/202207/1380340-20220718174650101-1820890617.png)

å›¾1ï¼šPodçš„è°ƒåº¦ä¸Šä¸‹æ–‡

_Sourceï¼š_https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework

å½“äº†è§£äº†schedulerç»“æ„åï¼Œä¸‹é¢åˆ†æä¸‹è°ƒåº¦ä¸Šä¸‹æ–‡çš„è¿‡ç¨‹ã€‚çœ‹çœ‹æ‰©å±•ç‚¹æ˜¯æ€ä¹ˆå·¥ä½œçš„ã€‚è¿™ä¸ªæ—¶å€™åˆéœ€è¦æåˆ°å®˜ç½‘çš„è°ƒåº¦ä¸Šä¸‹æ–‡çš„å›¾ã€‚

### è°ƒåº¦æ¡†æ¶ [\[2\]](#2)

è°ƒåº¦æ¡†æ¶ (`scheduling framework` _SF_ ) æ˜¯kubernetesä¸º schedulerè®¾è®¡çš„ä¸€ä¸ªpluggableçš„æ¶æ„ã€‚SF å°†schedulerè®¾è®¡ä¸º _Plugin_ å¼çš„ APIï¼ŒAPIå°†ä¸Šä¸€ç« ä¸­æåˆ°çš„ä¸€äº›åˆ—è°ƒåº¦ç­–ç•¥å®ç°ä¸º `Plugin`ã€‚

åœ¨ _SF_ ä¸­ï¼Œå®šä¹‰äº†ä¸€äº›æ‰©å±•ç‚¹ ï¼ˆ`extension points` _EP_ ï¼‰ï¼Œè€Œè¢«å®ç°ä¸ºPluginçš„è°ƒåº¦ç¨‹åºå°†è¢«æ³¨å†Œåœ¨ä¸€ä¸ªæˆ–å¤šä¸ª _EP_ ä¸­ï¼Œæ¢å¥è¯æ¥è¯´ï¼Œåœ¨è¿™äº› _EP_ çš„æ‰§è¡Œè¿‡ç¨‹ä¸­å¦‚æœæ³¨å†Œåœ¨å¤šä¸ª _EP_ ä¸­ï¼Œå°†ä¼šåœ¨å¤šä¸ª _EP_ è¢«è°ƒç”¨ã€‚

æ¯æ¬¡è°ƒåº¦éƒ½åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œè°ƒåº¦å‘¨æœŸï¼ˆ`Scheduling Cycel`ï¼‰ä¸ç»‘å®šå‘¨æœŸï¼ˆ`Binding Cycle`ï¼‰ã€‚

*   _SC_ è¡¨ç¤ºä¸ºï¼Œä¸ºPodé€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹ï¼›_SC_ æ˜¯ä¸²è¡Œè¿è¡Œçš„ã€‚
*   _BC_ è¡¨ç¤ºä¸ºï¼Œå°† _SC_ å†³ç­–ç»“æœåº”ç”¨äºé›†ç¾¤ä¸­ï¼›_BC_ å¯ä»¥åŒæ—¶è¿è¡Œã€‚

è°ƒåº¦å‘¨æœŸä¸ç»‘å®šå‘¨æœŸç»“åˆä¸€èµ·ï¼Œè¢«ç§°ä¸º**è°ƒåº¦ä¸Šä¸‹æ–‡** ï¼ˆ`Scheduling Context`ï¼‰,ä¸‹å›¾åˆ™æ˜¯è°ƒåº¦ä¸Šä¸‹æ–‡çš„å·¥ä½œæµ

> æ³¨ï¼šå¦‚æœå†³ç­–ç»“æœä¸ºPodçš„è°ƒåº¦ç»“æœæ— å¯ç”¨èŠ‚ç‚¹ï¼Œæˆ–å­˜åœ¨å†…éƒ¨é”™è¯¯ï¼Œåˆ™ä¸­æ­¢ _SC_ æˆ– _BC_ã€‚Podå°†é‡å…¥é˜Ÿåˆ—é‡è¯•

### æ‰©å±•ç‚¹ [\[3\]](#3)

æ‰©å±•ç‚¹ï¼ˆ`Extension points`ï¼‰æ˜¯æŒ‡åœ¨_è°ƒåº¦ä¸Šä¸‹æ–‡_ä¸­çš„æ¯ä¸ªå¯æ‰©å±•APIï¼Œé€šè¿‡å›¾æç°ä¸º[\[å›¾1\]](#podsc)ã€‚å…¶ä¸­ `Filter` ç›¸å½“äº `Predicate` è€Œ `Scoring` ç›¸å½“äº `Priority`ã€‚

å¯¹äºè°ƒåº¦é˜¶æ®µä¼šé€šè¿‡ä»¥ä¸‹æ‰©å±•ç‚¹ï¼š

*   `Sort`ï¼šè¯¥æ’ä»¶æä¾›äº†æ’åºåŠŸèƒ½ï¼Œç”¨äºå¯¹åœ¨è°ƒåº¦é˜Ÿåˆ—ä¸­å¾…å¤„ç† Pod è¿›è¡Œæ’åºã€‚ä¸€æ¬¡åªèƒ½å¯ç”¨ä¸€ä¸ªé˜Ÿåˆ—æ’åºã€‚
    
*   `preFilter`ï¼šè¯¥æ’ä»¶ç”¨äºåœ¨è¿‡æ»¤ä¹‹å‰é¢„å¤„ç†æˆ–æ£€æŸ¥ Pod æˆ–é›†ç¾¤çš„ç›¸å…³ä¿¡æ¯ã€‚è¿™é‡Œä¼šç»ˆæ­¢è°ƒåº¦
    
*   `filter`ï¼šè¯¥æ’ä»¶ç›¸å½“äº_è°ƒåº¦ä¸Šä¸‹æ–‡_ä¸­çš„ `Predicates`ï¼Œç”¨äºæ’é™¤ä¸èƒ½è¿è¡Œ Pod çš„èŠ‚ç‚¹ã€‚Filter ä¼šæŒ‰é…ç½®çš„é¡ºåºè¿›è¡Œè°ƒç”¨ã€‚å¦‚æœæœ‰ä¸€ä¸ªfilterå°†èŠ‚ç‚¹æ ‡è®°ä½ä¸å¯ç”¨ï¼Œåˆ™å°† Pod æ ‡è®°ä¸ºä¸å¯è°ƒåº¦ï¼ˆå³ä¸ä¼šå‘ä¸‹æ‰§è¡Œï¼‰ã€‚
    
*   `postFilter`ï¼šå½“æ²¡æœ‰ä¸º pod æ‰¾åˆ°_FN_æ—¶ï¼Œè¯¥æ’ä»¶ä¼šæŒ‰ç…§é…ç½®çš„é¡ºåºè¿›è¡Œè°ƒç”¨ã€‚å¦‚æœä»»ä½•`postFilter`æ’ä»¶å°† Pod æ ‡è®°ä¸º_schedulable_ï¼Œåˆ™ä¸ä¼šè°ƒç”¨å…¶ä½™æ’ä»¶ã€‚å³ `filter` æˆåŠŸåä¸ä¼šè¿›è¡Œè¿™æ­¥éª¤
    
*   `preScore`ï¼šå¯ç”¨äºè¿›è¡Œé¢„Scoreå·¥ä½œï¼ˆé€šçŸ¥æ€§çš„æ‰©å±•ç‚¹ï¼‰ã€‚
    
*   `score`ï¼šè¯¥æ’ä»¶ä¸ºæ¯ä¸ªé€šè¿‡ `filter` é˜¶æ®µçš„Nodeæä¾›æ‰“åˆ†æœåŠ¡ã€‚ç„¶åSchedulerå°†é€‰æ‹©å…·æœ‰æœ€é«˜åŠ æƒåˆ†æ•°æ€»å’Œçš„Nodeã€‚
    
*   `reserve`ï¼šå› ä¸ºç»‘å®šäº‹ä»¶æ—¶å¼‚æ­¥å‘ç”Ÿçš„ï¼Œè¯¥æ’ä»¶æ˜¯ä¸ºäº†é¿å…Podåœ¨ç»‘å®šåˆ°èŠ‚ç‚¹å‰æ—¶ï¼Œè°ƒåº¦åˆ°æ–°çš„Podï¼Œä½¿èŠ‚ç‚¹ä½¿ç”¨èµ„æºè¶…è¿‡å¯ç”¨èµ„æºæƒ…å†µã€‚å¦‚æœåç»­é˜¶æ®µå‘ç”Ÿé”™è¯¯æˆ–å¤±è´¥ï¼Œå°†è§¦å‘ `UnReserve` å›æ»šï¼ˆé€šçŸ¥æ€§æ‰©å±•ç‚¹ï¼‰ã€‚è¿™ä¹Ÿæ˜¯ä½œä¸ºè°ƒåº¦å‘¨æœŸä¸­æœ€åä¸€ä¸ªçŠ¶æ€ï¼Œè¦ä¹ˆæˆåŠŸåˆ° `postBind` ï¼Œè¦ä¹ˆå¤±è´¥è§¦å‘ `UnReserve`ã€‚
    
*   `permit`ï¼šè¯¥æ’ä»¶å¯ä»¥é˜»æ­¢æˆ–å»¶è¿Ÿ Pod çš„ç»‘å®šï¼Œä¸€èˆ¬æƒ…å†µä¸‹è¿™æ­¥éª¤ä¼šåšä¸‰ä»¶äº‹ï¼š
    
    *   `appove` ï¼šè°ƒåº¦å™¨ç»§ç»­ç»‘å®šè¿‡ç¨‹
    *   `Deny`ï¼šå¦‚æœä»»ä½•ä¸€ä¸ªPremitæ‹’ç»äº†Podä¸èŠ‚ç‚¹çš„ç»‘å®šï¼Œé‚£ä¹ˆå°†è§¦å‘ `UnReserve` ï¼Œå¹¶é‡å…¥é˜Ÿåˆ—
    *   `Wait`ï¼š å¦‚æœ Permit æ’ä»¶è¿”å› `Wait`ï¼Œè¯¥ Pod å°†ä¿ç•™åœ¨å†…éƒ¨ `Wait` Pod åˆ—è¡¨ä¸­ï¼Œç›´åˆ°è¢« `Appove`ã€‚å¦‚æœå‘ç”Ÿè¶…æ—¶ï¼Œ`wait` å˜ä¸º `deny` ï¼Œå°†Podæ”¾å›è‡³è°ƒåº¦é˜Ÿåˆ—ä¸­ï¼Œå¹¶è§¦å‘ `Unreserve` å›æ»š ã€‚
*   `preBind`ï¼šè¯¥æ’ä»¶ç”¨äºåœ¨ bind Pod ä¹‹å‰æ‰§è¡Œæ‰€éœ€çš„å‰ç½®å·¥ä½œã€‚å¦‚ï¼Œ`preBind` å¯èƒ½ä¼šæä¾›ä¸€ä¸ªç½‘ç»œå·å¹¶å°†å…¶æŒ‚è½½åˆ°ç›®æ ‡èŠ‚ç‚¹ä¸Šã€‚å¦‚æœåœ¨è¯¥æ­¥éª¤ä¸­çš„ä»»æ„æ’ä»¶è¿”å›é”™è¯¯ï¼Œåˆ™Pod å°†è¢« `deny` å¹¶æ”¾ç½®åˆ°è°ƒåº¦é˜Ÿåˆ—ä¸­ã€‚
    
*   `bind`ï¼šåœ¨æ‰€æœ‰çš„ `preBind` å®Œæˆåï¼Œè¯¥æ’ä»¶å°†ç”¨äºå°†Podç»‘å®šåˆ°Nodeï¼Œå¹¶æŒ‰é¡ºåºè°ƒç”¨ç»‘å®šè¯¥æ­¥éª¤çš„æ’ä»¶ã€‚å¦‚æœæœ‰ä¸€ä¸ªæ’ä»¶å¤„ç†äº†è¿™ä¸ªäº‹ä»¶ï¼Œé‚£ä¹ˆåˆ™å¿½ç•¥å…¶ä½™æ‰€æœ‰æ’ä»¶ã€‚
    
*   `postBind`ï¼šè¯¥æ’ä»¶åœ¨ç»‘å®š Pod åè°ƒç”¨ï¼Œå¯ç”¨äºæ¸…ç†ç›¸å…³èµ„æºï¼ˆé€šçŸ¥æ€§çš„æ‰©å±•ç‚¹ï¼‰ã€‚
    
*   `multiPoint`ï¼šè¿™æ˜¯ä¸€ä¸ªä»…é…ç½®å­—æ®µï¼Œå…è®¸åŒæ—¶ä¸ºæ‰€æœ‰é€‚ç”¨çš„æ‰©å±•ç‚¹å¯ç”¨æˆ–ç¦ç”¨æ’ä»¶ã€‚
    

è€Œ _scheduler_ å¯¹äºè°ƒåº¦ä¸Šä¸‹æ–‡åœ¨ä»£ç ä¸­çš„å®ç°å°±æ˜¯ `scheduleOne` ï¼Œä¸‹é¢å°±æ˜¯çœ‹è¿™ä¸ªè°ƒåº¦ä¸Šä¸‹æ–‡

### Sort

`Sort` æ’ä»¶æä¾›äº†æ’åºåŠŸèƒ½ï¼Œç”¨äºå¯¹åœ¨è°ƒåº¦é˜Ÿåˆ—ä¸­å¾…å¤„ç† Pod è¿›è¡Œæ’åºã€‚ä¸€æ¬¡åªèƒ½å¯ç”¨ä¸€ä¸ªé˜Ÿåˆ—æ’åºã€‚

åœ¨è¿›å…¥ `scheduleOne` åï¼Œ`NextPod` ä» `activeQ` ä¸­é˜Ÿåˆ—ä¸­å¾—åˆ°ä¸€ä¸ªPodï¼Œç„¶åçš„ `frameworkForPod` ä¼šåšæ‰“åˆ†çš„åŠ¨ä½œå°±æ˜¯è°ƒåº¦ä¸Šä¸‹æ–‡çš„ç¬¬ä¸€ä¸ªæ‰©å±•ç‚¹ `sort`

    func (sched *Scheduler) scheduleOne(ctx context.Context) {
    	podInfo := sched.NextPod()
    	// pod could be nil when schedulerQueue is closed
    	if podInfo == nil || podInfo.Pod == nil {
    		return
    	}
    	pod := podInfo.Pod
    	fwk, err := sched.frameworkForPod(pod)
    ...
        
    func (sched *Scheduler) frameworkForPod(pod *v1.Pod) (framework.Framework, error) {
        // è·å–æŒ‡å®šçš„profile
    	fwk, ok := sched.Profiles[pod.Spec.SchedulerName]
    	if !ok {
    		return nil, fmt.Errorf("profile not found for scheduler name %q", pod.Spec.SchedulerName)
    	}
    	return fwk, nil
    }
    

å›é¡¾ï¼Œå› ä¸ºåœ¨New scheduleræ—¶ä¼šåˆå§‹åŒ–è¿™ä¸ª sort å‡½æ•°

    podQueue := internalqueue.NewSchedulingQueue(
        profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
        informerFactory,
        internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
        internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
        internalqueue.WithPodNominator(nominator),
        internalqueue.WithClusterEventMap(clusterEventMap),
        internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
    )
    

### preFilter

preFilterä½œä¸ºç¬¬ä¸€ä¸ªæ‰©å±•ç‚¹ï¼Œæ˜¯ç”¨äºåœ¨è¿‡æ»¤ä¹‹å‰é¢„å¤„ç†æˆ–æ£€æŸ¥ Pod æˆ–é›†ç¾¤çš„ç›¸å…³ä¿¡æ¯ã€‚è¿™é‡Œä¼šç»ˆæ­¢è°ƒåº¦

    func (sched *Scheduler) scheduleOne(ctx context.Context) {
    	podInfo := sched.NextPod()
    	// pod could be nil when schedulerQueue is closed
    	if podInfo == nil || podInfo.Pod == nil {
    		return
    	}
    	pod := podInfo.Pod
    	fwk, err := sched.frameworkForPod(pod)
    	if err != nil {
    		// This shouldn't happen, because we only accept for scheduling the pods
    		// which specify a scheduler name that matches one of the profiles.
    		klog.ErrorS(err, "Error occurred")
    		return
    	}
    	if sched.skipPodSchedule(fwk, pod) {
    		return
    	}
    
    	klog.V(3).InfoS("Attempting to schedule pod", "pod", klog.KObj(pod))
    
    	// Synchronously attempt to find a fit for the pod.
    	start := time.Now()
    	state := framework.NewCycleState()
    	state.SetRecordPluginMetrics(rand.Intn(100) < pluginMetricsSamplePercent)
    	// Initialize an empty podsToActivate struct, which will be filled up by plugins or stay empty.
    	podsToActivate := framework.NewPodsToActivate()
    	state.Write(framework.PodsToActivateKey, podsToActivate)
    
    	schedulingCycleCtx, cancel := context.WithCancel(ctx)
    	defer cancel()
        // è¿™é‡Œå°†è¿›å…¥prefilter
    	scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
    

[schedulePod](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L311-L360) å°è¯•å°†ç»™å®šçš„ pod è°ƒåº¦åˆ°èŠ‚ç‚¹åˆ—è¡¨ä¸­çš„èŠ‚ç‚¹ä¹‹ä¸€ã€‚å¦‚æœæˆåŠŸï¼Œå®ƒå°†è¿”å›èŠ‚ç‚¹çš„åç§°ã€‚

    func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
    	trace := utiltrace.New("Scheduling", utiltrace.Field{Key: "namespace", Value: pod.Namespace}, utiltrace.Field{Key: "name", Value: pod.Name})
    	defer trace.LogIfLong(100 * time.Millisecond)
    	// ç”¨äºå°†cacheæ›´æ–°ä¸ºå½“å‰å†…å®¹
    	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
    		return result, err
    	}
    	trace.Step("Snapshotting scheduler cache and node infos done")
    
    	if sched.nodeInfoSnapshot.NumNodes() == 0 {
    		return result, ErrNoNodesAvailable
    	}
    	// æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„podæ—¶ï¼Œä¼šæ‰§è¡Œæ‰©å±•ç‚¹
    	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
    	
        ...
    

[findNodesThatFitPod](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L364-L426) ä¼šæ‰§è¡Œå¯¹åº”çš„è¿‡æ»¤æ’ä»¶æ¥æ‰¾åˆ°æœ€é€‚åˆçš„Nodeï¼ŒåŒ…æ‹¬å¤‡æ³¨ï¼Œä»¥åŠæ–¹æ³•åéƒ½å¯ä»¥çœ‹åˆ°ï¼Œè¿™é‡Œè¿è¡Œçš„æ’ä»¶ğŸ˜ğŸ˜ï¼Œåé¢ä¼šåˆ†æç®—æ³•å†…å®¹ï¼Œåªå¯¹workflowå­¦ä¹ ã€‚

    func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
    	diagnosis := framework.Diagnosis{
    		NodeToStatusMap:      make(framework.NodeToStatusMap),
    		UnschedulablePlugins: sets.NewString(),
    	}
    
    	// Run "prefilter" plugins.
    	preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod)
    	allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List()
    	if err != nil {
    		return nil, diagnosis, err
    	}
    	if !s.IsSuccess() {
    		if !s.IsUnschedulable() {
    			return nil, diagnosis, s.AsError()
    		}
    		// All nodes will have the same status. Some non trivial refactoring is
    		// needed to avoid this copy.
    		for _, n := range allNodes {
    			diagnosis.NodeToStatusMap[n.Node().Name] = s
    		}
    		// Status satisfying IsUnschedulable() gets injected into diagnosis.UnschedulablePlugins.
    		if s.FailedPlugin() != "" {
    			diagnosis.UnschedulablePlugins.Insert(s.FailedPlugin())
    		}
    		return nil, diagnosis, nil
    	}
    
    	// "NominatedNodeName" can potentially be set in a previous scheduling cycle as a result of preemption.
    	// This node is likely the only candidate that will fit the pod, and hence we try it first before iterating over all nodes.
    	if len(pod.Status.NominatedNodeName) > 0 {
    		feasibleNodes, err := sched.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis)
    		if err != nil {
    			klog.ErrorS(err, "Evaluation failed on nominated node", "pod", klog.KObj(pod), "node", pod.Status.NominatedNodeName)
    		}
    		// Nominated node passes all the filters, scheduler is good to assign this node to the pod.
    		if len(feasibleNodes) != 0 {
    			return feasibleNodes, diagnosis, nil
    		}
    	}
    
    	nodes := allNodes
    	if !preRes.AllNodes() {
    		nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames))
    		for n := range preRes.NodeNames {
    			nInfo, err := sched.nodeInfoSnapshot.NodeInfos().Get(n)
    			if err != nil {
    				return nil, diagnosis, err
    			}
    			nodes = append(nodes, nInfo)
    		}
    	}
    	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)
    	if err != nil {
    		return nil, diagnosis, err
    	}
    
    	feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
    	if err != nil {
    		return nil, diagnosis, err
    	}
    	return feasibleNodes, diagnosis, nil
    }
    

### filter

filteræ’ä»¶ç›¸å½“äº_è°ƒåº¦ä¸Šä¸‹æ–‡_ä¸­çš„ `Predicates`ï¼Œç”¨äºæ’é™¤ä¸èƒ½è¿è¡Œ Pod çš„èŠ‚ç‚¹ã€‚Filter ä¼šæŒ‰é…ç½®çš„é¡ºåºè¿›è¡Œè°ƒç”¨ã€‚å¦‚æœæœ‰ä¸€ä¸ªfilterå°†èŠ‚ç‚¹æ ‡è®°ä½ä¸å¯ç”¨ï¼Œåˆ™å°† Pod æ ‡è®°ä¸ºä¸å¯è°ƒåº¦ï¼ˆå³ä¸ä¼šå‘ä¸‹æ‰§è¡Œï¼‰ã€‚

å¯¹äºä»£ç ä¸­æ¥è®²ï¼Œfilterè¿˜æ˜¯å¤„äº [findNodesThatFitPod](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L364-L426) å‡½æ•°ä¸­ï¼Œ`findNodesThatPassFilters` å°±æ˜¯è·å–åˆ° FNï¼Œå³å¯è¡ŒèŠ‚ç‚¹ï¼Œè€Œè¿™ä¸ªè¿‡ç¨‹å°±æ˜¯ _filter_ æ‰©å±•ç‚¹

    func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
    	...
        
    	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)
    	if err != nil {
    		return nil, diagnosis, err
    	}
    
    	feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
    	if err != nil {
    		return nil, diagnosis, err
    	}
    	return feasibleNodes, diagnosis, nil
    }
    

### Postfilter

å½“æ²¡æœ‰ä¸º pod æ‰¾åˆ°_FN_æ—¶ï¼Œè¯¥æ’ä»¶ä¼šæŒ‰ç…§é…ç½®çš„é¡ºåºè¿›è¡Œè°ƒç”¨ã€‚å¦‚æœä»»ä½•`postFilter`æ’ä»¶å°† Pod æ ‡è®°ä¸º_schedulable_ï¼Œåˆ™ä¸ä¼šè°ƒç”¨å…¶ä½™æ’ä»¶ã€‚å³ `filter` æˆåŠŸåä¸ä¼šè¿›è¡Œè¿™æ­¥éª¤ï¼Œé‚£æˆ‘ä»¬æ¥éªŒè¯ä¸‹è¿™é‡ŒæŠŠğŸ˜Š

è¿˜æ˜¯åœ¨ scheduleOne ä¸­ï¼Œå½“æˆ‘ä»¬è¿è¡Œçš„ SchedulePod å®Œæˆåï¼ˆæˆåŠŸæˆ–å¤±è´¥ï¼‰ï¼Œè¿™æ—¶ä¼šè¿”å›ä¸€ä¸ªerrï¼Œè€Œ `postfilter` ä¼šæ ¹æ®è¿™ä¸ª errè¿›è¡Œé€‰æ‹©æ‰§è¡Œæˆ–ä¸æ‰§è¡Œï¼Œç¬¦åˆå®˜æ–¹ç»™å‡ºçš„è¯´æ³•ã€‚

    scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
    	if err != nil {
    		// SchedulePod() may have failed because the pod would not fit on any host, so we try to
    		// preempt, with the expectation that the next time the pod is tried for scheduling it
    		// will fit due to the preemption. It is also possible that a different pod will schedule
    		// into the resources that were preempted, but this is harmless.
    		var nominatingInfo *framework.NominatingInfo
    		if fitError, ok := err.(*framework.FitError); ok {
    			if !fwk.HasPostFilterPlugins() {
    				klog.V(3).InfoS("No PostFilter plugins are registered, so no preemption will be performed")
    			} else {
    				// Run PostFilter plugins to try to make the pod schedulable in a future scheduling cycle.
    				result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap)
    				if status.Code() == framework.Error {
    					klog.ErrorS(nil, "Status after running PostFilter plugins for pod", "pod", klog.KObj(pod), "status", status)
    				} else {
    					fitError.Diagnosis.PostFilterMsg = status.Message()
    					klog.V(5).InfoS("Status after running PostFilter plugins for pod", "pod", klog.KObj(pod), "status", status)
    				}
    				if result != nil {
    					nominatingInfo = result.NominatingInfo
    				}
    			}
    			// Pod did not fit anywhere, so it is counted as a failure. If preemption
    			// succeeds, the pod should get counted as a success the next time we try to
    			// schedule it. (hopefully)
    			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
    		} else if err == ErrNoNodesAvailable {
    			nominatingInfo = clearNominatedNode
    			// No nodes available is counted as unschedulable rather than an error.
    			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
    		} else {
    			nominatingInfo = clearNominatedNode
    			klog.ErrorS(err, "Error selecting node for pod", "pod", klog.KObj(pod))
    			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
    		}
    		sched.handleSchedulingFailure(ctx, fwk, podInfo, err, v1.PodReasonUnschedulable, nominatingInfo)
    		return
    	}
    

### PreScore,Score

å¯ç”¨äºè¿›è¡Œé¢„Scoreå·¥ä½œï¼Œä½œä¸ºé€šçŸ¥æ€§çš„æ‰©å±•ç‚¹ï¼Œä¼šåœ¨åœ¨filterå®Œä¹‹åç›´æ¥ä¼šå…³è” preScore æ’ä»¶è¿›è¡Œç»§ç»­å·¥ä½œï¼Œè€Œä¸æ˜¯è¿”å›ï¼Œå¦‚æœé…ç½®çš„è¿™äº›æ’ä»¶æœ‰ä»»ä½•ä¸€ä¸ªè¿”å›å¤±è´¥ï¼Œåˆ™Podå°†è¢«æ‹’ç»ã€‚

    
    func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
    	trace := utiltrace.New("Scheduling", utiltrace.Field{Key: "namespace", Value: pod.Namespace}, utiltrace.Field{Key: "name", Value: pod.Name})
    	defer trace.LogIfLong(100 * time.Millisecond)
    
    	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
    		return result, err
    	}
    	trace.Step("Snapshotting scheduler cache and node infos done")
    
    	if sched.nodeInfoSnapshot.NumNodes() == 0 {
    		return result, ErrNoNodesAvailable
    	}
    
    	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
    	if err != nil {
    		return result, err
    	}
    	trace.Step("Computing predicates done")
    
    	if len(feasibleNodes) == 0 {
    		return result, &framework.FitError{
    			Pod:         pod,
    			NumAllNodes: sched.nodeInfoSnapshot.NumNodes(),
    			Diagnosis:   diagnosis,
    		}
    	}
    
    	// When only one node after predicate, just use it.
    	if len(feasibleNodes) == 1 {
    		return ScheduleResult{
    			SuggestedHost:  feasibleNodes[0].Name,
    			EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap),
    			FeasibleNodes:  1,
    		}, nil
    	}
    	// è¿™é‡Œä¼šå®Œæˆprescoreï¼Œscore
    	priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes)
    	if err != nil {
    		return result, err
    	}
    
    	host, err := selectHost(priorityList)
    	trace.Step("Prioritizing done")
    
    	return ScheduleResult{
    		SuggestedHost:  host,
    		EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap),
    		FeasibleNodes:  len(feasibleNodes),
    	}, err
    }
    

[priorityNodes](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L605-L705) ä¼šé€šè¿‡é…ç½®çš„æ’ä»¶ç»™Nodeæ‰“åˆ†ï¼Œå¹¶è¿”å›æ¯ä¸ªNodeçš„åˆ†æ•°ï¼Œå°†æ¯ä¸ªæ’ä»¶æ‰“åˆ†ç»“æœè®¡ç®—æ€»å’Œè·å¾—Nodeçš„åˆ†æ•°ï¼Œæœ€åè·å¾—èŠ‚ç‚¹çš„åŠ æƒæ€»åˆ†æ•°ã€‚

    func prioritizeNodes(
    	ctx context.Context,
    	extenders []framework.Extender,
    	fwk framework.Framework,
    	state *framework.CycleState,
    	pod *v1.Pod,
    	nodes []*v1.Node,
    ) (framework.NodeScoreList, error) {
    	// If no priority configs are provided, then all nodes will have a score of one.
    	// This is required to generate the priority list in the required format
    	if len(extenders) == 0 && !fwk.HasScorePlugins() {
    		result := make(framework.NodeScoreList, 0, len(nodes))
    		for i := range nodes {
    			result = append(result, framework.NodeScore{
    				Name:  nodes[i].Name,
    				Score: 1,
    			})
    		}
    		return result, nil
    	}
    
    	// Run PreScore plugins.
    	preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes)
    	if !preScoreStatus.IsSuccess() {
    		return nil, preScoreStatus.AsError()
    	}
    
    	// Run the Score plugins.
    	scoresMap, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes)
    	if !scoreStatus.IsSuccess() {
    		return nil, scoreStatus.AsError()
    	}
    
    	// Additional details logged at level 10 if enabled.
    	klogV := klog.V(10)
    	if klogV.Enabled() {
    		for plugin, nodeScoreList := range scoresMap {
    			for _, nodeScore := range nodeScoreList {
    				klogV.InfoS("Plugin scored node for pod", "pod", klog.KObj(pod), "plugin", plugin, "node", nodeScore.Name, "score", nodeScore.Score)
    			}
    		}
    	}
    
    	// Summarize all scores.
    	result := make(framework.NodeScoreList, 0, len(nodes))
    
    	for i := range nodes {
    		result = append(result, framework.NodeScore{Name: nodes[i].Name, Score: 0})
    		for j := range scoresMap {
    			result[i].Score += scoresMap[j][i].Score
    		}
    	}
    
    	if len(extenders) != 0 && nodes != nil {
    		var mu sync.Mutex
    		var wg sync.WaitGroup
    		combinedScores := make(map[string]int64, len(nodes))
    		for i := range extenders {
    			if !extenders[i].IsInterested(pod) {
    				continue
    			}
    			wg.Add(1)
    			go func(extIndex int) {
    				metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Inc()
    				defer func() {
    					metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Dec()
    					wg.Done()
    				}()
    				prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes)
    				if err != nil {
    					// Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities
    					klog.V(5).InfoS("Failed to run extender's priority function. No score given by this extender.", "error", err, "pod", klog.KObj(pod), "extender", extenders[extIndex].Name())
    					return
    				}
    				mu.Lock()
    				for i := range *prioritizedList {
    					host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score
    					if klogV.Enabled() {
    						klogV.InfoS("Extender scored node for pod", "pod", klog.KObj(pod), "extender", extenders[extIndex].Name(), "node", host, "score", score)
    					}
    					combinedScores[host] += score * weight
    				}
    				mu.Unlock()
    			}(i)
    		}
    		// wait for all go routines to finish
    		wg.Wait()
    		for i := range result {
    			// MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore,
    			// therefore we need to scale the score returned by extenders to the score range used by the scheduler.
    			result[i].Score += combinedScores[result[i].Name] * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority)
    		}
    	}
    
    	if klogV.Enabled() {
    		for i := range result {
    			klogV.InfoS("Calculated node's final score for pod", "pod", klog.KObj(pod), "node", result[i].Name, "score", result[i].Score)
    		}
    	}
    	return result, nil
    }
    

### Reserve

[Reserve](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L153-L163) å› ä¸ºç»‘å®šäº‹ä»¶æ—¶å¼‚æ­¥å‘ç”Ÿçš„ï¼Œè¯¥æ’ä»¶æ˜¯ä¸ºäº†é¿å…Podåœ¨ç»‘å®šåˆ°èŠ‚ç‚¹å‰æ—¶ï¼Œè°ƒåº¦åˆ°æ–°çš„Podï¼Œä½¿èŠ‚ç‚¹ä½¿ç”¨èµ„æºè¶…è¿‡å¯ç”¨èµ„æºæƒ…å†µã€‚å¦‚æœåç»­é˜¶æ®µå‘ç”Ÿé”™è¯¯æˆ–å¤±è´¥ï¼Œå°†è§¦å‘ `UnReserve` å›æ»šï¼ˆé€šçŸ¥æ€§æ‰©å±•ç‚¹ï¼‰ã€‚è¿™ä¹Ÿæ˜¯ä½œä¸ºè°ƒåº¦å‘¨æœŸä¸­æœ€åä¸€ä¸ªçŠ¶æ€ï¼Œè¦ä¹ˆæˆåŠŸåˆ° `postBind` ï¼Œè¦ä¹ˆå¤±è´¥è§¦å‘ `UnReserve`ã€‚

    // Run the Reserve method of reserve plugins.
    if sts := fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() { // å½“å¤„ç†ä¸æˆåŠŸæ—¶
        metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
        // è§¦å‘ un-reserve æ¥æ¸…ç†ç›¸å…³Podçš„çŠ¶æ€
        fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
        if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
            klog.ErrorS(forgetErr, "Scheduler cache ForgetPod failed")
        }
        sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, sts.AsError(), SchedulerError, clearNominatedNode)
        return
    }
    

### permit

[Permit](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L165-L183) æ’ä»¶å¯ä»¥é˜»æ­¢æˆ–å»¶è¿Ÿ Pod çš„ç»‘å®š

    	// Run "permit" plugins.
    	runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
    	if !runPermitStatus.IsWait() && !runPermitStatus.IsSuccess() {
    		var reason string
    		if runPermitStatus.IsUnschedulable() {
    			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
    			reason = v1.PodReasonUnschedulable
    		} else {
    			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
    			reason = SchedulerError
    		}
            // åªè¦å…¶ä¸­ä¸€ä¸ªæ’ä»¶è¿”å›çš„çŠ¶æ€ä¸æ˜¯ success æˆ–è€… wait
    		fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
            // ä»cacheä¸­å¿˜æ‰pod
    		if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
    			klog.ErrorS(forgetErr, "Scheduler cache ForgetPod failed")
    		}
    		sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, runPermitStatus.AsError(), reason, clearNominatedNode)
    		return
    	}
    
    

### Binding Cycle

åœ¨é€‰æ‹©å¥½ _FN_ ååˆ™åšä¸€ä¸ªå‡è®¾ç»‘å®šï¼Œå¹¶æ›´æ–°åˆ°cacheä¸­ï¼Œæ¥ä¸‹æ¥å›å»æ‰§è¡ŒçœŸæ­£çš„bindæ“ä½œï¼Œä¹Ÿå°±æ˜¯ `binding cycle`

    func (sched *Scheduler) scheduleOne(ctx context.Context) {
    	...
        ...
    	// binding cycle æ˜¯ä¸€ä¸ªå¼‚æ­¥çš„æ“ä½œï¼Œè¿™é‡Œè¡¨ç°å°±æ˜¯goåç¨‹
    	go func() {
    		bindingCycleCtx, cancel := context.WithCancel(ctx)
    		defer cancel()
    		metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Inc()
    		defer metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Dec()
    		// è¿è¡ŒWaitOnPermitæ’ä»¶ï¼Œå¦‚æœå¤±è´¥åˆ™ï¼ŒunReserveå›æ»š
    		waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod)
    		if !waitOnPermitStatus.IsSuccess() {
    			var reason string
    			if waitOnPermitStatus.IsUnschedulable() {
    				metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
    				reason = v1.PodReasonUnschedulable
    			} else {
    				metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
    				reason = SchedulerError
    			}
    			// trigger un-reserve plugins to clean up state associated with the reserved Pod
    			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
    			if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
    				klog.ErrorS(forgetErr, "scheduler cache ForgetPod failed")
    			} else {
    				// "Forget"ing an assumed Pod in binding cycle should be treated as a PodDelete event,
    				// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
    				// TODO(#103853): de-duplicate the logic.
    				// Avoid moving the assumed Pod itself as it's always Unschedulable.
    				// It's intentional to "defer" this operation; otherwise MoveAllToActiveOrBackoffQueue() would
    				// update `q.moveRequest` and thus move the assumed pod to backoffQ anyways.
    				defer sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, func(pod *v1.Pod) bool {
    					return assumedPod.UID != pod.UID
    				})
    			}
    			sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, waitOnPermitStatus.AsError(), reason, clearNominatedNode)
    			return
    		}
    
    	// è¿è¡ŒPrebind æ’ä»¶
    		preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
    		if !preBindStatus.IsSuccess() {
    			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
    			// trigger un-reserve plugins to clean up state associated with the reserved Pod
    			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
    			if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
    				klog.ErrorS(forgetErr, "scheduler cache ForgetPod failed")
    			} else {
    				// "Forget"ing an assumed Pod in binding cycle should be treated as a PodDelete event,
    				// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
    				// TODO(#103853): de-duplicate the logic.
    				sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, nil)
    			}
    			sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, preBindStatus.AsError(), SchedulerError, clearNominatedNode)
    			return
    		}
    		// bindæ˜¯çœŸæ­£çš„ç»‘å®šæ“ä½œ
    		err := sched.bind(bindingCycleCtx, fwk, assumedPod, scheduleResult.SuggestedHost, state)
    		if err != nil {
    			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
    			// å¦‚æœå¤±è´¥äº†å°±è§¦å‘ un-reserve plugins 
    			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
    			if err := sched.Cache.ForgetPod(assumedPod); err != nil {
    				klog.ErrorS(err, "scheduler cache ForgetPod failed")
    			} else {
    				// "Forget"ing an assumed Pod in binding cycle should be treated as a PodDelete event,
    				// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
    				// TODO(#103853): de-duplicate the logic.
    				sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, nil)
    			}
    			sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, fmt.Errorf("binding rejected: %w", err), SchedulerError, clearNominatedNode)
    			return
    		}
    		// Calculating nodeResourceString can be heavy. Avoid it if klog verbosity is below 2.
    		klog.V(2).InfoS("Successfully bound pod to node", "pod", klog.KObj(pod), "node", scheduleResult.SuggestedHost, "evaluatedNodes", scheduleResult.EvaluatedNodes, "feasibleNodes", scheduleResult.FeasibleNodes)
    		metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start))
    		metrics.PodSchedulingAttempts.Observe(float64(podInfo.Attempts))
    		metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(podInfo)).Observe(metrics.SinceInSeconds(podInfo.InitialAttemptTimestamp))
    
    		// è¿è¡Œ "postbind" æ’ä»¶
            // æ˜¯é€šçŸ¥æ€§çš„æ‰©å±•ç‚¹ï¼Œè¯¥æ’ä»¶åœ¨ç»‘å®š Pod åè°ƒç”¨ï¼Œå¯ç”¨äºæ¸…ç†ç›¸å…³èµ„æºï¼ˆï¼‰ã€‚
    		fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
    
    		// At the end of a successful binding cycle, move up Pods if needed.
    		if len(podsToActivate.Map) != 0 {
    			sched.SchedulingQueue.Activate(podsToActivate.Map)
    			// Unlike the logic in scheduling cycle, we don't bother deleting the entries
    			// as `podsToActivate.Map` is no longer consumed.
    		}
    	}()
    }
    

è°ƒåº¦ä¸Šä¸‹æ–‡ä¸­çš„å¤±è´¥æµç¨‹
-----------

ä¸Šé¢è¯´åˆ°çš„éƒ½æ˜¯æ­£å¸¸çš„è¯·æ±‚ï¼Œä¸‹é¢ä¼šå¯¹å¤±è´¥çš„è¯·æ±‚æ˜¯å¦‚ä½•é‡è¯•çš„è¿›è¡Œåˆ†æï¼Œè€Œ _scheduler_ ä¸­å…³äºå¤±è´¥å¤„ç†æ–¹é¢ç›¸å…³çš„å±æ€§ä¼šæ¶‰åŠåˆ°ä¸Šé¢ _scheduler_ ç»“æ„ä¸­çš„ `backoffQ` ä¸ `unschedulablePods`

*   `backoffQ`ï¼šä¹Ÿæ˜¯ä¸€ä¸ª _heap_ ç±»å‹çš„ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œå­˜æ”¾çš„æ˜¯ä¸å¯è°ƒåº¦çš„Pod
*   `unschedulablePods` ï¼šä¿å­˜ç¡®å®šä¸å¯è¢«è°ƒåº¦çš„Podï¼Œä¸€ä¸ªmapç±»å‹

backoffQ ä¸ unschedulablePods ä¼šåœ¨åˆå§‹åŒ– _scheduler_ æ—¶åˆå§‹åŒ–ï¼Œ

    func NewPriorityQueue(
    	lessFn framework.LessFunc,
    	informerFactory informers.SharedInformerFactory,
    	opts ...Option,
    ) *PriorityQueue {
    	options := defaultPriorityQueueOptions
    	for _, opt := range opts {
    		opt(&options)
    	}
    
    	comp := func(podInfo1, podInfo2 interface{}) bool {
    		pInfo1 := podInfo1.(*framework.QueuedPodInfo)
    		pInfo2 := podInfo2.(*framework.QueuedPodInfo)
    		return lessFn(pInfo1, pInfo2)
    	}
    
    	if options.podNominator == nil {
    		options.podNominator = NewPodNominator(informerFactory.Core().V1().Pods().Lister())
    	}
    
    	pq := &PriorityQueue{
    		PodNominator:                      options.podNominator,
    		clock:                             options.clock,
    		stop:                              make(chan struct{}),
    		podInitialBackoffDuration:         options.podInitialBackoffDuration,
    		podMaxBackoffDuration:             options.podMaxBackoffDuration,
    		podMaxInUnschedulablePodsDuration: options.podMaxInUnschedulablePodsDuration,
    		activeQ:                           heap.NewWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()),
    		unschedulablePods:                 newUnschedulablePods(metrics.NewUnschedulablePodsRecorder()),
    		moveRequestCycle:                  -1,
    		clusterEventMap:                   options.clusterEventMap,
    	}
    	pq.cond.L = &pq.lock
        // åˆå§‹åŒ–backoffQ
        // NewWithRecorderä½œä¸ºä¸€ä¸ªå¯é€‰çš„ metricRecorder çš„ Heap å¯¹è±¡ã€‚
        // podInfoKeyFuncæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè¿”å›é”™è¯¯ä¸å­—ç¬¦ä¸²
        // pq.podsCompareBackoffCompleted æ¯”è¾ƒä¸¤ä¸ªpodçš„å›é€€æ—¶é—´ï¼Œå¦‚æœç¬¬ä¸€ä¸ªåœ¨ç¬¬äºŒä¸ªä¹‹å‰ä¸ºtrueï¼Œ
        // åä¹‹ false
    	pq.podBackoffQ = heap.NewWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder())
    	pq.nsLister = informerFactory.Core().V1().Namespaces().Lister()
    
    	return pq
    }
    

å¯¹äºåˆå§‹åŒ– backoffQ ä¼šäº§ç”Ÿçš„ä¸¤ä¸ªå‡½æ•°ï¼Œ[getBackoffTime](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L757-L761) ä¸ [calculateBackoffDuration](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L765-L775)

    // getBackoffTime returns the time that podInfo completes backoff
    func (p *PriorityQueue) getBackoffTime(podInfo *framework.QueuedPodInfo) time.Time {
    	duration := p.calculateBackoffDuration(podInfo)
    	backoffTime := podInfo.Timestamp.Add(duration)
    	return backoffTime
    }
    
    // calculateBackoffDuration is a helper function for calculating the backoffDuration
    // based on the number of attempts the pod has made.
    func (p *PriorityQueue) calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration {
    	duration := p.podInitialBackoffDuration
    	for i := 1; i < podInfo.Attempts; i++ {
    		// Use subtraction instead of addition or multiplication to avoid overflow.
    		if duration > p.podMaxBackoffDuration-duration {
    			return p.podMaxBackoffDuration
    		}
    		duration += duration
    	}
    	return duration
    }
    

å¯¹äºæ•´ä¸ªæ•…éšœé”™è¯¯ä¼šæŒ‰ç…§å¦‚ä¸‹æµç¨‹è¿›è¡Œï¼Œåœ¨åˆå§‹åŒ– _scheduler_ ä¼šæ³¨å†Œä¸€ä¸ª Error å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ç”¨ä½œå¯¹ä¸å¯è°ƒåº¦Podè¿›è¡Œå¤„ç†ï¼Œå®é™…ä¸Šè¢«æ³¨å†Œçš„å‡½æ•°æ˜¯ MakeDefaultErrorFuncã€‚è¿™ä¸ªå‡½æ•°å°†ä½œä¸º Error å‡½æ•°è¢«è°ƒç”¨ã€‚

    sched := newScheduler(
        schedulerCache,
        extenders,
        internalqueue.MakeNextPodFunc(podQueue),
        MakeDefaultErrorFunc(client, podLister, podQueue, schedulerCache),
        stopEverything,
        podQueue,
        profiles,
        client,
        snapshot,
        options.percentageOfNodesToScore,
    )
    

è€Œåœ¨ è°ƒåº¦å‘¨æœŸä¸­ï¼Œä¹Ÿå°±æ˜¯ [scheduleOne](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L66-L132) å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªæ‰©å±•ç‚¹æ“ä½œå¤±è´¥åéƒ½ä¼šè°ƒç”¨ [handleSchedulingFailure](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L812-L834) è€Œè¯¥å‡½æ•°ï¼Œä½¿ç”¨äº†æ³¨å†Œçš„ _Error_ å‡½æ•°æ¥å¤„ç†Pod

    func (sched *Scheduler) scheduleOne(ctx context.Context) {
    	...
    	defer cancel()
    	scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
    	if err != nil {
    
    		var nominatingInfo *framework.NominatingInfo
    		if fitError, ok := err.(*framework.FitError); ok {
    			if !fwk.HasPostFilterPlugins() {
    				klog.V(3).InfoS("No PostFilter plugins are registered, so no preemption will be performed")
    			} else {
    			
    				result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap)
    				if status.Code() == framework.Error {
    					klog.ErrorS(nil, "Status after running PostFilter plugins for pod", "pod", klog.KObj(pod), "status", status)
    				} else {
    					fitError.Diagnosis.PostFilterMsg = status.Message()
    					klog.V(5).InfoS("Status after running PostFilter plugins for pod", "pod", klog.KObj(pod), "status", status)
    				}
    				if result != nil {
    					nominatingInfo = result.NominatingInfo
    				}
    			}
    	
    			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
    		} else if err == ErrNoNodesAvailable {
    			nominatingInfo = clearNominatedNode
    			// No nodes available is counted as unschedulable rather than an error.
    			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
    		} else {
    			nominatingInfo = clearNominatedNode
    			klog.ErrorS(err, "Error selecting node for pod", "pod", klog.KObj(pod))
    			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
    		}
            // å¤„ç†ä¸å¯è°ƒåº¦Pod
    		sched.handleSchedulingFailure(ctx, fwk, podInfo, err, v1.PodReasonUnschedulable, nominatingInfo)
    		return
    	}
    
    

æ¥åˆ°äº†æ³¨å†Œçš„ _Error_ å‡½æ•° `MakeDefaultErrorFunc`

    func MakeDefaultErrorFunc(client clientset.Interface, podLister corelisters.PodLister, podQueue internalqueue.SchedulingQueue, schedulerCache internalcache.Cache) func(*framework.QueuedPodInfo, error) {
    	return func(podInfo *framework.QueuedPodInfo, err error) {
    		pod := podInfo.Pod
    		if err == ErrNoNodesAvailable {
    			klog.V(2).InfoS("Unable to schedule pod; no nodes are registered to the cluster; waiting", "pod", klog.KObj(pod))
    		} else if fitError, ok := err.(*framework.FitError); ok {
    			// Inject UnschedulablePlugins to PodInfo, which will be used later for moving Pods between queues efficiently.
    			podInfo.UnschedulablePlugins = fitError.Diagnosis.UnschedulablePlugins
    			klog.V(2).InfoS("Unable to schedule pod; no fit; waiting", "pod", klog.KObj(pod), "err", err)
    		} else if apierrors.IsNotFound(err) {
    			klog.V(2).InfoS("Unable to schedule pod, possibly due to node not found; waiting", "pod", klog.KObj(pod), "err", err)
    			if errStatus, ok := err.(apierrors.APIStatus); ok && errStatus.Status().Details.Kind == "node" {
    				nodeName := errStatus.Status().Details.Name
    				// when node is not found, We do not remove the node right away. Trying again to get
    				// the node and if the node is still not found, then remove it from the scheduler cache.
    				_, err := client.CoreV1().Nodes().Get(context.TODO(), nodeName, metav1.GetOptions{})
    				if err != nil && apierrors.IsNotFound(err) {
    					node := v1.Node{ObjectMeta: metav1.ObjectMeta{Name: nodeName}}
    					if err := schedulerCache.RemoveNode(&node); err != nil {
    						klog.V(4).InfoS("Node is not found; failed to remove it from the cache", "node", node.Name)
    					}
    				}
    			}
    		} else {
    			klog.ErrorS(err, "Error scheduling pod; retrying", "pod", klog.KObj(pod))
    		}
    
    		// Check if the Pod exists in informer cache.
    		cachedPod, err := podLister.Pods(pod.Namespace).Get(pod.Name)
    		if err != nil {
    			klog.InfoS("Pod doesn't exist in informer cache", "pod", klog.KObj(pod), "err", err)
    			return
    		}
    
    		// In the case of extender, the pod may have been bound successfully, but timed out returning its response to the scheduler.
    		// It could result in the live version to carry .spec.nodeName, and that's inconsistent with the internal-queued version.
    		if len(cachedPod.Spec.NodeName) != 0 {
    			klog.InfoS("Pod has been assigned to node. Abort adding it back to queue.", "pod", klog.KObj(pod), "node", cachedPod.Spec.NodeName)
    			return
    		}
    
    		// As <cachedPod> is from SharedInformer, we need to do a DeepCopy() here.
    		podInfo.PodInfo = framework.NewPodInfo(cachedPod.DeepCopy())
            // æ·»åŠ åˆ°unschedulableé˜Ÿåˆ—ä¸­
    		if err := podQueue.AddUnschedulableIfNotPresent(podInfo, podQueue.SchedulingCycle()); err != nil {
    			klog.ErrorS(err, "Error occurred")
    		}
    	}
    }
    

ä¸‹é¢æ¥åˆ° `AddUnschedulableIfNotPresent` ï¼Œè¿™ä¸ªä¹Ÿæ˜¯æ“ä½œ `backoffQ` å’Œ `unschedulablePods` çš„çœŸæ­£çš„åŠ¨ä½œ

`AddUnschedulableIfNotPresent` å‡½æ•°ä¼šå§æ— æ³•è°ƒåº¦çš„ pod æ’å…¥é˜Ÿåˆ—ï¼Œé™¤éå®ƒå·²ç»åœ¨é˜Ÿåˆ—ä¸­ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œ`PriorityQueue` å°†ä¸å¯è°ƒåº¦çš„ Pod æ”¾åœ¨ `unschedulablePods` ä¸­ã€‚ä½†å¦‚æœæœ€è¿‘æœ‰ move requestï¼Œåˆ™å°† pod æ”¾å…¥ `podBackoffQ` ä¸­ã€‚

    func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error {
    	p.lock.Lock()
    	defer p.lock.Unlock()
    	pod := pInfo.Pod
        // å¦‚æœå·²ç»å­˜åœ¨åˆ™ä¸æ·»åŠ 
    	if p.unschedulablePods.get(pod) != nil {
    		return fmt.Errorf("Pod %v is already present in unschedulable queue", klog.KObj(pod))
    	}
    	// æ£€æŸ¥æ˜¯å¦åœ¨activeQä¸­
    	if _, exists, _ := p.activeQ.Get(pInfo); exists {
    		return fmt.Errorf("Pod %v is already present in the active queue", klog.KObj(pod))
    	}
        // æ£€æŸ¥æ˜¯å¦åœ¨podBackoffQä¸­
    	if _, exists, _ := p.podBackoffQ.Get(pInfo); exists {
    		return fmt.Errorf("Pod %v is already present in the backoff queue", klog.KObj(pod))
    	}
    
    	// åœ¨é‡æ–°æ·»åŠ æ—¶ï¼Œä¼šåˆ·æ–° Podæ—¶é—´ä¸ºæœ€æ–°æ“ä½œçš„æ—¶é—´
    	pInfo.Timestamp = p.clock.Now()
    
    	for plugin := range pInfo.UnschedulablePlugins {
    		metrics.UnschedulableReason(plugin, pInfo.Pod.Spec.SchedulerName).Inc()
    	}
        // å¦‚æœæ¥å—åˆ°move requesté‚£ä¹ˆåˆ™æ”¾å…¥BackoffQ
    	if p.moveRequestCycle >= podSchedulingCycle {
    		if err := p.podBackoffQ.Add(pInfo); err != nil {
    			return fmt.Errorf("error adding pod %v to the backoff queue: %v", pod.Name, err)
    		}
    		metrics.SchedulerQueueIncomingPods.WithLabelValues("backoff", ScheduleAttemptFailure).Inc()
    	} else {
            // å¦åˆ™å°†æ”¾å…¥åˆ° unschedulablePods
    		p.unschedulablePods.addOrUpdate(pInfo)
    		metrics.SchedulerQueueIncomingPods.WithLabelValues("unschedulable", ScheduleAttemptFailure).Inc()
    
    	}
    
    	p.PodNominator.AddNominatedPod(pInfo.PodInfo, nil)
    	return nil
    }
    

åœ¨å¯åŠ¨ _scheduler_ æ—¶ï¼Œä¼šå°†è¿™ä¸¤ä¸ªé˜Ÿåˆ—å¼‚æ­¥å¯ç”¨ä¸¤ä¸ªloopæ¥æ“ä½œé˜Ÿåˆ—ã€‚è¡¨ç°åœ¨ [Run()](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L292-L295)

    func (p *PriorityQueue) Run() {
    	go wait.Until(p.flushBackoffQCompleted, 1.0*time.Second, p.stop)
    	go wait.Until(p.flushUnschedulablePodsLeftover, 30*time.Second, p.stop)
    }
    

å¯ä»¥çœ‹åˆ° [flushBackoffQCompleted](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L431-L458) ä½œä¸º `BackoffQ` å®ç°ï¼›è€Œ [flushUnschedulablePodsLeftover](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L462-L478) ä½œä¸º `UnschedulablePods` å®ç°ã€‚

`flushBackoffQCompleted` æ˜¯ç”¨äºå°†æ‰€æœ‰å·²å®Œæˆå›é€€çš„ pod ä» `backoffQ` ç§»åˆ° `activeQ` ä¸­

    func (p *PriorityQueue) flushBackoffQCompleted() {
    	p.lock.Lock()
    	defer p.lock.Unlock()
    	broadcast := false
    	for { // è¿™å°±æ˜¯heapå®ç°çš„æ–¹æ³•ï¼Œçª¥è§†ä¸‹ï¼Œä½†ä¸å¼¹å‡º
    		rawPodInfo := p.podBackoffQ.Peek()
    		if rawPodInfo == nil {
    			break
    		}
    		pod := rawPodInfo.(*framework.QueuedPodInfo).Pod
    		boTime := p.getBackoffTime(rawPodInfo.(*framework.QueuedPodInfo))
    		if boTime.After(p.clock.Now()) {
    			break
    		}
    		_, err := p.podBackoffQ.Pop() // å¼¹å‡ºä¸€ä¸ª
    		if err != nil {
    			klog.ErrorS(err, "Unable to pop pod from backoff queue despite backoff completion", "pod", klog.KObj(pod))
    			break
    		}
    		p.activeQ.Add(rawPodInfo) // æ”¾å…¥åˆ°æ´»åŠ¨é˜Ÿåˆ—ä¸­
    		metrics.SchedulerQueueIncomingPods.WithLabelValues("active", BackoffComplete).Inc()
    		broadcast = true
    	}
    
    	if broadcast {
    		p.cond.Broadcast()
    	}
    }
    

`flushUnschedulablePodsLeftover` å‡½æ•°ç”¨äºå°†åœ¨ `unschedulablePods` ä¸­çš„å­˜æ”¾æ—¶é—´è¶…è¿‡ `podMaxInUnschedulablePodsDuration` å€¼çš„ pod ç§»åŠ¨åˆ° `backoffQ` æˆ– `activeQ` ä¸­ã€‚

`podMaxInUnschedulablePodsDuration` ä¼šæ ¹æ®é…ç½®ä¼ å…¥ï¼Œå½“æ²¡æœ‰ä¼ å…¥ï¼Œä¹Ÿå°±æ˜¯ä½¿ç”¨äº† _[Deprecated](https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/cmd/kube-scheduler/app/options/options.go#L77-L84)_ é‚£ä¹ˆä¼šä¸º5åˆ†é’Ÿã€‚

    func NewOptions() *Options {
    	o := &Options{
    		SecureServing:  apiserveroptions.NewSecureServingOptions().WithLoopback(),
    		Authentication: apiserveroptions.NewDelegatingAuthenticationOptions(),
    		Authorization:  apiserveroptions.NewDelegatingAuthorizationOptions(),
    		Deprecated: &DeprecatedOptions{
    			PodMaxInUnschedulablePodsDuration: 5 * time.Minute,
    		},
    

å¯¹äº `flushUnschedulablePodsLeftover` å°±æ˜¯åšä¸€ä¸ªæ—¶é—´å¯¹æ¯”ï¼Œç„¶åæ·»åŠ åˆ°å¯¹åº”çš„é˜Ÿåˆ—ä¸­

    func (p *PriorityQueue) flushUnschedulablePodsLeftover() {
    	p.lock.Lock()
    	defer p.lock.Unlock()
    
    	var podsToMove []*framework.QueuedPodInfo
    	currentTime := p.clock.Now()
    	for _, pInfo := range p.unschedulablePods.podInfoMap {
    		lastScheduleTime := pInfo.Timestamp
    		if currentTime.Sub(lastScheduleTime) > p.podMaxInUnschedulablePodsDuration {
    			podsToMove = append(podsToMove, pInfo)
    		}
    	}
    
    	if len(podsToMove) > 0 {
    		p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout)
    	}
    }
    

æ€»ç»“è°ƒåº¦ä¸Šä¸‹æ–‡æµç¨‹
---------

*   åœ¨æ„å»ºä¸€ä¸ª _scheduler_ æ—¶ç»å†å¦‚ä¸‹æ­¥éª¤ï¼š
    *   å‡†å¤‡cacheï¼Œinformerï¼Œqueueï¼Œé”™è¯¯å¤„ç†å‡½æ•°ç­‰
    *   æ·»åŠ äº‹ä»¶å‡½æ•°ï¼Œä¼šç›‘å¬èµ„æºï¼ˆå¦‚Podï¼‰ï¼Œå½“æœ‰å˜åŠ¨åˆ™è§¦å‘å¯¹åº”äº‹ä»¶å‡½æ•°ï¼Œè¿™æ˜¯å…¥ç«™ `activeQ`
*   æ„å»ºå®Œæˆåä¼š runï¼Œrunæ—¶ä¼šrunä¸€ä¸ª `SchedulingQueue`ï¼Œè¿™ä¸ªæ˜¯ä½œä¸ºä¸å¯è°ƒåº¦é˜Ÿåˆ—
    *   `BackoffQ`
    *   `UnschedulablePods`
    *   ä¸å¯è°ƒåº¦é˜Ÿåˆ—ä¼šæ ¹æ®æ³¨å†Œæ—¶å®šæœŸæ¶ˆè´¹é˜Ÿåˆ—ä¸­Podå°†å…¶æ·»åŠ åˆ° `activeQ` ä¸­
*   å¯åŠ¨ä¸€ä¸ª `scheduleOne` çš„loopï¼Œè¿™ä¸ªæ˜¯è°ƒåº¦ä¸Šä¸‹æ–‡ä¸­æ‰€æœ‰çš„æ‰©å±•ç‚¹çš„æ‰§è¡Œï¼Œä¹Ÿæ˜¯ `activeQ` çš„æ¶ˆè´¹ç«¯
    *   `scheduleOne` è·å– pod
    *   æ‰§è¡Œå„ä¸ªæ‰©å±•ç‚¹ï¼Œå¦‚æœå‡ºé”™åˆ™ _Error_ å‡½æ•° `MakeDefaultErrorFunc` å°†å…¶æ·»åŠ åˆ°ä¸å¯è°ƒåº¦é˜Ÿåˆ—ä¸­
    *   å›åˆ°ä¸å¯è°ƒåº¦é˜Ÿåˆ—ä¸­æ¶ˆè´¹éƒ¨åˆ†

> **Reference**
> 
> \[1\] [kubernetes scheduler extender](https://www.sobyte.net/post/2022-02/kubernetes-scheduling-framework-and-extender/)  
> \[2\] [scheduling framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)  
> \[3\] [Extension points](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#extension-points)

ä½œè€…ï¼š[é’¢é—¸é—¨](http://lc161616.cnblogs.com/)  

å‡ºå¤„ï¼š[http://lc161616.cnblogs.com/](http://lc161616.cnblogs.com/)

æœ¬æ–‡ç‰ˆæƒå½’ä½œè€…å’Œåšå®¢å›­å…±æœ‰ï¼Œæ¬¢è¿è½¬è½½ï¼Œä½†æœªç»ä½œè€…åŒæ„å¿…é¡»ä¿ç•™æ­¤æ®µå£°æ˜ï¼Œä¸”åœ¨æ–‡ç« é¡µé¢æ˜æ˜¾ä½ç½®ç»™å‡ºåŸæ–‡è¿æ¥ï¼Œå¦åˆ™ä¿ç•™è¿½ç©¶æ³•å¾‹è´£ä»»çš„æƒåˆ©ã€‚

**é˜¿é‡Œäº‘ä¼˜æƒ ï¼š[ç‚¹å‡»åŠ›äº«ä½ä»·](https://www.aliyun.com/minisite/goods?userCode=l1acknzz&share_source=copy_link)**

**å¢¨å¢¨å­¦è‹±è¯­ï¼š[å¸®å¿™ç‚¹ä¸€ä¸‹](
https://www.maimemo.com/share/page?uid=11934970&pid=f21c0f2a3a395fd55a2ce7619fdc7738&tid=3c095460df2440b7f16205a536893440)**