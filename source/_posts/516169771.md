---
layout: post
title: "深入浅出理解SVM支持向量机算法"
date: "2022-07-05T04:38:03.294Z"
---
深入浅出理解SVM支持向量机算法
----------------

  支持向量机是Vapnik等人于1995年首先提出的，它是基于VC维理论和结构风险最小化原则的学习机器。它在解决小样本、非线性和高维模式识别问题中表现出许多特有的优势，并在一定程度上克服了“维数灾难”和“过学习”等传统困难，再加上它具有坚实的理论基础，简单明了的数学模型，使得支持向量机从提出以来受到广泛的关注，并取得了长足的发展 。支持向量机(Support Vector Machine, SVM)本身是一个二元分类算法，是对[感知机算法模型](https://www.cnblogs.com/yj179101536/p/16437051.html)的一种扩展，现在的 SVM 算法支持线性分类和非线性分类的分类应用，并且也能够直接将 SVM 应用于回归应用中，同时通过 OvR 或者 OvO 的方式我们也可以将 SVM 应用在多元分类领域中。在不考虑集成学习算法，不考虑特定的数据集的时候，在分类算法中 SVM 可以说是特别优秀的。

### SVM算法思想   

　　支持向量机最初是在模式识别中提出的。假定训练样本集合(xi, yi)，i-1，… ，l， xi∈R ，y∈{-1，+1}，可以被超平面x·w+b=0无错误地分开 ，并且离超平面最近的向量离超平面的距离是最大的 ，则这个超平面称为最优超平面。 对于下图哪条分界线最好，这便是SVM要解决的问题

![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704162319281-860080726.jpg)

假设未来拿到的数据含有一部分噪声，那么不同的超平面对于噪声的容忍度是不同的，最右边的线是最 robust 的。

![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704162949809-1579222906.jpg)

所以说我们只要让离超平面比较近的点尽可能的远离这个超平面，那么我们的模型分类效果应该就会比较不错。SVM 其实就是这个思想。 

### 函数间隔

  在超平面![[公式]](https://www.zhihu.com/equation?tex=w%5Ccdot+x%2Bb%3D0)确定的情况下，![[公式]](https://www.zhihu.com/equation?tex=%7Cw%5Ccdot+x%2Bb%7C)可以相对地表示点![[公式]](https://www.zhihu.com/equation?tex=x)距离超平面的远近。对于两类分类问题，如果![[公式]](https://www.zhihu.com/equation?tex=w%5Ccdot+x%2Bb%3E0)，则![[公式]](https://www.zhihu.com/equation?tex=x%0A%0A)的类别被判定为1；否则判定为\-1。所以如果![[公式]](https://www.zhihu.com/equation?tex=y%28w%5Ccdot+x%2Bb%29%3E0)，则认为![[公式]](https://www.zhihu.com/equation?tex=x)的分类结果是正确的，否则是错误的。且![[公式]](https://www.zhihu.com/equation?tex=y%28w%5Ccdot+x%2Bb%29)的值越大，分类结果的确信度越大。反之亦然。所以样本点![[公式]](https://www.zhihu.com/equation?tex=%28x_%7Bi%7D%2C+y_%7Bi%7D%29)与超平面![[公式]](https://www.zhihu.com/equation?tex=%28w%2C+b%29)之间的函数间隔定义为![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma_%7Bi%7D+%3D+y_%7Bi%7D+%28w%5Ccdot+x_%7Bi%7D+%2B+b%29++)。但是该定义存在问题：即![[公式]](https://www.zhihu.com/equation?tex=w)和![[公式]](https://www.zhihu.com/equation?tex=b)同时缩小或放大M倍后，超平面并没有变化，但是函数间隔却变化了。所以，需要将![[公式]](https://www.zhihu.com/equation?tex=w)的大小固定，如![[公式]](https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C%3D1)，使得函数间隔固定。这时的间隔也就是几何间隔 。

### 几何间隔

  定义超平面关于样本点（xi,yi）的几何间隔为：

                      ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma_%7Bi%7D+%3D+y_%7Bi%7D+%28%5Cfrac%7Bw%7D%7B%7C%7Cw%7C%7C%7D%5Ccdot+x_%7Bi%7D+%2B+%5Cfrac%7Bb%7D%7B%7C%7Cw%7C%7C%7D%29++)

  实际上，几何间隔就是点到超平面的距离。高中学习的点![[公式]](https://www.zhihu.com/equation?tex=%28x_i%2C+y_i%29)到直线![[公式]](https://www.zhihu.com/equation?tex=ax%2Bby%2Bc%3D0)的距离公式：

                       ![[公式]](https://www.zhihu.com/equation?tex=d%28x_i%2C+y_i%29+%3D+%5Cfrac%7B%7Cax_i%2Bby_i%2Bc%7C%7D%7B%5Csqrt%7Ba%5E2%2Bb%5E2%7D%7D)

所以在二维空间中，几何间隔就是点到直线的距离。在三维及以上空间中，就是点到超平面的距离。而函数距离，就是上述距离公式中的分子，即未归一化的距离。

  定义训练集到超平面的最小几何间隔是：![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma+%3D+min_%7Bi%3D1%2C...%2Cn%7D+%5Cgamma_%7Bi%7D+)

  可以发现函数间隔与几何间隔有如下关系：

                      ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704190839572-1957830134.png)

  **当我们取分母即W的L2范数为1时，函数间隔和几何间隔相等**。  
  如果超平面参数w和b成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。

SVM训练分类器的方法是寻找到超平面，使正负样本在超平面的两侧，且样本到超平面的几何间隔最大。所以SVM可以表述为求解下列优化问题：

                            ![[公式]](https://www.zhihu.com/equation?tex=%5Cunderset%7Bw%2C+b%7D%7Bmax%7D+%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B%5C%3B+%5Cgamma%0A)

                      ![[公式]](https://www.zhihu.com/equation?tex=s.t.+%5C%3B%5C%3B%5C%3B+y_%7Bi%7D+%28%5Cfrac%7Bw%7D%7B%7C%7Cw%7C%7C%7D%5Ccdot+x_%7Bi%7D+%2B+%5Cfrac%7Bb%7D%7B%7C%7Cw%7C%7C%7D%29%5Cgeq+%5Cgamma++)

**以上关于间隔问题在《统计学习方法》中有详细讲解**

**​线性可分支持向量机**
--------------

给定一个线性可分的训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为:

​                               w · x + b = 0​

决策函数为：

                           f(x)\=sign(wx+b)

**在二维空间上，两类点被一条直线完全分开叫做线性可分。**

### **硬间隔最大化**

我们寻找的超平面应当满足：这个超平面不仅可以将数据集中的所有正负实例点分开，而且对最难区分的实例点（离超平面最近的点，也就是支持向量）也有足够大的间隔将它们分开。

**寻找几何间隔最大的分离超平面问题可表示为下面的约束最优化问题:**

                              **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704191710537-288348677.png)**

                    ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704191738567-1167536882.png)

即我们要最大化超平面关于数据集T的几何间隔γ，约束条件则表示训练集中每个点与超平面的几何间隔都大于等于γ。  
事实上我们就是要找到一对支持向量（一正例一负例两个样本点，超平面在这两个点的正中间，使得超平面对两个点的距离最远），同时这个超平面对于更远处的其他所有正负实例点都能正确区分。  
**可将上式的几何间隔用函数间隔表示，这个问题可改写为:**

                            ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704191858641-794649876.png)

                    ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704191925827-456233213.png)

此时如果我们将w和b放大λ倍，那么右边的函数间隔也会放大λ倍。也就是说约束条件左右两边都整体放大了λ倍，对不等式约束没有影响。因此，我们不妨取:

                              ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704192009716-2083820968.png)

取1是为了方便推导和优化，且这样做对目标函数的优化没有影响。

**那么上面的最优化问题变为:**

                           **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704192107773-848232231.png)**

                   **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704192142971-1188805068.png)**

最大化1/||w||与最小化（1/2）||w||2 是等价的，乘以1/2是为了后面求导时计算方便。**  
则我们可以得到下面的线性可分支持向量机学习的最优化问题:**

                         **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704192236582-56219276.png)**

                  **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704192305842-263224473.png)**

### 线性可分支持向量机的原始问题转化为对偶问题

我们将上面最优化问题称之为原始问题

**我们可以列出一个拉格朗日函数来求解原始问题。我们定义拉格朗日函数为:**

                   **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704192649169-1722465257.png)**

**原始问题是一个极小极大问题**

                          **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704192755113-58596981.png)**

直接求原始问题并不好解，我们可以应用拉格朗日对偶性，通过求解对偶问题来得到原始问题的最优解。

### 对偶问题求解过程

**根据拉格朗日对偶性，对偶问题是极大极小问题，即求:**

                         **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704192924007-1677307614.png)**

**对偶问题的求解过程:**

先对L（w,b,α）求对w、b的极小。我们需要求L对w、b的偏导数并令其为0。

                       ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193028129-1340638505.png)

                       ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193052592-226792057.png)

得到两个等式:

                 ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193141402-920799421.png)                                ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193203163-1543015820.png)

将上面两式代入拉格朗日函数，得:

         ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193333251-1921242309.png)

                                                      ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193425002-1586922701.png)

再求min L（w,b,α）对α的极大，即:

                        ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193608162-952368657.png)

**我们将上面第一个式子乘以\-1，即可得到等价的对偶最优化问题:**

                        **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193703415-609692860.png)**

**我们可以求出上面问题对α的解向量（α即为所有拉格朗日乘子）为:**

                              **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193739124-330651194.png)**

**则按上面求L对w、b的偏导数并令其为0得到的等式，我们可得:**

                          **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193839330-1471496852.png)**

**那么超平面就为**

                        **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704193922245-1065407097.png)**

**决策函数为**

                     **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704194011326-2131064193.png)**

**我们注意到对原始的最优化问题:**

                  **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704194103895-2084554147.png)**

**有不等式约束，因此上述过程必须满足KKT条件:**

                        **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704194151503-1299012123.png)**

线性支持向量机
=======

我们经常遇到这样的情况：数据集T中大部分数据线性可分，但有少部分数据线性不可分，这时使用上面的线性可分支持向量机并不可行。这时我们就要使用学习策略为软间隔最大化的线性支持向量机。

软间隔最大化
------

所谓软间隔最大化，就是在硬间隔最大化的基础上，对每个样本点（xi, yi）引进一个松弛变量ξi，于是我们的约束不等式变为:  
                       ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704194452761-663568715.png)

同时，我们的目标函数中对每个松弛变量ξi需要支付一个代价ξi

**即目标函数变为:**

                       **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704194543198-2068645495.png)**

这里C>0称为惩罚参数。C较大时对误分类的惩罚增大，C较小时对误分类的惩罚减小。  
上面的目标函数包含两层含义:一是使间隔尽量大，二是使误分类点的个数尽量小。  
松弛变量ξi表示xi在超平面下不符合真实分类，但是与超平面距离<ξi，因此我们将其归到另一类（也就是真实分类）。松弛变量用来解决异常点，它可以"容忍"异常点在错误的区域，使得支持向量机计算出的超平面不至于产生过拟合的问题，但同时，我们又不希望松弛变量太大导致超平面分类性能太差。

线性支持向量机的原始问题转化为对偶问题
-------------------

**原始问题**

                     **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704194725572-2105327590.png)**

### 对偶问题及解题过程

**原始问题的对偶问题为:**

                     **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704194832608-1095334031.png)**

**拉格朗日函数为:**

            **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704194925211-1197321834.png)**

类似的，先求L对w、b、ξ的极小，求L对w、b、ξ的偏导数并令偏导数为0，最后得:

                             ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704195023203-1066130419.png)

上面三式代回L，得:

                  ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704195115682-1344357173.png)

再对上式求α的极大，得到对偶问题:

                       ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704195158316-228622710.png)

**设**

                          **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704195246564-1728960425.png)**

是上面对偶问题的拉格朗日乘子的解，那么有:

                           ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704195330480-2123349995.png)

**那么超平面为:**

                           **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704195422291-923057147.png)**

**决策函数为：**

                                          **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704195458115-1018154559.png)**

非线性支持向量机
========

如果我们的数据集样本是非线性可分的，我们可以使用一个映射将样本从原始空间映射到一个合适的高维空间，在这个高维空间中数据集线性可分。

核函数(Kernel Function)
--------------------

设X是输入空间，H是特征空间（即高维空间）,如果存在一个映射:

                             ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704195801507-434950081.png)

**使得**

                           **![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704195853099-1132906621.png)**

那么称K（x,z）为核函数。Φ（x）为映射函数，等式右边是内积。

### **常用的核函数：**

### 1\. Linear Kernel

线性核是最简单的核函数，核函数的数学公式：k(x, y) = xt · y

### 2\. Polynomial Kernel

多项式核实一种非标准核函数，它非常适合于正交归一化后的数据，其具体形式：k(x, y) = (axt y+c)d       比较好用的，就是参数比较多，但是还算稳定

### 3\. Gaussian Kernel

这里说一种经典的鲁棒径向基核，即高斯核函数，鲁棒径向基核对于数据中的噪音有着较好的抗干扰能力，其参数决定了函数作用范围，超过了这个范围，数据的作用就“基本消失”。高斯核函数是这一族核函数的优秀代表，也是必须尝试的核函数，其数学形式如下：

                         ![](https://img-blog.csdnimg.cn/20191109212914504.png)

虽然被广泛使用，但是这个核函数的性能对参数十分敏感

### 4\. Exponential Kernel

指数核函数就是高斯核函数的变种，它仅仅是将向量之间的L2距离调整为L1距离，这样改动会对参数的依赖性降低，但是适用范围相对狭窄。其数学形式如下：

                         ![](https://img-blog.csdn.net/20150724004246915)![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704201418520-1214267060.png)

### 5\. Sigmoid Kernel

Sigmoid 核来源于神经网络，现在已经大量应用于深度学习，它是S型的，所以被用作于“激活函数”,表达式：k(x, y) = tanh(axty+c)

核技巧
---

现在假设我们用映射:

                            ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704201821774-778429133.png)

将超平面从原本低维空间的形式变成高维空间的形式:

                          ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704201920995-1906053058.png)

这时我们原本在低维的非线性边界在高维就变成了一个超平面，在高维中数据集变得线性可分了。  
但是如果我们先将数据集从低维映射到高维，再去求超平面，由于高维空间中维数要多的多，计算量会变得很大。  
**核技巧是一种方法，我们用它可以简化计算。它就是用低维空间的内积来求高维空间的内积，省去了先做映射变换再在高维空间中求内积的麻烦。**

  
**举例:**  
以多项式核为例，若样本x有d个分量:

                  ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202025556-906431606.png)

我们做一个二维的多项式变换，将d维扩展:

                  ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202056041-969031750.png)

注意上面的式子中有很多重复项可以合并。  
我们对变换后的样本作内积:

                  ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202141799-1770931692.png)

其中，最后一项可以拆成:

                 ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202225874-1830265291.png)

又低维空间的内积可表示为:

                     ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202254430-1191320992.png)

所以上面变换后的多项式内积可以写成:

                  ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202330123-1080227015.png)

我们就可以不先进行高维变换，再在高维空间做内积，而是直接利用低维空间的内积计算即可。

**SVM中的核技巧:**  
不使用核函数时支持向量机的最优化问题为:

                      ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202427033-846942450.png)

使用核函数将数据集样本从低维映射到高维空间后最优化问题变为:

                       ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202506382-946983203.png)

于是我们可以得到类似的对偶问题:

                  ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202549467-2118794398.png)

我们在计算其中的

                       ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202625140-1572131970.png)

内积时就可以使用核技巧。使用核函数表示对偶问题为:

                  ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202700050-1993489198.png)

**分类决策函数**表示为:

              ![](https://img2022.cnblogs.com/blog/2576900/202207/2576900-20220704202735452-405600124.png)

SVM算法小结
-------

  SVM 算法是一个很优秀的算法，在集成学习和神经网络之类的算法没有表现出优越性能之前，SVM 算法基本占据了分类模型的统治地位。目前在大数据时代的大样本背景下，SVM由于其在大样本时超级大的计算量，热度有所下降，但仍然是一个常用的机器学习算法。

### **优点：**

**1) 解决高维特征的分类问题和回归问题很有效，在特征维度大于样本数时依然有很好的  
效果。  
2) 仅仅使用一部分支持向量来做超平面的决策，无需依赖全部数据。  
3) 有大量的核函数可以使用，从而可以很灵活的来解决各种非线性的分类回归问题。  
4) 样本量不是海量数据的时候，分类准确率高，泛化能力强。**

### **缺点：**

**1) 如果特征维度远远大于样本数，则 SVM 表现一般。  
2) SVM 在样本量非常大，核函数映射维度非常高时，计算量过大，不太适合使用。  
3) 非线性问题的核函数的选择没有通用标准，难以选择一个合适的核函数。  
4) SVM 对缺失数据敏感。**

posted on 2022-07-05 07:51  [叶小小qaq](https://www.cnblogs.com/yj179101536/)  阅读(75)  评论(0)  [编辑](https://i.cnblogs.com/EditPosts.aspx?postid=16439550)  [收藏](javascript:void(0))  [举报](javascript:void(0))