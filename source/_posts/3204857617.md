---
layout: post
title: "推理框架概览"
date: "2022-03-22T18:21:22.991Z"
---
推理框架概览
======

目录

*   [信息汇总](#信息汇总)
*   [使用体验](#使用体验)
    *   [1\. MNN VS NCNN](#1-mnn-vs-ncnn)
    *   [2.TNN](#2tnn)
    *   [3.Paddle-Lite](#3paddle-lite)
    *   [4.OpenVino](#4openvino)
    *   [5.TensorRT](#5tensorrt)
*   [社区支持](#社区支持)
*   [发展趋势](#发展趋势)
*   [其他](#其他)

  
工作中涉及到在各种各样的硬件上做神经网络的推理，对使用到的一些框架做一个梳理汇总。

信息汇总
----

框架

项目地址

平台支持

所属公司

特点

NCNN

[https://github.com/Tencent/ncnn](https://github.com/Tencent/ncnn)

![image](https://storage.live.com/items/F749ABC6249A7092!5164?authkey=APQrfGBrZ55_xwU)

腾讯

开放时间比较早，资源较多

TNN

[https://github.com/Tencent/TNN](https://github.com/Tencent/TNN)

![image](https://storage.live.com/items/F749ABC6249A7092!5165?authkey=APQrfGBrZ55_xwU)

腾讯

与其他框架相比，支持跨模型的内存复用，对于内存比较吃紧的情况有用

MNN

[https://github.com/alibaba/MNN](https://github.com/alibaba/MNN)

ARM CPU, ARM GPU, X86 CPU, CUDA

阿里

支持的设备类型多，优化较好

Paddle-Lite

[https://github.com/PaddlePaddle/Paddle-Lite](https://github.com/PaddlePaddle/Paddle-Lite)

![image](https://storage.live.com/items/F749ABC6249A7092!5166?authkey=APQrfGBrZ55_xwU)

百度

有配套训练框架，方便做量化重训练

OpenVino

[https://docs.openvino.ai/latest/index.html](https://docs.openvino.ai/latest/index.html)

Intel CPU, GPU, VPU

Intel

Intel CPU官方框架

TensorRT

[https://docs.nvidia.com/deeplearning/tensorrt/](https://docs.nvidia.com/deeplearning/tensorrt/)

NVIDIA GPU

英伟达

英伟达显卡的官方框架

使用体验
----

### 1\. MNN VS NCNN

最初从NCNN往MNN切的时候（19年），MNN总体比NCNN快了25%，不知道现在的情况是怎么样，据说ncnn后来也优化了，这就不了解了，没有再继续对比了，加上那时NCNN还没有支持arm的GPU,整体上比MNN弱了些。MNN的跨平台做的挺好，Windows上用也很方便。  
_**小插曲**_：在把MNN用到RK3399的GPU上的时候，UI会比较卡顿，原因是模型推理时，GPU使用率会到100%，最后是参考小米的MACE,对OpenCL的命令队列做了个限制，详情参考：[https://github.com/alibaba/MNN/issues/495](https://github.com/alibaba/MNN/issues/495)

### 2.TNN

使用TNN的契机是有一个项目要在3518ev300上做，内存比较吃紧。刚开始使用MNN，内存不够，然后又尝试了Paddle-Lite跑Int8模型，还是起不来，偶然看到了TNN的一个特性：

    内存优化
    
    高效”内存池”实现：通过 DAG 网络计算图分析，实现无计算依赖的节点间复用内存，降低 90% 内存资源消耗
    跨模型内存复用：支持外部实时指定用于网络内存，实现“多个模型，单份内存”。
    

主要是第二点，在其他框架没有发现。使用这个特性，就可以只消耗最大的模型所需要的内存了，最终，也是确实只有他能够在板子上起来。不过那时候TNN还比较早期，对X86 CPU支持得不好，很多算子都不能用，现在好像是直接集成OpenVino了。

### 3.Paddle-Lite

Paddle-Lite主要特点是有个Paddle训练框架，比较方便做量化重训练，量化也支持某些层不量化。速度比较快，当时和Tengine的商业版本对比了下，甚至比它快。缺点就是对X86 CPU支持不好，没好好做，因为百度还有个叫Padding Inference的东西专门用来做X86上的推理。

### 4.OpenVino

OpenVino就比较专注了，专门用来做Intel CPU和VPU之类。速度上，有一次参加FAT的比赛，要求是单线程推理在1S之内，MNN是刚刚卡线，换成OpenVino, 直接缩短到了100ms。不过后来看MNN的发布说明，对X86 CPU做了优化，据说已经比OpenVino还快了···具体我也没测过了。OpenVino 使用场景比较局限，而且只支持64位的。

### 5.TensorRT

算是显卡上部署必备了，像TNN、MNN、PaddleLite对GPU的支持，也是接入了TensorRT。像Jetson nx之类的也只能用它了。

社区支持
----

国内这几个都建有QQ群或钉钉群，不过MNN群里的管理员基本上是不怎么理人的，Paddle-Lite是最活跃的，TNN也不错。TensorRT的话有官方论坛，回复有时候还算及时，OpenVino暂时不知道。

发展趋势
----

从最近这些框架特别是国内这几个的更新看，都在走大而全的道路，比如MNN从最初集中优化arm CPU，arm GPU, 到优化X86 CPU, NV GPU, 现在甚至开始了扩充NPU。TNN也是同样的发展路径, 个人觉得这也是个正确的道路，毕竟这么多平台，谁都想做到开发一次，只需要改改配置就能到处运行。对于这些硬件的支持，既有自己做优化（MNN的X86 CPU), 也有直接兼容成熟的框架（TNN接入了OpenVino来支持X86 CPU, 接入了TensorRT来支持显卡，MNN对显卡的支持也是通过TensorRT），这种方法能够极大的减少开发量，集中精力优化通用设备，而特定硬件的厂家的优化应该是最强的。随着芯片的发展，现在越来越多的芯片都是自带NPU了，直接在arm CPU上做推理的场景也越来越少了，后续可能再拼就是拼对各种NPU的支持了。

其他
--

比较出名的还有TVM、TFlite等，使用不多，就不说了，个人感觉这块还是国内做的更易用些，纯属个人看法。

本文来自博客园，作者：[haoliuhust](https://www.cnblogs.com/haoliuhust/)，转载请注明原文链接：[https://www.cnblogs.com/haoliuhust/p/16041851.html](https://www.cnblogs.com/haoliuhust/p/16041851.html)