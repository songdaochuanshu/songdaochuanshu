---
layout: post
title: "深度学习和图形学渲染的结合和应用"
date: "2022-05-28T05:18:07.320Z"
---
深度学习和图形学渲染的结合和应用
================

大家好~这一个月以来，我从0开始学习和实现“深度学习”的技术。现在与大家分享下我的学习经历，以及我了解到的深度学习在渲染中的应用方向，希望对大家有所帮助！谢谢！

大家好~这一个月以来，我从0开始学习和实现“深度学习”的技术。  
现在与大家分享下我的学习经历，以及我了解到的深度学习在渲染中的应用方向，希望对大家有所帮助！谢谢！

目录

*   [为什么开始学习深度学习？](#为什么开始学习深度学习)
*   [了解到“谷歌地图基于神经渲染实现了3D地图”](#了解到谷歌地图基于神经渲染实现了3d地图)
*   [NeRF的改进方向](#nerf的改进方向)
*   [在Web上使用硬件来加速训练和推理？](#在web上使用硬件来加速训练和推理)
*   [更多的资料](#更多的资料)

为什么开始学习深度学习？
============

其实我以前在实现与路径追踪相关的降噪算法时，就了解到可以基于深度学习来实现降噪，并且发现这方面的论文近年来越来越多。所以我初步判定深度学习是降噪领域中的发展方向。

![image](https://img2022.cnblogs.com/blog/419321/202205/419321-20220528121826777-114914754.png)

但因为深度学习跟图形学是完全不同的学科，跨学科学习的成本太高，需要从0开始，所以我那时候没有采用深度学习的方法，而是采用更偏向于图形学的方法来实现降噪（比如SVGF/BMFR算法）。

那为什么我现在下决心从0开始学习深度学习了呢？这要感谢今年参加我开的[“离线渲染（二期）”培训课](https://www.bilibili.com/video/BV1134y1s7Su)的同学的反馈意见~他们表示希望多学习下实时渲染的技术，或者是能够将课程的离线渲染技术（如路径追踪）应用到实时渲染中。  
经过我的研究，我发现了有两个可行的方案：  
1、基于[DDGI](https://zhuanlan.zhihu.com/p/404520592)以及衍生的技术方案（如[SDFDDGI](https://new.qq.com/omn/20201217/20201217A0IA0U00.html)）  
2、路径追踪+降噪+SDF

第一个方案属于工业上的成熟方案，但是也有很多限制（如只支持漫反射表面），工程上也不易维护（因为是混合了光栅化和光追渲染，比较复杂）；  
第二个方案而是只用光追渲染，工程上容易维护；并且也支持高光反射、透明物体等材质。

所以我决定采用第二个方案。这个方案的技术难点就是降噪（路径追踪我已经实现了），所以我决定优先实现它。  
前面已经提到了我知道深度学习在降噪中很有前景，所以我下定决心从0学习深度学习！

使用深度学习来降噪的相关论文资料：  
[AI图像降噪](https://www.zhihu.com/column/deepvision)  
[ISO随便开！神经网络学习降噪算法解析](https://zhuanlan.zhihu.com/p/39987943)  
[【每周CV论文】深度学习图像降噪应该从阅读哪些文章开始](https://zhuanlan.zhihu.com/p/153165931)  
[可复现的图像降噪算法总结](https://zhuanlan.zhihu.com/p/32502816)

了解到“谷歌地图基于神经渲染实现了3D地图”
======================

本来我学习深度学习是一心为了用在降噪中，但是我在QQ群里与同学分享我在深度学习方面的实现进展后，有个同学提到了NeRF，说这个最近很火。

我初步研究了下，发现它使用了神经网络，用于从2D图像中重建3D渲染的。我认为我还是需要3D->2D，而不是2D->3D。也就是说，我是要渲染3D模型为2D图像的。所以我认为我目前暂时不需要用到NeRF。

但是，后来我在微信朋友圈中，看到有人分享了“谷歌地图”的发布会，它基于NeRF实现了3D地图。  
然后我又在QQ群里看到有同学再一次分享了“谷歌地图”的这个发布会，于是我就再次调研了下相关的技术。

相关的视频：  
[用AI建模？谷歌3D地图的背后技术](https://www.bilibili.com/video/BV1aS4y1z7ji)

通过调查后，我还是很看好这个技术！NeRF属于“神经渲染”领域，有希望取代目前传统的基于几何模型的渲染！因为它只需要几张图片，就可以渲染出3D画面了，而不再需要几何模型！  
使用NeRF得到的3D渲染还可以进行风格变换，以及各种光照变换！

NeRF相关资料：  
[NeRF：用深度学习完成3D渲染任务的蹿红](https://zhuanlan.zhihu.com/p/390848839)  
[神经渲染最新进展与算法（二）：NeRF及其演化](https://posts.careerengine.us/p/6146b8a241bcd20fc4545786)

NeRF的改进方向
=========

NeRF目前主要用在静态场景中，我还不清楚如何将其用在动态场景中。  
不过对于NeRF的其它的缺点，已经有相关的论文对其改进：

**提高训练速度**  
NeRF训练时间太长，相关的改进资料如下：  
[神经渲染最新进展与算法（二）：NeRF及其演化](https://posts.careerengine.us/p/6146b8a241bcd20fc4545786) -> NeRF的加速  
FastNeRF: High-Fidelity NeuralRendering at 200FPS  
Baking Neural Radiance Fields for real-Time View Synthesis  
AutoInt: Automatic Integration for Fast Neural Volume Rendering  
[不可思议！英伟达新技术训练NeRF模型最快只需5秒](https://cloud.tencent.com/developer/article/1961326?from=article.detail.1972906)  
![image](https://img2022.cnblogs.com/blog/419321/202205/419321-20220528123903425-1325328805.gif)

还有人提出了不用神经网络的方法，资料如下：  
[本科生新算法打败NeRF，不用神经网络照片也能动起来，提速100倍](https://baijiahao.baidu.com/s?id=1720002965899087781&wfr=spider&for=pc&searchword=nerf%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)  
![image](https://img2022.cnblogs.com/blog/419321/202205/419321-20220528123812608-369938676.gif)

**只要一张图片**  
NeRF需要多个角度下的图片。  
有论文提出了只要一张图片的方法：  
[一张照片就能生成3D模型，GAN和自动编码器碰撞出奇迹](https://cloud.tencent.com/developer/article/1950416?from=article.detail.1961326)  
![image](https://img2022.cnblogs.com/blog/419321/202205/419321-20220528123948576-1711934021.png)

**将体素渲染转换为真实渲染**  
把“我的世界”的体素风格转换为真实渲染：  
[GANcraft ：将玩家变成 3D 艺术家](https://developer.nvidia.com/zh-cn/blog/gancraft-turning-gamers-into-3d-artists/)  
![image](https://img2022.cnblogs.com/blog/419321/202205/419321-20220528123957146-1986256161.gif)

**支持透明物体**  
有论文提出了支持透明物体的方法：  
[Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects](https://sites.google.com/view/dex-nerf)  
![image](https://img2022.cnblogs.com/blog/419321/202205/419321-20220528124000742-965449264.png)

**支持超大场景**  
这个就是谷歌地图实现的技术了，他们还发表了论文：  
[Block-NeRF: Scalable Large Scene Neural View Synthesis](https://arxiv.org/abs/2202.05263)  
![image](https://img2022.cnblogs.com/blog/419321/202205/419321-20220528124007121-1196657153.png)

**编辑场景内容**  
NeRF方法仅能从已有的固定的场景生成渲染图像，无法直接按照主观意图编辑场景内容  
改进的论文如下：  
[GRAF: Generative Radiance Fields for3D-Aware Image](https://autonomousvision.github.io/graf/)  
GIRAFFE: RepresentingScenes as Compositional Generative Neural Feature Fields

**黑暗中的高光渲染**  
[NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images](https://bmild.github.io/rawnerf/)  
![image](https://img2022.cnblogs.com/blog/419321/202205/419321-20220528124013730-72866173.png)

**用于降噪**  
NeRF甚至可以用在降噪中，不过我没有具体研究。  
相关论文如下：  
[NAN: Noise-Aware NeRFs for Burst-Denoising](https://arxiv.org/abs/2204.04668)

在Web上使用硬件来加速训练和推理？
==================

因为我是Web3D领域的开发者，我知道深度学习的Web后端可以为：CPU、WebGL、WebGPU（我不考虑WebAssembly）

我也了解到nvidia显卡有专门的神经网络硬件，但我不清楚如何使用它！

通过群里同学的提醒，我通过研究了解到现在的硬件除了CPU、GPU，还有NPU，而这个NPU是专门为深度学习设计的硬件

NPU相关介绍：[NPU的发展概况](http://www.4k8k.xyz/article/CHAO_bismarck/106651814)

那么在Web上能使用NPU硬件吗？答案是有的！目前Web上已经制定了[Web Neural Network API标准](https://webmachinelearning.github.io/webnn/)（简称WebNN），通过该API即可调用NPU硬件!  
目前Chrome浏览器正在实现中，貌似还没有发布计划，所以暂时不能使用！

看到有人进行了测评，它的性能比WebGPU快数倍！

参考资料为：  
[WebNN API - 将硬件加速的深度学习带入开放式 Web 平台](https://static001.geekbang.org/con/42/pdf/4134778686/file/%E5%85%83%E5%87%AF-WebNN%20API%20-%20%E5%B0%86%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%A6%E5%85%A5%E5%BC%80%E6%94%BE%E5%BC%8F%20Web%20%E5%B9%B3%E5%8F%B0.pdf)

更多的资料
=====

[AI算法与图像处理](https://space.bilibili.com/288489574/video)

欢迎来到Wonder~

扫码加入我的QQ群：

![](https://img2020.cnblogs.com/blog/419321/202012/419321-20201228104448953-1235302601.png)

扫码加入免费知识星球-YYC的Web3D旅程：

![](https://img2018.cnblogs.com/blog/419321/201912/419321-20191203125111510-1737718475.png)

扫码关注Wonder微信公众号

![](https://ldr1-18716f-1302358347.tcloudbaseapp.com/img/wechat_subscription.jpg)