---
layout: post
title: "解密Prompt系列2. 冻结Prompt微调LM： T5 & PET & LM-BFF"
date: "2023-02-24T01:14:15.206Z"
---
解密Prompt系列2. 冻结Prompt微调LM： T5 & PET & LM-BFF
============================================

![解密Prompt系列2. 冻结Prompt微调LM： T5 &amp; PET &amp; LM-BFF](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082349872-1883702620.png) 这一章我们介绍固定prompt微调LM的相关模型，他们的特点都是针对不同的下游任务设计不同的prompt模板，在微调过程中固定模板对预训练模型进行微调。以下按时间顺序介绍，支持任意NLP任务的T5，针对文本分类的两篇PET和LM-BFF。

这一章我们介绍固定prompt微调LM的相关模型，他们的特点都是针对不同的下游任务设计不同的prompt模板，在微调过程中固定模板对预训练模型进行微调。以下按时间顺序介绍，支持任意NLP任务的T5，针对文本分类的两篇PET和LM-BFF。

在小样本场景，固定prompt微调LM对比常规微调的优点，在分类任务上比较直观我能想到的有三点(在下面PET中会细说）

*   无需额外的分类层的参数引入，微调成本低
*   标签词本身前置语义信息的引入，无需重头学习可类比MRC
*   微调和预训练的Gap更小，任务转化成LM任务后一致性高

T5
--

> *   paper: 2019.10 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
> *   Task: Everything
> *   Prompt: 前缀式人工prompt
> *   Model: Encoder-Decoder
> *   Take Away: 加入前缀Prompt，所有NLP任务都可以转化为文本生成任务

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412663-716982604.png)

T5论文的初衷如标题所言，是为了全面公平的对比不同预训练和迁移策略的贡献和效果，避免在A模型上效果不好的预训练目标在B上可能效果更优的情况，对比项包括

*   预训练目标：语言模型，乱序还原，MLM(不同的掩码率)，Span掩码, etc
*   预训练数据：构建C4数据集，从C4抽取不同领域语料来训练
*   模型架构: Encoder-Decoder，Decoder Only，Encoder Only
*   迁移策略：逐步解冻，全量微调，局部微调
*   其他：多任务预训练，模型大小

说句题外话，再看论文结果发现Encoder-Decoder的模型结果+SpanMLM损失函数效果最好。不知道这是否是谷歌押注T5，而没有像OpenAI一样选择Deocder结构的原因。

具体对比结果这里不细说，本文只关注T5为了公平对比以上差异，提出的Text2Text的通用建模框架：用相同的模型，相同的预训练，相同的损失函数和解码方式，把文本分类，摘要，翻译，QA都转化成了生成任务，而转化的方式就是通过加入前缀prompt。

针对不同的下游微调任务，我们看下T5提出的Text2Text是如何构建prompt模板的

1.  WMT英语到德语的翻译任务，输入是'translate English to German:'+input, 输出是翻译结果
2.  CNN Mail摘要任务: 文本摘要任务，输入是‘Summarize:'+input，输出是摘要
3.  MNLI任务：输入是'mnli hypothesis:'+假设+'premise:'+叙述，输出是contradiction, entailment，neutral
4.  STS文本相似任务：输入是'stsb sentence1:'+input1+‘sentence2：’+input2, 输出是1~5的打分（离散化）
5.  问答SQuAD任务：输入是'question:'+提问+ 'context:'+上下文，输出是答案

不难发现在T5的时代，prompt模板的构建还比较粗糙，更多是单纯的任务名称+任务类型来区分不同的NLP任务，只是让模型在解码时多一层条件概率，既给定不同prompt前缀在解码时采用不同的条件概率（attention）。并没有太多从语义和上下文关联的角度去进行prompt模板的构建，我猜这是T5在论文中提到他们尝试了不同的prompt模板发现效果影响有限的原因（哈哈因为都不太好所以没啥差异），不不能否定T5在通用LM上做出的贡献~

PET-TC(a)
---------

> *   paper a: 2020.1 Exploiting Cloze Questions for Few Shot Text Classification and Natural
> *   prompt: 单字完形填空式人工Prompt
> *   Task： Text Classification
> *   Model: Roberta-large, XLM-R
> *   Take Away: 加入完形填空式Prompt把文本分类任务转化成单字MLM

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412988-600172457.png)

和第一章的LAMA相似，PET-TC也是把输入映射成完形填空式的prompt模板，对掩码词进行预测作为分类标签。不过PET没有直接使用prompt，而是用了半监督的方案。用多个prompt模板微调模型后，对大规模无监督数据进行预测，然后在伪标签上进行常规的模型微调，哈哈绕了一个圈最后还是输出的常规微调的模型。我大胆猜测作者很看好prompt范式在微调时引入的前置语义信息，以及无额外参数的设定，但是对不同prompt和answer模板带来的不稳定性感到头疼，于是搞出这么个折中的方法~

### prompt & Answer Engineer

PET针对每个数据集人工设计了prompt模板和Answer词对标签的映射。针对单双文本输入分别举两个例子，以下a，b为原始输入文本，'\_'位置为MASK词

*   单输入：Yelp评论1~5星打分，标签词分别为terrible, bad，okay，good，great

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412659-953920883.png)

*   双输入：AG's News新闻四分类问题, 标签词分别为分类名称Worlds，Sports, Business, Science/Tech,

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412864-1206494143.png)

可以看出作者构建prompt模板的思路是尽可能还原文本所在的上下文场景，Answer词的选取是一对一的构建模式，每个label只选取一个词来表示。

### 固定prompt微调LM

完形填空式的prompt模板在微调时的优势，我认为主要有以下三点

*   没有额外参数的引入，常规微调需要引入hidden\_size \* label\_size的额外参数（classify head）作为每个标签对应的空间表征，这部分需要针对下游任务重头学习。而完形填空的token是在原始vocab中的，于是只需要调整标签词的预训练表征让它在label上线性可分即可
*   前置语义信息的引入，因为标签词的选取本身符合label的原始语义，例如以上YELP评论打分中的5个形容词本身就是隐含了评论质量信息的，所以会引入部分前置信息，避免重头学习，这一点和MRC有些相似
*   预训练和微调的一致性高，都是解决完形填空问题，学习目标一致

微调的损失函数是交叉熵，作者没有引入额外参数，而是把MASK位置上模型的预估logits在label上归一化来得到分类预测。例如上面的AG新闻分类任务，先得到MASK位置worlds，sports，business，science这四个词的预测logits，然后归一化得到预估概率，再和分类标签计算交叉熵。

为了避免灾难遗忘作者在下游任务微调时加入了预训练的MLM任务，于是微调的损失函数如下

\\\[L = (1-\\alpha) L\_{CE} + \\alpha L\_{MLM} \\\]

### 半监督+蒸馏

这部分的设计可以和prompt的部分分开来看，是一个半监督方案。以上每个任务对应的多个prompt模板，分别固定prompt微调LM得到一版模型，然后在大量的未标注样本上进行预测，再对多个模型的预测值进行加权得到伪标签。

最终在为标签上使用常规的微调方案（加classifier head），训练模型作为输出，这一步类比知识蒸馏。所以PET最后输出的还是常规的监督微调模型，Prompt只是被当做了一种半监督方案。效果上在小样本的设定上比直接使用监督微调都有一定的效果提升。

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412735-801057323.png)

作者还做了iPET对以上过程通过迭代逐步扩大数据集，提高伪标签准确率的方案，不过这么麻烦的实现一点都不适合我这种懒人，哈哈就不细说了~

针对PET有几点疑问

*   完形填空类的prompt，在微调过程中可能的灾难遗忘，是否因为对label词的微调偏离了词在原始文本中语义表征，以及和其他词的相对位置
*   prompt模板差异带来的效果差异尚未解决，人工构建的prompt模板不一定是最优的
*   Answer词单token，以及和label一一对应的设定，限制性较强。这部分在后面的续作里作者做了改良

后面介绍的几个模型，大多是基于PET上述问题的改良~

PET-TC(B)
---------

> *   paper b: 2020.9 It’s not just size that matters: Small language models are also few-shot learners.
> *   Prompt： 多字完形填空式人工Prompt
> *   Task：Text Classification
> *   Model: Albert-xxlarge-v2
> *   Take Away: 支持多字的完形填空Prompt，效果超越GPT3

这篇paper和上面的PET-TC是同一作者，算是上文的续作，主要优化了Answer词单token设定，支持多个token作为标签词，不过限制性依旧较强是预先设定任务最大的token数，然后使用最大token数作为MASK数量，而非动态的任意数量的MASK填充。

论文对推理和训练中的多MASK填充做了不同的处理。在推理中需要向前传导K次，如下图所示

1.  使用标签最大的label词长度K，生成k个MASK位置
2.  对K个位置同时预估得到K个预估词，选取概率最高的1个词进行填充
3.  针对填充后的新文本，对剩余K-1个位置再进行预估
4.  直到所有位置都被填充，分类概率由所有填充标签词的概率累乘得到

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412858-350417308.png)

\\\[p(Y=-1|x) = P\_M^1(ble|x) \* P\_M^2(terri|\\tilde{x}) \\\]

在训练过程中为了提升效率，论文使用了一次向前传导对多个位置同时完成预估，这时MASK长度是所有标签的最大长度。例如情感分类问题terr##ble长度为2，great长度为1，这时MASK填充长度为2,great只取第一个MASK词的概率，后面的忽略，概率计算如下

\\\[\\begin{align} p(Y=-1|x) &= P\_M^1(ble|x) \* P\_M^2(terri|\\tilde{x}) \\\\ p(Y=1|x) &= P\_M^1(great|x) \\\\ \\end{align} \\\]

其他部分和PET基本一样这里不再重复。效果上这篇论文换成了Albert-xxlarge-v2模型和GPT-3 few-shot在superGLUE上进行效果对比。不过以下参数对比并不太合理，虽然Albert是层共享参数，但是推理速度并无提升，12层的xxlarge模型参与计算的参数量级应该是223M\*12~2B，所以并不是严格意义上的小模型。调整参数后，32个小样本上PET的效果也是超过同等量级甚至更大的GPT3在few-shot上的效果的

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082413427-1137138389.png)

LM-BFF
------

> *   paper: 2020.12 Making Pre-trained Language Models Better Few-shot Learners
> *   Prompt: 完形填空自动搜索prompt
> *   Task: Text Classification
> *   Model: Bert or Roberta
> *   Take Away: 把人工构建prompt模板和标签词优化为自动搜索

LM-BFF是陈丹琦团队在20年底提出的针对few-shot场景，自动搜索模板和触发词的Prompt方案，prompt模板延续了PET的完型填空形式，把人工构建prompt和标签词的构建优化成了自动搜索。论文先是验证了相同模板不同标签词,和相同标签词不同模板对模型效果都有显著影响，如下

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412762-1616599030.png)

以下介绍自动搜索的部分

### 标签词搜索

考虑在全vocab上搜索标签词搜索空间太大，在少量样本上直接微调选择最优的标签词会存在过拟合的问题。作者先通过zero-shot缩小候选词范围，再通过微调选择最优标签词。

如下，固定prompt模板(L)，作者用训练集中每个分类(c)的数据，在预训练模型上分别计算该分类下MASK词的概率分布，选择概率之和在Top-k的单词作为候选词。再结合所有分类Top-K的候选词，得到n个标签词组合。这里的n和k都是超参，在100~1000不等。

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412864-1100944624.png)

然后在n个候选标签词组合中，针对微调后在验证集的准确率，选择效果最好的标签词组合。

### prompt模板搜索

固定标签词，作者使用T5来进行模板生成，让T5负责在标签词前、后生成符合上下文语义的prompt指令，再在所有训练样本中选择整体表现最优的prompt模板。

如下, 固定二分类的标签词是great和terrible，T5的模型输入为Input+MASK+标签对应标签词+MASK，让模型来完成对MASK部分的填充。现在预训练模型中通过Beam-Search得到多个模板，再在下游任务中微调得到表现最好的一个或多个prompt模板

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412687-85074690.png)

以上自动搜索prompt和标签词得到的部分结果如下，该说不说这种方案得到的标签词，至少直观看上去比AutoPrompt合（人）理（类）不（能）少（懂）：

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412863-1878723573.png)

### 固定prompt微调LM

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412911-611656461.png)

经过以上搜素得到最优标签词组合和prompt模板后，作者的微调过程模仿了GPT3的few-shot构建方式。如上图，先把输入填充进prompt模板，再从各个分类中各采样1个样本作为指令样本拼接进输入，为待预测文本补充更丰富的上下文，一起输入模型。在训练和推理时，补充的指令样本都是从训练集中采样。

同时为了避免加入的指令样本和待预测样本之间差异较大，导致模型可能直接无视接在prompt后面的指令样本，作者使用Sentence-Bert来筛选语义相似的样本作为指令样本。

效果上，作者给出了每类采样16个样本的小样本场景下， Roberta-Large的效果，可以得到以下insights

*   部分场景下自动模板是要优于手工模板的，整体上可以打平，自动搜索是人工成本的平价替代
*   加入指令样本对效果有显著提升
*   在16个样本的few-shot场景下，prompt微调效果是显著优于常规微调和GPT3 few-shot效果的

![](https://img2023.cnblogs.com/blog/1326688/202302/1326688-20230224082412811-917310076.png)

* * *

Reference
---------

1.  苏神的[必须要GPT-3吗？不，BERT的MLM模型也能小样本学习](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw%3D%3D&chksm=96ea6fe7a19de6f1be86b965e268df1b9c6320810cf32b6d64ddd3d238bf9088be41fb36adfe&idx=1&mid=2247512167&scene=21&sn=cc7695d92362e3b18a6e8969fb14dc27#wechat_redirect)