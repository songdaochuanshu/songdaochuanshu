---
layout: post
title: "论文笔记 - GRAD-MATCH: A Gradient Matching Based Data Subset Selection For Efficient Learning"
date: "2022-11-06T15:20:55.197Z"
---
论文笔记 - GRAD-MATCH: A Gradient Matching Based Data Subset Selection For Efficient Learning
=========================================================================================

Analysis
========

Coreset 是带有权重的数据子集，目的是在某个方面模拟完整数据的表现（例如损失函数的梯度，既可以是在训练数据上的损失，也可以是在验证数据上的损失）；

给出优化目标的定义：

![](https://img2022.cnblogs.com/blog/2849310/202211/2849310-20221106224427471-601765258.png)

$w^t$ 是 t 轮得到的 coreset 权重，$X\_t$ 是 t 轮得到的 coreset，$L$ 既可以是在训练数据上的损失，也可以是在验证数据上的损失，$L\_T$ 是在 coreset 上的损失函数，$\\theta\_t$ 是 t 轮得到模型参数；

最小化 ERR 来使 Coreset 最好地模拟损失函数（训练集或验证集）的梯度。

![](https://img2022.cnblogs.com/blog/2849310/202211/2849310-20221106225243149-460223571.png)

如何优化这个问题
--------

![](https://img2022.cnblogs.com/blog/2849310/202211/2849310-20221106225326853-502002000.png)

将其转化为次模函数：

![](https://img2022.cnblogs.com/blog/2849310/202211/2849310-20221106225357677-761640851.png)![](https://img2022.cnblogs.com/blog/2849310/202211/2849310-20221106225408255-465255451.png)

之后可以用贪心算法快速解决。

Tricks
------

*   只计算最后一层的梯度；
*   现在完整的数据集上跑几个 epoch，获得一个较为靠近的模型权重（类似于 warm-up 和 pre-training）；
*   每过 R 个 epoch 再更新 coreset。