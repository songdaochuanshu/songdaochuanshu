---
layout: post
title: "THFuse: An infrared and visible image fusion network using transformer and hybrid feature extractor 论文解读"
date: "2023-03-29T01:14:58.036Z"
---
THFuse: An infrared and visible image fusion network using transformer and hybrid feature extractor 论文解读
========================================================================================================

### THFuse: An infrared and visible image fusion network using transformer and hybrid feature extractor

**一种基于Transformer和混合特征提取器的红外与可见光图像融合网络**

#### 研究背景：

*   现有的图像融合方法主要是基于卷积神经网络(CNN)，由于CNN的感受野较小，很难对图像的长程依赖性进行建模，忽略了图像的长程相关性，导致融合网络不能生成具有良好互补性的图像，感受野的限制直接影响融合图像的质量。

研究方法：

*   考虑到transformer的全局注意力机制，提出了一种结合CNN和vision transformer的端到端图像融合方法来解决上述问题。

#### 整体框架

网络是一个端到端的架构，框架包括三个部分：卷积神经网络模块(CNN-module)、Vision Transformer模块（VIT-module）和图像重建模块。 前两个模块称为混合块，即混合特征提取器。

![](https://img2023.cnblogs.com/blog/2147078/202303/2147078-20230328215257674-1064294010.png)

卷积神经网络模块(CNN-module)：由细节分支和结构分支两部分组成。

*   细节分支：四个卷积层，卷积层之间有密集的连接操作来提取图像的深层特征。 输入特征数为16，每层卷积后的通道数为8、16、24、32。 每个卷积层的核大小为3 3，步幅为1。 为了保持特征的大小不变，我们使用反射模式来填充图像。 细节将最终输出一组大小为256x256的特征，包含32个通道。
*   结构分支：三个卷积层。 每次卷积运算后，特征的大小是前一步的一半，以达到下采样的目的。 每个卷积层的核大小为3 x3，步幅为2。 输入特征的大小为256x256，通道数为16。 每次卷积运算后特征的大小分别为128x128、64x64和32x32，特征的通道数分别为32、64和32。 为了保证该分支的通道大小和数目与前一分支一致，这里增加了一个双线性上采样层，输出32个通道、大小为256x256的特征。

Vision Transformer模块：有**空间transformer**和**通道transformer**两部分组成。如下图

*   空间transformer：将图像划分成许多patch块（每个通道都划分patch块,其中一个patch与所有通道的patch进行注意力操作），将每个patch拉成向量，patch块之间进行注意力操作。
*   通道transformer：按照图像的通道进行划分，将通道拉成向量，不同通道之间进行注意力操作。

![](https://img2023.cnblogs.com/blog/2147078/202303/2147078-20230328215320195-1306163232.png)

图像重建模块： 因为前面图像的大小没有进行下采样，所以重建的时候不需要上采样，只要把图像的通道维度降下来。图像重建器设置了四个卷积层，输入特征数为64个，每层卷积后的特征通道数为64、32、16、8和1。 每个卷积层的核大小为3 3，步幅为1。 为了保持特征的大小不变，我们使用反射模式来填充图像。

![](https://img2023.cnblogs.com/blog/2147078/202303/2147078-20230328215338256-1092403866.png)

#### 损失函数

损失函数由**像素损失**和**感知损失**两部分组成。

![](https://img2023.cnblogs.com/blog/2147078/202303/2147078-20230328215355823-893970522.png)

像素损失：由三部分组成，其中LMSE是均方误差(MSE)损失函数。LSSIM表示结构相似度(SSIM)损失函数。LTV表示总方差(TV)损失函数。

*   LMSE是均方误差(MSE)损失函数：对融合图像与源图像的每一点像素求差值然后平方，最后平均。

![](https://img2023.cnblogs.com/blog/2147078/202303/2147078-20230328215414854-98400401.png)

*   LSSIM表示结构相似度(SSIM)损失函数：融合图像与源图像的结构相似程度。

![](https://img2023.cnblogs.com/blog/2147078/202303/2147078-20230328215431884-157458584.png)

*   LTV表示总方差(TV)损失函数：该函数是抑制噪声和保留梯度信息。 噪声体现在图像中就是某一点梯度突变或者像素强度突变。

p,q是图像某一点的坐标，R(p,q)是融合图像与源图像之间的像素强度差值，R(p+1,q)和R(p,q+1)是临近点。通过约束融合图像这一点和临近点的差值，来保留原图像的梯度信息，并且抑制噪声信息。

![](https://img2023.cnblogs.com/blog/2147078/202303/2147078-20230328215448165-1520310723.png)

**像素损失无法代替感知损失。 例如，两个相距仅几个像素的相同图像，尽管在感知上相似，但当按每个像素的损失来衡量时，可能会有很大的不同。或者像素损失不大，但是图像感知差别很大。**

**我们通过两个图像特征图中的语义信息来判断两个图像的最后的感知，所以我们要从特征图入手，上面像素损失没有考虑到特征图的像素的重要性。**

感知损失：使用预训练的VGG19网络去提取（融合图像，源图像）多尺度的特征。主要是想用两个特征的语义信息来确定两个特征的感知信息，进而确定两个图像的感知信息。

![](https://img2023.cnblogs.com/blog/2147078/202303/2147078-20230328215505786-1893917206.png)

融合图像和可见光图像的感知损失: 使用提取的相对浅层特征(第一层)进行计算，由于浅层特征包含较多的结构信息和细节信息。

计算融合图像与红外图像的感知损失: 使用深层次的特征(第四层)来计算。因为红外图像中由更多的显著特征，语义信息。

![](https://img2023.cnblogs.com/blog/2147078/202303/2147078-20230328215521082-1166225617.png)

#### 结论

提出了一种基于VIT和卷积神经网络的红外与可见光图像融合方法。 由于我们的网络是端到端的类型，所以不需要对融合结果进行后期处理。 混合块集成了CNN-模块和VIT-模块，双分支CNN-模块具有更强的特征提取能力。 VIT-module的加入使网络能够同时考虑图像的局部信息和全局信息，避免了传统CNN网络远程依赖性差的问题。 另外，我们利用预训练的VGG19网络提取不同的特征来计算损失，有针对性地保留不同类型的图像信息。

图像融合的最终目的是与其他计算机视觉任务相结合并使之更好，因此我们接下来将尝试在其他计算机视觉任务的驱动下利用图像融合来改善原有的结果。

虽然本文的重点是红外和可见光图像融合，但本文提出的网络可以用于其他图像融合领域。 今后我们将尝试将该方法应用于多曝光和医学图像融合。

贡献点

*   提出了一种混合特征提取器，将双分支CNN和VIT相结合，实现了图像局部信息和全局信息的同时提取。
    
*   对VIT的网络结构进行了改进，使其更适合于图像融合。 另外，**将transfomer使用在图像的通道维度上**。
    
*   设计了一个有针对性的感知损失函数。 通过计算不同深度特征的损失，融合图像可以保留更多的纹理细节和显著信息。
    

参考原文：[https://www.x-mol.com/paper/1613633839666642944?adv](https://www.x-mol.com/paper/1613633839666642944?adv)