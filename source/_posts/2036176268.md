---
layout: post
title: "分布式协议与算法-Raft算法"
date: "2023-01-30T05:15:19.254Z"
---
分布式协议与算法-Raft算法
===============

> 本文总结自：极客时间**韩健**老师的**分布式协议与算法实战课程。**  
> 大家都知道，Raft算法属于**Multi-Paxos算法**，它是在Multi-Paxos思想的基础上，做了一些简化和限制。关于Paxos算法，博主在之前的文章有过总结，大家可以从这里**跳转**：[分布式协议与算法-Paxos算法](https://www.cnblogs.com/akai-yuan/p/17071522.html)  
> 关于Raft算法相关的开源社区有很多，如果你想深入理解RAFT算法，博主在这里推荐**蚂蚁金服**的[SOFAJRaft](https://github.com/sofastack/sofa-jraft)，它是基于JAVA语言开发的一个**生产级高性能共识算法实现**。  
> 博主总结过一些关于SOFAJRaft的源码，有兴趣的读者可以访问：[SOFAJRaft源码阅读](https://www.cnblogs.com/akai-yuan/category/2265786.html)。  
> @Author：Akai-yuan  
> @更新时间：2023/1/30

1.如何选举领导者
=========

### 1.1成员身份

成员身份，又叫做服务器节点状态，Raft 算法支持领导者（Leader）、跟随者（Follower）和候选人（Candidate） 3 种状态。  
![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003231217-444241865.png)

**跟随者**：就相当于普通群众，默默地接收和处理来自领导者的消息，当等待领导者心跳信息超时的时候，就主动站出来，推荐自己当候选人。  
**候选人**：候选人将向其他节点发送请求投票（RequestVote）RPC 消息，通知其他节点来投票，如果赢得了大多数选票，就晋升当领导者。  
**领导者**：蛮不讲理的霸道总裁，一切以我为准，平常的主要工作内容就是 3 部分，处理写请求、管理日志复制和不断地发送心跳信息，通知其他节点“我是领导者，我还活着，你们现在不要发起新的选举，找个新领导者来替代我。”

### 1.2选举过程：

*   首先，在初始状态下，集群中所有的节点都是跟随者的状态。

![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003242441-1512198869.png)

*   Raft 算法实现了随机超时时间的特性。也就是说，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。通过上面的图片你可以看到，集群中没有领导者，而节点 A 的等待超时时间最小（150ms），它会最先因为没有等到领导者的心跳信息，发生超时。这个时候，**节点 A 就增加自己的任期编号，并推举自己为候选人，先给自己投上一张选票，然后向其他节点发送请求投票 RPC 消息，请它们选举自己为领导者**。

![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003251503-929540391.png)

*   如果其他节点接收到候选人 A 的请求投票 RPC 消息，在编号为 1 的这届任期内，也还没有进行过投票，那么它将把选票投给节点 A，并增加自己的任期编号。

![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003316887-184151940.png)

*   如果候选人在选举超时时间内赢得了大多数的选票，那么它就会成为本届任期内新的领导者。

![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003323552-1020504670.png)

*   节点 A 当选领导者后，他将周期性地发送心跳消息，通知其他服务器我是领导者，阻止跟随者发起新的选举，篡权。

![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003337489-168022770.png)

2.节点间如何通信
=========

在 Raft 算法中，服务器节点间的沟通联络采用的是远程过程调用（RPC），在领导者选举  
中，需要用到这样两类的 RPC：

1.  **请求投票（RequestVote）RPC**，是由候选人在选举期间发起，通知各节点进行投票；
2.  **日志复制（AppendEntries）RPC**，是由领导者发起，用来复制日志和提供心跳消息。日志复制 RPC 只能由领导者发起，这是实现强领导者模型的关键之一。

3.什么是任期
=======

我们知道，议会选举中的领导者是有任期的，领导者任命到期后，要重新开会再次选举。Raft 算法中的领导者也是有任期的，每个任期由**单调递增**的数字（任期编号）标识，比如节点 A 的任期编号是 1。任期编号是随着选举的举行而变化的，这是在说下面几点。

1.  **跟随者在等待领导者心跳信息超时后，推举自己为候选人时，会增加自己的任期号。**比如节点 A 的当前任期编号为 0，那么在推举自己为候选人时，会将自己的任期编号增加为 1。
2.  **如果一个服务器节点，发现自己的任期编号比其他节点小，那么它会更新自己的编号到较大的编号值。**比如节点 B 的任期编号是 0，当收到来自节点 A 的请求投票 RPC 消息时，因为消息中包含了节点 A 的任期编号，且编号为 1，那么节点 B 将把自己的任期编号更新为 1。

_但是，与现实议会选举中的领导者的任期不同，Raft 算法中的任期不只是时间段，而且任期编号的大小，会影响领导者选举和请求的处理。_

1.  **在 Raft 算法中约定，如果一个候选人或者领导者，发现自己的任期编号比其他节点小，那么它会立即恢复成跟随者状态。**比如分区错误恢复后，任期编号为 3 的领导者节点B，收到来自新领导者的，包含任期编号为 4 的心跳消息，那么节点 B 将立即恢复成跟随者状态。
2.  **还约定如果一个节点接收到一个包含较小的任期编号值的请求，那么它会直接拒绝这个请求。**比如节点 C 的任期编号为 4，收到包含任期编号为 3 的请求投票 RPC 消息，那么它将拒绝这个消息。

在这里，你可以看到，Raft 算法中的任期比议会选举中的任期要复杂。同样，在 Raft 算法中，选举规则的内容也会比较多。

4.选举有哪些规则
=========

1.  领导者周期性地向所有跟随者发送心跳消息（即不包含日志项的日志复制 RPC 消息），通知大家我是领导者，阻止跟随者发起新的选举。
2.  如果在指定时间内，跟随者没有接收到来自领导者的消息，那么它就认为当前没有领导者，推举自己为候选人，发起领导者选举。
3.  在一次选举中，赢得大多数选票的候选人，将晋升为领导者。
4.  在一个任期内，领导者一直都会是领导者，直到它自身出现问题（比如宕机），或者因为网络延迟，其他节点发起一轮新的选举。
5.  在一次选举中，每一个服务器节点最多会对一个任期编号投出一张选票，并且按照“先来先服务”的原则进行投票。比如节点 C 的任期编号为 3，先收到了 1 个包含任期编号为 4 的投票请求（来自节点 A），然后又收到了 1 个包含任期编号为 4 的投票请求（来自节点 B）。那么节点 C 将会把唯一一张选票投给节点 A，当再收到节点 B 的投票请求RPC 消息时，对于编号为 4 的任期，已没有选票可投了。
6.  当任期编号相同时，日志完整性高的跟随者（也就是最后一条日志项对应的任期编号值更大，索引号更大），拒绝投票给日志完整性低的候选人。比如节点 B、C 的任期编号都是 3，节点 B 的最后一条日志项对应的任期编号为 3，而节点 C 为 2，那么当节点 C请求节点 B 投票给自己时，节点 B 将拒绝投票。

![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003347550-389547569.png)

5.如何理解随机超时时间
============

在议会选举中，常出现未达到指定票数，选举无效，需要重新选举的情况。在 Raft 算法的选举中，也存在类似的问题，那它是如何处理选举无效的问题呢？  
其实，Raft 算法巧妙地使用随机选举超时时间的方法，把超时时间都分散开来，在大多数情况下只有一个服务器节点先发起选举，而不是同时发起选举，这样就能减少因选票瓜分导致选举失败的情况。  
我想强调的是，在 Raft 算法中，随机超时时间是有 2 种含义的，这里是很多同学容易理解出错的地方，需要你注意一下：

1.  跟随者等待领导者心跳信息超时的时间间隔，是随机的；
2.  当没有候选人赢得过半票数，选举无效了，这时需要等待一个随机时间间隔，也就是说，等待选举超时的时间间隔，是随机的。

6.如何复制日志
========

### 6.1如何理解日志

副本数据是以日志的形式存在的，日志是由日志项组成。日志项是一种数据格式，它主要包含用户指定的数据，也就是**指令（Command）**，还包含一些附加信息，比如**索引值（Log index）、任期编号（Term）**。  
![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003358198-1242948951.png)

### 6.2日志复制过程

你可以把 Raft 的日志复制理解成一个优化后的二阶段提交（将二阶段优化成了一阶段），减少了一半的往返消息，也就是降低了一半的消息延迟。那日志复制的具体过程是什么呢？  
首先，领导者进入第一阶段，通过**日志复制（AppendEntries）RPC 消息**，将日志项复制到集群其他节点上。  
接着，如果领导者接收到大多数的“复制成功”响应后，它将日志项提交到它的状态机，并返回成功给客户端。如果领导者没有接收到大多数的“复制成功”响应，那么就返回错误给客户端。  
学到这里，有同学可能有这样的疑问了，领导者将日志项提交到它的状态机，怎么没通知跟随者提交日志项呢？  
**这是 Raft 中的一个优化，领导者不直接发送消息通知其他节点提交指定日志项。因为领导者的日志复制 RPC 消息或心跳消息，包含了当前最大的，将会被提交的日志项索引值。所以通过日志复制 RPC 消息或心跳消息，跟随者就可以知道领导者的日志提交位置信息**。  
因此，**当其他节点接受领导者的心跳消息，或者新的日志复制 RPC 消息后，就会将这条日志项提交到它的状态机。而这个优化，降低了处理客户端请求的延迟，将二阶段提交优化为了一段提交，降低了一半的消息延迟。**  
![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003407827-796835987.png)

### 6.3日志一致的实现

在 Raft 算法中，领导者通过强制跟随者直接复制自己的日志项，处理不一致日志。也就是说，Raft 是通过以领导者的日志为准，来实现各节点日志的一致的。具体有 2 个步骤。  
首先，领导者通过日志复制 RPC 的一致性检查，找到跟随者节点上，与自己相同日志项的最大索引值。也就是说，这个索引值之前的日志，领导者和跟随者是一致的，之后的日志是不一致的了。  
然后，领导者强制跟随者更新覆盖的不一致日志项，实现日志的一致。

为了方便演示，我们引入 2 个新变量：

1.  PrevLogEntry：表示当前要复制的日志项，前面一条日志项的索引值。比如在图中，如果领导者将索引值为 8 的日志项发送给跟随者，那么此时 PrevLogEntry 值为 7。
2.  PrevLogTerm：表示当前要复制的日志项，前面一条日志项的任期编号，比如在图中，如果领导者将索引值为 8 的日志项发送给跟随者，那么此时 PrevLogTerm 值为 4。

![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003417884-2135619489.png)

1.  领导者通过日志复制 RPC 消息，发送当前最新日志项到跟随者（为了演示方便，假设当前需要复制的日志项是最新的），这个消息的 PrevLogEntry 值为 7，PrevLogTerm 值为 4。
2.  如果跟随者在它的日志中，找不到与 PrevLogEntry 值为 7、PrevLogTerm 值为 4 的日志项，也就是说它的日志和领导者的不一致了，那么跟随者就会拒绝接收新的日志项，并返回失败信息给领导者。
3.  这时，领导者会递减要复制的日志项的索引值，并发送新的日志项到跟随者，这个消息的 PrevLogEntry 值为 6，PrevLogTerm 值为 3。
4.  如果跟随者在它的日志中，找到了 PrevLogEntry 值为 6、PrevLogTerm 值为 3 的日志项，那么日志复制 RPC 返回成功，这样一来，领导者就知道在 PrevLogEntry 值为 6、PrevLogTerm 值为 3 的位置，跟随者的日志项与自己相同。
5.  领导者通过日志复制 RPC，复制并更新覆盖该索引值之后的日志项（也就是不一致的日志项），最终实现了集群各节点日志的一致。

7.成员变更
======

单节点变更，就是通过一次变更一个节点实现成员变更。如果需要变更多个节点，那你需要执行多次单节点变更。比如将 3 节点集群扩容为 5 节点集群，这时你需要执行 2 次单节点变更，先将 3 节点集群变更为 4 节点集群，然后再将 4 节点集群变更为 5 节点集群，就像下图的样子。  
![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003428924-1948331666.png)

现在，让我们回到开篇的思考题，看看如何用单节点变更的方法，解决这个问题。为了演示  
方便，我们假设节点 A 是领导者：  
![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003435377-1268602809.png)

目前的集群配置为\[A, B, C\]，我们先向集群中加入节点 D，这意味着新配置为\[A, B, C, D\]。  
成员变更，是通过这么两步实现的：  
第一步，领导者（节点 A）向新节点（节点 D）同步数据；  
第二步，领导者（节点 A）将新配置\[A, B, C, D\]作为一个日志项，复制到新配置中所有节点（节点 A、B、C、D）上，然后将新配置的日志项提交到本地状态机，完成单节点变更。  
![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003442507-2034777112.png)

在变更完成后，现在的集群配置就是\[A, B, C, D\]，我们再向集群中加入节点 E，也就是说，  
新配置为\[A, B, C, D, E\]。成员变更的步骤和上面类似：  
第一步，领导者（节点 A）向新节点（节点 E）同步数据；  
第二步，领导者（节点 A）将新配置\[A, B, C, D, E\]作为一个日志项，复制到新配置中的所有节点（A、B、C、D、E）上，然后再将新配置的日志项提交到本地状态机，完成单节点变更。  
![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003453539-1932139277.png)

这样一来，我们就通过一次变更一个节点的方式，完成了成员变更，保证了集群中始终只有一个领导者，而且集群也在稳定运行，持续提供服务。  
在正常情况下，不管旧的集群配置是怎么组成的，旧配置的“大多数”和新配置的“大多数”都会有一个节点是重叠的。 也就是说，不会同时存在旧配置和新配置 2个“大多数”。  
![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003502451-1061938766.png)

![](https://img2023.cnblogs.com/blog/2784327/202301/2784327-20230130003509968-1075969838.png)

从上图中你可以看到，不管集群是偶数节点，还是奇数节点，不管是增加节点，还是移除节点，新旧配置的“大多数”都会存在重叠（图中的橙色节点）。  
需要你注意的是，在分区错误、节点故障等情况下，如果我们并发执行单节点变更，那么就可能出现一次单节点变更尚未完成，新的单节点变更又在执行，导致集群出现 2 个领导者的情况。  
如果你遇到这种情况，可以在领导者启动时，**创建一个 NO\_OP 日志项（也就是空日志项）**，只有当领导者将 NO\_OP 日志项提交后，再执行成员变更请求。这个解决办法，你记住就可以了，可以自己在课后试着研究下。具体的实现，可参考 \*\*Hashicorp Raft \*\*的源码，也就是 **runLeader() 函数**中：

    noop := &logFuture{
    log: Log{
    Type: LogNoop,
    },
    }
    r.dispatchLogs([]*logFuture{noop})