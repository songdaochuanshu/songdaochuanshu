---
layout: post
title: "论文解读《Measuring and Relieving the Over-smoothing Problem for Graph NeuralNetworks from the Topological View》"
date: "2022-05-04T01:46:50.907Z"
---
论文解读《Measuring and Relieving the Over-smoothing Problem for Graph NeuralNetworks from the Topological View》
===========================================================================================================

论文信息
====

> 论文标题：Measuring and Relieving the Over-smoothing Problem for Graph NeuralNetworks from the Topological View  
> 论文作者：Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, Xu Sun  
> 论文来源：2020, AAAI  
> 论文地址：[download](https://arxiv.org/abs/1909.03211)  
> 论文代码：download

1 Introduction
==============

 　　过平滑的标准定义：当 GNN 叠加多层时，不同类的图节点的表示将变得难以区分。

　　产生过平滑问题的原因：融合结构信息所带来噪声，即类内的交互可以带来有用的信息，而类间的交互可能会导致类之间难以区分的表示。

2 Measuring Over-smoothing Problem from the Topological View
============================================================

　　在本节中，将研究产生过平滑的原因。为此提出两个指标 MAD 和 MADGap 去测量图表示的平滑（smoothness）和过平滑（over-smoothness）。

2.1 MAD: Metric for Smoothness
------------------------------

　　Mean Average Distance (MAD) ，MAD 通过计算从节点到其他节点的平均距离的平均值来反映图表示的平滑性。

　　首先，计算余弦距离：

　　　　$D\_{i j}=1-\\frac{\\boldsymbol{H}\_{i,:} \\cdot \\boldsymbol{H}\_{j,:}}{\\left|\\boldsymbol{H}\_{i,:}\\right| \\cdot\\left|\\boldsymbol{H}\_{j,:}\\right|} \\quad i, j \\in\[1,2, \\cdots, n\]    $

　　其中：

*   *   $\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$ 代表图表示矩阵；
    *   $\\boldsymbol{D} \\in \\mathbb{R}^{n \\times n}$ 代表着距离矩阵；
    *   $D\_{i j}\\in \[0,2\]$ ；

　　其次，将 $D$ 和掩模矩阵（mask matrix）做哈达玛积：

　　　　$\\boldsymbol{D}^{t g t}=\\boldsymbol{D} \\circ M^{t g t}\\quad\\quad\\quad(2)$

　　其中，$\\boldsymbol{M}^{\\text {tgt }} \\in   \\{0,1\\}^{n \\times n}$ ，$\\boldsymbol{M}\_{i j}^{t g t}=1$ 代表着需要计算节点 $i$ 和节点 $j$ 之间的余弦距离。【可以从局部领域考虑平滑性，也可以从全局考虑平滑性】

　　然后，沿着 $\\overline{\\boldsymbol{D}}^{\\text {tgt }} $ 中每一行的非零值的平均距离 $\\boldsymbol{D}^{\\text {tgt }}$：

　　　　$\\overline{\\boldsymbol{D}}\_{i}^{t g t}=\\frac{\\sum\_{j=0}^{n} \\boldsymbol{D}\_{i j}^{t g t}}{\\sum\_{j=0}^{n} \\mathbb{1}\\left(\\boldsymbol{D}\_{i j}^{t g t}\\right)} \\quad\\quad\\quad(3)$

　　其中，如果 $x>0$ ， $\\mathbb{1}(x)=1$ ，否则为 $0$。

　　最后计算整个图的 MAD：

　　　　$\\operatorname{MAD}^{\\mathrm{tgt}}=\\frac{\\sum\_{i=0}^{n} \\overline{\\boldsymbol{D}}\_{i}^{t g t}}{\\sum\_{i=0}^{n} \\mathbb{1}\\left(\\overline{\\boldsymbol{D}}\_{i}^{t g t}\\right)} \\quad\\quad\\quad(4)$

　　通过考虑所有节点对来计算 MAD 值 $MAD^{Glogal}$，即 $ M^{tgt}$ 中的所有值都是 $1$，以度量学习图表示的全局平滑性。在 Cora 数据集上的 MAD 如下所示：

　　![](https://img2022.cnblogs.com/blog/1664108/202205/1664108-20220503210259624-987649563.png)

　　结果显示：多层 GNN 的 MAD 值接近于 $0$，说明所有的节点表示都变得难以区分。

2.2 Information-to-noise Ratio Largely Affects Over-smoothness
--------------------------------------------------------------

　　过平滑的标准定义：当 GNN 叠加多层时，不同类的图节点的表示将变得难以区分。

　　信噪比 （information-to-noise ratio）：

*   *   局部邻居角度：在二阶时，每个节点的信息噪比是同一类节点在所有一阶和二阶邻居中的比例；
    *   全图角度：整个图的信息噪比是两步节点对在所有节点对中可以接触的类内对的比例；

　　$\\text{Figure 3}$ 将展示在 Core 数据集上的不同 $\\text{order}$ 的全图的信噪比：

　　 ![](https://img2022.cnblogs.com/blog/1664108/202205/1664108-20220503212634488-1090291749.png)

　　观察结果：发现在低阶上有更多的类内节点对，反之亦然。当模型层数变大时，信噪比变小，高阶邻居之间的相互作用带来了太多的噪声，稀释了有用的信息，这是导致过度平滑问题的原因。

　　基于上述分析，提出 MADGap 去测量图表示过平滑。从 $\\text{Figure 3}$ 中我们注意到，两个拓扑距离较小的节点（低阶邻居）更有可能属于同一类别。因此，我们建议利用图的拓扑结构来近似节点类别，并计算区分远程节点和邻居节点的 MAD 值的间隙来估计图表示的过光滑性：

　　　　$\\mathrm{MADGap}=\\mathrm{MAD}^{\\mathrm{rmt}}-\\mathrm{MAD}^{\\mathrm{neb}} \\quad\\quad\\quad(5)$

　　其中，$MAD^{\\text {rmt }}$ 为图拓扑中远程节点的 $MAD$ 值，$MAD^{\\text {neb }}$ 为相邻节点的 $MAD$ 值。本文基于 $orders  \\leq 3$ 的节点计算 $MAD^{\\text {neb }}$，基于 $orders  \\geq 8$ 的节点计算 $MAD^{\\text {rmt }}$。

　　根据假设，$\\text{MADGap}$ 值越大($\\ge 0$)，表示节点接收到的有用信息要大于噪声。此时，GNN 进行了合理程度的平滑处理，模型表现良好。相反，小的或负的 $\\text{MADGap}$ 值意味着过度平滑和较差的性能。

　　为了验证 $\\text{MADGap }$ 的有效性，我们计算了 $\\text{MADGap}$ 值，并计算了 $\\text{MADGap}$ 与预测精度之间的皮尔逊系数。

　　在 Table 2 中报告了在 CORA、CiteSeer 和 PubMed 数据集上具有不同层的 GNN 的皮尔逊系数。

　　  ![](https://img2022.cnblogs.com/blog/1664108/202205/1664108-20220503215825065-883222807.png)

　　可以发现 MADGap 与模型性能之间存在显著的高相关性，这表明 MADGap 是度量图表示过平滑度的可靠度量。此外，MADGap 还可以作为一个观察指标，在不看到标签的情况下，基于图的拓扑结构来估计模型的性能。需要注意的是 $1$ 层 GNN 的 MADGap 和预测精度通常较小（ Figure 1），这是由于信息不足造成的，而高层GNN的过平滑问题是由于接收过多的噪声造成的。

　　在 Figure 4 中，我们展示了在同一模型中具有不同信息-噪声比的节点集的 MADGap 和预测精度。我们可以发现，即使使用相同的模型和传播步长，信息噪比率较高的节点通常具有较高的预测精度，过平滑度较小。

　　![](https://img2022.cnblogs.com/blog/1664108/202205/1664108-20220503221320058-916012774.png)

　　为了验证我们的假设，我们通过去除类间边和添加基于标签的类内边来优化图的拓扑结构。在 CORA 数据集上的结果如 Figure 5 所示。

 　　![](https://img2022.cnblogs.com/blog/1664108/202205/1664108-20220503221549765-1587511107.png)

　　可以发现，随着更多的类间边被删除，MADGap 值不断上升添加了更多的类内边，从而获得更好的模型性能。因此，优化图的拓扑结构有助于缓解过平滑问题，提高模型的性能。 

　　综上所述，我们发现图的拓扑结构对图表示的平滑性和模型的性能有很大的影响。然而，在自然连接和下游任务之间仍然存在不一致。现有的工作主要集中在设计新的 GNN 架构，而较少关注已建立的图拓扑。因此，我们进一步研究通过优化图拓扑来提高 GNN 的性能。

3 Relieving Over-smoothing Problem from the Topological View
============================================================

　　受前面分析的启发，我们提出了两种方法来从拓扑的角度缓解过度平滑问题：

*   *   MADReg：我们在训练目标中添加了一个 MADGap-based 的正则化器；
    *   Adaptive Edge Optimization (AdaEdge)：通过迭代训练 GNN 模型，并根据预测结果进行边缘去除/添加操作，从而自适应地调整图的拓扑结构；

3.1 MADReg: MADGap as Regularizer
---------------------------------

　　我们在训练目标中添加了 MADGap，使图节点接收到更多有用的信息和更少的干扰噪声：

　　　　$\\mathcal{L}=\\sum-l \\log p(\\hat{l} \\mid \\boldsymbol{X}, \\boldsymbol{A}, \\Theta)-\\lambda \\mathrm{MADGap} \\quad\\quad\\quad(6)$

　　其中，$\\boldsymbol{X}$ 为输入特征矩阵，$\\boldsymbol{A}$ 为邻接矩阵，$\\hat{l}$ 和 $l$ 分别为节点的预测标签和真实标签。$\\Theta$ 是GNN的参数，$\\lambda$ 是控制 MADReg 影响的正则化系数。我们在训练集上计算 MADGap，以与交叉熵损失相一致。

3.2 AdaEdge: Adaptive Edge Optimization
---------------------------------------

　　如前一节中所讨论的，在基于真实标签优化拓扑之后(添加类内边，并去除类间边)，过平滑问题明显缓解，模型性能大大提高。受此启发，我们提出了一种名为 AdaEdge 的自训练算法，基于模型的预测结果来优化图的拓扑结构，以自适应地调整图的拓扑结构，使其对特定的任务目标更加合理。具体来说，我们首先在原始图上训练GNN，并根据模型的预测结果，通过删除类间边和添加类内边来调整图的拓扑结构。然后，我们从头开始在更新后的图上重新训练GNN模型。我们将多次执行上述图的拓扑优化操作。AdaEdge算法的详细介绍如下：

　　![](https://img2022.cnblogs.com/blog/1664108/202205/1664108-20220504083640160-2010910551.png)

3.3 Relieving Over-smoothing in High-order Layers
-------------------------------------------------

　　为了验证这两种方法的有效性，我们在CORA/CiteSeer/PubMed数据集上对所有10个基线GNN模型进行了对照实验。我们计算了 4 层、过平滑问题严重的GNN模型的预测精度和 MADGap 值。结果如 $\\text{Figure 6}$ 所示。由于空间限制，我们给出了 10 个模型中的 6 个结果。可以发现，在过平滑问题严重的高阶层情况下，MADReg 和 AdaEdge 方法可以有效地缓解过平滑问题，提高所有三个数据集中大多数模型的模型性能。MADReg 和 AdaEdge 的有效性进一步验证了我们的假设，并为缓解过平滑问题提供了一个普遍而有效的解决方案。 

　　![](https://img2022.cnblogs.com/blog/1664108/202205/1664108-20220504084829325-359777822.png)

3.4 Improving Performance of GNNs
---------------------------------

　　在 $\\text{Table 3}$ 中，我们展示了在所有 7 个数据集上训练的 GNN 模型和在 AdaEdge 方法上训练的更新图模型的受控实验。我们在原始图上训练 GNN 时选择最佳的超参数，并在更新的图上进行训练时修复所有这些超参数。

　　![](https://img2022.cnblogs.com/blog/1664108/202205/1664108-20220504090447745-817841120.png)

　　实验结果表明，AdaEdge方法在大多数情况下都可以有效地提高模型的性能，这证明了图拓扑结构的优化对提高模型的性能有很大的帮助。我们分析了 AdaEdge 方法几乎没有改进或没有改进的情况，发现这是由于调整拓扑时的错误操作不当造成的。因此，当错误操作的比例太大时，会对模型训练造成严重的干扰，导致很少或没有改善。

　　![](https://img2022.cnblogs.com/blog/1664108/202205/1664108-20220504090624284-961710770.png)

　　在 $\\text{Table 3}$ 中，展示了 MADReg 的结果。通常，基线在少量的 GNN 层下达到了最好的性能，其中过平滑问题并不严重。在这种情况下，MADReg 很难通过扩大 MADGap 值来提高性能。然而，当过平滑问题变得更严重，而 GNN 层数变得更大时，MADReg 仍然能够显著地提高基线的性能。最重要的是，AdaEdge 和MADReg 都能有效地提高 GNN 的性能，并且当过平滑问题不严重时，AdaEdge 可以更好地推广。

4 Conclusion
============

　　在这项工作中，我们对gnn所面临的过度平滑问题进行了系统和定量的研究。我们首先设计了两个定量指标：光滑度的MAD和过平滑度的MADGap。从多个gnn和图数据集的定量测量结果中，我们发现平滑是gnn的本质；过光滑性是由信息和噪声的过度混合造成的。此外，我们发现MADGap与模型性能之间存在显著的高相关性。此外，我们还证明了信息噪比与图拓扑有关，并通过优化图拓扑来使其更适合下游任务，从而缓解过平滑问题。接下来，我们提出了两种方法来缓解gnn中的过平滑问题：MADReg方法和AdaEdge方法。大量的研究结果证明，这两种方法可以有效地缓解过平滑问题，并在一般情况下提高模型的性能。

　　虽然我们已经证明了优化图拓扑是提高gnn性能的有效方法，但我们提出的AdaEdge方法仍然存在错误的图调整操作问题。如何减少这些操作是一个很有前途的研究方向。

因上求缘，果上努力~~~~ 作者：[Learner-](https://www.cnblogs.com/BlairGrowing/)，转载请注明原文链接：[https://www.cnblogs.com/BlairGrowing/p/16218809.html](https://www.cnblogs.com/BlairGrowing/p/16218809.html)