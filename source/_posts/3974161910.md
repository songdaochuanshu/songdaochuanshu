---
layout: post
title: "3D视觉算法初学概述"
date: "2022-12-30T15:15:50.812Z"
---
3D视觉算法初学概述
==========

![3D视觉算法初学概述](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153617802-1087267579.png) SLAM 是Simultaneous Localization and Mapping的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器（单目、双目、RGB-D相机、Lidar）的主体，在没有环境先验信息的情况下，在运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉SLAM”；如果传感器位激光，则为激光 SLAM。

*   [背景知识](#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86)
    *   [RGB-D相机](#rgb-d%E7%9B%B8%E6%9C%BA)
*   [一，基于3DMM的三维人脸重建技术概述](#%E4%B8%80%E5%9F%BA%E4%BA%8E3dmm%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E8%84%B8%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF%E6%A6%82%E8%BF%B0)
    *   [1.1，3D 人脸重建概述](#113d-%E4%BA%BA%E8%84%B8%E9%87%8D%E5%BB%BA%E6%A6%82%E8%BF%B0)
    *   [1.2，初版 3DMM](#12%E5%88%9D%E7%89%88-3dmm)
*   [二，视觉SLAM算法基础概述](#%E4%BA%8C%E8%A7%86%E8%A7%89slam%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0)
    *   [2.1，视觉里程计](#21%E8%A7%86%E8%A7%89%E9%87%8C%E7%A8%8B%E8%AE%A1)
    *   [2.2，后端优化](#22%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96)
    *   [2.3，回环检测](#23%E5%9B%9E%E7%8E%AF%E6%A3%80%E6%B5%8B)
    *   [2.4，建图](#24%E5%BB%BA%E5%9B%BE)
*   [三，三维点云语义分割和实例分割综述](#%E4%B8%89%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E7%BB%BC%E8%BF%B0)
    *   [3.1，三维数据的表示方法](#31%E4%B8%89%E7%BB%B4%E6%95%B0%E6%8D%AE%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
        *   [3.1.1，点云定义](#311%E7%82%B9%E4%BA%91%E5%AE%9A%E4%B9%89)
        *   [3.1.2，点云的属性：](#312%E7%82%B9%E4%BA%91%E7%9A%84%E5%B1%9E%E6%80%A7)
        *   [3.1.3，点云获取](#313%E7%82%B9%E4%BA%91%E8%8E%B7%E5%8F%96)
        *   [3.1.4，点云存储格式](#314%E7%82%B9%E4%BA%91%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F)
        *   [3.1.5，三维点云的多种表示方法](#315%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91%E7%9A%84%E5%A4%9A%E7%A7%8D%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
    *   [3.2，基于点云的分类和检测](#32%E5%9F%BA%E4%BA%8E%E7%82%B9%E4%BA%91%E7%9A%84%E5%88%86%E7%B1%BB%E5%92%8C%E6%A3%80%E6%B5%8B)
    *   [3.3，基于点云的语义分割](#33%E5%9F%BA%E4%BA%8E%E7%82%B9%E4%BA%91%E7%9A%84%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2)
        *   [3.3.1，PointNet 网络](#331pointnet-%E7%BD%91%E7%BB%9C)
*   [四，参考资料](#%E5%9B%9B%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99)

> 3D 视觉算法包括很多内容，此文仅当作入门了解些概念和知识概括。

背景知识
----

**3D图像描述有多种方法，常见的如下：**

*   **点云**
*   网格（meshes）
*   基于视图的描述
*   深度图像（depth images）

### RGB-D相机

一般普通的相机拍出来的图像，其每个像素坐标（x, y）可以获得三种颜色属性（R, G, B）。但在 `RGB-D` 图像中，每个（x, y）坐标将对应于四个属性`（深度 D，R，G，B）`。

一，基于3DMM的三维人脸重建技术概述
-------------------

### 1.1，3D 人脸重建概述

3D 人脸重建定义：从一张或多张2D图像中重建出人脸的3D模型。数学表达式：

\\(M = (S,T)\\)

其中 `S` 表示人脸 3D 坐标形状向量（shape-vector），`T` 表示对应点的纹理信息向量（texture-vector）。

3D 人脸重建算法分类：

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153235025-229056261.png)

### 1.2，初版 3DMM

1999 年论文 《A Morphable Model For The Synthesis Of 3D Faces》提出三维形变模型（3DMM），三维形变模型建立在**三维人脸数据库**的基础上，以**人脸形状和人脸纹理**统计为约束，同时考虑到了人脸的姿态和光照因素的影响，因而生成的三维人脸模型精度高。每一个人脸模型都由相应的形状向量 \\(S\_i\\) 和 \\(T\_i\\)组成，其定义如下：

\\(S\_{newModel} = \\bar{S} + \\sum\_{i=1}^{m-1} \\alpha\_{i} s\_{i}\\)

\\(T\_{newModel} = \\bar{T} + \\sum\_{i=1}^{m-1} bata\_{i} t\_{i}\\)

其中 \\(\\bar{S}\\) 表示平均脸部形状模型，\\(s\_i\\)表示 shape 的 PCA 部分（按照特征值降序排列的协方差阵的特征向量），\\(\\alpha\_i\\)表示对应形状系数；纹理模型符号定义类似。通过调整形状、纹理系数系数可生成不同的人脸 3D 模型。

二，视觉SLAM算法基础概述
--------------

> SLAM问题的本质:对运动主体自身和周围环境空间不确定性的估计。为了解决SLAM问题，我们需要状 态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，估计状态的均值和不确定性(方差)。

`SLAM` 是Simultaneous Localization and Mapping的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器（单目、双目、RGB-D相机、Lidar）的主体，在没有环境先验信息的情况下，**在运动过程中建立环境的模型**，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉SLAM”；如果传感器位激光，则为激光 SLAM，两者对比如下：

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153235884-1309708847.png)

`SLAM` 主要解决**定位**和**地图构建**两个问题**。**视觉 SLAM 流程图如下：

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153236542-776412945.png)

整个视觉 SLAM 流程包括以下几个步骤：

1.  **传感器信息读取**。视觉 SLAM 中主要指摄像头图像数据读取与预处理。
2.  **视觉里程计**（`Visual Odometry`, `VO`）。视觉里程计的任务是**估算相邻图像间相机的运动**，以及局部地图的样子。`VO` 又称为前端 (`Front End`)。
3.  **后端优化**（`Optimization`）。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。由于接在 `VO` 之后，又称为后端(`Back End`)。
4.  \*\*回环检测 \*\*（`Loop Closing`）。回环检测判断机器人是否到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。
5.  **建图**（`Mapping`）。它根据估计的轨迹，建立与任务要求对应的地图。

### 2.1，视觉里程计

视觉里程计 `VO` 目的是**通过相邻帧间的图像估计相机运动**，并恢复场景的空间结构。其中为了定量地估计相机运动，必须先了解相机与空间点的几何关系。同时，仅通过视觉里程计来估计轨迹，将不可避免地出现累积漂移 (`Accumulating Drift`)，即每次估计都有误差的情况下，先前时刻的误差将会传递到下一个时刻，导致经过一段时间累积之后，估计的轨迹将不再准确，如下图所示。

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153237053-1667449826.png)

### 2.2，后端优化

概述性的说，后端优化主要目是为了处理 `SLAM` 过程中噪声的问题。后端优化要考虑的问题，就是如何从这些带有噪声的数据中估计整 个系统的状态，以及这个状态估计的不确定性有多大—这称为最大后 验概率估计(Maximum-a-Posteriori，MAP)。这里的状态既包括机器 人自身的轨迹，也包含地图。

前端与后端的关系：前端给后端提供待优化的数据，以及这些数据的初始值。后端只关心数据的优化过程，不关系这些数据来源于什么传感器。因此在视觉 `SLAM` 中，前端和计算机视觉研究领域更为相关，比如图像的特征提取与匹配等，后端则主要是**滤波与非线性优化算法**。

### 2.3，回环检测

回环检测（又称闭环检测 Loop Closure Detection），主要目的是为了**解决位置估计随时间漂移的问题**。

可以通过**图像相似性**来完成回环检测。在检测到回环之后，我们会把“A与B是同一个点”这样的信息告诉 后端优化算法。然后，后端根据这些新的信息，把轨迹和地图调整到符合回环检测结果的样子。这样，如果我们有充分而且正确的回环检测， 就可以消除累积误差，得到全局一致的轨迹和地图。

### 2.4，建图

建图(`Mapping`)是指构建地图的过程。这里的地图是对环境的描述，但这个描述并不是固定的，需要视SLAM 的应用而定。

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153237766-1909593098.png)

建图技术，根据用途的不同，可以分为稀疏重建和稠密重建，稀疏重建通常是重建一些图像特征点的三维坐标，**稀疏重建主要用于定位**。**稠密建图又称三维重建**，是对整个图像或图像中绝大部分像素进行重建，在导航、避障等方面起着举足轻重的作用。

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153238932-917669514.png)

三，三维点云语义分割和实例分割综述
-----------------

### 3.1，三维数据的表示方法

> 三维图像 = 普通的 RGB 三通道彩色图像 + Depth Map。

三维数据有四种表示方法，分别是 point cloud（点云），Mesh（网格），Voxel（体素）以及 Multi-View（多角度图片）。由此也衍生出了对应的三维数据语义和示例分割的算法，但主要是针对 point cloud 的算法越来越多。三维数据集有 ShapeNet、S3DIS、ModelNet40 等。

#### 3.1.1，点云定义

点云简单来说就是一堆三维点的集合，必须包括各个点的三维坐标信息，其他信息比如各个点的法向量、颜色、分类值、强度值、时间等均是可选。

点云在组成特点上分为两种，一种是有序点云，一种是无序点云。

*   **有序点云**：一般由深度图还原的点云，有序点云按照图方阵一行一行的，从左上角到右下角排列，当然其中有一些无效点因为。有序点云按顺序排列，可以很容易的找到它的相邻点信息。有序点云在某些处理的时候还是很便利的，但是很多情况下是无法获取有序点云的。
*   **无序点云**：无序点云就是其中的点的集合，点排列之间没有任何顺序，点的顺序交换后没有任何影响。是比较普遍的点云形式，有序点云也可看做无序点云来处理。

#### 3.1.2，点云的属性：

*   空间分辨率、点位精度、表面法向量等。
*   点云可以表达物体的空间轮廓和具体位置，我们能看到街道、房屋的形状，物体距离摄像机的距离也是可知的；其次，点云本身和视角无关，可以任意旋转，从不同角度和方向观察一个点云，而且不同的点云只要在同一个坐标系下就可以直接融合。

#### 3.1.3，点云获取

点云一般需要通过三维成像传感器获得，比如**双目相机、RGB-D相机和 LiDAR激光传感器**。

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153239617-792498239.png)

根据激光测量原理得到的点云，包括三维坐标（XYZ）和激光反射强度（Intensity），强度信息与目标的表面材质、粗糙度、入射角方向以及仪器的发射能量、激光波长有关。根据摄影测量原理得到的点云，包括三维坐标（XYZ）和颜色信息（RGB）。结合激光测量和摄影测量原理得到点云，包括三维坐标（`XYZ`）、激光反射强度（`Intensity`）和颜色信息（`RGB`）。

#### 3.1.4，点云存储格式

点云的文件格式可以有很多种，包括 .xyz，npy，ply，obj，off 等（mesh 可以通过泊松采样等方式转化成点云）。对于单个点云，如果你使用np.loadtxt得到的实际上就是一个维度为 的张量，num\_channels一般为 3，表示点云的三维坐标。

*   **pts** 点云文件格式是最简便的点云格式，直接按 `XYZ` 顺序存储点云数据， 可以是整型或者浮点型。

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153240044-55127976.png)

*   **LAS** 是激光雷达数据（LiDAR），存储格式比 pts 复杂，旨在提供一种开放的格式标准，允许不同的硬件和软件提供商输出可互操作的统一格式。LAS 格式点云截图，其中 C：class(所属类)，F：flight(航线号)，T：time(GPS 时间)，I：intensity(回波强度)，R：return(第几次回波)，N：number of return(回波次数)，A：scan angle(扫描角)，RGB：red green blue(RGB 颜色值)。

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153240583-1855952100.png)

*   **.xyz** 一种文本格式，前面 3 个数字表示点坐标，后面 3 个数字是点的法向量，数字间以空格分隔。

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153240962-1818635723.png)

*   **.pcap** 是一种通用的数据流格式，现在流行的 Velodyne 公司出品的激光雷达默认采集数据文件格式。它是一种二进制文件

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153241309-1512462629.png)

#### 3.1.5，三维点云的多种表示方法

三维点云除了原始点云表示还要网格 (Mesh) 表示和体素表示，如下图所示：

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153241800-445577606.png)

### 3.2，基于点云的分类和检测

**背景：**相比于图像数据，点云不直接包含空间结构，因此点云的深度模型必须解决三个主要问题:

1.  如何从稀疏的点云找到高信息密度的表示。
2.  如何构建一个网络满足必要的限制如 size-variance 和 permutation-invariance。
3.  如何以较低的时间和计算资源消耗处理大量数据。

对点云的分类通常称为三维形状分类。与图像分类模型相似，三维形状分类模型通常是先通过聚合编码器生成全局嵌入，然后将嵌入通过几个完全连通的层来获得最终结果。基于点云聚合方法，分类模型大致可分为两类: **基于投影的方法**和**基于点的方法。**

### 3.3，基于点云的语义分割

基于点云的语义分割方法大致可分为**基于投影的方法和基于点的方法。**

#### 3.3.1，PointNet 网络

PointNet 是第一个可以直接处理原始三维点云的深度神经网络，简单来说 `PointNet` 所作的事情就是对点云做特征学习，并将学习到的特征去做不同的应用：分类（shape-wise feature）、分割（point-wise feature）等。

无论是分类还是分割，本质上都还是分类任务，只是粒度不同罢了。因此损失函数 `loss` 一定有有监督分类任务中常用的交叉熵 loss，另外 loss 还有之前 `alignment network`（用于实现网络对于仿射变换、刚体变换等变换的无关性）的约束 loss，也就是上面的 `mat_diff_loss` 。

PointNet 网络结构如下所示：

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153242495-1506115375.png)

其大致的运算流程如下（来自[【3D视觉】PointNet和PointNet++](https://zhuanlan.zhihu.com/p/336496973)）：

1.  输入为一帧的全部点云数据的集合，表示为一个 nx3 的 2d tensor，其中 n 代表点云数量，3 对应 xyz 坐标。
2.  输入数据先通过和一个 `T-Net`**学习到的转换矩阵**相乘来对齐，保证了模型的对特定空间转换的不变性。
3.  通过多次 mlp 对各点云数据进行特征提取后，再用一个 T-Net 对特征进行对齐。
4.  在特征的各个维度上执行 **maxpooling **操作来得到最终的**全局特征**。
5.  对分类任务，将全局特征通过 mlp 来预测最后的分类分数。
6.  对分割任务，将全局特征和之前学习到的各点云的局部特征进行串联，再通过 mlp 得到每个数据点的分类结果。

分割任务**针对于每一个点做分类**，在下面的图中，把全局的特征复制成 `n` 份然后与之前的 `64` 维特征进行拼接，然后接着做一个 `mlp`，最后的输出 `nxm` 就是每一个点的分类结果。

![image](https://img2023.cnblogs.com/blog/2989634/202212/2989634-20221230153243310-1816732926.png)

四，参考资料
------

1.  [细嚼慢咽读论文：PointNet论文及代码详细解析](https://zhuanlan.zhihu.com/p/264627148?utm_source=wechat_session&utm_medium=social&utm_oi=1135649954939883520&utm_campaign=shareopn)
2.  [3D点云基础知识](https://zhuanlan.zhihu.com/p/344635951)
3.  [【3D视觉】PointNet和PointNet++](https://zhuanlan.zhihu.com/p/336496973)
4.  [点云+深度学习的开山之作–Pointnet](https://www.aimlab.top/1694.html)