---
layout: post
title: "爬虫基础知识"
date: "2023-03-16T01:13:24.893Z"
---
爬虫基础知识
======

目录

*   [一、requests模块](#一requests模块)
*   [二、requests携带参数的方式](#二requests携带参数的方式)
*   [三、get请求URL解码编码](#三get请求url解码编码)
*   [四、post请求携带数据编码格式](#四post请求携带数据编码格式)
*   [五、get请求携带请求头](#五get请求携带请求头)
*   [六、post请求携带参数](#六post请求携带参数)
*   [七、requests.session的使用(可以不用带cookie)](#七requestssession的使用可以不用带cookie)
*   [八、response对象](#八response对象)
*   [九、爬取图片和视频(前提是没有做防盗链)](#九爬取图片和视频前提是没有做防盗链)

  
爬虫是什么？；爬虫就是程序--->从互联网中，各个网站上，爬取数据\[前提是：你能浏览的页面才能爬\]，做数据清洗，保存到数据库的过程。

爬虫本质：模拟方式发送http请求、然后获取数据、再做数据清洗、最后保存到数据库

本篇爬虫知识主要包括(不会一次性写完，分好几篇文章写完)：

1.  模拟发送http请求(requests模块、selenium)
2.  反扒(封IP：IP代理、封账号：cookie池)
3.  解析数据(bs4)
4.  入库(保存到MySQL、Redis等数据库以及普通文件)
5.  爬虫框架(scrapy)

一、requests模块
============

使用python为什么能发送http请求，原因是该模块封装了Python内置模块urllib(起之前用到的urllib，requests模块的api更加便捷,本质就是封装了urllib3)

安装模块

    pip install requests
    

基本使用

    import requests
    
    
    res = requests.get('网址')  # 获取数据发送get请求
    print(res.text)  # text 参数可以打印出页面HTML代码
    

二、requests携带参数的方式
=================

    第一：直接在URL后面英文问号拼参数
     res = requests.get('网址')
        
    第二：使用params参数(其实最后效果跟第一种方式一样)
    res = requests.get('网址',params={'name':'mire','age':18})
    

三、get请求URL解码编码
==============

    import requests
    from urllib.parse import quote, unquote
    
    res = requests.get('网址', params={'name': '阿丽米热', 'age': 18})
    print(res.url)  # https://www.cnblogs.com/almira998/p/17208674.html?name=%E9%98%BF%E4%B8%BD%E7%B1%B3%E7%83%AD&age=18
    
    """
    我们平时在浏览器上搜关键字，有时候当前网址后面拼搜索关键字但是其已经url编码的具体样子如下
    https://www.cnblogs.com/almira998/p/17208674.html?name=%E9%98%BF%E4%B8%BD%E7%B1%B3%E7%83%AD&age=18
    怎么样？是不是很眼熟啊，这就是url编码后的效果
    """
    
    '''
    那么我们可以研究下如编码和解码
    首先要导入模块  from urllib.parse import quote, unquote
    quote：代表编码
    unquote：代表解码
    '''
    
    # 演示一下编码和解码
    res = quote('阿丽米热')
    print(res)  # %E9%98%BF%E4%B8%BD%E7%B1%B3%E7%83%AD
    
    res1 = unquote('%E9%98%BF%E4%B8%BD%E7%B1%B3%E7%83%AD')
    print(res1)  # 阿丽米热
    

四、post请求携带数据编码格式
================

    import requests
    
    # data对应字典，这样写，编码方式是urlencoded
    requests.post(url='xxxxxxxx',data={'xxx':'yyy'})
    # json对应字典，这样写，编码方式是json格式
    requests.post(url='xxxxxxxx',json={'xxx':'yyy'})
    # 终极方案，编码就是json格式
    requests.post(url='',
                  data={'':1,},
                  headers={
                      'content-type':'application/json'
                  })
    

五、get请求携带请求头
============

    # (反爬虫措施之一)
    Connection: keep-alive  # 该参数的作用保持链接状态，因为http协议本质是基于TCP协议的(无连接、无状态)
    Cookie: 很长的字符串删掉;  # 带着cookie就可以信任为登录状态，请求比较容易成功
    User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.3  # 表示客户端的类型 比如操作系统浏览器类型等等
    

    import requests
    
    
    # 先用字典的形式包装好
    headers = {
        'Connection':'keep-alive',
        'Cookie':'一堆字符串',
        'User-Agent':''
    }
    
    res = requests.get('网址', headers=headers)  # headers=headers表明携带请求头
    print(res.text)
    
    

六、post请求携带参数
============

    # 发送post请求需要以下参数
    """
    1、携带登录信息(用户名、密码、验证码)
    2、携带cookie
    3、携带请求头headers
    """
    

    import requests 
    data = {
        'username': '',
        'password': '',
        'captcha': '3456',
        'remember': 1,
        'ref': '网址',
        'act': 'act_login'
    }
    
    headers = {
        'Connection':'keep-alive',
        'Cookie':'一堆字符串',
        'User-Agent':''
    }
    res = requests.get('网址', data=data, headers=headers)
    print(res.text)
    
    
    """
    登录一般都是post请求，需要细带cookie
    但是有些网站啊有双token认证 严重影响了爬虫的效率
    使用requests.session就不用带cookie
    """
    # 使用方法
    session = requests.session()
    res = session.post('网址', data=data)
    res1 = session.get('网址')  # 自动保持登录状态，自动携带cookie
    
    

七、requests.session的使用(可以不用带cookie)
==================================

    import requests
    
    
    # 在data参数里需要携带用户名密码验证码等基本信息
    data = {
        'username': '3014885197@qq.com',
        'password': 'almira10054X',
        'referer': '网址'
    }
    # 发post请求，参数里面传data参数
    
    session = requests.session()
    res = session.post('网址, data=data)
    
    print(res.text)
    

八、response对象
============

    # Response相应对象的属性和方法
    import requests
    headers = {
        'User-Agent': '一些pc和浏览器配置信息'
    }
    respone=requests.get('网址',headers=headers)
    # respone属性
    print(respone.text) # 响应体转成了字符串
    print(respone.content) # 响应体的二进制内容
    
    print(respone.status_code) # 响应状态码
    print(respone.headers)   # 响应头
    print(respone.cookies)  # cookie是在响应头，cookie很重要，它单独做成了一个属性
    print(respone.cookies.get_dict()) # cookieJar对象---》转成字段
    print(respone.cookies.items())  # cookie的键值对
    
    print(respone.url)    # 请求地址
    print(respone.history) # 不用关注
    
    print(respone.encoding)  # 响应编码格式
    
    

九、爬取图片和视频(前提是没有做防盗链)
====================

    import requests
    
    url = '网址'
    
    res = requests.get(url)
    with open('狗.jpg', 'wb') as f:
        for line in res.iter_content():
            f.write(line)
    
    url = '网址'
    
    res1 = requests.get(url)
    with open('视频.mp4', 'wb') as f:
        for line in res1.iter_content():
            f.write(line)