---
layout: post
title: "åªèƒ½ç”¨äºæ–‡æœ¬ä¸å›¾åƒæ•°æ®ï¼ŸNoï¼çœ‹TabTransformerå¯¹ç»“æ„åŒ–ä¸šåŠ¡æ•°æ®ç²¾å‡†å»ºæ¨¡"
date: "2022-10-31T20:22:49.438Z"
---
åªèƒ½ç”¨äºæ–‡æœ¬ä¸å›¾åƒæ•°æ®ï¼ŸNoï¼çœ‹TabTransformerå¯¹ç»“æ„åŒ–ä¸šåŠ¡æ•°æ®ç²¾å‡†å»ºæ¨¡
==========================================

![åªèƒ½ç”¨äºæ–‡æœ¬ä¸å›¾åƒæ•°æ®ï¼ŸNoï¼çœ‹TabTransformerå¯¹ç»“æ„åŒ–ä¸šåŠ¡æ•°æ®ç²¾å‡†å»ºæ¨¡](https://img2022.cnblogs.com/blog/2637458/202210/2637458-20221026183705188-347103993.png) äºšé©¬é€Šæå‡ºçš„TabTransformerç½‘ç»œç»“æ„ï¼Œé¢ è¦†äº†NLP/CVï¼Œä¸ºå„ç§éç»“æ„åŒ–æ•°æ®ä¸šåŠ¡å¸¦æ¥äº†å·¨å¤§çªç ´ã€‚å½“ç„¶ï¼ŒTabTransformerä¹Ÿæ“…äºæ•æ‰ä¼ ç»Ÿç»“æ„åŒ–è¡¨æ ¼æ•°æ®ä¸­ä¸åŒç±»å‹çš„æ•°æ®ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç»“åˆä»¥å®Œæˆé¢„ä¼°ä»»åŠ¡ã€‚æœ¬æ–‡å°±è®²è§£å¦‚ä½•æ„å»ºTabTransformerå¹¶å°†å…¶åº”ç”¨äºç»“æ„åŒ–æ•°æ®ã€‚

![](https://img-blog.csdnimg.cn/img_convert/51b7018b1c5845b6d3b142e3117e3957.png)

> ğŸ’¡ ä½œè€…ï¼š[éŸ©ä¿¡å­](https://github.com/HanXinzi-AI)@[ShowMeAI](https://www.showmeai.tech/)  
> ğŸ“˜ [æ·±åº¦å­¦ä¹ å®æˆ˜ç³»åˆ—](https://www.showmeai.tech/tutorials/42)ï¼š[https://www.showmeai.tech/tutorials/42](https://www.showmeai.tech/tutorials/42)  
> ğŸ“˜ [TensorFlow å®æˆ˜ç³»åˆ—](https://www.showmeai.tech/tutorials/43)ï¼š[https://www.showmeai.tech/tutorials/43](https://www.showmeai.tech/tutorials/43)  
> ğŸ“˜ [æœ¬æ–‡åœ°å€](https://www.showmeai.tech/article-detail/315)ï¼š[https://www.showmeai.tech/article-detail/315](https://www.showmeai.tech/article-detail/315)  
> ğŸ“¢ å£°æ˜ï¼šç‰ˆæƒæ‰€æœ‰ï¼Œè½¬è½½è¯·è”ç³»å¹³å°ä¸ä½œè€…å¹¶æ³¨æ˜å‡ºå¤„  
> ğŸ“¢ æ”¶è—[ShowMeAI](https://www.showmeai.tech/)æŸ¥çœ‹æ›´å¤šç²¾å½©å†…å®¹

![](https://img-blog.csdnimg.cn/img_convert/24c858898e40f7e77022a46ce4f0cc28.png)

è‡ª Transformers å‡ºç°ä»¥æ¥ï¼ŒåŸºäºå®ƒçš„ç»“æ„å·²ç»é¢ è¦†äº†è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ï¼Œå¸¦æ¥å„ç§éç»“æ„åŒ–æ•°æ®ä¸šåŠ¡åœºæ™¯å’Œä»»åŠ¡çš„å·¨å¤§æ•ˆæœçªç ´ï¼Œæ¥ç€å¤§å®¶æŠŠç›®å…‰è½¬å‘äº†ç»“æ„åŒ–ä¸šåŠ¡æ•°æ®ï¼Œå®ƒæ˜¯å¦èƒ½åœ¨ç»“æ„åŒ–è¡¨æ ¼æ•°æ®ä¸ŠåŒæ ·æœ‰æƒŠäººçš„æ•ˆæœè¡¨ç°å‘¢ï¼Ÿ

ç­”æ¡ˆæ˜¯YESï¼äºšé©¬é€Šåœ¨è®ºæ–‡ä¸­æå‡ºçš„ ğŸ“˜[**TabTransformer**](https://arxiv.org/abs/2012.06678)ï¼Œæ˜¯ä¸€ç§æŠŠç»“æ„è°ƒæ•´åé€‚åº”äºç»“æ„åŒ–è¡¨æ ¼æ•°æ®çš„ç½‘ç»œç»“æ„ï¼Œå®ƒæ›´æ“…é•¿äºæ•æ‰ä¼ ç»Ÿç»“æ„åŒ–è¡¨æ ¼æ•°æ®ä¸­ä¸åŒç±»å‹çš„æ•°æ®ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç»“åˆä»¥å®Œæˆé¢„ä¼°ä»»åŠ¡ã€‚ä¸‹é¢[ShowMeAI](https://www.showmeai.tech/)ç»™å¤§å®¶è®²è§£æ„å»º TabTransformer å¹¶å°†å…¶åº”ç”¨äºç»“æ„åŒ–æ•°æ®ä¸Šçš„è¿‡ç¨‹ã€‚

![](https://img-blog.csdnimg.cn/img_convert/fe547ab8a170dbfec07de28a89d1b3e1.png)

ğŸ’¡ ç¯å¢ƒè®¾ç½®
=======

æœ¬ç¯‡ä½¿ç”¨åˆ°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸ºTensorFlowï¼Œå¤§å®¶éœ€è¦å®‰è£…2.7æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œ æˆ‘ä»¬è¿˜éœ€è¦å®‰è£…ä¸€ä¸‹ ğŸ“˜[**TensorFlowæ’ä»¶addons**](https://www.tensorflow.org/addons/overview)ï¼Œå®‰è£…çš„è¿‡ç¨‹å¤§å®¶å¯ä»¥é€šè¿‡ä¸‹è¿°å‘½ä»¤å®Œæˆï¼š

    pip install -U tensorflow tensorflow-addons
    

![](https://img-blog.csdnimg.cn/img_convert/912c9f6d053b97c812dfe840fd8add8b.png)

> å…³äºæœ¬ç¯‡ä»£ç å®ç°ä¸­ä½¿ç”¨åˆ°çš„TensorFlowå·¥å…·åº“ï¼Œå¤§å®¶å¯ä»¥æŸ¥çœ‹[ShowMeAI](https://www.showmeai.tech/)åˆ¶ä½œçš„TensorFlowé€ŸæŸ¥æ‰‹å†Œå¿«å­¦å¿«ç”¨ï¼š
> 
> *   [**AIå‚ç›´é¢†åŸŸå·¥å…·åº“é€ŸæŸ¥è¡¨ | TensorFlow2å»ºæ¨¡é€ŸæŸ¥&åº”ç”¨é€ŸæŸ¥**](https://www.showmeai.tech/article-detail/109)

æ¥ä¸‹æ¥æˆ‘ä»¬å¯¼å…¥å·¥å…·åº“

    import math
    import numpy as np
    import pandas as pd
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    import tensorflow_addons as tfa
    import matplotlib.pyplot as plt
    

ğŸ’¡ æ•°æ®è¯´æ˜
=======

[ShowMeAI](https://www.showmeai.tech/)åœ¨æœ¬ä¾‹ä¸­ä½¿ç”¨åˆ°çš„æ˜¯ ğŸ†[**ç¾å›½äººå£æ™®æŸ¥æ”¶å…¥æ•°æ®é›†**](https://archive.ics.uci.edu/ml/datasets/census+income)ï¼Œä»»åŠ¡æ˜¯æ ¹æ®äººå£åŸºæœ¬ä¿¡æ¯é¢„æµ‹å…¶å¹´æ”¶å…¥æ˜¯å¦å¯èƒ½è¶…è¿‡ 50,000 ç¾å…ƒï¼Œæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ã€‚

![](https://img-blog.csdnimg.cn/img_convert/8c33cc8a0d6c35a4fe88c425f714902a.png)

æ•°æ®é›†å¯ä»¥åœ¨ä»¥ä¸‹åœ°å€ä¸‹è½½ï¼š

ğŸ“˜ [https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult)

ğŸ“˜ [https://archive.ics.uci.edu/ml/machine-learning-databases/adult/](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/)

> æ•°æ®ä»ç¾å›½1994å¹´äººå£æ™®æŸ¥æ•°æ®åº“æŠ½å–è€Œæ¥ï¼Œå¯ä»¥ç”¨æ¥é¢„æµ‹å±…æ°‘æ”¶å…¥æ˜¯å¦è¶…è¿‡50K/yearã€‚è¯¥æ•°æ®é›†ç±»å˜é‡ä¸ºå¹´æ”¶å…¥æ˜¯å¦è¶…è¿‡50kï¼Œå±æ€§å˜é‡åŒ…å«å¹´é¾„ã€å·¥ç§ã€å­¦å†ã€èŒä¸šã€äººç§ç­‰é‡è¦ä¿¡æ¯ï¼Œå€¼å¾—ä¸€æçš„æ˜¯ï¼Œ14ä¸ªå±æ€§å˜é‡ä¸­æœ‰7ä¸ªç±»åˆ«å‹å˜é‡ã€‚æ•°æ®é›†å„å±æ€§æ˜¯ï¼šå…¶ä¸­åºå·0~13æ˜¯å±æ€§ï¼Œ14æ˜¯ç±»åˆ«ã€‚

å­—æ®µåºå·

å­—æ®µå

å«ä¹‰

ç±»å‹

0

age

å¹´é¾„

Double

1

workclass

å·¥ä½œç±»å‹\*

string

2

fnlwgt

åºå·

string

3

education

æ•™è‚²ç¨‹åº¦\*

string

4

education\_num

å—æ•™è‚²æ—¶é—´

double

5

maritial\_status

å©šå§»çŠ¶å†µ\*

string

6

occupation

èŒä¸š\*

string

7

relationship

å…³ç³»\*

string

8

race

ç§æ—\*

string

9

sex

æ€§åˆ«\*

string

10

capital\_gain

èµ„æœ¬æ”¶ç›Š

string

11

capital\_loss

èµ„æœ¬æŸå¤±

string

12

hours\_per\_week

æ¯å‘¨å·¥ä½œå°æ—¶æ•°

double

13

native\_country

åŸç±\*

string

14(label)

income

æ”¶å…¥æ ‡ç­¾

string

![](https://img-blog.csdnimg.cn/img_convert/4146260ec120f49d64a9f14d9fa1900f.png)

æˆ‘ä»¬å…ˆç”¨pandasè¯»å–æ•°æ®åˆ°dataframeä¸­ï¼š

    CSV_HEADER = [
        "age",
        "workclass",
        "fnlwgt",
        "education",
        "education_num",
        "marital_status",
        "occupation",
        "relationship",
        "race",
        "gender",
        "capital_gain",
        "capital_loss",
        "hours_per_week",
        "native_country",
        "income_bracket",
    ]
    
    train_data_url = (
        "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
    )
    train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)
    
    test_data_url = (
        "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"
    )
    test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)
    
    print(f"Train dataset shape: {train_data.shape}")
    print(f"Test dataset shape: {test_data.shape}")
    Train dataset shape: (32561, 15)
    Test dataset shape: (16282, 15)
    

æˆ‘ä»¬åšç‚¹æ•°æ®æ¸…æ´—ï¼ŒæŠŠæµ‹è¯•é›†ç¬¬ä¸€æ¡è®°å½•å‰”é™¤ï¼ˆå®ƒä¸æ˜¯æœ‰æ•ˆçš„æ•°æ®ç¤ºä¾‹ï¼‰ï¼ŒæŠŠç±»æ ‡ç­¾ä¸­çš„å°¾éšçš„â€œç‚¹â€å»æ‰ã€‚

    test_data = test_data[1:]
    test_data.income_bracket = test_data.income_bracket.apply(
        lambda value: value.replace(".", "")
    )
    

å†æŠŠè®­ç»ƒé›†å’Œæµ‹è¯•é›†å­˜å›å•ç‹¬çš„ CSV æ–‡ä»¶ä¸­ã€‚

    train_data_file = "train_data.csv"
    test_data_file = "test_data.csv"
    
    train_data.to_csv(train_data_file, index=False, header=False)
    test_data.to_csv(test_data_file, index=False, header=False)
    

ğŸ’¡ æ¨¡å‹åŸç†
=======

TabTransformerçš„æ¨¡å‹æ¶æ„å¦‚ä¸‹æ‰€ç¤ºï¼š

![](https://img-blog.csdnimg.cn/img_convert/ac4ebb1f098a5b41636ab8bf3dd1f913.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç±»åˆ«å‹çš„ç‰¹å¾ï¼Œå¾ˆé€‚åˆåœ¨ embedding åï¼Œé€å…¥ transformer æ¨¡å—è¿›è¡Œæ·±åº¦äº¤å‰ç»„åˆä¸ä¿¡æ¯æŒ–æ˜ï¼Œå¾—åˆ°çš„ä¿¡æ¯ä¸å³ä¾§çš„è¿ç»­å€¼ç‰¹å¾è¿›è¡Œæ‹¼æ¥ï¼Œå†é€å…¥å…¨è¿æ¥çš„ MLP æ¨¡å—è¿›è¡Œç»„åˆå’Œå®Œæˆæœ€åçš„ä»»åŠ¡ï¼ˆåˆ†ç±»æˆ–è€…å›å½’ï¼‰ã€‚

ğŸ’¡ æ¨¡å‹å®ç°
=======

ğŸ“Œ å®šä¹‰æ•°æ®é›†å…ƒæ•°æ®
-----------

è¦å®ç°æ¨¡å‹ï¼Œæˆ‘ä»¬å…ˆå¯¹è¾“å…¥æ•°æ®å­—æ®µï¼ŒåŒºåˆ†ä¸åŒçš„ç±»å‹ï¼ˆæ•°å€¼å‹ç‰¹å¾ä¸ç±»åˆ«å‹ç‰¹å¾ï¼‰ã€‚æˆ‘ä»¬ä¼šå¯¹ä¸åŒç±»å‹çš„ç‰¹å¾ï¼Œä½¿ç”¨ä¸åŒçš„æ–¹å¼è¿›è¡Œå¤„ç†å’Œå®Œæˆç‰¹å¾å·¥ç¨‹ï¼ˆä¾‹å¦‚æ•°å€¼å‹çš„ç‰¹å¾è¿›è¡Œå¹…åº¦ç¼©æ”¾ï¼Œç±»åˆ«å‹çš„ç‰¹å¾è¿›è¡Œç¼–ç å¤„ç†ï¼‰ã€‚

    ## æ•°å€¼ç‰¹å¾å­—æ®µ
    NUMERIC_FEATURE_NAMES = [
        "age",
        "education_num",
        "capital_gain",
        "capital_loss",
        "hours_per_week",
    ]
    ## ç±»åˆ«å‹ç‰¹å¾å­—æ®µåŠå…¶å–å€¼åˆ—è¡¨
    CATEGORICAL_FEATURES_WITH_VOCABULARY = {
        "workclass": sorted(list(train_data["workclass"].unique())),
        "education": sorted(list(train_data["education"].unique())),
        "marital_status": sorted(list(train_data["marital_status"].unique())),
        "occupation": sorted(list(train_data["occupation"].unique())),
        "relationship": sorted(list(train_data["relationship"].unique())),
        "race": sorted(list(train_data["race"].unique())),
        "gender": sorted(list(train_data["gender"].unique())),
        "native_country": sorted(list(train_data["native_country"].unique())),
    }
    ## æƒé‡å­—æ®µ
    WEIGHT_COLUMN_NAME = "fnlwgt"
    ## ç±»åˆ«å‹å­—æ®µåç§°
    CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())
    ## æ‰€æœ‰çš„è¾“å…¥ç‰¹å¾
    FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES
    ## é»˜è®¤å¡«å……çš„å–å€¼
    COLUMN_DEFAULTS = [
        [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else ["NA"]
        for feature_name in CSV_HEADER
    ]
    ## ç›®æ ‡å­—æ®µ
    TARGET_FEATURE_NAME = "income_bracket"
    ## ç›®æ ‡å­—æ®µå–å€¼
    TARGET_LABELS = [" <=50K", " >50K"]
    

ğŸ“Œ é…ç½®è¶…å‚æ•°
--------

æˆ‘ä»¬ä¸ºç¥ç»ç½‘ç»œçš„ç»“æ„å’Œè®­ç»ƒè¿‡ç¨‹çš„è¶…å‚æ•°è¿›è¡Œè®¾ç½®ï¼Œå¦‚ä¸‹ã€‚

    # å­¦ä¹ ç‡
    LEARNING_RATE = 0.001
    # å­¦ä¹ ç‡è¡°å‡
    WEIGHT_DECAY = 0.0001
    # éšæœºå¤±æ´» æ¦‚ç‡å‚æ•°
    DROPOUT_RATE = 0.2
    # æ‰¹æ•°æ®å¤§å°
    BATCH_SIZE = 265
    # æ€»è®­ç»ƒè½®æ¬¡æ•°
    NUM_EPOCHS = 15
    
    # transformerå—çš„æ•°é‡
    NUM_TRANSFORMER_BLOCKS = 3
    # æ³¨æ„åŠ›å¤´çš„æ•°é‡
    NUM_HEADS = 4
    # ç±»åˆ«å‹embeddingåµŒå…¥çš„ç»´åº¦
    EMBEDDING_DIMS = 16
    # MLPéšå±‚å•å…ƒæ•°é‡
    MLP_HIDDEN_UNITS_FACTORS = [
        2,
        1,
    ]
    # MLPå—çš„æ•°é‡
    NUM_MLP_BLOCKS = 2
    

ğŸ“Œ å®ç°æ•°æ®è¯»å–ç®¡é“
-----------

ä¸‹é¢æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè¾“å…¥å‡½æ•°ï¼Œå®ƒè´Ÿè´£è¯»å–å’Œè§£ææ–‡ä»¶ï¼Œå¹¶å¯¹ç‰¹å¾å’Œæ ‡ç­¾å¤„ç†ï¼Œæ”¾å…¥ `tf.data.Dataset`ï¼Œä»¥ä¾¿åç»­è®­ç»ƒå’Œè¯„ä¼°ã€‚

    target_label_lookup = layers.StringLookup(
        vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0
    )
    
    
    def prepare_example(features, target):
        target_index = target_label_lookup(target)
        weights = features.pop(WEIGHT_COLUMN_NAME)
        return features, target_index, weights
    
    # ä»csvä¸­è¯»å–æ•°æ®
    def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):
        dataset = tf.data.experimental.make_csv_dataset(
            csv_file_path,
            batch_size=batch_size,
            column_names=CSV_HEADER,
            column_defaults=COLUMN_DEFAULTS,
            label_name=TARGET_FEATURE_NAME,
            num_epochs=1,
            header=False,
            na_value="?",
            shuffle=shuffle,
        ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)
        return dataset.cache()
    

ğŸ“Œ æ¨¡å‹æ„å»ºä¸è¯„ä¼°
----------

    def run_experiment(
        model,
        train_data_file,
        test_data_file,
        num_epochs,
        learning_rate,
        weight_decay,
        batch_size,
    ):
        # ä¼˜åŒ–å™¨
        optimizer = tfa.optimizers.AdamW(
            learning_rate=learning_rate, weight_decay=weight_decay
        )
        # æ¨¡å‹ç¼–è¯‘
        model.compile(
            optimizer=optimizer,
            loss=keras.losses.BinaryCrossentropy(),
            metrics=[keras.metrics.BinaryAccuracy(name="accuracy")],
        )
        # è®­ç»ƒé›†ä¸éªŒè¯é›†
        train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)
        validation_dataset = get_dataset_from_csv(test_data_file, batch_size)
        
        # æ¨¡å‹è®­ç»ƒ
        print("Start training the model...")
        history = model.fit(
            train_dataset, epochs=num_epochs, validation_data=validation_dataset
        )
        print("Model training finished")
        
        # æ¨¡å‹è¯„ä¼°
        _, accuracy = model.evaluate(validation_dataset, verbose=0)
    
        print(f"Validation accuracy: {round(accuracy * 100, 2)}%")
    
        return history
    

### â‘  åˆ›å»ºæ¨¡å‹è¾“å…¥

åŸºäº TensorFlow çš„è¾“å…¥è¦æ±‚ï¼Œæˆ‘ä»¬å°†æ¨¡å‹çš„è¾“å…¥å®šä¹‰ä¸ºå­—å…¸ï¼Œå…¶ä¸­ã€key/é”®ã€æ˜¯ç‰¹å¾åç§°ï¼Œã€value/å€¼ã€ä¸º `keras.layers.Input`å…·æœ‰ç›¸åº”ç‰¹å¾å½¢çŠ¶çš„å¼ é‡å’Œæ•°æ®ç±»å‹ã€‚

    def create_model_inputs():
        inputs = {}
        for feature_name in FEATURE_NAMES:
            if feature_name in NUMERIC_FEATURE_NAMES:
                inputs[feature_name] = layers.Input(
                    name=feature_name, shape=(), dtype=tf.float32
                )
            else:
                inputs[feature_name] = layers.Input(
                    name=feature_name, shape=(), dtype=tf.string
                )
        return inputs
    

### â‘¡ ç¼–ç ç‰¹å¾

æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª`encode_inputs`å‡½æ•°ï¼Œè¿”å›`encoded_categorical_feature_list`å’Œ `numerical_feature_list`ã€‚æˆ‘ä»¬å°†åˆ†ç±»ç‰¹å¾ç¼–ç ä¸ºåµŒå…¥ï¼Œä½¿ç”¨å›ºå®šçš„`embedding_dims`å¯¹äºæ‰€æœ‰åŠŸèƒ½ï¼Œ æ— è®ºä»–ä»¬çš„è¯æ±‡é‡å¤§å°ã€‚ è¿™æ˜¯ Transformer æ¨¡å‹æ‰€å¿…éœ€çš„ã€‚

    def encode_inputs(inputs, embedding_dims):
    
        encoded_categorical_feature_list = []
        numerical_feature_list = []
    
        for feature_name in inputs:
            if feature_name in CATEGORICAL_FEATURE_NAMES:
    
                # è·å–ç±»åˆ«å‹ç‰¹å¾çš„ä¸åŒå–å€¼(vocabulary)
                vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]
    
                # æ„å»ºlookup tableå»æ„å»º ç±»åˆ«å‹å–å€¼ å’Œ ç´¢å¼• çš„ç›¸äº’æ˜ å°„
                lookup = layers.StringLookup(
                    vocabulary=vocabulary,
                    mask_token=None,
                    num_oov_indices=0,
                    output_mode="int",
                )
    
                # ç±»åˆ«å‹å­—ç¬¦ä¸²å–å€¼ è½¬ä¸º æ•´å‹ç´¢å¼•
                encoded_feature = lookup(inputs[feature_name])
    
                # æ„å»ºembeddingå±‚
                embedding = layers.Embedding(
                    input_dim=len(vocabulary), output_dim=embedding_dims
                )
    
                # ä¸ºç´¢å¼•æ„å»ºembeddingåµŒå…¥
                encoded_categorical_feature = embedding(encoded_feature)
                encoded_categorical_feature_list.append(encoded_categorical_feature)
    
            else:
    
                # æ•°å€¼å‹ç‰¹å¾
                numerical_feature = tf.expand_dims(inputs[feature_name], -1)
                numerical_feature_list.append(numerical_feature)
    
        return encoded_categorical_feature_list, numerical_feature_list
    

### â‘¢ MLPæ¨¡å—å®ç°

ç½‘ç»œä¸­ä¸å¯æˆ–ç¼ºçš„éƒ¨åˆ†æ˜¯ MLP å…¨è¿æ¥æ¿å—ï¼Œä¸‹é¢æ˜¯å®ƒçš„ç®€å•å®ç°ï¼š

    def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):
    
        mlp_layers = []
        for units in hidden_units:
            mlp_layers.append(normalization_layer),
            mlp_layers.append(layers.Dense(units, activation=activation))
            mlp_layers.append(layers.Dropout(dropout_rate))
    
        return keras.Sequential(mlp_layers, name=name)
    

### â‘£ æ¨¡å‹å®ç°1ï¼šåŸºçº¿æ¨¡å‹

ä¸ºäº†å¯¹æ¯”æ•ˆæœï¼Œæˆ‘ä»¬å…ˆç®€å•ä½¿ç”¨MLPï¼ˆå¤šå±‚å‰é¦ˆç½‘ç»œï¼‰è¿›è¡Œå»ºæ¨¡ï¼Œä»£ç å’Œæ³¨é‡Šå¦‚ä¸‹ã€‚

    def create_baseline_model(
        embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate
    ):
    
        # åˆ›å»ºè¾“å…¥.
        inputs = create_model_inputs()
        # ç‰¹å¾ç¼–ç 
        encoded_categorical_feature_list, numerical_feature_list = encode_inputs(
            inputs, embedding_dims
        )
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        features = layers.concatenate(
            encoded_categorical_feature_list + numerical_feature_list
        )
        # å‰å‘è®¡ç®—
        feedforward_units = [features.shape[-1]]
    
        # æ„å»ºå…¨è¿æ¥ï¼Œå¹¶ä¸”æ·»åŠ è·³è·ƒè¿æ¥(skip-connection)
        for layer_idx in range(num_mlp_blocks):
            features = create_mlp(
                hidden_units=feedforward_units,
                dropout_rate=dropout_rate,
                activation=keras.activations.gelu,
                normalization_layer=layers.LayerNormalization(epsilon=1e-6),
                name=f"feedforward_{layer_idx}",
            )(features)
    
        # MLPå…¨è¿æ¥çš„éšå±‚ç»“æœ
        mlp_hidden_units = [
            factor * features.shape[-1] for factor in mlp_hidden_units_factors
        ]
        # æœ€ç»ˆçš„MLPç½‘ç»œ
        features = create_mlp(
            hidden_units=mlp_hidden_units,
            dropout_rate=dropout_rate,
            activation=keras.activations.selu,
            normalization_layer=layers.BatchNormalization(),
            name="MLP",
        )(features)
    
        # æ·»åŠ sigmoidæ„å»ºäºŒåˆ†ç±»å™¨
        outputs = layers.Dense(units=1, activation="sigmoid", name="sigmoid")(features)
        model = keras.Model(inputs=inputs, outputs=outputs)
        return model
    
    # å®Œæ•´çš„æ¨¡å‹
    baseline_model = create_baseline_model(
        embedding_dims=EMBEDDING_DIMS,
        num_mlp_blocks=NUM_MLP_BLOCKS,
        mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,
        dropout_rate=DROPOUT_RATE,
    )
    
    print("Total model weights:", baseline_model.count_params())
    keras.utils.plot_model(baseline_model, show_shapes=True, rankdir="LR")
    # Total model weights: 109629
    

ä¸Šè¿°æ¨¡å‹æ„å»ºå®Œæˆä¹‹åï¼Œæˆ‘ä»¬é€šè¿‡plot\_modelæ“ä½œï¼Œç»˜åˆ¶å‡ºæ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š

![](https://img-blog.csdnimg.cn/img_convert/705a97eaafb83079ba7ba7c2150ccd2d.png)

æ¥ä¸‹æ¥æˆ‘ä»¬è®­ç»ƒå’Œè¯„ä¼°ä¸€ä¸‹åŸºçº¿æ¨¡å‹ï¼š

    history = run_experiment(
        model=baseline_model,
        train_data_file=train_data_file,
        test_data_file=test_data_file,
        num_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        batch_size=BATCH_SIZE,
    )
    

è¾“å‡ºçš„è®­ç»ƒè¿‡ç¨‹æ—¥å¿—å¦‚ä¸‹ï¼š

    Start training the model...
    Epoch 1/15
    123/123 [==============================] - 6s 25ms/step - loss: 110178.8203 - accuracy: 0.7478 - val_loss: 92703.0859 - val_accuracy: 0.7825
    Epoch 2/15
    123/123 [==============================] - 2s 14ms/step - loss: 90979.8125 - accuracy: 0.7675 - val_loss: 71798.9219 - val_accuracy: 0.8001
    Epoch 3/15
    123/123 [==============================] - 2s 14ms/step - loss: 77226.5547 - accuracy: 0.7902 - val_loss: 68581.0312 - val_accuracy: 0.8168
    Epoch 4/15
    123/123 [==============================] - 2s 14ms/step - loss: 72652.2422 - accuracy: 0.8004 - val_loss: 70084.0469 - val_accuracy: 0.7974
    Epoch 5/15
    123/123 [==============================] - 2s 14ms/step - loss: 71207.9375 - accuracy: 0.8033 - val_loss: 66552.1719 - val_accuracy: 0.8130
    Epoch 6/15
    123/123 [==============================] - 2s 14ms/step - loss: 69321.4375 - accuracy: 0.8091 - val_loss: 65837.0469 - val_accuracy: 0.8149
    Epoch 7/15
    123/123 [==============================] - 2s 14ms/step - loss: 68839.3359 - accuracy: 0.8099 - val_loss: 65613.0156 - val_accuracy: 0.8187
    Epoch 8/15
    123/123 [==============================] - 2s 14ms/step - loss: 68126.7344 - accuracy: 0.8124 - val_loss: 66155.8594 - val_accuracy: 0.8108
    Epoch 9/15
    123/123 [==============================] - 2s 14ms/step - loss: 67768.9844 - accuracy: 0.8147 - val_loss: 66705.8047 - val_accuracy: 0.8230
    Epoch 10/15
    123/123 [==============================] - 2s 14ms/step - loss: 67482.5859 - accuracy: 0.8151 - val_loss: 65668.3672 - val_accuracy: 0.8143
    Epoch 11/15
    123/123 [==============================] - 2s 14ms/step - loss: 66792.6875 - accuracy: 0.8181 - val_loss: 66536.3828 - val_accuracy: 0.8233
    Epoch 12/15
    123/123 [==============================] - 2s 14ms/step - loss: 65610.4531 - accuracy: 0.8229 - val_loss: 70377.7266 - val_accuracy: 0.8256
    Epoch 13/15
    123/123 [==============================] - 2s 14ms/step - loss: 63930.2500 - accuracy: 0.8282 - val_loss: 68294.8516 - val_accuracy: 0.8289
    Epoch 14/15
    123/123 [==============================] - 2s 14ms/step - loss: 63420.1562 - accuracy: 0.8323 - val_loss: 63050.5859 - val_accuracy: 0.8204
    Epoch 15/15
    123/123 [==============================] - 2s 14ms/step - loss: 62619.4531 - accuracy: 0.8345 - val_loss: 66933.7500 - val_accuracy: 0.8177
    Model training finished
    Validation accuracy: 81.77%
    

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åŸºçº¿æ¨¡å‹(å…¨è¿æ¥MLPç½‘ç»œ)å®ç°äº†çº¦ 82% çš„éªŒè¯å‡†ç¡®åº¦ã€‚

### â‘¤ æ¨¡å‹å®ç°2ï¼šTabTransformer

![](https://img-blog.csdnimg.cn/img_convert/4fa8084f7efb6ca6f4435d3ded8d922f.png)

TabTransformer æ¶æ„çš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š

*   æ‰€æœ‰ç±»åˆ«å‹ç‰¹å¾éƒ½è¢«ç¼–ç ä¸ºåµŒå…¥ï¼Œä½¿ç”¨ç›¸åŒçš„ `embedding_dims`ã€‚
*   å°†åˆ—åµŒå…¥ï¼ˆæ¯ä¸ªç±»åˆ«å‹ç‰¹å¾çš„ä¸€ä¸ªåµŒå…¥å‘é‡ï¼‰æ·»åŠ ç±»åˆ«å‹ç‰¹å¾åµŒå…¥ä¸­ã€‚
*   åµŒå…¥çš„ç±»åˆ«å‹ç‰¹å¾è¢«è¾“å…¥åˆ°ä¸€ç³»åˆ—çš„ Transformer å—ä¸­ã€‚ æ¯ä¸ª Transformer å—ç”±ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›å±‚å’Œä¸€ä¸ªå‰é¦ˆå±‚ç»„æˆã€‚
*   æœ€ç»ˆ Transformer å±‚çš„è¾“å‡ºï¼Œ ä¸è¾“å…¥çš„æ•°å€¼å‹ç‰¹å¾è¿æ¥ï¼Œå¹¶è¾“å…¥åˆ°æœ€ç»ˆçš„ MLP å—ä¸­ã€‚
*   å°¾éƒ¨ç”±ä¸€ä¸ª `softmax`ç»“æ„å®Œæˆåˆ†ç±»ã€‚

    def create_tabtransformer_classifier(
        num_transformer_blocks,
        num_heads,
        embedding_dims,
        mlp_hidden_units_factors,
        dropout_rate,
        use_column_embedding=False,
    ):
    
        # æ„å»ºè¾“å…¥
        inputs = create_model_inputs()
        # ç¼–ç ç‰¹å¾
        encoded_categorical_feature_list, numerical_feature_list = encode_inputs(
            inputs, embedding_dims
        )
        # å †å ç±»åˆ«å‹ç‰¹å¾çš„embeddingsï¼Œä¸ºè¾“å…¥Tansformeråšå‡†å¤‡
        encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)
        # æ‹¼æ¥æ•°å€¼å‹ç‰¹å¾
        numerical_features = layers.concatenate(numerical_feature_list)
    
        # embedding
        if use_column_embedding:
            num_columns = encoded_categorical_features.shape[1]
            column_embedding = layers.Embedding(
                input_dim=num_columns, output_dim=embedding_dims
            )
            column_indices = tf.range(start=0, limit=num_columns, delta=1)
            encoded_categorical_features = encoded_categorical_features + column_embedding(
                column_indices
            )
    
        # æ„å»ºTransformerå—
        for block_idx in range(num_transformer_blocks):
            # å¤šå¤´è‡ªæ³¨æ„åŠ›
            attention_output = layers.MultiHeadAttention(
                num_heads=num_heads,
                key_dim=embedding_dims,
                dropout=dropout_rate,
                name=f"multihead_attention_{block_idx}",
            )(encoded_categorical_features, encoded_categorical_features)
            # ç¬¬1ä¸ªè·³æ¥/Skip connection
            x = layers.Add(name=f"skip_connection1_{block_idx}")(
                [attention_output, encoded_categorical_features]
            )
            # ç¬¬1ä¸ªå±‚å½’ä¸€åŒ–/Layer normalization
            x = layers.LayerNormalization(name=f"layer_norm1_{block_idx}", epsilon=1e-6)(x)
            # å…¨è¿æ¥å±‚
            feedforward_output = create_mlp(
                hidden_units=[embedding_dims],
                dropout_rate=dropout_rate,
                activation=keras.activations.gelu,
                normalization_layer=layers.LayerNormalization(epsilon=1e-6),
                name=f"feedforward_{block_idx}",
            )(x)
            # ç¬¬2ä¸ªè·³æ¥/Skip connection
            x = layers.Add(name=f"skip_connection2_{block_idx}")([feedforward_output, x])
            # ç¬¬2ä¸ªå±‚å½’ä¸€åŒ–/Layer normalization
            encoded_categorical_features = layers.LayerNormalization(
                name=f"layer_norm2_{block_idx}", epsilon=1e-6
            )(x)
    
        # å±•å¹³embeddings
        categorical_features = layers.Flatten()(encoded_categorical_features)
        # å¯¹æ•°å€¼å‹ç‰¹å¾åšå±‚å½’ä¸€åŒ–
        numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)
        # æ‹¼æ¥ä½œä¸ºæœ€ç»ˆMLPçš„è¾“å…¥
        features = layers.concatenate([categorical_features, numerical_features])
    
        # è®¡ç®—MLPéšå±‚å•å…ƒ
        mlp_hidden_units = [
            factor * features.shape[-1] for factor in mlp_hidden_units_factors
        ]
        # æ„å»ºæœ€ç»ˆçš„MLP.
        features = create_mlp(
            hidden_units=mlp_hidden_units,
            dropout_rate=dropout_rate,
            activation=keras.activations.selu,
            normalization_layer=layers.BatchNormalization(),
            name="MLP",
        )(features)
    
        # æ·»åŠ sigmoidæ„å»ºäºŒåˆ†ç±»
        outputs = layers.Dense(units=1, activation="sigmoid", name="sigmoid")(features)
        model = keras.Model(inputs=inputs, outputs=outputs)
        return model
    
    
    tabtransformer_model = create_tabtransformer_classifier(
        num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,
        num_heads=NUM_HEADS,
        embedding_dims=EMBEDDING_DIMS,
        mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,
        dropout_rate=DROPOUT_RATE,
    )
    
    print("Total model weights:", tabtransformer_model.count_params())
    keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir="LR")
    #Total model weights: 87479
    

æœ€ç»ˆè¾“å‡ºçš„æ¨¡å‹ç»“æ„ç¤ºæ„å›¾å¦‚ä¸‹ï¼ˆå› ä¸ºæ¨¡å‹ç»“æ„è¾ƒæ·±ï¼Œæ€»ä½“å¾ˆé•¿ï¼Œç‚¹å‡»æ”¾å¤§ï¼‰

![](https://img-blog.csdnimg.cn/img_convert/8ab5400ab55c8051c6bcd6ee82bd9da5.png)

ä¸‹é¢æˆ‘ä»¬è®­ç»ƒå’Œè¯„ä¼°ä¸€ä¸‹TabTransformer æ¨¡å‹çš„æ•ˆæœï¼š

    history = run_experiment(
        model=tabtransformer_model,
        train_data_file=train_data_file,
        test_data_file=test_data_file,
        num_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        batch_size=BATCH_SIZE,
    )
    Start training the model...
    Epoch 1/15
    123/123 [==============================] - 13s 61ms/step - loss: 82503.1641 - accuracy: 0.7944 - val_loss: 64260.2305 - val_accuracy: 0.8421
    Epoch 2/15
    123/123 [==============================] - 6s 51ms/step - loss: 68677.9375 - accuracy: 0.8251 - val_loss: 63819.8633 - val_accuracy: 0.8389
    Epoch 3/15
    123/123 [==============================] - 6s 51ms/step - loss: 66703.8984 - accuracy: 0.8301 - val_loss: 63052.8789 - val_accuracy: 0.8428
    Epoch 4/15
    123/123 [==============================] - 6s 51ms/step - loss: 65287.8672 - accuracy: 0.8342 - val_loss: 61593.1484 - val_accuracy: 0.8451
    Epoch 5/15
    123/123 [==============================] - 6s 52ms/step - loss: 63968.8594 - accuracy: 0.8379 - val_loss: 61385.4531 - val_accuracy: 0.8442
    Epoch 6/15
    123/123 [==============================] - 6s 51ms/step - loss: 63645.7812 - accuracy: 0.8394 - val_loss: 61332.3281 - val_accuracy: 0.8447
    Epoch 7/15
    123/123 [==============================] - 6s 51ms/step - loss: 62778.6055 - accuracy: 0.8412 - val_loss: 61342.5352 - val_accuracy: 0.8461
    Epoch 8/15
    123/123 [==============================] - 6s 51ms/step - loss: 62815.6992 - accuracy: 0.8398 - val_loss: 61220.8242 - val_accuracy: 0.8460
    Epoch 9/15
    123/123 [==============================] - 6s 52ms/step - loss: 62191.1016 - accuracy: 0.8416 - val_loss: 61055.9102 - val_accuracy: 0.8452
    Epoch 10/15
    123/123 [==============================] - 6s 51ms/step - loss: 61992.1602 - accuracy: 0.8439 - val_loss: 61251.8047 - val_accuracy: 0.8441
    Epoch 11/15
    123/123 [==============================] - 6s 50ms/step - loss: 61745.1289 - accuracy: 0.8429 - val_loss: 61364.7695 - val_accuracy: 0.8445
    Epoch 12/15
    123/123 [==============================] - 6s 51ms/step - loss: 61696.3477 - accuracy: 0.8445 - val_loss: 61074.3594 - val_accuracy: 0.8450
    Epoch 13/15
    123/123 [==============================] - 6s 51ms/step - loss: 61569.1719 - accuracy: 0.8436 - val_loss: 61844.9688 - val_accuracy: 0.8456
    Epoch 14/15
    123/123 [==============================] - 6s 51ms/step - loss: 61343.0898 - accuracy: 0.8445 - val_loss: 61702.8828 - val_accuracy: 0.8455
    Epoch 15/15
    123/123 [==============================] - 6s 51ms/step - loss: 61355.0547 - accuracy: 0.8504 - val_loss: 61272.2852 - val_accuracy: 0.8495
    Model training finished
    Validation accuracy: 84.55%
    

TabTransformer æ¨¡å‹å®ç°äº†çº¦ 85% çš„éªŒè¯å‡†ç¡®åº¦ï¼Œç›¸æ¯”äºç›´æ¥ä½¿ç”¨å…¨è¿æ¥ç½‘ç»œæ•ˆæœæœ‰ä¸€å®šçš„æå‡ã€‚

å‚è€ƒèµ„æ–™
====

*   ğŸ“˜ **TabTransformer**ï¼š[https://arxiv.org/abs/2012.06678](https://arxiv.org/abs/2012.06678)
*   ğŸ“˜ **TensorFlowæ’ä»¶addons**ï¼š[https://www.tensorflow.org/addons/overview](https://www.tensorflow.org/addons/overview)
*   ğŸ“˜**AIå‚ç›´é¢†åŸŸå·¥å…·åº“é€ŸæŸ¥è¡¨ | TensorFlow2å»ºæ¨¡é€ŸæŸ¥&åº”ç”¨é€ŸæŸ¥**ï¼š[https://www.showmeai.tech/article-detail/109](https://www.showmeai.tech/article-detail/109)

[![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e9190f41b8de4af38c8a1a0c96f0513b~tplv-k3u1fbpfcp-zoom-1.image)](https://www.showmeai.tech/)