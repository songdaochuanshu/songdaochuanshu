---
layout: post
title: "【机器学习】李宏毅——线性降维"
date: "2022-12-16T13:19:48.312Z"
---
【机器学习】李宏毅——线性降维
===============

![【机器学习】李宏毅——线性降维](https://img2023.cnblogs.com/blog/2966067/202212/2966067-20221216192323228-382767304.png) 关于李宏毅老师的线性降维课程章节的学习内容记录，包括完整知识脉络及思路

降维，可以用下面这张图来很简单的描述，**就是将不同的、复杂的多种树都抽象成最简单的树的描述，也就是我们不关心这棵树长什么样子有什么特别的特征，我们只需要降维，知道它是一棵树即可**。  
![在这里插入图片描述](https://img-blog.csdnimg.cn/51dda2d2160e41d69960be3cd7395115.png#pic_center)

维度下降实际上就是**找到一个function，使得输入x得到输出z，而输出z的维度要比输入x的维度小**。具体有几种方面，下面就先将PCA（主成分分析）

### PCA

PCA认为，function实际上可以看成一个矩阵，即：

\\\[z=Wx \\\]

可以通过一个向量与矩阵的运算来描述这件事。那么当前假设x为二维向量，而要求降到一维的向量z，而w的范数等于1，则可以看成**z就是x在w上的投影**

![在这里插入图片描述](https://img-blog.csdnimg.cn/2808f5943f0e4b05953727193daec6d8.png#pic_center)

不同方向的w会导致投影出来的z不一样，因此我们的目标是**找到一个w，它能够使得投影之后的z的差异性能够最大，而不是都挤在一起**，如下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/edd7ea2b8ea641739d3d96ac940ddfc6.png#pic_center)

那么如果是降到多维的话也是同理，**首先先找某一维度能够让对应方差最大，再找另一维度能够让对应方差最大且\\(w^2\\)与\\(w^1\\)是正交的，以此类推**：

![在这里插入图片描述](https://img-blog.csdnimg.cn/5b2686272e6c436e9d4a6030eb0d53bc.png#pic_center)

最终得到的\\(W\\)是**正交矩阵**。

而经过一系列的推导（此部分推导可以具体看影片，之后我也会把统计学习方法中的推导过程补上）得到的结论就是：**假设x的协方差矩阵为\\(S=Cov(x)\\)，而\\(w^i\\)就是矩阵S所有特征值中第i大的特征值对应的特征向量**。

#### 从另一种角度来看PCA

假设当前**有许多个基础的组成成分，而我们每张图片都可以看成是由多个组成成分和一个基础的均值（全数样本的均值）相加而成的，看成在图片均值的基础上拥有自身差异性的部分，那么根据该图片中是否有对应的基础成分就可以写出来一个向量\\(C=\[c\_1,c\_2,...\]\\)，如果组成成分不是特别多而图片特别多，那我们用向量C来表示一张图片是非常有意义的**。

![在这里插入图片描述](https://img-blog.csdnimg.cn/f747c797cfa1476a94e3999343700fae.png#pic_center)

那么就可以写成

![在这里插入图片描述](https://img-blog.csdnimg.cn/8a1946b3fc3e42b298d30c75e06ee447.png#pic_center)

因此我们的思路就转换成**找到一组向量\\(\\{u^1,u^2,...,u^k\\}\\)能够使得 \\(x-\\bar{x}\\)与\\(\\hat{x}\\)之间的距离最小化**，即

![在这里插入图片描述](https://img-blog.csdnimg.cn/d1c11db978fe45589e5a395045c74f30.png#pic_center)

而可以证明，**由PCA方法找出来的向量组\\(\\{w^1,w^2,...,w^k\\}\\)就是我们要找的目标向量组**。而将上述运算转换成矩阵形式：

![在这里插入图片描述](https://img-blog.csdnimg.cn/6ab19efc8db341b393ebe0300d908b23.png#pic_center)

目标就是**找出矩阵u和矩阵c，使得他们相乘之后和矩阵X的差距最小**。而回顾学过的SVD，矩阵X可以进行分解：

![在这里插入图片描述](https://img-blog.csdnimg.cn/b9b67b807b024a2287525f82f271333c.png#pic_center)

**其中矩阵U的k个列向量就是矩阵\\(XX^T\\)的最大的前k个特征向量，而\\(XX^T\\)就是\\(S=Cov(x)\\)，因此这也就是我们想要找到\\(\\{w^1,w^2,...w^k\\}\\)**，因此只要进行SVD分解就可以找到目标的\\(\\{u^1,u^2,...u^k\\}\\)。那么下一个问题就是求解矩阵C。

由于我们已知了矩阵U，那么对于某个样本，就有：

\\\[\\hat{x}\_i=\\sum\_{k=1}^K c^i\_kw^k\\approx x\_i - \\bar{x} \\\]

而由于**矩阵W中每一个向量都是相互正交的**，就可以有：

\\\[c\_k^i=(x\_i-\\bar{x})·w^k \\\]

(此部分我也不太理解怎么推导出来的，希望会的大神教教)。接下来就可以将这一个过程用神经网络的形式来表示：（注意图中应该是c应该是下标不是上标）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/8fb56fe5f9f74c849be2d7e057409d76.png#pic_center)

**但是如果现在我们不是从SVD中解出矩阵W，而是从这神经网络之中来进行梯度下降求解，这样求出来的结果和用SVD求出来的结果是不一样的！因为在SVD中求出来的结果还有限制它们彼此之间是正交的，而神经网络是没有的。而将PCA看成是具有一个隐含层的神经网络的方式称为Autoencoder**。

但其实用SVD的方式求解起来更快更好，而要学习这种神经网络的方式是因为其隐含层可以加层来实现更复杂的操作。

#### PCA的缺点

首先是无监督性质，如果样本本身就具有一定的类别信息，那么就会出现下面这样的问题：

![在这里插入图片描述](https://img-blog.csdnimg.cn/06d68d206a7e4683bd11ce7559ac9164.png#pic_center)

**可以看到如果有类别区分那么做PCA就会将它们混淆在一起**。

其次它是线性变换，无法做非线性的事情：

![在这里插入图片描述](https://img-blog.csdnimg.cn/bda4b47e43fa45edb55e6936a6472f94.png#pic_center)

#### PCA的应用

如果将PCA对人脸数据进行处理：

![在这里插入图片描述](https://img-blog.csdnimg.cn/2d7222e703e54c25813d2eb9c4318963.png#pic_center)

可以看到**各个出来的特征向量并不是我们想象中的基本组成部分（比如嘴巴什么的），更像是一张完整的脸**，这是为什么呢？

![在这里插入图片描述](https://img-blog.csdnimg.cn/743309e13560406dbc9b22717a2c6533.png#pic_center)

**因为这些向量的组成成分的加权数字并不一定是正的，如果是负数就相当于先画出一个很复杂的东西然后再减去某个元素**。这样就很不直观。如果想要加权的参数都是正的，**可以采用NMF，它能够使得参数\\(a\_i\\)都是正的，其次是每个\\(w^i\\)里面每个维度的数值都是正的，这是PCA无法保证的，因为在图像中\\(w^i\\)就象征第i个组成部分的图像，如果某个像素是负的那将无法处理**，PCA就会出现这个问题。

![在这里插入图片描述](https://img-blog.csdnimg.cn/0d00e703c05e4d1f853977c8f466ef36.png#pic_center)  
![在这里插入图片描述](https://img-blog.csdnimg.cn/73aab23da14d4ba7abc051ecdf005437.png#pic_center)

能够更明显地看出组成部分，符合我们的预期。