---
layout: post
title: "史上最全学习率调整策略lr_scheduler"
date: "2022-07-06T19:15:02.701Z"
---
史上最全学习率调整策略lr\_scheduler
========================

> 学习率是深度学习训练中至关重要的参数，很多时候一个合适的学习率才能发挥出模型的较大潜力。所以学习率调整策略同样至关重要，这篇博客介绍一下Pytorch中常见的学习率调整方法。

    import torch
    import numpy as np
    from torch.optim import SGD
    from torch.optim import lr_scheduler
    from torch.nn.parameter import Parameter
    
    model = [Parameter(torch.randn(2, 2, requires_grad=True))]
    optimizer = SGD(model, lr=0.1)
    

以上是一段通用代码，这里将基础学习率设置为0.1。接下来仅仅展示学习率调节器的代码，以及对应的学习率曲线。

1\. StepLR
----------

这是最简单常用的学习率调整方法，每过step\_size轮，将此前的学习率乘以gamma。

    scheduler=lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/e656b8114f9a4ca2b6d507cffef9221d.png#pic_center)

2\. MultiStepLR
---------------

MultiStepLR同样也是一个非常常见的学习率调整策略，它会在每个milestone时，将此前学习率乘以gamma。

    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.5)
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/e73fc155053a43899aeb74d75a5b1c2a.png#pic_center)

3\. ExponentialLR
-----------------

ExponentialLR是指数型下降的学习率调节器，每一轮会将学习率乘以gamma，所以这里千万注意gamma不要设置的太小，不然几轮之后学习率就会降到0。

    scheduler=lr_scheduler.ExponentialLR(optimizer, gamma=0.9) 
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/893832d626814558b6495c2aaa0cd717.png#pic_center)

4\. LinearLR
------------

LinearLR是线性学习率，给定起始factor和最终的factor，LinearLR会在中间阶段做线性插值，比如学习率为0.1，起始factor为1，最终的factor为0.1，那么第0次迭代，学习率将为0.1，最终轮学习率为0.01。下面设置的总轮数total\_iters为80,所以超过80时，学习率恒为0.01。

    scheduler=lr_scheduler.LinearLR(optimizer,start_factor=1,end_factor=0.1,total_iters=80)
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/fb6f0d57e4d94ffcb6fc26f63365317f.png#pic_center)

5\. CyclicLR
------------

    scheduler=lr_scheduler.CyclicLR(optimizer,base_lr=0.1,max_lr=0.2,step_size_up=30,step_size_down=10)
    

CyclicLR的参数要更多一些，它的曲线看起来就像是不断的上坡与下坡，base\_lr为谷底的学习率，max\_lr为顶峰的学习率，step\_size\_up是从谷底到顶峰需要的轮数，step\_size\_down时从顶峰到谷底的轮数。至于为啥这样设置，可以参见[论文](https://arxiv.org/pdf/1506.01186.pdf),简单来说最佳学习率会在base\_lr和max\_lr，CyclicLR不是一味衰减而是出现增大的过程是为了避免陷入鞍点。

    scheduler=lr_scheduler.CyclicLR(optimizer,base_lr=0.1,max_lr=0.2,step_size_up=30,step_size_down=10)
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/2dbf8e9bbc5e4689adf50764c9f54ded.png#pic_center)

6\. OneCycleLR
--------------

OneCycleLR顾名思义就像是CyclicLR的一周期版本，它也有多个参数，max\_lr就是最大学习率，pct\_start是学习率上升部分所占比例，一开始的学习率为max\_lr/div\_factor,最终的学习率为max\_lr/final\_div\_factor，总的迭代次数为total\_steps。

    scheduler=lr_scheduler.OneCycleLR(optimizer,max_lr=0.1,pct_start=0.5,total_steps=120,div_factor=10,final_div_factor=10)
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/b6e68ddbeb654d5789ddbb968c82280c.png#pic_center)

7\. CosineAnnealingLR
---------------------

CosineAnnealingLR是余弦退火学习率，T\_max是周期的一半，最大学习率在optimizer中指定，最小学习率为eta\_min。这里同样能够帮助逃离鞍点。值得注意的是最大学习率不宜太大，否则loss可能出现和学习率相似周期的上下剧烈波动。

    scheduler=lr_scheduler.CosineAnnealingLR(optimizer,T_max=20,eta_min=0.05)
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/f9e7040ec6724a049be095e3028e681b.png#pic_center)

7\. CosineAnnealingWarmRestarts
-------------------------------

这里相对负责一些，公式如下，其中T\_0是第一个周期，会从optimizer中的学习率下降至eta\_min，之后的每个周期变成了前一周期乘以T\_mult。

\\(eta\_t = \\eta\_{min} + \\frac{1}{2}(\\eta\_{max} - \\eta\_{min})\\left(1 + \\cos\\left(\\frac{T\_{cur}}{T\_{i}}\\pi\\right)\\right)\\)

    scheduler=lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2, eta_min=0.01)
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/a98e033f647943af857d40d457942f9a.png#pic_center)

8\. LambdaLR
------------

LambdaLR其实没有固定的学习率曲线，名字中的lambda指的是可以将学习率自定义为一个有关epoch的lambda函数，比如下面我们定义了一个指数函数，实现了ExponentialLR的功能。

    scheduler=lr_scheduler.LambdaLR(optimizer,lr_lambda=lambda epoch:0.9**epoch)
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/48f130a070a54ebcbb18f7d54e384252.png#pic_center)

9.SequentialLR
--------------

SequentialLR可以将多个学习率调整策略按照顺序串联起来,在milestone时切换到下一个学习率调整策略。下面就是将一个指数衰减的学习率和线性衰减的学习率结合起来。

    scheduler=lr_scheduler.SequentialLR(optimizer,schedulers=[lr_scheduler.ExponentialLR(optimizer, gamma=0.9),lr_scheduler.LinearLR(optimizer,start_factor=1,end_factor=0.1,total_iters=80)],milestones=[50])
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/65631328a5b34a91bde300816787c232.png#pic_center)

10.ChainedScheduler
-------------------

ChainedScheduler和SequentialLR类似，也是按照顺序调用多个串联起来的学习率调整策略，不同的是ChainedScheduler里面的学习率变化是连续的。

    scheduler=lr_scheduler.ChainedScheduler([lr_scheduler.LinearLR(optimizer,start_factor=1,end_factor=0.5,total_iters=10),lr_scheduler.ExponentialLR(optimizer, gamma=0.95)])
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/3b8e5c4aa3b74b51b8f6b0d3881f322e.png#pic_center)

11.ConstantLR
-------------

ConstantLRConstantLR非常简单，在total\_iters轮内将optimizer里面指定的学习率乘以factor,total\_iters轮外恢复原学习率。

    scheduler=lr_scheduler.ConstantLRConstantLR(optimizer,factor=0.5,total_iters=80)
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/3528eda7f90d4dea943e0eccbe41f2e5.png#pic_center)

12.ReduceLROnPlateau
--------------------

ReduceLROnPlateau参数非常多，其功能是自适应调节学习率，它在step的时候会观察验证集上的loss或者准确率情况，loss当然是越低越好，准确率则是越高越好，所以使用loss作为step的参数时，mode为min，使用准确率作为参数时，mode为max。factor是每次学习率下降的比例，新的学习率等于老的学习率乘以factor。patience是能够容忍的次数，当patience次后，网络性能仍未提升，则会降低学习率。threshold是测量最佳值的阈值，一般只关注相对大的性能提升。min\_lr是最小学习率，eps指最小的学习率变化，当新旧学习率差别小于eps时，维持学习率不变。  
因为参数相对复杂，这里可以看一份完整的代码[实操](https://github.com/milesial/Pytorch-UNet/blob/master/train.py#L69)。

    scheduler=lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.5,patience=5,threshold=1e-4,threshold_mode='abs',cooldown=0,min_lr=0.001,eps=1e-8)
    scheduler.step(val_score)