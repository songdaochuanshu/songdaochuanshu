---
layout: post
title: "关于linux的一点好奇心（五）：进程线程的创建"
date: "2022-09-25T10:23:07.506Z"
---
关于linux的一点好奇心（五）：进程线程的创建
========================

　　一直以来，进程和线程的区别，这种问题一般会被面试官拿来考考面试者，可见这事就不太简单。简单说一点差异是，进程拥有独立的内存资源信息，而线程则共享父进程的资源信息。也就是说线程不拥有内存资源，所以对系统消耗会更小。所以，线程也有轻量级进程的说法。

　　除了从资源消耗的角度来讲进程线程的差别，还有一个值得说明的是，内存的可见性问题。进程与进程间的资源是隔离的，所以要想共享变量之类的，是很难做到的，但是共享线程变量则容易许多，因为多线程共享内存资源，所以一个线程对内存的改变，其他线程是可以感知到的。这就给应用做一些共享和并发控制带来了很大的方便（其实也有进程间共享信息，进程间通信，只是太重了，应用层无法承受之重）。所以，一般的编程语言都会提供多线程的创建和使用方法。

　　那么，进程线程是如何创建的呢？我们知道，所有上层语言，都会依赖于操作系统底层的基础实现，进线程的创建作为非常核心的概念，自然也是最终要转给操作系统处理。所以，我们想看看操作系统是如何创建进程线程的。

### 1\. linux创建进程概述

　　linux中进程和线程，拥有同样的数据结构，也就是说没有特别区分进程和线程的差别。这样做的好处是，进程和线程都是作为独立的调度单元对待，不需要特殊处理，减少了调度算法的复杂性。

　　一般的，我们可以通过 fork() 函数直接创建进程，通过 pthread\_create() 函数进行线程的创建，当然这些都是外层的api函数。我们可以以此二者作为入口进行进线程的创建研究。

　　进线程作为进程调度的单元，它自然有一个对应的描述类，即 task\_struct, 它描述了一个进程到底有哪些内容和能力，是一个非常重要的抽象。

// include/linux/sched.h
struct task\_struct {
#ifdef CONFIG\_THREAD\_INFO\_IN\_TASK
    /\*
     \* For reasons of header soup (see current\_thread\_info()), this
     \* must be the first element of task\_struct.
     \*/
    struct thread\_info        thread\_info;
#endif
    /\* -1 unrunnable, 0 runnable, >0 stopped: \*/
    volatile long            state;

    /\*
     \* This begins the randomizable portion of task\_struct. Only
     \* scheduling-critical items should be added above here.
     \*/
    randomized\_struct\_fields\_start

    void                \*stack;
    atomic\_t            usage;
    /\* Per task flags (PF\_\*), defined further below: \*/
    unsigned int            flags;
    unsigned int            ptrace;

#ifdef CONFIG\_SMP
    struct llist\_node        wake\_entry;
    int                on\_cpu;
#ifdef CONFIG\_THREAD\_INFO\_IN\_TASK
    /\* Current CPU: \*/
    unsigned int            cpu;
#endif
    unsigned int            wakee\_flips;
    unsigned long            wakee\_flip\_decay\_ts;
    struct task\_struct        \*last\_wakee;

    /\*
     \* recent\_used\_cpu is initially set as the last CPU used by a task
     \* that wakes affine another task. Waker/wakee relationships can
     \* push tasks around a CPU where each wakeup moves to the next one.
     \* Tracking a recently used CPU allows a quick search for a recently
     \* used CPU that may be idle.
     \*/
    int                recent\_used\_cpu;
    int                wake\_cpu;
#endif
    int                on\_rq;

    int                prio;
    int                static\_prio;
    int                normal\_prio;
    unsigned int            rt\_priority;

    const struct sched\_class    \*sched\_class;
    struct sched\_entity        se;
    struct sched\_rt\_entity        rt;
#ifdef CONFIG\_CGROUP\_SCHED
    struct task\_group        \*sched\_task\_group;
#endif
    struct sched\_dl\_entity        dl;

#ifdef CONFIG\_PREEMPT\_NOTIFIERS
    /\* List of struct preempt\_notifier: \*/
    struct hlist\_head        preempt\_notifiers;
#endif

#ifdef CONFIG\_BLK\_DEV\_IO\_TRACE
    unsigned int            btrace\_seq;
#endif

    unsigned int            policy;
    int                nr\_cpus\_allowed;
    cpumask\_t            cpus\_allowed;

#ifdef CONFIG\_PREEMPT\_RCU
    int                rcu\_read\_lock\_nesting;
    union rcu\_special        rcu\_read\_unlock\_special;
    struct list\_head        rcu\_node\_entry;
    struct rcu\_node            \*rcu\_blocked\_node;
#endif /\* #ifdef CONFIG\_PREEMPT\_RCU \*/

#ifdef CONFIG\_TASKS\_RCU
    unsigned long            rcu\_tasks\_nvcsw;
    u8                rcu\_tasks\_holdout;
    u8                rcu\_tasks\_idx;
    int                rcu\_tasks\_idle\_cpu;
    struct list\_head        rcu\_tasks\_holdout\_list;
#endif /\* #ifdef CONFIG\_TASKS\_RCU \*/

    struct sched\_info        sched\_info;

    struct list\_head        tasks;
#ifdef CONFIG\_SMP
    struct plist\_node        pushable\_tasks;
    struct rb\_node            pushable\_dl\_tasks;
#endif

    struct mm\_struct        \*mm;
    struct mm\_struct        \*active\_mm;

    /\* Per-thread vma caching: \*/
    struct vmacache            vmacache;

#ifdef SPLIT\_RSS\_COUNTING
    struct task\_rss\_stat        rss\_stat;
#endif
    int                exit\_state;
    int                exit\_code;
    int                exit\_signal;
    /\* The signal sent when the parent dies: \*/
    int                pdeath\_signal;
    /\* JOBCTL\_\*, siglock protected: \*/
    unsigned long            jobctl;

    /\* Used for emulating ABI behavior of previous Linux versions: \*/
    unsigned int            personality;

    /\* Scheduler bits, serialized by scheduler locks: \*/
    unsigned            sched\_reset\_on\_fork:1;
    unsigned            sched\_contributes\_to\_load:1;
    unsigned            sched\_migrated:1;
    unsigned            sched\_remote\_wakeup:1;
    /\* Force alignment to the next boundary: \*/
    unsigned            :0;

    /\* Unserialized, strictly 'current' \*/

    /\* Bit to tell LSMs we're in execve(): \*/
    unsigned            in\_execve:1;
    unsigned            in\_iowait:1;
#ifndef TIF\_RESTORE\_SIGMASK
    unsigned            restore\_sigmask:1;
#endif
#ifdef CONFIG\_MEMCG
    unsigned            memcg\_may\_oom:1;
#ifndef CONFIG\_SLOB
    unsigned            memcg\_kmem\_skip\_account:1;
#endif
#endif
#ifdef CONFIG\_COMPAT\_BRK
    unsigned            brk\_randomized:1;
#endif
#ifdef CONFIG\_CGROUPS
    /\* disallow userland-initiated cgroup migration \*/
    unsigned            no\_cgroup\_migration:1;
#endif

    unsigned long            atomic\_flags; /\* Flags requiring atomic access. \*/

    struct restart\_block        restart\_block;

    pid\_t                pid;
    pid\_t                tgid;

#ifdef CONFIG\_STACKPROTECTOR
    /\* Canary value for the -fstack-protector GCC feature: \*/
    unsigned long            stack\_canary;
#endif
    /\*
     \* Pointers to the (original) parent process, youngest child, younger sibling,
     \* older sibling, respectively.  (p->father can be replaced with
     \* p->real\_parent->pid)
     \*/

    /\* Real parent process: \*/
    struct task\_struct \_\_rcu    \*real\_parent;

    /\* Recipient of SIGCHLD, wait4() reports: \*/
    struct task\_struct \_\_rcu    \*parent;

    /\*
     \* Children/sibling form the list of natural children:
     \*/
    struct list\_head        children;
    struct list\_head        sibling;
    struct task\_struct        \*group\_leader;

    /\*
     \* 'ptraced' is the list of tasks this task is using ptrace() on.
     \*
     \* This includes both natural children and PTRACE\_ATTACH targets.
     \* 'ptrace\_entry' is this task's link on the p->parent->ptraced list.
     \*/
    struct list\_head        ptraced;
    struct list\_head        ptrace\_entry;

    /\* PID/PID hash table linkage. \*/
    struct pid\_link            pids\[PIDTYPE\_MAX\];
    struct list\_head        thread\_group;
    struct list\_head        thread\_node;

    struct completion        \*vfork\_done;

    /\* CLONE\_CHILD\_SETTID: \*/
    int \_\_user            \*set\_child\_tid;

    /\* CLONE\_CHILD\_CLEARTID: \*/
    int \_\_user            \*clear\_child\_tid;

    u64                utime;
    u64                stime;
#ifdef CONFIG\_ARCH\_HAS\_SCALED\_CPUTIME
    u64                utimescaled;
    u64                stimescaled;
#endif
    u64                gtime;
    struct prev\_cputime        prev\_cputime;
#ifdef CONFIG\_VIRT\_CPU\_ACCOUNTING\_GEN
    struct vtime            vtime;
#endif

#ifdef CONFIG\_NO\_HZ\_FULL
    atomic\_t            tick\_dep\_mask;
#endif
    /\* Context switch counts: \*/
    unsigned long            nvcsw;
    unsigned long            nivcsw;

    /\* Monotonic time in nsecs: \*/
    u64                start\_time;

    /\* Boot based time in nsecs: \*/
    u64                real\_start\_time;

    /\* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: \*/
    unsigned long            min\_flt;
    unsigned long            maj\_flt;

#ifdef CONFIG\_POSIX\_TIMERS
    struct task\_cputime        cputime\_expires;
    struct list\_head        cpu\_timers\[3\];
#endif

    /\* Process credentials: \*/

    /\* Tracer's credentials at attach: \*/
    const struct cred \_\_rcu        \*ptracer\_cred;

    /\* Objective and real subjective task credentials (COW): \*/
    const struct cred \_\_rcu        \*real\_cred;

    /\* Effective (overridable) subjective task credentials (COW): \*/
    const struct cred \_\_rcu        \*cred;

    /\*
     \* executable name, excluding path.
     \*
     \* - normally initialized setup\_new\_exec()
     \* - access it with \[gs\]et\_task\_comm()
     \* - lock it with task\_lock()
     \*/
    char                comm\[TASK\_COMM\_LEN\];

    struct nameidata        \*nameidata;

#ifdef CONFIG\_SYSVIPC
    struct sysv\_sem            sysvsem;
    struct sysv\_shm            sysvshm;
#endif
#ifdef CONFIG\_DETECT\_HUNG\_TASK
    unsigned long            last\_switch\_count;
#endif
    /\* Filesystem information: \*/
    struct fs\_struct        \*fs;

    /\* Open file information: \*/
    struct files\_struct        \*files;

    /\* Namespaces: \*/
    struct nsproxy            \*nsproxy;

    /\* Signal handlers: \*/
    struct signal\_struct        \*signal;
    struct sighand\_struct        \*sighand;
    sigset\_t            blocked;
    sigset\_t            real\_blocked;
    /\* Restored if set\_restore\_sigmask() was used: \*/
    sigset\_t            saved\_sigmask;
    struct sigpending        pending;
    unsigned long            sas\_ss\_sp;
    size\_t                sas\_ss\_size;
    unsigned int            sas\_ss\_flags;

    struct callback\_head        \*task\_works;

    struct audit\_context        \*audit\_context;
#ifdef CONFIG\_AUDITSYSCALL
    kuid\_t                loginuid;
    unsigned int            sessionid;
#endif
    struct seccomp            seccomp;

    /\* Thread group tracking: \*/
    u32                parent\_exec\_id;
    u32                self\_exec\_id;

    /\* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems\_allowed, mempolicy: \*/
    spinlock\_t            alloc\_lock;

    /\* Protection of the PI data structures: \*/
    raw\_spinlock\_t            pi\_lock;

    struct wake\_q\_node        wake\_q;

#ifdef CONFIG\_RT\_MUTEXES
    /\* PI waiters blocked on a rt\_mutex held by this task: \*/
    struct rb\_root\_cached        pi\_waiters;
    /\* Updated under owner's pi\_lock and rq lock \*/
    struct task\_struct        \*pi\_top\_task;
    /\* Deadlock detection and priority inheritance handling: \*/
    struct rt\_mutex\_waiter        \*pi\_blocked\_on;
#endif

#ifdef CONFIG\_DEBUG\_MUTEXES
    /\* Mutex deadlock detection: \*/
    struct mutex\_waiter        \*blocked\_on;
#endif

#ifdef CONFIG\_TRACE\_IRQFLAGS
    unsigned int            irq\_events;
    unsigned long            hardirq\_enable\_ip;
    unsigned long            hardirq\_disable\_ip;
    unsigned int            hardirq\_enable\_event;
    unsigned int            hardirq\_disable\_event;
    int                hardirqs\_enabled;
    int                hardirq\_context;
    unsigned long            softirq\_disable\_ip;
    unsigned long            softirq\_enable\_ip;
    unsigned int            softirq\_disable\_event;
    unsigned int            softirq\_enable\_event;
    int                softirqs\_enabled;
    int                softirq\_context;
#endif

#ifdef CONFIG\_LOCKDEP
# define MAX\_LOCK\_DEPTH            48UL
    u64                curr\_chain\_key;
    int                lockdep\_depth;
    unsigned int            lockdep\_recursion;
    struct held\_lock        held\_locks\[MAX\_LOCK\_DEPTH\];
#endif

#ifdef CONFIG\_UBSAN
    unsigned int            in\_ubsan;
#endif

    /\* Journalling filesystem info: \*/
    void                \*journal\_info;

    /\* Stacked block device info: \*/
    struct bio\_list            \*bio\_list;

#ifdef CONFIG\_BLOCK
    /\* Stack plugging: \*/
    struct blk\_plug            \*plug;
#endif

    /\* VM state: \*/
    struct reclaim\_state        \*reclaim\_state;

    struct backing\_dev\_info        \*backing\_dev\_info;

    struct io\_context        \*io\_context;

    /\* Ptrace state: \*/
    unsigned long            ptrace\_message;
    siginfo\_t            \*last\_siginfo;

    struct task\_io\_accounting    ioac;
#ifdef CONFIG\_TASK\_XACCT
    /\* Accumulated RSS usage: \*/
    u64                acct\_rss\_mem1;
    /\* Accumulated virtual memory usage: \*/
    u64                acct\_vm\_mem1;
    /\* stime + utime since last update: \*/
    u64                acct\_timexpd;
#endif
#ifdef CONFIG\_CPUSETS
    /\* Protected by ->alloc\_lock: \*/
    nodemask\_t            mems\_allowed;
    /\* Seqence number to catch updates: \*/
    seqcount\_t            mems\_allowed\_seq;
    int                cpuset\_mem\_spread\_rotor;
    int                cpuset\_slab\_spread\_rotor;
#endif
#ifdef CONFIG\_CGROUPS
    /\* Control Group info protected by css\_set\_lock: \*/
    struct css\_set \_\_rcu        \*cgroups;
    /\* cg\_list protected by css\_set\_lock and tsk->alloc\_lock: \*/
    struct list\_head        cg\_list;
#endif
#ifdef CONFIG\_INTEL\_RDT
    u32                closid;
    u32                rmid;
#endif
#ifdef CONFIG\_FUTEX
    struct robust\_list\_head \_\_user    \*robust\_list;
#ifdef CONFIG\_COMPAT
    struct compat\_robust\_list\_head \_\_user \*compat\_robust\_list;
#endif
    struct list\_head        pi\_state\_list;
    struct futex\_pi\_state        \*pi\_state\_cache;
#endif
#ifdef CONFIG\_PERF\_EVENTS
    struct perf\_event\_context    \*perf\_event\_ctxp\[perf\_nr\_task\_contexts\];
    struct mutex            perf\_event\_mutex;
    struct list\_head        perf\_event\_list;
#endif
#ifdef CONFIG\_DEBUG\_PREEMPT
    unsigned long            preempt\_disable\_ip;
#endif
#ifdef CONFIG\_NUMA
    /\* Protected by alloc\_lock: \*/
    struct mempolicy        \*mempolicy;
    short                il\_prev;
    short                pref\_node\_fork;
#endif
#ifdef CONFIG\_NUMA\_BALANCING
    int                numa\_scan\_seq;
    unsigned int            numa\_scan\_period;
    unsigned int            numa\_scan\_period\_max;
    int                numa\_preferred\_nid;
    unsigned long            numa\_migrate\_retry;
    /\* Migration stamp: \*/
    u64                node\_stamp;
    u64                last\_task\_numa\_placement;
    u64                last\_sum\_exec\_runtime;
    struct callback\_head        numa\_work;

    struct list\_head        numa\_entry;
    struct numa\_group        \*numa\_group;

    /\*
     \* numa\_faults is an array split into four regions:
     \* faults\_memory, faults\_cpu, faults\_memory\_buffer, faults\_cpu\_buffer
     \* in this precise order.
     \*
     \* faults\_memory: Exponential decaying average of faults on a per-node
     \* basis. Scheduling placement decisions are made based on these
     \* counts. The values remain static for the duration of a PTE scan.
     \* faults\_cpu: Track the nodes the process was running on when a NUMA
     \* hinting fault was incurred.
     \* faults\_memory\_buffer and faults\_cpu\_buffer: Record faults per node
     \* during the current scan window. When the scan completes, the counts
     \* in faults\_memory and faults\_cpu decay and these values are copied.
     \*/
    unsigned long            \*numa\_faults;
    unsigned long            total\_numa\_faults;

    /\*
     \* numa\_faults\_locality tracks if faults recorded during the last
     \* scan window were remote/local or failed to migrate. The task scan
     \* period is adapted based on the locality of the faults with different
     \* weights depending on whether they were shared or private faults
     \*/
    unsigned long            numa\_faults\_locality\[3\];

    unsigned long            numa\_pages\_migrated;
#endif /\* CONFIG\_NUMA\_BALANCING \*/

#ifdef CONFIG\_RSEQ
    struct rseq \_\_user \*rseq;
    u32 rseq\_len;
    u32 rseq\_sig;
    /\*
     \* RmW on rseq\_event\_mask must be performed atomically
     \* with respect to preemption.
     \*/
    unsigned long rseq\_event\_mask;
#endif

    struct tlbflush\_unmap\_batch    tlb\_ubc;

    struct rcu\_head            rcu;

    /\* Cache last used pipe for splice(): \*/
    struct pipe\_inode\_info        \*splice\_pipe;

    struct page\_frag        task\_frag;

#ifdef CONFIG\_TASK\_DELAY\_ACCT
    struct task\_delay\_info        \*delays;
#endif

#ifdef CONFIG\_FAULT\_INJECTION
    int                make\_it\_fail;
    unsigned int            fail\_nth;
#endif
    /\*
     \* When (nr\_dirtied >= nr\_dirtied\_pause), it's time to call
     \* balance\_dirty\_pages() for a dirty throttling pause:
     \*/
    int                nr\_dirtied;
    int                nr\_dirtied\_pause;
    /\* Start of a write-and-pause period: \*/
    unsigned long            dirty\_paused\_when;

#ifdef CONFIG\_LATENCYTOP
    int                latency\_record\_count;
    struct latency\_record        latency\_record\[LT\_SAVECOUNT\];
#endif
    /\*
     \* Time slack values; these are used to round up poll() and
     \* select() etc timeout values. These are in nanoseconds.
     \*/
    u64                timer\_slack\_ns;
    u64                default\_timer\_slack\_ns;

#ifdef CONFIG\_KASAN
    unsigned int            kasan\_depth;
#endif

#ifdef CONFIG\_FUNCTION\_GRAPH\_TRACER
    /\* Index of current stored address in ret\_stack: \*/
    int                curr\_ret\_stack;

    /\* Stack of return addresses for return function tracing: \*/
    struct ftrace\_ret\_stack        \*ret\_stack;

    /\* Timestamp for last schedule: \*/
    unsigned long long        ftrace\_timestamp;

    /\*
     \* Number of functions that haven't been traced
     \* because of depth overrun:
     \*/
    atomic\_t            trace\_overrun;

    /\* Pause tracing: \*/
    atomic\_t            tracing\_graph\_pause;
#endif

#ifdef CONFIG\_TRACING
    /\* State flags for use by tracers: \*/
    unsigned long            trace;

    /\* Bitmask and counter of trace recursion: \*/
    unsigned long            trace\_recursion;
#endif /\* CONFIG\_TRACING \*/

#ifdef CONFIG\_KCOV
    /\* Coverage collection mode enabled for this task (0 if disabled): \*/
    unsigned int            kcov\_mode;

    /\* Size of the kcov\_area: \*/
    unsigned int            kcov\_size;

    /\* Buffer for coverage collection: \*/
    void                \*kcov\_area;

    /\* KCOV descriptor wired with this task or NULL: \*/
    struct kcov            \*kcov;
#endif

#ifdef CONFIG\_MEMCG
    struct mem\_cgroup        \*memcg\_in\_oom;
    gfp\_t                memcg\_oom\_gfp\_mask;
    int                memcg\_oom\_order;

    /\* Number of pages to reclaim on returning to userland: \*/
    unsigned int            memcg\_nr\_pages\_over\_high;
#endif

#ifdef CONFIG\_UPROBES
    struct uprobe\_task        \*utask;
#endif
#if defined(CONFIG\_BCACHE) || defined(CONFIG\_BCACHE\_MODULE)
    unsigned int            sequential\_io;
    unsigned int            sequential\_io\_avg;
#endif
#ifdef CONFIG\_DEBUG\_ATOMIC\_SLEEP
    unsigned long            task\_state\_change;
#endif
    int                pagefault\_disabled;
#ifdef CONFIG\_MMU
    struct task\_struct        \*oom\_reaper\_list;
#endif
#ifdef CONFIG\_VMAP\_STACK
    struct vm\_struct        \*stack\_vm\_area;
#endif
#ifdef CONFIG\_THREAD\_INFO\_IN\_TASK
    /\* A live task holds one reference: \*/
    atomic\_t            stack\_refcount;
#endif
#ifdef CONFIG\_LIVEPATCH
    int patch\_state;
#endif
#ifdef CONFIG\_SECURITY
    /\* Used by LSM modules for access restriction: \*/
    void                \*security;
#endif

    /\*
     \* New fields for task\_struct should be added above here, so that
     \* they are included in the randomized portion of task\_struct.
     \*/
    randomized\_struct\_fields\_end

    /\* CPU-specific state of this task: \*/
    struct thread\_struct        thread;

    /\*
     \* WARNING: on x86, 'thread\_struct' contains a variable-sized
     \* structure.  It \*MUST\* be at the end of 'task\_struct'.
     \*
     \* Do not put anything below here!
     \*/
};

　　涉及到的数据结构很多，因为很多后续进程的操作，都要基于它进行，如：内存、io、cpu、调度器、文件等，自然任务艰巨。

### 2\. 进程的创建 fork

　　这里说的fork, 是应用层的api fork。用法简单: pid\_t pid = fork(); 就可以得到新的进程了，而且新的进程已经自动运行。

　　fork方法被调用一次，成功就会有两次返回；在父进程中返回一次，返回的是子进程的pid（非0）在子进程中返回一次，返回值为0 。 

　　这里的fork和系统级的fork 是差不多的，下面会有详细讲解。

### 3\. 线程的创建 pthread\_create

　　线程的创建方法为 pthread\_create()，在glibc 的源码实现源码: [https://sourceware.org/git/?p=glibc.git;a=blob;f=nptl/pthread\_create.c;h=308db65cd4c148f8a119ed9025af194946aa2c80;hb=HEAD](https://sourceware.org/git/?p=glibc.git;a=blob;f=nptl/pthread_create.c;h=308db65cd4c148f8a119ed9025af194946aa2c80;hb=HEAD)

　　首先，pthread\_create() 函数是通过 versioned\_symbol 进行导出使用的。而其内部实现函数为 \_\_pthread\_create\_2\_1(), 如下:

versioned\_symbol (libpthread, \_\_pthread\_create\_2\_1, pthread\_create, GLIBC\_2\_1);

　　所以我们可以通过 \_\_pthread\_create\_2\_1 进行研究其他创建线程过程。

int
\_\_pthread\_create\_2\_1 (pthread\_t \*newthread, const pthread\_attr\_t \*attr,
              void \*(\*start\_routine) (void \*), void \*arg)
{
  STACK\_VARIABLES;

  const struct pthread\_attr \*iattr = (struct pthread\_attr \*) attr;
  struct pthread\_attr default\_attr;
  bool free\_cpuset = false;
  bool c11 = (attr == ATTR\_C11\_THREAD);
  if (iattr == NULL || c11)
    {
      lll\_lock (\_\_default\_pthread\_attr\_lock, LLL\_PRIVATE);
      default\_attr \= \_\_default\_pthread\_attr;
      size\_t cpusetsize \= default\_attr.cpusetsize;
      if (cpusetsize > 0)
    {
      cpu\_set\_t \*cpuset;
      if (\_\_glibc\_likely (\_\_libc\_use\_alloca (cpusetsize)))
        cpuset \= \_\_alloca (cpusetsize);
      else
        {
          cpuset \= malloc (cpusetsize);
          if (cpuset == NULL)
        {
          lll\_unlock (\_\_default\_pthread\_attr\_lock, LLL\_PRIVATE);
          return ENOMEM;
        }
          free\_cpuset \= true;
        }
      memcpy (cpuset, default\_attr.cpuset, cpusetsize);
      default\_attr.cpuset \= cpuset;
    }
      lll\_unlock (\_\_default\_pthread\_attr\_lock, LLL\_PRIVATE);
      iattr \= &default\_attr;
    }

  struct pthread \*pd = NULL;
  int err = ALLOCATE\_STACK (iattr, &pd);
  int retval = 0;

  if (\_\_glibc\_unlikely (err != 0))
    /\* Something went wrong.  Maybe a parameter of the attributes is
       invalid or we could not allocate memory.  Note we have to
       translate error codes.  \*/
    {
      retval \= err == ENOMEM ? EAGAIN : err;
      goto out;
    }


  /\* Initialize the TCB.  All initializations with zero should be
     performed in 'get\_cached\_stack'.  This way we avoid doing this if
     the stack freshly allocated with 'mmap'.  \*/

#if TLS\_TCB\_AT\_TP
  /\* Reference to the TCB itself.  \*/
  pd\->header.self = pd;

  /\* Self-reference for TLS.  \*/
  pd\->header.tcb = pd;
#endif

  /\* Store the address of the start routine and the parameter.  Since
     we do not start the function directly the stillborn thread will
     get the information from its thread descriptor.  \*/
  pd\->start\_routine = start\_routine;
  pd\->arg = arg;
  pd\->c11 = c11;

  /\* Copy the thread attribute flags.  \*/
  struct pthread \*self = THREAD\_SELF;
  pd\->flags = ((iattr->flags & ~(ATTR\_FLAG\_SCHED\_SET | ATTR\_FLAG\_POLICY\_SET))
           | (self->flags & (ATTR\_FLAG\_SCHED\_SET | ATTR\_FLAG\_POLICY\_SET)));

  /\* Initialize the field for the ID of the thread which is waiting
     for us.  This is a self-reference in case the thread is created
     detached.  \*/
  pd\->joinid = iattr->flags & ATTR\_FLAG\_DETACHSTATE ? pd : NULL;

  /\* The debug events are inherited from the parent.  \*/
  pd\->eventbuf = self->eventbuf;


  /\* Copy the parent's scheduling parameters.  The flags will say what
     is valid and what is not.  \*/
  pd\->schedpolicy = self->schedpolicy;
  pd\->schedparam = self->schedparam;

  /\* Copy the stack guard canary.  \*/
#ifdef THREAD\_COPY\_STACK\_GUARD
  THREAD\_COPY\_STACK\_GUARD (pd);
#endif

  /\* Copy the pointer guard value.  \*/
#ifdef THREAD\_COPY\_POINTER\_GUARD
  THREAD\_COPY\_POINTER\_GUARD (pd);
#endif

  /\* Setup tcbhead.  \*/
  tls\_setup\_tcbhead (pd);

  /\* Verify the sysinfo bits were copied in allocate\_stack if needed.  \*/
#ifdef NEED\_DL\_SYSINFO
  CHECK\_THREAD\_SYSINFO (pd);
#endif

  /\* Inform start\_thread (above) about cancellation state that might
     translate into inherited signal state.  \*/
  pd\->parent\_cancelhandling = THREAD\_GETMEM (THREAD\_SELF, cancelhandling);

  /\* Determine scheduling parameters for the thread.  \*/
  if (\_\_builtin\_expect ((iattr->flags & ATTR\_FLAG\_NOTINHERITSCHED) != 0, 0)
      && (iattr->flags & (ATTR\_FLAG\_SCHED\_SET | ATTR\_FLAG\_POLICY\_SET)) != 0)
    {
      /\* Use the scheduling parameters the user provided.  \*/
      if (iattr->flags & ATTR\_FLAG\_POLICY\_SET)
        {
          pd\->schedpolicy = iattr->schedpolicy;
          pd\->flags |= ATTR\_FLAG\_POLICY\_SET;
        }
      if (iattr->flags & ATTR\_FLAG\_SCHED\_SET)
        {
          /\* The values were validated in pthread\_attr\_setschedparam.  \*/
          pd\->schedparam = iattr->schedparam;
          pd\->flags |= ATTR\_FLAG\_SCHED\_SET;
        }

      if ((pd->flags & (ATTR\_FLAG\_SCHED\_SET | ATTR\_FLAG\_POLICY\_SET))
          != (ATTR\_FLAG\_SCHED\_SET | ATTR\_FLAG\_POLICY\_SET))
        collect\_default\_sched (pd);
    }

  if (\_\_glibc\_unlikely (\_\_nptl\_nthreads == 1))
    \_IO\_enable\_locks ();

  /\* Pass the descriptor to the caller.  \*/
  \*newthread = (pthread\_t) pd;

  LIBC\_PROBE (pthread\_create, 4, newthread, attr, start\_routine, arg);

  /\* One more thread.  We cannot have the thread do this itself, since it
     might exist but not have been scheduled yet by the time we've returned
     and need to check the value to behave correctly.  We must do it before
     creating the thread, in case it does get scheduled first and then
     might mistakenly think it was the only thread.  In the failure case,
     we momentarily store a false value; this doesn't matter because there
     is no kosher thing a signal handler interrupting us right here can do
     that cares whether the thread count is correct.  \*/
  atomic\_increment (&\_\_nptl\_nthreads);

  /\* Our local value of stopped\_start and thread\_ran can be accessed at
     any time. The PD->stopped\_start may only be accessed if we have
     ownership of PD (see CONCURRENCY NOTES above).  \*/
  bool stopped\_start = false; bool thread\_ran = false;

  /\* Start the thread.  \*/
  if (\_\_glibc\_unlikely (report\_thread\_creation (pd)))
    {
      stopped\_start \= true;

      /\* We always create the thread stopped at startup so we can
     notify the debugger.  \*/
      retval \= create\_thread (pd, iattr, &stopped\_start,
                  STACK\_VARIABLES\_ARGS, &thread\_ran);
      if (retval == 0)
    {
      /\* We retain ownership of PD until (a) (see CONCURRENCY NOTES
         above).  \*/

      /\* Assert stopped\_start is true in both our local copy and the
         PD copy.  \*/
      assert (stopped\_start);
      assert (pd\->stopped\_start);

      /\* Now fill in the information about the new thread in
         the newly created thread's data structure.  We cannot let
         the new thread do this since we don't know whether it was
         already scheduled when we send the event.  \*/
      pd\->eventbuf.eventnum = TD\_CREATE;
      pd\->eventbuf.eventdata = pd;

      /\* Enqueue the descriptor.  \*/
      do
        pd\->nextevent = \_\_nptl\_last\_event;
      while (atomic\_compare\_and\_exchange\_bool\_acq (&\_\_nptl\_last\_event,
                               pd, pd\->nextevent)
         != 0);

      /\* Now call the function which signals the event.  See
         CONCURRENCY NOTES for the nptl\_db interface comments.  \*/
      \_\_nptl\_create\_event ();
    }
    }
  else
    retval \= create\_thread (pd, iattr, &stopped\_start,
                STACK\_VARIABLES\_ARGS, &thread\_ran);

  if (\_\_glibc\_unlikely (retval != 0))
    {
      if (thread\_ran)
    /\* State (c) or (d) and we may not have PD ownership (see
       CONCURRENCY NOTES above).  We can assert that STOPPED\_START
       must have been true because thread creation didn't fail, but
       thread attribute setting did.  \*/
    /\* See bug 19511 which explains why doing nothing here is a
       resource leak for a joinable thread.  \*/
    assert (stopped\_start);
      else
    {
      /\* State (e) and we have ownership of PD (see CONCURRENCY
         NOTES above).  \*/

      /\* Oops, we lied for a second.  \*/
      atomic\_decrement (&\_\_nptl\_nthreads);

      /\* Perhaps a thread wants to change the IDs and is waiting for this
         stillborn thread.  \*/
      if (\_\_glibc\_unlikely (atomic\_exchange\_acq (&pd->setxid\_futex, 0)
                \== -2))
        futex\_wake (&pd->setxid\_futex, 1, FUTEX\_PRIVATE);

      /\* Free the resources.  \*/
      \_\_deallocate\_stack (pd);
    }

      /\* We have to translate error codes.  \*/
      if (retval == ENOMEM)
    retval \= EAGAIN;
    }
  else
    {
      /\* We don't know if we have PD ownership.  Once we check the local
         stopped\_start we'll know if we're in state (a) or (b) (see
     CONCURRENCY NOTES above).  \*/
      if (stopped\_start)
    /\* State (a), we own PD. The thread blocked on this lock either
       because we're doing TD\_CREATE event reporting, or for some
       other reason that create\_thread chose.  Now let it run
       free.  \*/
    lll\_unlock (pd\->lock, LLL\_PRIVATE);

      /\* We now have for sure more than one thread.  The main thread might
     not yet have the flag set.  No need to set the global variable
     again if this is what we use.  \*/
      THREAD\_SETMEM (THREAD\_SELF, header.multiple\_threads, 1);
    }

 out:
  if (\_\_glibc\_unlikely (free\_cpuset))
    free (default\_attr.cpuset);

  return retval;
}

　　以上主要是构建创建线程的必要信息，然后调用 create\_thread() 进行线程的创建，而create\_thread并非是内核的调用，我们需要再深入了解下。

// sysdeps/unix/sysv/linux/createThread.c
static int
create\_thread (struct pthread \*pd, const struct pthread\_attr \*attr,
           bool \*stopped\_start, STACK\_VARIABLES\_PARMS, bool \*thread\_ran)
{
  /\* Determine whether the newly created threads has to be started
     stopped since we have to set the scheduling parameters or set the
     affinity.  \*/
  if (attr != NULL
      && (\_\_glibc\_unlikely (attr->cpuset != NULL)
      || \_\_glibc\_unlikely ((attr->flags & ATTR\_FLAG\_NOTINHERITSCHED) != 0)))
    \*stopped\_start = true;

  pd\->stopped\_start = \*stopped\_start;
  if (\_\_glibc\_unlikely (\*stopped\_start))
    /\* See CONCURRENCY NOTES in nptl/pthread\_creat.c.  \*/
    lll\_lock (pd\->lock, LLL\_PRIVATE);

  /\* We rely heavily on various flags the CLONE function understands:

     CLONE\_VM, CLONE\_FS, CLONE\_FILES
    These flags select semantics with shared address space and
    file descriptors according to what POSIX requires.

     CLONE\_SIGHAND, CLONE\_THREAD
    This flag selects the POSIX signal semantics and various
    other kinds of sharing (itimers, POSIX timers, etc.).

     CLONE\_SETTLS
    The sixth parameter to CLONE determines the TLS area for the
    new thread.

     CLONE\_PARENT\_SETTID
    The kernels writes the thread ID of the newly created thread
    into the location pointed to by the fifth parameters to CLONE.

    Note that it would be semantically equivalent to use
    CLONE\_CHILD\_SETTID but it is be more expensive in the kernel.

     CLONE\_CHILD\_CLEARTID
    The kernels clears the thread ID of a thread that has called
    sys\_exit() in the location pointed to by the seventh parameter
    to CLONE.

     The termination signal is chosen to be zero which means no signal
     is sent.  \*/
  const int clone\_flags = (CLONE\_VM | CLONE\_FS | CLONE\_FILES | CLONE\_SYSVSEM
               | CLONE\_SIGHAND | CLONE\_THREAD
               | CLONE\_SETTLS | CLONE\_PARENT\_SETTID
               | CLONE\_CHILD\_CLEARTID
               | 0);

  TLS\_DEFINE\_INIT\_TP (tp, pd);
  // \_\_clone 系统调用
  if (\_\_glibc\_unlikely (ARCH\_CLONE (&start\_thread, STACK\_VARIABLES\_ARGS,
                    clone\_flags, pd, &pd->tid, tp, &pd->tid)
            \== -1))
    return errno;

  /\* It's started now, so if we fail below, we'll have to cancel it
     and let it clean itself up.  \*/
  \*thread\_ran = true;

  /\* Now we have the possibility to set scheduling parameters etc.  \*/
  if (attr != NULL)
    {
      INTERNAL\_SYSCALL\_DECL (err);
      int res;

      /\* Set the affinity mask if necessary.  \*/
      if (attr->cpuset != NULL)
    {
      assert (\*stopped\_start);

      res \= INTERNAL\_SYSCALL (sched\_setaffinity, err, 3, pd->tid,
                  attr\->cpusetsize, attr->cpuset);

      if (\_\_glibc\_unlikely (INTERNAL\_SYSCALL\_ERROR\_P (res, err)))
      err\_out:
        {
          /\* The operation failed.  We have to kill the thread.
         We let the normal cancellation mechanism do the work.  \*/

          pid\_t pid \= \_\_getpid ();
          INTERNAL\_SYSCALL\_DECL (err2);
          (void) INTERNAL\_SYSCALL\_CALL (tgkill, err2, pid, pd->tid,
                        SIGCANCEL);

          return INTERNAL\_SYSCALL\_ERRNO (res, err);
        }
    }

      /\* Set the scheduling parameters.  \*/
      if ((attr->flags & ATTR\_FLAG\_NOTINHERITSCHED) != 0)
    {
      assert (\*stopped\_start);

      res \= INTERNAL\_SYSCALL (sched\_setscheduler, err, 3, pd->tid,
                  pd\->schedpolicy, &pd->schedparam);

      if (\_\_glibc\_unlikely (INTERNAL\_SYSCALL\_ERROR\_P (res, err)))
        goto err\_out;
    }
    }

  return 0;
}

　　可见最终是调用到了 \_\_clone() 方法了，\_\_clone 是一个api级的linux调用，此处将会根据标识，创建一个线程。

### 4\. 系统创建进程 do\_fork

　　上面的fork()和pthread\_create(), 其实都是上层的封装，在操作系统层面，创建进程和线程都是使用的 do\_fork();

// kernel/fork.c
/\* For compatibility with architectures that call do\_fork directly rather than
 \* using the syscall entry points below. \*/
long do\_fork(unsigned long clone\_flags,
          unsigned long stack\_start,
          unsigned long stack\_size,
          int \_\_user \*parent\_tidptr,
          int \_\_user \*child\_tidptr)
{
    // 调用统一的创建进程方法
    return \_do\_fork(clone\_flags, stack\_start, stack\_size,
            parent\_tidptr, child\_tidptr, 0);
}

/\*
 \*  Ok, this is the main fork-routine.
 \*
 \* It copies the process, and if successful kick-starts
 \* it and waits for it to finish using the VM if required.
 \*/
long \_do\_fork(unsigned long clone\_flags,
          unsigned long stack\_start,
          unsigned long stack\_size,
          int \_\_user \*parent\_tidptr,
          int \_\_user \*child\_tidptr,
          unsigned long tls)
{
    struct completion vfork;
    struct pid \*pid;
    struct task\_struct \*p;
    int trace = 0;
    long nr;

    /\*
     \* Determine whether and which event to report to ptracer.  When
     \* called from kernel\_thread or CLONE\_UNTRACED is explicitly
     \* requested, no event is reported; otherwise, report if the event
     \* for the type of forking is enabled.
     \*/
    if (!(clone\_flags & CLONE\_UNTRACED)) {
        if (clone\_flags & CLONE\_VFORK)
            trace \= PTRACE\_EVENT\_VFORK;
        else if ((clone\_flags & CSIGNAL) != SIGCHLD)
            trace \= PTRACE\_EVENT\_CLONE;
        else
            trace \= PTRACE\_EVENT\_FORK;

        if (likely(!ptrace\_event\_enabled(current, trace)))
            trace \= 0;
    }
    // 复制当前进程上下文信息
    p = copy\_process(clone\_flags, stack\_start, stack\_size,
             child\_tidptr, NULL, trace, tls, NUMA\_NO\_NODE);
    add\_latent\_entropy();

    if (IS\_ERR(p))
        return PTR\_ERR(p);

    /\*
     \* Do this prior waking up the new thread - the thread pointer
     \* might get invalid after that point, if the thread exits quickly.
     \*/
    trace\_sched\_process\_fork(current, p);
    // 获取新创建的进程id
    pid = get\_task\_pid(p, PIDTYPE\_PID);
    nr \= pid\_vnr(pid);

    if (clone\_flags & CLONE\_PARENT\_SETTID)
        put\_user(nr, parent\_tidptr);

    if (clone\_flags & CLONE\_VFORK) {
        p\->vfork\_done = &vfork;
        init\_completion(&vfork);
        get\_task\_struct(p);
    }
    // 将新进程放入调度队列，以便后续可以被执行
    wake\_up\_new\_task(p);

    /\* forking complete and child started to run, tell ptracer \*/
    if (unlikely(trace))
        ptrace\_event\_pid(trace, pid);

    if (clone\_flags & CLONE\_VFORK) {
        if (!wait\_for\_vfork\_done(p, &vfork))
            ptrace\_event\_pid(PTRACE\_EVENT\_VFORK\_DONE, pid);
    }
    // 进程id管理
    put\_pid(pid);
    return nr;
}

　　框架简单清晰，先复制当前进程，然后将其唤醒等待调度，然后返回进程id。所以重点是进程如何复制和唤醒，当然其中的标识位设置是很重要的，它决定如何创建进程。

#### 4.1 进程都复制了些什么？

/\*
 \* This creates a new process as a copy of the old one,
 \* but does not actually start it yet.
 \*
 \* It copies the registers, and all the appropriate
 \* parts of the process environment (as per the clone
 \* flags). The actual kick-off is left to the caller.
 \*/
static \_\_latent\_entropy struct task\_struct \*copy\_process(
                    unsigned long clone\_flags,
                    unsigned long stack\_start,
                    unsigned long stack\_size,
                    int \_\_user \*child\_tidptr,
                    struct pid \*pid,
                    int trace,
                    unsigned long tls,
                    int node)
{
    int retval;
    struct task\_struct \*p;

    /\*
     \* Don't allow sharing the root directory with processes in a different
     \* namespace
     \*/
    if ((clone\_flags & (CLONE\_NEWNS|CLONE\_FS)) == (CLONE\_NEWNS|CLONE\_FS))
        return ERR\_PTR(-EINVAL);

    if ((clone\_flags & (CLONE\_NEWUSER|CLONE\_FS)) == (CLONE\_NEWUSER|CLONE\_FS))
        return ERR\_PTR(-EINVAL);

    /\*
     \* Thread groups must share signals as well, and detached threads
     \* can only be started up within the thread group.
     \*/
    if ((clone\_flags & CLONE\_THREAD) && !(clone\_flags & CLONE\_SIGHAND))
        return ERR\_PTR(-EINVAL);

    /\*
     \* Shared signal handlers imply shared VM. By way of the above,
     \* thread groups also imply shared VM. Blocking this case allows
     \* for various simplifications in other code.
     \*/
    if ((clone\_flags & CLONE\_SIGHAND) && !(clone\_flags & CLONE\_VM))
        return ERR\_PTR(-EINVAL);

    /\*
     \* Siblings of global init remain as zombies on exit since they are
     \* not reaped by their parent (swapper). To solve this and to avoid
     \* multi-rooted process trees, prevent global and container-inits
     \* from creating siblings.
     \*/
    if ((clone\_flags & CLONE\_PARENT) &&
                current\->signal->flags & SIGNAL\_UNKILLABLE)
        return ERR\_PTR(-EINVAL);

    /\*
     \* If the new process will be in a different pid or user namespace
     \* do not allow it to share a thread group with the forking task.
     \*/
    if (clone\_flags & CLONE\_THREAD) {
        if ((clone\_flags & (CLONE\_NEWUSER | CLONE\_NEWPID)) ||
            (task\_active\_pid\_ns(current) !=
                current\->nsproxy->pid\_ns\_for\_children))
            return ERR\_PTR(-EINVAL);
    }

    retval \= -ENOMEM;
    // 复制当前进程的 task\_struct 结构体
    p = dup\_task\_struct(current, node);
    if (!p)
        goto fork\_out;

    /\*
     \* This \_must\_ happen before we call free\_task(), i.e. before we jump
     \* to any of the bad\_fork\_\* labels. This is to avoid freeing
     \* p->set\_child\_tid which is (ab)used as a kthread's data pointer for
     \* kernel threads (PF\_KTHREAD).
     \*/
    p\->set\_child\_tid = (clone\_flags & CLONE\_CHILD\_SETTID) ? child\_tidptr : NULL;
    /\*
     \* Clear TID on mm\_release()?
     \*/
    p\->clear\_child\_tid = (clone\_flags & CLONE\_CHILD\_CLEARTID) ? child\_tidptr : NULL;

    ftrace\_graph\_init\_task(p);

    rt\_mutex\_init\_task(p);

#ifdef CONFIG\_PROVE\_LOCKING
    DEBUG\_LOCKS\_WARN\_ON(!p->hardirqs\_enabled);
    DEBUG\_LOCKS\_WARN\_ON(!p->softirqs\_enabled);
#endif
    retval \= -EAGAIN;
    if (atomic\_read(&p->real\_cred->user->processes) >=
            task\_rlimit(p, RLIMIT\_NPROC)) {
        if (p->real\_cred->user != INIT\_USER &&
            !capable(CAP\_SYS\_RESOURCE) && !capable(CAP\_SYS\_ADMIN))
            goto bad\_fork\_free;
    }
    current\->flags &= ~PF\_NPROC\_EXCEEDED;

    retval \= copy\_creds(p, clone\_flags);
    if (retval < 0)
        goto bad\_fork\_free;

    /\*
     \* If multiple threads are within copy\_process(), then this check
     \* triggers too late. This doesn't hurt, the check is only there
     \* to stop root fork bombs.
     \*/
    retval \= -EAGAIN;
    if (nr\_threads >= max\_threads)
        goto bad\_fork\_cleanup\_count;

    delayacct\_tsk\_init(p);    /\* Must remain after dup\_task\_struct() \*/
    p\->flags &= ~(PF\_SUPERPRIV | PF\_WQ\_WORKER | PF\_IDLE);
    p\->flags |= PF\_FORKNOEXEC;
    INIT\_LIST\_HEAD(&p->children);
    INIT\_LIST\_HEAD(&p->sibling);
    rcu\_copy\_process(p);
    p\->vfork\_done = NULL;
    spin\_lock\_init(&p->alloc\_lock);

    init\_sigpending(&p->pending);

    p\->utime = p->stime = p->gtime = 0;
#ifdef CONFIG\_ARCH\_HAS\_SCALED\_CPUTIME
    p\->utimescaled = p->stimescaled = 0;
#endif
    prev\_cputime\_init(&p->prev\_cputime);

#ifdef CONFIG\_VIRT\_CPU\_ACCOUNTING\_GEN
    seqcount\_init(&p->vtime.seqcount);
    p\->vtime.starttime = 0;
    p\->vtime.state = VTIME\_INACTIVE;
#endif

#if defined(SPLIT\_RSS\_COUNTING)
    memset(&p->rss\_stat, 0, sizeof(p->rss\_stat));
#endif

    p\->default\_timer\_slack\_ns = current->timer\_slack\_ns;

    task\_io\_accounting\_init(&p->ioac);
    acct\_clear\_integrals(p);

    posix\_cpu\_timers\_init(p);

    p\->start\_time = ktime\_get\_ns();
    p\->real\_start\_time = ktime\_get\_boot\_ns();
    p\->io\_context = NULL;
    audit\_set\_context(p, NULL);
    cgroup\_fork(p);
#ifdef CONFIG\_NUMA
    p\->mempolicy = mpol\_dup(p->mempolicy);
    if (IS\_ERR(p->mempolicy)) {
        retval \= PTR\_ERR(p->mempolicy);
        p\->mempolicy = NULL;
        goto bad\_fork\_cleanup\_threadgroup\_lock;
    }
#endif
#ifdef CONFIG\_CPUSETS
    p\->cpuset\_mem\_spread\_rotor = NUMA\_NO\_NODE;
    p\->cpuset\_slab\_spread\_rotor = NUMA\_NO\_NODE;
    seqcount\_init(&p->mems\_allowed\_seq);
#endif
#ifdef CONFIG\_TRACE\_IRQFLAGS
    p\->irq\_events = 0;
    p\->hardirqs\_enabled = 0;
    p\->hardirq\_enable\_ip = 0;
    p\->hardirq\_enable\_event = 0;
    p\->hardirq\_disable\_ip = \_THIS\_IP\_;
    p\->hardirq\_disable\_event = 0;
    p\->softirqs\_enabled = 1;
    p\->softirq\_enable\_ip = \_THIS\_IP\_;
    p\->softirq\_enable\_event = 0;
    p\->softirq\_disable\_ip = 0;
    p\->softirq\_disable\_event = 0;
    p\->hardirq\_context = 0;
    p\->softirq\_context = 0;
#endif

    p\->pagefault\_disabled = 0;

#ifdef CONFIG\_LOCKDEP
    p\->lockdep\_depth = 0; /\* no locks held yet \*/
    p\->curr\_chain\_key = 0;
    p\->lockdep\_recursion = 0;
    lockdep\_init\_task(p);
#endif

#ifdef CONFIG\_DEBUG\_MUTEXES
    p\->blocked\_on = NULL; /\* not blocked yet \*/
#endif
#ifdef CONFIG\_BCACHE
    p\->sequential\_io    = 0;
    p\->sequential\_io\_avg    = 0;
#endif

    // 复制其他独立信息到新进程中
    /\* Perform scheduler related setup. Assign this task to a CPU. \*/
    // 设置调度器，及cpu分配
    retval = sched\_fork(clone\_flags, p);
    if (retval)
        goto bad\_fork\_cleanup\_policy;

    retval \= perf\_event\_init\_task(p);
    if (retval)
        goto bad\_fork\_cleanup\_policy;
    retval \= audit\_alloc(p);
    if (retval)
        goto bad\_fork\_cleanup\_perf;
    /\* copy all the process information \*/
    shm\_init\_task(p);
    retval \= security\_task\_alloc(p, clone\_flags);
    if (retval)
        goto bad\_fork\_cleanup\_audit;
    retval \= copy\_semundo(clone\_flags, p);
    if (retval)
        goto bad\_fork\_cleanup\_security;
    // 文件复制，实际上是 fd 的复制
    retval = copy\_files(clone\_flags, p);
    if (retval)
        goto bad\_fork\_cleanup\_semundo;
    // 复制fs结构
    retval = copy\_fs(clone\_flags, p);
    if (retval)
        goto bad\_fork\_cleanup\_files;
    retval \= copy\_sighand(clone\_flags, p);
    if (retval)
        goto bad\_fork\_cleanup\_fs;
    // 复制信号监听，如果是线程则不需要
    retval = copy\_signal(clone\_flags, p);
    if (retval)
        goto bad\_fork\_cleanup\_sighand;
    // 复制内存，重量级操作，创建线程时则不会真的复制
    retval = copy\_mm(clone\_flags, p);
    if (retval)
        goto bad\_fork\_cleanup\_signal;
    // 复制各大命名空间, pid,uts,ipc,net,cgroup
    retval = copy\_namespaces(clone\_flags, p);
    if (retval)
        goto bad\_fork\_cleanup\_mm;
    retval \= copy\_io(clone\_flags, p);
    if (retval)
        goto bad\_fork\_cleanup\_namespaces;
    // 复制线程本地存储(thread local storage)
    retval = copy\_thread\_tls(clone\_flags, stack\_start, stack\_size, p, tls);
    if (retval)
        goto bad\_fork\_cleanup\_io;

    if (pid != &init\_struct\_pid) {
        pid \= alloc\_pid(p->nsproxy->pid\_ns\_for\_children);
        if (IS\_ERR(pid)) {
            retval \= PTR\_ERR(pid);
            goto bad\_fork\_cleanup\_thread;
        }
    }

#ifdef CONFIG\_BLOCK
    p\->plug = NULL;
#endif
#ifdef CONFIG\_FUTEX
    p\->robust\_list = NULL;
#ifdef CONFIG\_COMPAT
    p\->compat\_robust\_list = NULL;
#endif
    INIT\_LIST\_HEAD(&p->pi\_state\_list);
    p\->pi\_state\_cache = NULL;
#endif
    /\*
     \* sigaltstack should be cleared when sharing the same VM
     \*/
    if ((clone\_flags & (CLONE\_VM|CLONE\_VFORK)) == CLONE\_VM)
        sas\_ss\_reset(p);

    /\*
     \* Syscall tracing and stepping should be turned off in the
     \* child regardless of CLONE\_PTRACE.
     \*/
    user\_disable\_single\_step(p);
    clear\_tsk\_thread\_flag(p, TIF\_SYSCALL\_TRACE);
#ifdef TIF\_SYSCALL\_EMU
    clear\_tsk\_thread\_flag(p, TIF\_SYSCALL\_EMU);
#endif
    clear\_all\_latency\_tracing(p);

    /\* ok, now we should be set up.. \*/
    p\->pid = pid\_nr(pid);
    if (clone\_flags & CLONE\_THREAD) {
        p\->exit\_signal = -1;
        p\->group\_leader = current->group\_leader;
        p\->tgid = current->tgid;
    } else {
        if (clone\_flags & CLONE\_PARENT)
            p\->exit\_signal = current->group\_leader->exit\_signal;
        else
            p\->exit\_signal = (clone\_flags & CSIGNAL);
        p\->group\_leader = p;
        p\->tgid = p->pid;
    }

    p\->nr\_dirtied = 0;
    p\->nr\_dirtied\_pause = 128 >> (PAGE\_SHIFT - 10);
    p\->dirty\_paused\_when = 0;

    p\->pdeath\_signal = 0;
    INIT\_LIST\_HEAD(&p->thread\_group);
    p\->task\_works = NULL;

    cgroup\_threadgroup\_change\_begin(current);
    /\*
     \* Ensure that the cgroup subsystem policies allow the new process to be
     \* forked. It should be noted the the new process's css\_set can be changed
     \* between here and cgroup\_post\_fork() if an organisation operation is in
     \* progress.
     \*/
    retval \= cgroup\_can\_fork(p);
    if (retval)
        goto bad\_fork\_free\_pid;

    /\*
     \* Make it visible to the rest of the system, but dont wake it up yet.
     \* Need tasklist lock for parent etc handling!
     \*/
    write\_lock\_irq(&tasklist\_lock);

    /\* CLONE\_PARENT re-uses the old parent \*/
    if (clone\_flags & (CLONE\_PARENT|CLONE\_THREAD)) {
        p\->real\_parent = current->real\_parent;
        p\->parent\_exec\_id = current->parent\_exec\_id;
    } else {
        p\->real\_parent = current;
        p\->parent\_exec\_id = current->self\_exec\_id;
    }

    klp\_copy\_process(p);

    spin\_lock(&current->sighand->siglock);

    /\*
     \* Copy seccomp details explicitly here, in case they were changed
     \* before holding sighand lock.
     \*/
    copy\_seccomp(p);

    rseq\_fork(p, clone\_flags);

    /\*
     \* Process group and session signals need to be delivered to just the
     \* parent before the fork or both the parent and the child after the
     \* fork. Restart if a signal comes in before we add the new process to
     \* it's process group.
     \* A fatal signal pending means that current will exit, so the new
     \* thread can't slip out of an OOM kill (or normal SIGKILL).
    \*/
    recalc\_sigpending();
    if (signal\_pending(current)) {
        retval \= -ERESTARTNOINTR;
        goto bad\_fork\_cancel\_cgroup;
    }
    if (unlikely(!(ns\_of\_pid(pid)->pid\_allocated & PIDNS\_ADDING))) {
        retval \= -ENOMEM;
        goto bad\_fork\_cancel\_cgroup;
    }

    if (likely(p->pid)) {
        ptrace\_init\_task(p, (clone\_flags & CLONE\_PTRACE) || trace);

        init\_task\_pid(p, PIDTYPE\_PID, pid);
        if (thread\_group\_leader(p)) {
            init\_task\_pid(p, PIDTYPE\_PGID, task\_pgrp(current));
            init\_task\_pid(p, PIDTYPE\_SID, task\_session(current));

            if (is\_child\_reaper(pid)) {
                ns\_of\_pid(pid)\->child\_reaper = p;
                p\->signal->flags |= SIGNAL\_UNKILLABLE;
            }

            p\->signal->leader\_pid = pid;
            p\->signal->tty = tty\_kref\_get(current->signal->tty);
            /\*
             \* Inherit has\_child\_subreaper flag under the same
             \* tasklist\_lock with adding child to the process tree
             \* for propagate\_has\_child\_subreaper optimization.
             \*/
            p\->signal->has\_child\_subreaper = p->real\_parent->signal->has\_child\_subreaper ||
                             p\->real\_parent->signal->is\_child\_subreaper;
            list\_add\_tail(&p->sibling, &p->real\_parent->children);
            list\_add\_tail\_rcu(&p->tasks, &init\_task.tasks);
            attach\_pid(p, PIDTYPE\_PGID);
            attach\_pid(p, PIDTYPE\_SID);
            \_\_this\_cpu\_inc(process\_counts);
        } else {
            current\->signal->nr\_threads++;
            atomic\_inc(&current->signal->live);
            atomic\_inc(&current->signal->sigcnt);
            list\_add\_tail\_rcu(&p->thread\_group,
                      &p->group\_leader->thread\_group);
            list\_add\_tail\_rcu(&p->thread\_node,
                      &p->signal->thread\_head);
        }
        attach\_pid(p, PIDTYPE\_PID);
        nr\_threads++;
    }

    total\_forks++;
    spin\_unlock(&current->sighand->siglock);
    syscall\_tracepoint\_update(p);
    write\_unlock\_irq(&tasklist\_lock);

    proc\_fork\_connector(p);
    cgroup\_post\_fork(p);
    cgroup\_threadgroup\_change\_end(current);
    perf\_event\_fork(p);

    trace\_task\_newtask(p, clone\_flags);
    uprobe\_copy\_process(p, clone\_flags);

    return p;

bad\_fork\_cancel\_cgroup:
    spin\_unlock(&current->sighand->siglock);
    write\_unlock\_irq(&tasklist\_lock);
    cgroup\_cancel\_fork(p);
bad\_fork\_free\_pid:
    cgroup\_threadgroup\_change\_end(current);
    if (pid != &init\_struct\_pid)
        free\_pid(pid);
bad\_fork\_cleanup\_thread:
    exit\_thread(p);
bad\_fork\_cleanup\_io:
    if (p->io\_context)
        exit\_io\_context(p);
bad\_fork\_cleanup\_namespaces:
    exit\_task\_namespaces(p);
bad\_fork\_cleanup\_mm:
    if (p->mm)
        mmput(p\->mm);
bad\_fork\_cleanup\_signal:
    if (!(clone\_flags & CLONE\_THREAD))
        free\_signal\_struct(p\->signal);
bad\_fork\_cleanup\_sighand:
    \_\_cleanup\_sighand(p\->sighand);
bad\_fork\_cleanup\_fs:
    exit\_fs(p); /\* blocking \*/
bad\_fork\_cleanup\_files:
    exit\_files(p); /\* blocking \*/
bad\_fork\_cleanup\_semundo:
    exit\_sem(p);
bad\_fork\_cleanup\_security:
    security\_task\_free(p);
bad\_fork\_cleanup\_audit:
    audit\_free(p);
bad\_fork\_cleanup\_perf:
    perf\_event\_free\_task(p);
bad\_fork\_cleanup\_policy:
    lockdep\_free\_task(p);
#ifdef CONFIG\_NUMA
    mpol\_put(p\->mempolicy);
bad\_fork\_cleanup\_threadgroup\_lock:
#endif
    delayacct\_tsk\_free(p);
bad\_fork\_cleanup\_count:
    atomic\_dec(&p->cred->user->processes);
    exit\_creds(p);
bad\_fork\_free:
    p\->state = TASK\_DEAD;
    put\_task\_stack(p);
    free\_task(p);
fork\_out:
    return ERR\_PTR(retval);
}

　　可以看出创建一个新进程，还是一件非常重的事，copy了很多的信息，可以说肯定会非常耗性能和资源，所以不要随便创建不必要的进程，它是宝贵的。下面我们就几个简单的复制过程了解下，都干了什么。

// 复制线程结构外壳
static struct task\_struct \*dup\_task\_struct(struct task\_struct \*orig, int node)
{
    struct task\_struct \*tsk;
    unsigned long \*stack;
    struct vm\_struct \*stack\_vm\_area;
    int err;

    if (node == NUMA\_NO\_NODE)
        node \= tsk\_fork\_get\_node(orig);
    tsk \= alloc\_task\_struct\_node(node);
    if (!tsk)
        return NULL;

    stack \= alloc\_thread\_stack\_node(tsk, node);
    if (!stack)
        goto free\_tsk;

    stack\_vm\_area \= task\_stack\_vm\_area(tsk);

    err \= arch\_dup\_task\_struct(tsk, orig);

    /\*
     \* arch\_dup\_task\_struct() clobbers the stack-related fields.  Make
     \* sure they're properly initialized before using any stack-related
     \* functions again.
     \*/
    tsk\->stack = stack;
#ifdef CONFIG\_VMAP\_STACK
    tsk\->stack\_vm\_area = stack\_vm\_area;
#endif
#ifdef CONFIG\_THREAD\_INFO\_IN\_TASK
    atomic\_set(&tsk->stack\_refcount, 1);
#endif

    if (err)
        goto free\_stack;

#ifdef CONFIG\_SECCOMP
    /\*
     \* We must handle setting up seccomp filters once we're under
     \* the sighand lock in case orig has changed between now and
     \* then. Until then, filter must be NULL to avoid messing up
     \* the usage counts on the error path calling free\_task.
     \*/
    tsk\->seccomp.filter = NULL;
#endif

    setup\_thread\_stack(tsk, orig);
    clear\_user\_return\_notifier(tsk);
    clear\_tsk\_need\_resched(tsk);
    set\_task\_stack\_end\_magic(tsk);

#ifdef CONFIG\_STACKPROTECTOR
    tsk\->stack\_canary = get\_random\_canary();
#endif

    /\*
     \* One for us, one for whoever does the "release\_task()" (usually
     \* parent)
     \*/
    atomic\_set(&tsk->usage, 2);
#ifdef CONFIG\_BLK\_DEV\_IO\_TRACE
    tsk\->btrace\_seq = 0;
#endif
    tsk\->splice\_pipe = NULL;
    tsk\->task\_frag.page = NULL;
    tsk\->wake\_q.next = NULL;

    account\_kernel\_stack(tsk, 1);

    kcov\_task\_init(tsk);

#ifdef CONFIG\_FAULT\_INJECTION
    tsk\->fail\_nth = 0;
#endif

    return tsk;

free\_stack:
    free\_thread\_stack(tsk);
free\_tsk:
    free\_task\_struct(tsk);
    return NULL;
}

// 复制内存空间
static int copy\_mm(unsigned long clone\_flags, struct task\_struct \*tsk)
{
    struct mm\_struct \*mm, \*oldmm;
    int retval;

    tsk\->min\_flt = tsk->maj\_flt = 0;
    tsk\->nvcsw = tsk->nivcsw = 0;
#ifdef CONFIG\_DETECT\_HUNG\_TASK
    tsk\->last\_switch\_count = tsk->nvcsw + tsk->nivcsw;
#endif

    tsk\->mm = NULL;
    tsk\->active\_mm = NULL;

    /\*
     \* Are we cloning a kernel thread?
     \*
     \* We need to steal a active VM for that..
     \*/
    oldmm \= current->mm;
    if (!oldmm)
        return 0;

    /\* initialize the new vmacache entries \*/
    vmacache\_flush(tsk);
    // 一般情况下可以直接复用原有的内存信息
    if (clone\_flags & CLONE\_VM) {
        // 增加内存的使用计数，然后复用原有的指针即可
        mmget(oldmm);
        mm \= oldmm;
        goto good\_mm;
    }

    retval \= -ENOMEM;
    // 重量级操作，内存复制
    mm = dup\_mm(tsk);
    if (!mm)
        goto fail\_nomem;

good\_mm:
    tsk\->mm = mm;
    tsk\->active\_mm = mm;
    return 0;

fail\_nomem:
    return retval;
}

/\*
 \* Allocate a new mm structure and copy contents from the
 \* mm structure of the passed in task structure.
 \*/
static struct mm\_struct \*dup\_mm(struct task\_struct \*tsk)
{
    struct mm\_struct \*mm, \*oldmm = current->mm;
    int err;

    mm \= allocate\_mm();
    if (!mm)
        goto fail\_nomem;

    memcpy(mm, oldmm, sizeof(\*mm));

    if (!mm\_init(mm, tsk, mm->user\_ns))
        goto fail\_nomem;

    err \= dup\_mmap(mm, oldmm);
    if (err)
        goto free\_pt;

    mm\->hiwater\_rss = get\_mm\_rss(mm);
    mm\->hiwater\_vm = mm->total\_vm;

    if (mm->binfmt && !try\_module\_get(mm->binfmt->module))
        goto free\_pt;

    return mm;

free\_pt:
    /\* don't put binfmt in mmput, we haven't got module yet \*/
    mm\->binfmt = NULL;
    mmput(mm);

fail\_nomem:
    return NULL;
}

　　针对复制过程，每个方法有其独特的用处，这也体现了设计模式中的单一职责原则。复制进程结构就是复制最外层的结构体，而复制内存，则是针对各区域的内存进行依次的复制。

#### 4.2 将新建进程唤醒

　　进程创建好之后，自然是要交给调度系统处理的。理论上，只需要将将新建的进程加入到调度队列，后续的事则可以不用fork() 管了。具体是否是这样呢？

// kernel/sched/core.c
// 将新建进程加入到调度队列，并设置唤醒标识
/\*
 \* wake\_up\_new\_task - wake up a newly created task for the first time.
 \*
 \* This function will do some initial scheduler statistics housekeeping
 \* that must be done for every newly created context, then puts the task
 \* on the runqueue and wakes it.
 \*/
void wake\_up\_new\_task(struct task\_struct \*p)
{
    struct rq\_flags rf;
    struct rq \*rq;

    raw\_spin\_lock\_irqsave(&p->pi\_lock, rf.flags);
    p\->state = TASK\_RUNNING;
#ifdef CONFIG\_SMP
    /\*
     \* Fork balancing, do it here and not earlier because:
     \*  - cpus\_allowed can change in the fork path
     \*  - any previously selected CPU might disappear through hotplug
     \*
     \* Use \_\_set\_task\_cpu() to avoid calling sched\_class::migrate\_task\_rq,
     \* as we're not fully set-up yet.
     \*/
    p\->recent\_used\_cpu = task\_cpu(p);
    \_\_set\_task\_cpu(p, select\_task\_rq(p, task\_cpu(p), SD\_BALANCE\_FORK, 0));
#endif
    rq \= \_\_task\_rq\_lock(p, &rf);
    update\_rq\_clock(rq);
    post\_init\_entity\_util\_avg(&p->se);

    activate\_task(rq, p, ENQUEUE\_NOCLOCK);
    p\->on\_rq = TASK\_ON\_RQ\_QUEUED;
    trace\_sched\_wakeup\_new(p);
    check\_preempt\_curr(rq, p, WF\_FORK);
#ifdef CONFIG\_SMP
    if (p->sched\_class->task\_woken) {
        /\*
         \* Nothing relies on rq->lock after this, so its fine to
         \* drop it.
         \*/
        rq\_unpin\_lock(rq, &rf);
        p\->sched\_class->task\_woken(rq, p);
        rq\_repin\_lock(rq, &rf);
    }
#endif
    task\_rq\_unlock(rq, p, &rf);
}
// 新进程加入队列
void activate\_task(struct rq \*rq, struct task\_struct \*p, int flags)
{
    if (task\_contributes\_to\_load(p))
        rq\->nr\_uninterruptible--;

    enqueue\_task(rq, p, flags);
}

static inline void enqueue\_task(struct rq \*rq, struct task\_struct \*p, int flags)
{
    if (!(flags & ENQUEUE\_NOCLOCK))
        update\_rq\_clock(rq);

    if (!(flags & ENQUEUE\_RESTORE))
        sched\_info\_queued(rq, p);

    p\->sched\_class->enqueue\_task(rq, p, flags);
}

#### 4.3 put\_pid 是在管理pid吗？

　　最后，我们看下put\_pid() 都做了些什么？感觉像是将pid写入到全局空间中，进行统一管理，但是实际上却不是的，它实际上是一个引用计数的处理过程。

// kernel/pid.c
// 减少pid\_namespace的引用计数
void put\_pid(struct pid \*pid)
{
    struct pid\_namespace \*ns;

    if (!pid)
        return;

    ns \= pid->numbers\[pid->level\].ns;
    if ((atomic\_read(&pid->count) == 1) ||
         atomic\_dec\_and\_test(&pid->count)) {
        kmem\_cache\_free(ns\->pid\_cachep, pid);
        put\_pid\_ns(ns);
    }
}
EXPORT\_SYMBOL\_GPL(put\_pid);
// kernel/pid\_namespace.c
void put\_pid\_ns(struct pid\_namespace \*ns)
{
    struct pid\_namespace \*parent;
    // 放到init进程pid空间
    while (ns != &init\_pid\_ns) {
        parent \= ns->parent;
        if (!kref\_put(&ns->kref, free\_pid\_ns))
            break;
        ns \= parent;
    }
}
EXPORT\_SYMBOL\_GPL(put\_pid\_ns);

static void free\_pid\_ns(struct kref \*kref)
{
    struct pid\_namespace \*ns;

    ns \= container\_of(kref, struct pid\_namespace, kref);
    destroy\_pid\_namespace(ns);
}

　　创建进程线程，其实包含了大量的复杂操作，每个细节都值得我们深入研究，我们此处仅简单看几个方向，如内存的复制过程，也可以大致理解进线程的差别，相信也能在一定程度上为大家解惑。

不要害怕今日的苦，你要相信明天，更苦！