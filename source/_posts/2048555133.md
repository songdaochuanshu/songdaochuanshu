---
layout: post
title: "Spark简单介绍，Windows下安装Scala+Hadoop+Spark运行环境，集成到IDEA中"
date: "2022-11-03T05:23:47.903Z"
---
Spark简单介绍，Windows下安装Scala+Hadoop+Spark运行环境，集成到IDEA中
===================================================

一、前言
----

近几年大数据是异常的火爆，今天小编以java开发的身份来会会大数据，提高一下自己的层面！  
大数据技术也是有很多：

*   Hadoop
*   Spark
*   Flink

小编也只知道这些了，由于`Hadoop`，存在一定的缺陷（循环迭代式数据流处理：多  
并行运行的数据可复用场景`效率不行`）。所以`Spark`出来了，一匹黑马，8个月的时间从加入 `Apache`，直接成为顶级项目！！

选择`Spark`的主要原因是：

> Spark和Hadoop的根本差异是多个作业之间的数据通信问题 : Spark多个作业之间数据  
> 通信是基于内存，而 Hadoop 是基于磁盘。

二、Spark介绍
---------

[官网地址](https://spark.apache.org/)

`Spark 是用于大规模数据处理的统一分析引擎`。它提供了 Scala、Java、Python 和 R 中的高级 API，以及支持用于数据分析的通用计算图的优化引擎。它还支持一组丰富的高级工具，包括用于 SQL 和 DataFrames 的 Spark SQL、用于 Pandas 工作负载的 Spark 上的 Pandas API、用于机器学习的 MLlib、用于图形处理的 GraphX 和用于流处理的结构化流。

`spark`是使用`Scala`语言开发的，所以使用`Scala`更好！！

三、下载安装
------

### 1\. Scala下载

[Scala官网](https://www.scala-lang.org/)

点击安装

![在这里插入图片描述](https://img-blog.csdnimg.cn/ba29003112294e7b93ce26d4d48e73c4.png)

下载自己需要的版本

![在这里插入图片描述](https://img-blog.csdnimg.cn/be06a722405b488e86b5049631db5b9c.png)  
点击自己需要的版本：小编这里下载的是`2.12.11`

点击下载Windows二进制：  
![在这里插入图片描述](https://img-blog.csdnimg.cn/77c83f43cffd41d99ba651bc7f9e8bab.png)  
慢的话可以使用迅雷下载！

### 2\. 安装

安装就是下一步下一步，记住安装目录不要有空格，不然会报错的！！！

### 3\. 测试安装

`win+R`输入`cmd`：  
输入：

    scala
    

必须要有JDK环境哈，这个学大数据基本都有哈！！

![在这里插入图片描述](https://img-blog.csdnimg.cn/4cce8b5ac8504335b2567338966ba9a2.png)

### 4\. Hadoop下载

一个小技巧：  
`Hadoop和Spark`版本需要一致，我们先去看看spark，他上面名字就带着和他配套的Hadoop版本！！

[spark3.0对照](https://archive.apache.org/dist/spark/spark-3.0.0/)

![在这里插入图片描述](https://img-blog.csdnimg.cn/f2a157f0ef984acfb903c4eeea696d7d.png)  
得出我们下载Hadoop的版本为：`3.2`

[Hadoop下载地址](https://archive.apache.org/dist/hadoop/common/)

![在这里插入图片描述](https://img-blog.csdnimg.cn/5db939e83e184821b926e25affbab329.png)

### 5\. 解压配置环境

解压到即可使用，为了使用方便，要想jdk一样配置一下环境变量！

新建`HADOOP_HOME`  
值为安装目录：`D:\software\hadoop-3.2.1`  
在`Path`里添加：`%HADOOP_HOME%\bin`

cmd输入：`hadoop`：提示

    系统找不到指定的路径。
    Error: JAVA_HOME is incorrectly set.
    

这里先不用管，咱们只需要Hadoop的环境即可！

### 6\. 下载Spark

[Spark官网](https://spark.apache.org/)

点击找到历史版本：  
![在这里插入图片描述](https://img-blog.csdnimg.cn/f717e9434e45420d89097f90356bff31.png)  
点击下载：  
![在这里插入图片描述](https://img-blog.csdnimg.cn/5bea8f82dc21435d8bc2a5974441b8ea.png)

### 7\. 解压环境配置

新建：`SPARK_HOME`：`D:\spark\spark-3.3.1-bin-hadoop3`  
`Path`添加：`%SPARK_HOME%\bin`

### 8\. 测试安装

`win+R`输入`cmd`：  
输入：

    spark-shell
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/1a990ee1d5154050a4aad002f43ae98c.png)

四、集成Idea
--------

### 1\. 下载插件

    scala
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/c94e5e8319f540ce8adf4dbe584d2a81.png)

### 2\. 给项目添加Global Libraries

打开配置：  
![在这里插入图片描述](https://img-blog.csdnimg.cn/7e26da1533be461cbcc775d86adbb515.png)  
新增SDK

![在这里插入图片描述](https://img-blog.csdnimg.cn/80274dc338f34593b01eec371889d9bf.png)  
下载你需要的版本：小编这里是：`2.12.11`

![在这里插入图片描述](https://img-blog.csdnimg.cn/7a198f5c37ad488c98c2b6eae24c9160.png)  
右击项目，添加上`scala`：

![在这里插入图片描述](https://img-blog.csdnimg.cn/a5400e8bfb0441078f4f8307f1a8b0d4.png)

### 3\. 导入依赖

    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.12</artifactId>
        <version>3.0.0</version>
    </dependency>
    

### 4\. 第一个程序

![在这里插入图片描述](https://img-blog.csdnimg.cn/e9881340dfd94481bac6c85f99f30203.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/6d647a602b9141e48f4f56ae1ea8c803.png)

    object Test {
    
      def main(args: Array[String]): Unit = {
        println("hello")
        var sparkConf = new SparkConf().setMaster("local").setAppName("WordCount");
        var sc = new SparkContext(sparkConf);
        sc.stop();
      }
    }
    

### 5\. 测试bug1

    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    22/10/31 16:20:35 INFO SparkContext: Running Spark version 3.0.0
    22/10/31 16:20:35 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
    java.io.IOException: Could not locate executable D:\software\hadoop-3.2.1\bin\winutils.exe in the Hadoop binaries.
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/b829b3778b354ed6a5478831ff57805a.png)

原因就是缺少：`winutils`

[下载地址](https://github.com/cdarlint/winutils)

![在这里插入图片描述](https://img-blog.csdnimg.cn/dc1a81e24c194b709fae13fe56e293f0.png)  
把它发放`Hadoop`的bin目录下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/9ec96568566049e184169869538c09f1.png)

### 6\. 测试bug2

这个没办法复现，拔的网上的记录：

    
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    22/10/08 21:02:10 INFO SparkContext: Running Spark version 3.0.0
    22/10/08 21:02:10 ERROR SparkContext: Error initializing SparkContext.
    org.apache.spark.SparkException: A master URL must be set in your configuration
    	at org.apache.spark.SparkContext.<init>(SparkContext.scala:380)
    	at org.apache.spark.SparkContext.<init>(SparkContext.scala:120)
    	at test.wyh.wordcount.TestWordCount$.main(TestWordCount.scala:10)
    	at test.wyh.wordcount.TestWordCount.main(TestWordCount.scala)
    

就是这句：`A master URL must be set in your configuration`

解决方案：  
就是没有用到本地的地址

右击项目：  
![在这里插入图片描述](https://img-blog.csdnimg.cn/9636d222dd7f4259b4ca67494b849d91.png)

没有环境就添加上：  
![在这里插入图片描述](https://img-blog.csdnimg.cn/da793dc008124332bffdcfbf980db590.png)  
添加上：

    -Dspark.master=local
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/b4b8ce4b6c9249e9ba50e6d0092a704a.png)

### 7\. 测试完成

没有error，完美！！  
![在这里插入图片描述](https://img-blog.csdnimg.cn/9948561796d84575a8db74b9cfe0df88.png)

五、总结
----

这样就完成了，历尽千辛万苦，终于成功。第一次结束差点劝退，发现自己对这个东西还是不懂，后面再慢慢补`Scala`。先上手感受，然后再深度学习！！

如果对你有用，还请点赞关注下，支持一下一直是小编写作的动力！！

* * *

可以看下一小编的微信公众号，和网站文章首发看，欢迎关注，一起交流哈！！微信搜索：小王博客基地

![](https://img2022.cnblogs.com/blog/2471401/202211/2471401-20221103090451198-1239425921.jpg)

[点击访问！小编自己的网站，里面也是有很多好的文章哦！](https://wangzhenjun.xyz)