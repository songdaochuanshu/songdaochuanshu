---
layout: post
title: "Tensorflow2 深度学习十必知 "
date: "2022-06-30T14:16:47.579Z"
---
Tensorflow2 深度学习十必知
===================

博主根据自身多年的深度学习算法研发经验，整理分享以下十条必知。

含参考资料链接，部分附上相关代码实现。

独乐乐不如众乐乐，希望对各位看客有所帮助。

待回头有时间再展开细节说一说深度学习里的那些道道。 

有什么技术需求需要有偿解决的也可以邮件或者QQ联系博主。

邮箱QQ同ID：gaozhihan@vip.qq.com

当然除了这十条，肯定还有其他“必知”，

欢迎评论分享更多，这里只是暂时拟定的十条，别较真哈。

主要学习其中的思路，切记，以下思路在个别场景并不适用 。

1.数据回流

[\[1907.05550\] Faster Neural Network Training with Data Echoing](https://arxiv.org/abs/1907.05550)

def data\_echoing(factor): 
    return lambda image, label: tf.data.Dataset.from\_tensors((image, label)).repeat(factor)

作用:

数据集加载后，在数据增广前后重复当前批次进模型的次数，减少数据的加载耗时。

等价于让模型看n次当前的数据，或者看n个增广后的数据样本。

2.AMP 自动精度混合

[在bert4keras中使用混合精度和XLA加速训练 - 科学空间|Scientific Spaces](https://kexue.fm/archives/9059)

    tf.config.optimizer.set\_experimental\_options({"auto\_mixed\_precision": True})

作用:

降低显存占用，加速训练，将部分网络计算转为等价的低精度计算，以此降低计算量。

3.优化器节省显存

3.1  [\[1804.04235\]Adafactor: Adaptive Learning Rates with Sublinear Memory Cost](https://arxiv.org/abs/1804.04235)

[mesh/optimize.py at master · tensorflow/mesh · GitHub](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/optimize.py)

3.2 [\[1901.11150\] Memory-Efficient Adaptive Optimization](https://arxiv.org/abs/1901.11150)

[google-research/sm3 at master · google-research/google-research (github.com)](https://github.com/google-research/google-research/tree/master/sm3)

作用:

节省显存，加速训练，

主要是对二阶动量进行特例化解构，减少显存存储。

4.权重标准化(归一化)

[\[2102.06171\] High-Performance Large-Scale Image Recognition Without Normalization](https://arxiv.org/abs/2102.06171)

[deepmind-research/nfnets at master · deepmind/deepmind-research · GitHub](https://github.com/deepmind/deepmind-research/tree/master/nfnets)

class WSConv2D(tf.keras.layers.Conv2D):
    def \_\_init\_\_(self, \*args, \*\*kwargs):
        super(WSConv2D, self).\_\_init\_\_(
            kernel\_initializer=tf.keras.initializers.VarianceScaling(
                scale=1.0, mode='fan\_in', distribution='untruncated\_normal',
            ),
            use\_bias=False,
            kernel\_regularizer=tf.keras.regularizers.l2(1e-4), \*args, \*\*kwargs
        )
        self.gain = self.add\_weight(
            name='gain',
            shape=(self.filters,),
            initializer="ones",
            trainable=True,
            dtype=self.dtype
        )

    def standardize\_weight(self, eps):
        mean, var = tf.nn.moments(self.kernel, axes=\[0, 1, 2\], keepdims=True)
        fan\_in = np.prod(self.kernel.shape\[:-1\])
        # Manually fused normalization, eq. to (w - mean) \* gain / sqrt(N \* var)
        scale = tf.math.rsqrt(
            tf.math.maximum(
                var \* fan\_in,
                tf.convert\_to\_tensor(eps, dtype=self.dtype)
            )
        ) \* self.gain
        shift = mean \* scale
        return self.kernel \* scale - shift

    def call(self, inputs):
        eps = 1e-4
        weight = self.standardize\_weight(eps)
        return tf.nn.conv2d(
            inputs, weight, strides=self.strides,
            padding=self.padding.upper(), dilations=self.dilation\_rate
        ) if self.bias is None else tf.nn.bias\_add(
            tf.nn.conv2d(
                inputs, weight, strides=self.strides,
                padding=self.padding.upper(), dilations=self.dilation\_rate
            ), self.bias)

作用:

通过对kernel进行标准化或归一化，相当于对kernel做一个先验约束，以此加速模型训练收敛。

5.自适应梯度裁剪

[deepmind-research/agc\_optax.py at master · deepmind/deepmind-research · GitHub](https://github.com/deepmind/deepmind-research/blob/master/nfnets/agc_optax.py)

def unitwise\_norm(x):
    if len(tf.squeeze(x).shape) <= 1:  # Scalars and vectors
        axis = None
        keepdims = False
    elif len(x.shape) in \[2, 3\]:  # Linear layers of shape IO
        axis = 0
        keepdims = True
    elif len(x.shape) == 4:  # Conv kernels of shape HWIO
        axis = \[0, 1, 2, \]
        keepdims = True
    else:
        raise ValueError(f'Got a parameter with shape not in \[1, 2, 3, 4\]! {x}')
    square\_sum = tf.reduce\_sum(tf.square(x), axis, keepdims=keepdims)
    return tf.sqrt(square\_sum)


def gradient\_clipping(grad, var):
    clipping = 0.01
    max\_norm = tf.maximum(unitwise\_norm(var), 1e-3) \* clipping
    grad\_norm = unitwise\_norm(grad)
    trigger = (grad\_norm > max\_norm)
    clipped\_grad = (max\_norm / tf.maximum(grad\_norm, 1e-6))
    return grad \* tf.where(trigger, clipped\_grad, tf.ones\_like(clipped\_grad))

作用:

防止梯度爆炸，稳定训练。通过梯度和参数的关系，对梯度进行裁剪，约束学习率。

6.recompute\_grad

[\[1604.06174\] Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174)

[google-research/recompute\_grad.py at master · google-research/google-research (github.com)](https://github.com/google-research/google-research/blob/master/etcmodel/layers/recompute_grad.py)

[bojone/keras\_recompute: saving memory by recomputing for keras (github.com)](https://github.com/bojone/keras_recompute)

作用:

通过梯度重计算，节省显存。

7.归一化

[\[2003.05569\] Extended Batch Normalization (arxiv.org)](https://arxiv.org/abs/2003.05569)

from keras.layers.normalization.batch\_normalization import BatchNormalizationBase

class ExtendedBatchNormalization(BatchNormalizationBase):
    def \_\_init\_\_(self,
                 axis=-1,
                 momentum=0.99,
                 epsilon=1e-3,
                 center=True,
                 scale=True,
                 beta\_initializer='zeros',
                 gamma\_initializer='ones',
                 moving\_mean\_initializer='zeros',
                 moving\_variance\_initializer='ones',
                 beta\_regularizer=None,
                 gamma\_regularizer=None,
                 beta\_constraint=None,
                 gamma\_constraint=None,
                 renorm=False,
                 renorm\_clipping=None,
                 renorm\_momentum=0.99,
                 trainable=True,
                 name=None,
                 \*\*kwargs):
        # Currently we only support aggregating over the global batch size.
        super(ExtendedBatchNormalization, self).\_\_init\_\_(
            axis=axis,
            momentum=momentum,
            epsilon=epsilon,
            center=center,
            scale=scale,
            beta\_initializer=beta\_initializer,
            gamma\_initializer=gamma\_initializer,
            moving\_mean\_initializer=moving\_mean\_initializer,
            moving\_variance\_initializer=moving\_variance\_initializer,
            beta\_regularizer=beta\_regularizer,
            gamma\_regularizer=gamma\_regularizer,
            beta\_constraint=beta\_constraint,
            gamma\_constraint=gamma\_constraint,
            renorm=renorm,
            renorm\_clipping=renorm\_clipping,
            renorm\_momentum=renorm\_momentum,
            fused=False,
            trainable=trainable,
            virtual\_batch\_size=None,
            name=name,
            \*\*kwargs)

    def \_calculate\_mean\_and\_var(self, x, axes, keep\_dims):
        with tf.keras.backend.name\_scope('moments'):
            y = tf.cast(x, tf.float32) if x.dtype == tf.float16 else x
            replica\_ctx = tf.distribute.get\_replica\_context()
            if replica\_ctx:
                local\_sum = tf.math.reduce\_sum(y, axis=axes, keepdims=True)
                local\_squared\_sum = tf.math.reduce\_sum(tf.math.square(y), axis=axes,
                                                       keepdims=True)
                batch\_size = tf.cast(tf.shape(y)\[0\], tf.float32)
                y\_sum = replica\_ctx.all\_reduce(tf.distribute.ReduceOp.SUM, local\_sum)
                y\_squared\_sum = replica\_ctx.all\_reduce(tf.distribute.ReduceOp.SUM,
                                                       local\_squared\_sum)
                global\_batch\_size = replica\_ctx.all\_reduce(tf.distribute.ReduceOp.SUM,
                                                           batch\_size)
                axes\_vals = \[(tf.shape(y))\[i\] for i in range(1, len(axes))\]
                multiplier = tf.cast(tf.reduce\_prod(axes\_vals), tf.float32)
                multiplier = multiplier \* global\_batch\_size
                mean = y\_sum / multiplier
                y\_squared\_mean = y\_squared\_sum / multiplier
                # var = E(x^2) - E(x)^2
                variance = y\_squared\_mean - tf.math.square(mean)
            else:
                # Compute true mean while keeping the dims for proper broadcasting.
                mean = tf.math.reduce\_mean(y, axes, keepdims=True, name='mean')
                variance = tf.math.reduce\_mean(
                    tf.math.squared\_difference(y, tf.stop\_gradient(mean)),
                    axes,
                    keepdims=True,
                    name='variance')
            if not keep\_dims:
                mean = tf.squeeze(mean, axes)
                variance = tf.squeeze(variance, axes)
            variance = tf.math.reduce\_mean(variance)
            if x.dtype == tf.float16:
                return (tf.cast(mean, tf.float16),
                        tf.cast(variance, tf.float16))
            else:
                return mean, variance

作用:

一个简易改进版的Batch Normalization，思路简单有效。

8.学习率策略

[\[1506.01186\] Cyclical Learning Rates for Training Neural Networks (arxiv.org)](https://arxiv.org/abs/1506.01186)

作用:

一个推荐的学习率策略方案，特定情况下可以取得更好的泛化。

9.重参数化

[\[1908.03930\] ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks](https://arxiv.org/abs/1908.03930)

[https://zhuanlan.zhihu.com/p/361090497](https://zhuanlan.zhihu.com/p/361090497)

作用：

通过同时训练多份参数，合并权重的思路来提升模型泛化性。

10.长尾学习

[\[2110.04596\] Deep Long-Tailed Learning: A Survey (arxiv.org)](https://arxiv.org/abs/2110.04596)

[Jorwnpay/A-Long-Tailed-Survey: 本项目是 Deep Long-Tailed Learning: A Survey 文章的中译版 (github.com)](https://github.com/Jorwnpay/A-Long-Tailed-Survey)

作用:

解决长尾问题，可以加速收敛，提升模型泛化，稳定训练。