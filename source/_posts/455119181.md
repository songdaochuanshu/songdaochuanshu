---
layout: post
title: "分布式存储系统之Ceph集群存储池操作"
date: "2022-10-06T19:26:31.771Z"
---
分布式存储系统之Ceph集群存储池操作
===================

![分布式存储系统之Ceph集群存储池操作](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220930011449512-530339137.png) erasure-code-profile参数是用于指定纠删码池配置文件；未指定要使用的纠删编码配置文件时，创建命令会为其自动创建一个，并在创建相关的CRUSH规则集时使用到它；默认配置文件自动定义k=2和m=1，这意味着Ceph将通过三个OSD扩展对象数据，并且可以丢失其中一个OSD而不会丢失数据，因此，在冗余效果上，它相当于一个大小为2的副本池 ，不过，其存储空间有效利用率为2/3而非1/2。

　　前文我们了解了ceph的存储池、PG、CRUSH、客户端IO的简要工作过程、Ceph客户端计算PG\_ID的步骤的相关话题，回顾请参考[https://www.cnblogs.com/qiuhom-1874/p/16733806.html](https://www.cnblogs.com/qiuhom-1874/p/16733806.html)；今天我们来聊一聊在ceph上操作存储池相关命令的用法和说明；

　　在ceph上操作存储池不外乎就是查看列出、创建、重命名和删除等操作，常用相关的工具都是“ceph osd pool”的子命令，ls、create、rename和rm等；

　　1、创建存储池

　　副本型存储池创建命令格式

 ceph osd pool create <pool-name> <pg-num> \[pgp-num\] \[replicated\] \[crush-rule-name\] \[expected-num-objects\]

　　提示：创建副本型存储池上面的必要选项有存储池的名称和PG的数量，后面可以不用跟pgp和replicated来指定存储池的pgp的数量和类型为副本型；即默认创建不指定存储池类型，都是创建的是副本池；

　　纠删码池存储池创建命令格式

 ceph osd pool create <pool-name> <pg-num> <pgp-num> erasure \[erasure-code-profile\] \[crush-rule-name\] \[expected-num-objects\]

　　提示：创建纠删码池存储池，需要给定存储池名称、PG的数量、PGP的数量已经明确指定存储池类型为erasure；这里解释下PGP，所谓PGP（Placement Group for Placement purpose）就是用于归置的PG数量，其值应该等于PG的数量； crush-ruleset-name是用于指定此存储池所用的CRUSH规则集的名称，不过，引用的规则集必须事先存在；

　　erasure-code-profile参数是用于指定纠删码池配置文件；未指定要使用的纠删编码配置文件时，创建命令会为其自动创建一个，并在创建相关的CRUSH规则集时使用到它；默认配置文件自动定义k=2和m=1，这意味着Ceph将通过三个OSD扩展对象数据，并且可以丢失其中一个OSD而不会丢失数据，因此，在冗余效果上，它相当于一个大小为2的副本池 ，不过，其存储空间有效利用率为2/3而非1/2。

　　示例：创建一个副本池

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929232947108-2136747113.png)

　　示例：创建一个纠删码池

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929233116296-401067816.png)

　　2、获取存储池的相关信息

　　列出存储池：ceph osd pool ls \[detail\]

\[cephadm@ceph-admin ~\]$ ceph osd pool ls 
testpool
rbdpool
.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log
cephfs-metadatpool
cephfs-datapool
reppool
erasurepool
\[cephadm@ceph-admin ~\]$ ceph osd pool ls detail 
pool 1 'testpool' replicated size 3 min\_size 2 crush\_rule 0 object\_hash rjenkins pg\_num 16 pgp\_num 16 last\_change 42 flags hashpspool stripe\_width 0
pool 2 'rbdpool' replicated size 3 min\_size 2 crush\_rule 0 object\_hash rjenkins pg\_num 64 pgp\_num 64 last\_change 81 flags hashpspool,selfmanaged\_snaps stripe\_width 0 application rbd
        removed\_snaps \[1~3\]
pool 3 '.rgw.root' replicated size 3 min\_size 2 crush\_rule 0 object\_hash rjenkins pg\_num 8 pgp\_num 8 last\_change 84 owner 18446744073709551615 flags hashpspool stripe\_width 0 application rgw
pool 4 'default.rgw.control' replicated size 3 min\_size 2 crush\_rule 0 object\_hash rjenkins pg\_num 8 pgp\_num 8 last\_change 87 owner 18446744073709551615 flags hashpspool stripe\_width 0 application rgw
pool 5 'default.rgw.meta' replicated size 3 min\_size 2 crush\_rule 0 object\_hash rjenkins pg\_num 8 pgp\_num 8 last\_change 89 owner 18446744073709551615 flags hashpspool stripe\_width 0 application rgw
pool 6 'default.rgw.log' replicated size 3 min\_size 2 crush\_rule 0 object\_hash rjenkins pg\_num 8 pgp\_num 8 last\_change 91 owner 18446744073709551615 flags hashpspool stripe\_width 0 application rgw
pool 7 'cephfs-metadatpool' replicated size 3 min\_size 2 crush\_rule 0 object\_hash rjenkins pg\_num 64 pgp\_num 64 last\_change 99 flags hashpspool stripe\_width 0 application cephfs
pool 8 'cephfs-datapool' replicated size 3 min\_size 2 crush\_rule 0 object\_hash rjenkins pg\_num 128 pgp\_num 128 last\_change 99 flags hashpspool stripe\_width 0 application cephfs
pool 9 'reppool' replicated size 3 min\_size 2 crush\_rule 0 object\_hash rjenkins pg\_num 32 pgp\_num 32 last\_change 126 flags hashpspool stripe\_width 0
pool 10 'erasurepool' erasure size 3 min\_size 2 crush\_rule 1 object\_hash rjenkins pg\_num 32 pgp\_num 32 last\_change 130 flags hashpspool stripe\_width 8192

\[cephadm@ceph-admin ~\]$ 

　　提示：后面接detail表示列出存储池的详细信息；

　　获取存储池的统计数据：ceph osd pool stats \[pool-name\]

\[cephadm@ceph-admin ~\]$ ceph osd pool stats reppool
pool reppool id 9
  nothing is going on

\[cephadm@ceph-admin ~\]$ ceph osd pool stats 
pool testpool id 1
  nothing is going on

pool rbdpool id 2
  nothing is going on

pool .rgw.root id 3
  nothing is going on

pool default.rgw.control id 4
  nothing is going on

pool default.rgw.meta id 5
  nothing is going on

pool default.rgw.log id 6
  nothing is going on

pool cephfs-metadatpool id 7
  nothing is going on

pool cephfs-datapool id 8
  nothing is going on

pool reppool id 9
  nothing is going on

pool erasurepool id 10
  nothing is going on

\[cephadm@ceph-admin ~\]$ 

　　提示：不指定存储池名称表示查看所有存储池的统计数据；

　　显示存储池的用量信息：rados df 或者ceph df

\[cephadm@ceph-admin ~\]$ rados df
POOL\_NAME              USED OBJECTS CLONES COPIES MISSING\_ON\_PRIMARY UNFOUND DEGRADED RD\_OPS     RD WR\_OPS     WR 
.rgw.root           1.1 KiB       4      0     12                  0       0        0     27 18 KiB      4  4 KiB 
cephfs-datapool         0 B       0      0      0                  0       0        0      0    0 B      0    0 B 
cephfs-metadatpool  2.2 KiB      22      0     66                  0       0        0     49 51 KiB     46 13 KiB 
default.rgw.control     0 B       8      0     24                  0       0        0      0    0 B      0    0 B 
default.rgw.log         0 B     175      0    525                  0       0        0  16733 16 MiB  11158    0 B 
default.rgw.meta        0 B       0      0      0                  0       0        0      0    0 B      0    0 B 
erasurepool             0 B       0      0      0                  0       0        0      0    0 B      0    0 B 
rbdpool               389 B       5      0     15                  0       0        0     50 32 KiB     19 10 KiB 
reppool                 0 B       0      0      0                  0       0        0      0    0 B      0    0 B 
testpool                0 B       0      0      0                  0       0        0      2  2 KiB      2  1 KiB 

total\_objects    214
total\_used       10 GiB
total\_avail      890 GiB
total\_space      900 GiB
\[cephadm@ceph-admin ~\]$ 

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929233652617-930365197.png)

　　提示：rados df和ceph df显示稍微有点差别，rados 信息比较全面但偏底层；ceph df人类比较容易看懂；

　　3、存储池重命名

　　命令格式：ceph osd pool rename old-name new-name

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929233947580-1916848080.png)

　　4、删除存储池

　　我们知道删除存储池意味着数据的丢失；所以ceph为了防止我们意外删除存储池实施了两个机制；我们要删除存储池，必须先禁用这两个机制；

　　第一个机制是NODELETE标志，其值需要为false，默认也是false；即允许我们删除；第二个机制是集群范围的配置参数mon allow pool delete，其默认值为“false”，这表示默认不能删除存储池；即我们要删除存储池，需要将第二个机制mon allow pool delete 的值修改为true即可删除存储池；

　　查看nodelete的值命令格式：ceph osd pool get pool-name nodelete

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929234444012-2136302822.png)

　　修改命令nodelete的值命令格式：ceph osd pool set pool-name nodelete false|true

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929234600561-1159997667.png)

　　提示：我们要删存储池，需要将nodelete的值设置为false，即不允许删除为假，即表示允许删除；

　　修改mon allow pool delete的值命令格式：ceph tell mon.\* injectargs --mon-allow-pool-delete={true|false}

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929235046637-1616829457.png)

　　提示：删除之前将其值设置为true，删除完成后再改为false；

　　删除rep-pool存储池

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929235222143-176879269.png)

　　提示：虽然我们进用了上述的两个防止意外删除存储池的机制外，我们在直接删除存储池ceph还会提示我们需要将存储池的名称写两遍以及加--yes-i-really-really-mean-it选项来确定删除存储池的操作；

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929235819083-1019476220.png)

　　提示：删除需要删除的存储池以后，我们需要将mon allow pool delete的值修改为false防止后面误删除存储池；

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220929235930395-1202763710.png)

　　5、设置存储池配额

　　Ceph支持为存储池设置可存储对象的最大数量（max\_objects）和可占用的最大空间（max\_bytes）两个纬度的配额，命令格式ceph osd pool set-quota <pool-name> max\_objects|max\_bytes <val>；获取存储池配额的相关信息命令格式：ceph osd pool get-quota <pool-name>；

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220930000412199-697214388.png)

　　6、配置存储池参数

　　存储池的诸多配置属性保存于配置参数中，获取配置：ceph osd pool get <pool-name> <key>；设定配置：ceph osd pool set <pool-name> <key> <value>；

　　存储池常用的可配置参数

　　size：存储池中的对象副本数；

　　min\_size：I/O所需要的最小副本数；

　　pg\_num：存储池的PG数量；

　　pgp\_num：计算数据归置时要使用的PG的有效数量；

　　crush\_ruleset：用于在集群中映射对象归置的规则组；

　　nodelete：控制是否可删除存储池；

　　nopgchange：控制是否可更改存储池的pg\_num和pgp\_num；

　　nosizechange：控制是否可更改存储池的大小；

　　noscrub和nodeep-scrub：控制是否可整理或深层整理存储池以解决临时高I/O负载的问题；

　　scrub\_min\_interval：集群负载较低时整理存储池的最小时间间隔；默认值为0，表示其取值来自于配置文件中的osd\_scrub\_min\_interval参数；

　　scrub\_max\_interval：整理存储池的最大时间间隔；默认值为0，表示其取值来自于配置文件中的osd\_scrub\_max\_interval参数；

　　deep\_scrub\_interval：深层整理存储池的间隔；默认值为0，表示其取值来自于配置文件中的osd\_deep\_scrub参数；

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220930001343957-1576930262.png)

　　7、存储池快照

　　关于存储池快照

　　• 存储池快照是指整个存储池的状态快照；

　　• 通过存储池快照，可以保留存储池状态的历史；

　　• 创建存储池快照可能需要大量存储空间，具体取决于存储池的大小；

　　创建存储池快照命令格式： ceph osd pool mksnap <pool-name> <snap-name>或者rados -p <pool-name> mksnap <snap-name>

　　列出存储池的快照命令格式：rados -p <pool-name> lssnap

\[cephadm@ceph-admin ~\]$ ceph osd pool mksnap cephfs-metadatpool metadatasnap1
created pool cephfs-metadatpool snap metadatasnap1
\[cephadm@ceph-admin ~\]$ rados -p cephfs-metadatpool lssnap
1       metadatasnap1   2022.09.30 00:20:55
1 snaps
\[cephadm@ceph-admin ~\]$ 

　　回滚存储池至指定的快照命令格式： rados -p <pool-name> rollback <pool-name> <snap-name>

\[cephadm@ceph-admin ~\]$ rados -p cephfs-metadatpool lssnap
1       metadatasnap1   2022.09.30 00:20:55
2       metadatasnap2   2022.09.30 00:22:35
2 snaps
\[cephadm@ceph-admin ~\]$ rados -p cephfs-metadatpool rollback  cephfs-metadatpool metadatasnap2
rolled back pool cephfs-metadatpool to snapshot metadatasnap2
\[cephadm@ceph-admin ~\]$ 

　　删除存储池快照命令格式： ceph osd pool rmsnap <pool-name> <snap-name>或 rados -p <pool-name> rmsnap <snap-name>

\[cephadm@ceph-admin ~\]$ rados -p cephfs-metadatpool lssnap
1       metadatasnap1   2022.09.30 00:20:55
2       metadatasnap2   2022.09.30 00:22:35
2 snaps
\[cephadm@ceph-admin ~\]$ ceph osd pool rmsnap cephfs-metadatpool metadatasnap1
removed pool cephfs-metadatpool snap metadatasnap1
\[cephadm@ceph-admin ~\]$ rados -p cephfs-metadatpool lssnap                   
2       metadatasnap2   2022.09.30 00:22:35
1 snaps
\[cephadm@ceph-admin ~\]$ rados -p cephfs-metadatpool rmsnap metadatasnap2
removed pool cephfs-metadatpool snap metadatasnap2
\[cephadm@ceph-admin ~\]$ rados -p cephfs-metadatpool lssnap
0 snaps
\[cephadm@ceph-admin ~\]$ 

　　提示：不用的快照建议及时清除；

　　8、存储池数据压缩

　　BlueStore存储引擎提供即时数据压缩，以节省磁盘空间，启用压缩命令格式：ceph osd pool set <pool-name> compression\_algorithm snappy；压缩算法有none、zlib、lz4、zstd和snappy等几种，默认为snappy；zstd有较好的压缩比，但比较消耗CPU；lz4和snappy对CPU占用比例较低；不建议使用zlib；

　　设置压缩模式命令格式：ceph osd pool set <pool-name> compression\_mode aggressive ；压缩模式：none、aggressive、passive和force，默认值为none； none表示不压缩； passive表示若提示COMPRESSIBLE，则压缩；aggressive表示除非提示INCOMPRESSIBLE，否则就压缩； force表示始终压缩；

　　其它可用的压缩参数

　　compression\_required\_ratio：指定压缩比，取值格式为双精度浮点型，其值为SIZE\_COMPRESSED/SIZE\_ORIGINAL，即压缩后的大小与原始内容大小的比值，默认为.875；

　　compression\_max\_blob\_size：压缩对象的最大体积，无符号整数型数值，默认为0，表示没有限制；

　　compression\_min\_blob\_size：压缩对象的最小体积，无符号整数型数值，默认为0，表示没有限制；

![](https://img2022.cnblogs.com/blog/1503305/202209/1503305-20220930005311545-1012009507.png)

　　提示：压缩最小体积和最大体积都是以字节为单位；

　　全局压缩选项

　　可在ceph配置文件中设置压缩属性，它将对所有的存储池生效；可设置的相关参数如下

　　• bluestore\_compression\_algorithm  
　　• bluestore\_compression\_mode  
　　• bluestore\_compression\_required\_ratio  
　　• bluestore\_compression\_min\_blob\_size  
　　• bluestore\_compression\_max\_blob\_size  
　　• bluestore\_compression\_min\_blob\_size\_ssd  
　　• bluestore\_compression\_max\_blob\_size\_ssd  
　　• bluestore\_compression\_min\_blob\_size\_hdd  
　　• bluestore\_compression\_max\_blob\_size\_hdd

　　9、纠删码池配置文件

　　列出纠删码配置文件命令格式： ceph osd erasure-code-profile ls

　　获取指定的配置文件的相关内容：ceph osd erasure-code-profile get default

\[cephadm@ceph-admin ~\]$ ceph osd erasure-code-profile ls
default
\[cephadm@ceph-admin ~\]$ ceph osd erasure-code-profile get default
k=2
m=1
plugin=jerasure
technique=reed\_sol\_van
\[cephadm@ceph-admin ~\]$ 

　　自定义纠删码配置文件

　　命令格式：ceph osd erasure-code-profile set <name> \[<directory=directory>\]  \[<plugin=plugin>\] \[<crush-device-class>\] \[<crush-failure-domain>\] \[<key=value> ...\] \[--force\]

　　• - directory：加载纠删码插件的目录路径，默认为/usr/lib/ceph/erasure-code；  
　　• - plugin：用于生成及恢复纠删码块的插件名称，默认为jerasure；  
　　• - crush-device-class：设备类别，例如hdd或ssd，默认为none，即无视类别；  
　　• - crush-failure-domain：故障域，默认为host，支持使用的包括osd、host、rack、row和room等；

　　• - --force：强制覆盖现有的同名配置文件；

　　例如，如果所需的体系结构必须承受两个OSD的丢失，并且存储开销为30％；

\[cephadm@ceph-admin ~\]$ ceph osd erasure-code-profile ls
default
\[cephadm@ceph-admin ~\]$ ceph osd erasure-code-profile set myprofile k=4 m=2 crush-failure-domain=osd
\[cephadm@ceph-admin ~\]$ ceph osd erasure-code-profile ls
default
myprofile
\[cephadm@ceph-admin ~\]$ 

　　纠删码插件

　　Ceph支持以插件方式加载使用的纠删编码插件，存储管理员可根据存储场景的需要优化选择合用的插件。目前，Ceph支持的插件包括如下三个：

　　1、jerasure：最为通用的和灵活的纠删编码插件，它也是纠删码池默认使用的插件；不过，任何一个OSD成员的丢失，都需要余下的所有成员OSD参与恢复过程；另外，使用此类插件时，管理员还可以通过technique选项指定要使用的编码技术；

　　　　• reed\_sol\_van：最灵活的编码技术，管理员仅需提供k和m参数即可；

　　　　• cauchy\_good：更快的编码技术，但需要小心设置PACKETSIZE参数；  
　　　　• reed\_sol\_r6\_op、liberation、blaum\_roth或liber8tion：仅支持使用m=2的编码技术，功能特性类同于RAID 6；

　　2、 lrc：全称为Locally Repairable Erasure Code，即本地修复纠删码，除了默认的m个编码块之外，它会额外在本地创建指定数量（l）的奇偶校验块，从而在一个OSD丢失时，可以仅通过l个奇偶校验块完成恢复；

　　3、isa：仅支持运行在intel CPU之上的纠删编码插件，它支持reed\_sol\_van和cauchy两种技术；

　　例如，下面的命令创建了一个使用lrc插件的配置文件LRCprofile，其本地奇偶校验块为3，故障域为osd

\[cephadm@ceph-admin ~\]$ ceph osd erasure-code-profile set LRCprofile plugin=lrc k=4 m=2 l=3 crush-failure-domain=osd
\[cephadm@ceph-admin ~\]$ ceph osd erasure-code-profile ls
LRCprofile
default
myprofile
\[cephadm@ceph-admin ~\]$ 

作者：[Linux-1874](https://www.cnblogs.com/qiuhom-1874/)

出处：[https://www.cnblogs.com/qiuhom-1874/](https://www.cnblogs.com/qiuhom-1874/)

本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留追究法律责任的权利.