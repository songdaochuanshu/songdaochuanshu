---
layout: post
title: "重新定义性价比!人工智能AI聊天ChatGPT新接口模型gpt-3.5-turbo闪电更新,成本降90%,Python3.10接入"
date: "2023-03-06T01:15:09.629Z"
---
重新定义性价比!人工智能AI聊天ChatGPT新接口模型gpt-3.5-turbo闪电更新,成本降90%,Python3.10接入
=================================================================

![重新定义性价比!人工智能AI聊天ChatGPT新接口模型gpt-3.5-turbo闪电更新,成本降90%,Python3.10接入](https://img2023.cnblogs.com/blog/335778/202303/335778-20230306081845977-1758299591.png) 北国春迟，春寒料峭略带阴霾，但ChatGPT新接口模型gpt-3.5-turbo的更新为我们带来了一丝暖意，使用成本更加亲民，比高端产品ChatGPT Plus更实惠也更方便，毕竟ChatGPT Plus依然是通过网页端来输出，Api接口是以token的数量来计算价格的，0.002刀每1000个token，token可以理解为字数，说白了就是每1000个字合0.01381人民币，以ChatGPT无与伦比的产品力而言，如此低的使用成本让所有市面上其他所有类ChatGPT产品都黯然失光。

北国春迟，春寒料峭略带阴霾，但ChatGPT新接口模型gpt-3.5-turbo的更新为我们带来了一丝暖意，使用成本更加亲民，比高端产品ChatGPT Plus更实惠也更方便，毕竟ChatGPT Plus依然是通过网页端来输出，Api接口是以token的数量来计算价格的，0.002刀每1000个token，token可以理解为字数，说白了就是每1000个字合0.01381人民币，以ChatGPT无与伦比的产品力而言，如此低的使用成本让所有市面上其他所有类ChatGPT产品都黯然失光。

本次让我们使用Python3.10光速接入ChatGPT API的新模型gpt-3.5-turbo。

OpenAI库的SDK方式接入
---------------

OpenAI官方同步更新了接口Api的三方库openai，版本为0.27.0，如果要使用新的模型gpt-3.5-turbo，就必须同步安装最新版本：

    pip3 install openai==0.27.0
    

随后建立chat.py文件：

    import openai  
      
    openai.api_key = "openai的接口apikey"   
      
    completion = openai.ChatCompletion.create(  
      model="gpt-3.5-turbo",   
      messages=[{"role": "user", "content": "北国风光，千里冰封，万里雪飘，请接着续写，使用沁园春的词牌"}]  
    )  
      
    print(completion["choices"][0]["message"]["content"])
    

程序返回：

    瑶池冰缘，雪舞凄美， 隔窗寒意，似乎钻进衣袖。  
    寒塘渡鸭，雪中梅影， 孤独是一片银白的姿态。  
    冰雪如花，开放在草莓园里， 可爱的雪人，瑟瑟发抖着欢呼。  
    北风凛冽，寒暄难挡， 四季明媚，但冬日尤甜美。  
    千里冰封，万里雪飘， 窗外天下壮观，此时正是京城美。
    

闪电般秒回，让用惯了ChatGPT网页端的我们几乎不能适应。

gpt-3.5-turbo，对得起turbo的加成，带涡轮的ChatGPT就是不一样。

ChatGPT聊天上下文
------------

我们知道ChatGPT的最大特色就是可以联系语境中的上下文，换句话说，ChatGPT可以根据之前的回答来优化之后的回答，形成上下文关系，让人机对话更加连贯和富有逻辑性。

这里取决于输入参数中的role参数，每一个role的取值，对应的场景不一样，其中system用于在对话开始时给ChatGPT一个指示或声明，有点像引导词，使得后续的回答更具有个性化和专业化。user是用于给用户提问的或者说是用来给用户输入引导词的。assistant顾名思义，是用于输入ChatGPT的回答内容:

    import openai  
      
    openai.api_key = "apikey"   
      
      
    class ChatGPT:  
        def __init__(self,chat_list=[]) -> None:  
            # 初始化对话列表  
            self.chat_list = []  
          
        # 显示接口返回  
        def show_conversation(self,msg_list):  
            for msg in msg_list:  
                if msg['role'] == 'user':  
                    print(f"Me: {msg['content']}\n")  
                else:  
                    print(f"ChatGPT: {msg['content']}\n")  
      
        # 提示chatgpt  
        def ask(self,prompt):  
            self.chat_list.append({"role":"user","content":prompt})  
            response = openai.ChatCompletion.create(model="gpt-3.5-turbo",messages=self.chat_list)  
            answer = response.choices[0].message['content']  
            # 添加历史对话，形成上下文关系  
            self.chat_list.append({"role":"assistant","content":answer})  
            self.show_conversation(self.chat_list)
    

这里每一次会话都会加入chat\_list的聊天列表，角色为assistant，为了让ChatGPT分析上下文，下面开始测试：

    if __name__ == '__main__':  
      
        chat = ChatGPT()  
      
        chat.ask("你是一位南宋词人，词风婉约，有点类似李清照女士，请使用蝶恋花词牌描写北国春光")
    

程序返回：

    Me: 你是一位南宋词人，词风婉约，有点类似李清照女士，请使用蝶恋花词牌描写北国春光  
      
    ChatGPT: 北国春光，清冽宜人。望眼欲穿的远山如翠起伏，遥远而缥缈。层层叠叠的林木，新绿徜徉，婆娑摇曳。风儿吹起，沁人心脾，点点梅花飘至，宛如仙境。  
      
    花间蝶恋，春色满园。莺莺燕燕，鸟鸣花落，时时惹人遐思。碧空万里，蓝天白云，彩云飘飘，缤纷夺目。柳絮飘飘，轻羽翩翩，小河潺潺，流水声声，婉转动人。  
      
    清风拂面，落英缤纷。听着草虫唱起，充满阳光的气息，轻轻飘荡，仿佛一条无形的小河，展开春天的美好，留下美好的记忆。人间万象，却只有这春色无边，似乎奔向远方的快乐。
    

此时再次发问：

    chat.ask("请使用另外一种粗狂阳刚的风格再写一遍上面的词")
    

程序返回：

    Me: 请使用另外一种粗狂阳刚的风格再写一遍上面的词  
      
    ChatGPT: 北国春光，不柔不媚，金色的阳光照在地上，充满了男子气概。  
    草原上风吹不断，那些疯狂的野花，在春风中舞蹈。  
    看！那些猛禽静静地盘旋在高空，监视着整片草原，威武雄壮。  
    花丛间，一只雄性蜂鹰跃跃欲飞，看上去仿佛要冲破天际。  
    这里的春天有时带着风沙，但这并不能阻止狂放豪迈的草原奔腾前行，而这样的北国春光，怎会轻易被遗忘！
    

虽然内容有些尬，但确实联系了上下文。

需要注意的是，token不仅计算ChatGPT的接口返回内容，也会计算用户的发送内容，token的计算方法不是简单的一词一个，例如中文输入，一个中文汉字占2个字节数，而对于一次中文测试中，50个汉字被算为100个tokens，差不多是英文的一倍，而token还计算api发送中的角色字段，如果像上文一样实现上下文操作，就必须发送ChatGPT接口返回的历史聊天列表，这意味着ChatGPT上下文聊天的成本并不是我们想象中的那么低，需要谨慎使用。

原生ChatGPT接口异步访问
---------------

除了官方的SDK，新接口模型也支持原生的Http请求方式，比如使用requests库：

    pip3 install requests
    

直接请求openai官方接口：

    import requests  
    h = {  
        'Content-Type': 'application/json',  
        'Authorization': 'Bearer apikey'  
    }  
    d = {  
        "model": "gpt-3.5-turbo",  
        "messages":[{"role": "user", "content": "请解释同步请求和异步请求的区别"}],  
        "max_tokens": 100,  
        "temperature": 0  
    }  
    u = 'https://api.openai.com/v1/chat/completions'  
    r = requests.post(url=u, headers=h, json=d).json()  
    print(r)
    

程序返回：

    {'id': 'chatcmpl-6qDNQ9O4hZPDT1Ju902coxypjO0mY',   
    'object': 'chat.completion',   
    'created': 1677902496, 'model': 'gpt-3.5-turbo-0301',   
    'usage': {'prompt_tokens': 20, 'completion_tokens': 100, 'total_tokens': 120},   
    'choices': [{'message':   
    {'role': 'assistant',   
    'content': '\n\n同步请求和异步请求是指在客户端向服务器发送请求时，客户端等待服务器响应的方式不同。\n\n同步请求是指客户端发送请求后，必须等待服务器响应后才能继续执行后续的代码。在等待服务器响应的过程中，客户端的界面会被阻塞，用户无法进行'},   
    'finish_reason': 'length', 'index': 0}]}
    

ChatGPT原生接口也支持异步方式请求，这里使用httpx:

    pip3 install httpx
    

编写异步请求：

    h = {  
        'Content-Type': 'application/json',  
        'Authorization': 'Bearer apikey'  
    }  
    d = {  
        "model": "gpt-3.5-turbo",  
        "messages":[{"role": "user", "content": "请解释同步请求和异步请求的区别"}],  
        "max_tokens": 100,  
        "temperature": 0  
    }  
    u = 'https://api.openai.com/v1/chat/completions'  
      
    import asyncio  
    import httpx  
      
    async def main():  
        async with httpx.AsyncClient() as client:  
            resp = await client.post(url=u, headers=h, json=d)  
            result = resp.json()  
            print(result)  
      
    asyncio.run(main())
    

程序返回:

    {'id': 'chatcmpl-6qDNQ9O4hZPDT1Ju902coxypjO0mY',   
    'object': 'chat.completion',   
    'created': 1677902496, 'model': 'gpt-3.5-turbo-0301',   
    'usage': {'prompt_tokens': 20, 'completion_tokens': 100, 'total_tokens': 120},   
    'choices': [{'message':   
    {'role': 'assistant',   
    'content': '\n\n同步请求和异步请求是指在客户端向服务器发送请求时，客户端等待服务器响应的方式不同。\n\n同步请求是指客户端发送请求后，必须等待服务器响应后才能继续执行后续的代码。在等待服务器响应的过程中，客户端的界面会被阻塞，用户无法进行'},   
    'finish_reason': 'length', 'index': 0}]}
    

我们也可以将异步请求方式封装到对话类中，完整代码：

    import openai  
    import asyncio  
    import httpx  
      
    openai.api_key = "apikey"   
      
    h = {  
        'Content-Type': 'application/json',  
        'Authorization': f'Bearer {openai.api_key}'  
    }  
    d = {  
        "model": "gpt-3.5-turbo",  
        "messages":[{"role": "user", "content": "请解释同步请求和异步请求的区别"}],  
        "max_tokens": 100,  
        "temperature": 0  
    }  
    u = 'https://api.openai.com/v1/chat/completions'  
      
      
    class ChatGPT:  
        def __init__(self,chat_list=[]) -> None:  
            # 初始化对话列表  
            self.chat_list = []  
      
        # 异步访问  
        async def ask_async(self,prompt):  
      
            d["messages"][0]["content"] = prompt  
            async with httpx.AsyncClient() as client:  
                resp = await client.post(url=u, headers=h, json=d)  
                result = resp.json()  
                print(result)  
      
          
        # 显示接口返回  
        def show_conversation(self,msg_list):  
            for msg in msg_list:  
                if msg['role'] == 'user':  
                    print(f"Me: {msg['content']}\n")  
                else:  
                    print(f"ChatGPT: {msg['content']}\n")  
      
        # 提示chatgpt  
        def ask(self,prompt):  
            self.chat_list.append({"role":"user","content":prompt})  
            response = openai.ChatCompletion.create(model="gpt-3.5-turbo",messages=self.chat_list)  
            answer = response.choices[0].message['content']  
            # 添加历史对话，形成上下文关系  
            self.chat_list.append({"role":"assistant","content":answer})  
            self.show_conversation(self.chat_list)  
      
      
    if __name__ == '__main__':  
      
        chat = ChatGPT()  
      
        chat.ask("你是一位南宋词人，词风婉约，有点类似李清照女士，请使用蝶恋花词牌描写北国春光")  
      
        chat.ask("请使用另外一种粗狂阳刚的风格再写一遍上面的词")  
      
        asyncio.run(chat.ask_async("请解释同步请求接口和异步请求接口的区别"))
    

结语
--

低成本ChatGPT接口模型gpt-3.5-turbo更容易接入三方的客户端，比如微信、QQ、钉钉群之类，比起ChatGPT网页端，ChatGPT接口的响应速度更加迅速且稳定，ChatGPT，永远的神，没有之一，且不可替代，最后奉上异步上下文封装项目，与君共觞：github.com/zcxey2911/chatgpt\_api\_Contextual\_async