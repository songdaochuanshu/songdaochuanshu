---
layout: post
title: "解密Prompt系列3. 冻结LM微调Prompt: Prefix-Tuning & Prompt-Tuning & P-Tuning"
date: "2023-03-10T01:17:48.140Z"
---
解密Prompt系列3. 冻结LM微调Prompt: Prefix-Tuning & Prompt-Tuning & P-Tuning
===================================================================

![解密Prompt系列3. 冻结LM微调Prompt: Prefix-Tuning &amp; Prompt-Tuning &amp; P-Tuning](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083814779-1247080114.png) 这一章我们介绍在下游任务微调中固定LM参数，只微调Prompt的相关模型。这类模型的优势很直观就是微调的参数量小，能大幅降低LLM的微调参数量，是轻量级的微调替代品

这一章我们介绍在下游任务微调中固定LM参数，只微调Prompt的相关模型。这类模型的优势很直观就是微调的参数量小，能大幅降低LLM的微调参数量，是轻量级的微调替代品。和前两章微调LM和全部冻结的prompt模板相比，微调Prompt范式最大的区别就是prompt模板都是连续型（Embedding），而非和Token对应的离散型模板。核心在于我们并不关心prompt本身是否是自然语言，只关心prompt作为探针能否引导出预训练模型在下游任务上的特定能力。

固定LM微调Prompt的范式有以下几个优点

*   性价比高！微调参数少，冻结LM只微调prompt部分的参数
*   无人工参与！无需人工设计prompt模板，依赖模型微调即可
*   多任务共享模型！因为LM被冻结，只需训练针对不同任务的prompt即可。因此可以固定预训练模型，拔插式加入Prompt用于不同下游任务

Prefix-Tuning
-------------

> *   Paper: 2021.1 Optimizing Continuous Prompts for Generation
> *   Github：[https://github.com/XiangLi1999/PrefixTuning](https://github.com/XiangLi1999/PrefixTuning)
> *   Prompt: Continus Prefix Prompt
> *   Task & Model：BART(Summarization), GPT2(Table2Text)

最早提出Prompt微调的论文之一，其实是可控文本生成领域的延伸，因此只针对摘要和Table2Text这两个生成任务进行了评估。

先唠两句可控文本生成，哈哈其实整个Prompt范式也是通用的可控文本生成不是，只不过把传统的Topic控制，文本情绪控制，Data2Text等，更进一步泛化到了不同NLP任务的生成控制~~

Prefix-Tuning可以理解是CTRL\[1\]模型的连续化升级版，为了生成不同领域和话题的文本，CTRL是在预训练阶段在输入文本前加入了control code，例如好评前面加'Reviews Rating:5.0',差评前面加'Reviews Rating:1.0', 政治评论前面加‘Politics Title:’，把语言模型的生成概率，优化成了基于文本主题的条件概率。

Prefix-Tuning进一步把control code优化成了虚拟Token，每个NLP任务对应多个虚拟Token的Embedding（prefix），对于Decoder-Only的GPT，prefix只加在句首，对于Encoder-Decoder的BART，不同的prefix同时加在编码器和解码器的开头。在下游微调时，LM的参数被冻结，只有prefix部分的参数进行更新。不过这里的prefix参数不只包括embedding层而是虚拟token位置对应的每一层的activation都进行更新。

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748064-1984051245.png)

对于连续Prompt的设定，论文还讨论了几个细节如下

1.  prefix矩阵分解

作者发现直接更新多个虚拟token的参数效果很不稳定，因此作者在prefix层加了MLP，分解成了更小的embedding层 \* 更大的MLP层。原始的Embedding层参数是n\_prefix \* emb\_dim, 调整后变为n\_prefix \* n\_hidden + n\_hidden \* emb\_dim。训练完成后这部分就不再需要只保留MLP输出的参数进行推理即可

个人感觉MLP的加入是为了增加多个虚拟token之间的共享信息，因为它们和常规的连续文本存在差异，需要被作为一个整体考虑，可能对prefix位置编码进行特殊处理也阔以？？

2.  prefix长度

prefix部分到底使用多少个虚拟token，直接影响模型微调的参数量级，以及处理长文本的能力。默认的prefix长度为10，作者在不同任务上进行了微调，最优参数如下。整体上prompt部分的参数量都在原模型的~0.1%

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748049-1141414933.png)

3.  其他：作者还对比了把prefix放在不同位置，以及使用任务相关的Token来初始化prefix embedding的设定，前者局限性较大，后者在后面的paper做了更详细的消融实验

效果上在Table2Text任务上，只有0.1%参数量级的prompt tuning效果要优于微调，

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748054-1157647398.png)

在Xsum摘要任务上，prompt的效果要略差于微调。

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748075-2054235192.png)

Prompt-Tunning
--------------

> *   Paper: 2021.4 The Power of Scale for Parameter-Efficient Prompt Tuning
> *   prompt：Continus Prefix Prompt
> *   Github: [https://github.com/google-research/prompt-tuning](https://github.com/google-research/prompt-tuning)
> *   Task: SuperGLUE NLU任务
> *   Model: T5 1.1（在原T5上进行了细节优化）

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748076-1300319570.png)

Prompt-Tunning是以上prefix-Tunning的简化版本，面向NLU任务，进行了更全面的效果对比，并且在大模型上成功打平了LM微调的效果~

### 简化

对比Prefix-Tunning，prompt-tuning的主要差异如下，

论文使用100个prefix token作为默认参数，大于以上prefix-tuning默认的10个token，不过差异在于prompt-Tunning只对输入层(Embedding)进行微调，而Prefix是对虚拟Token对应的上游layer全部进行微调。因此Prompt-Tunning的微调参数量级要更小，且不需要修改原始模型结构，这是“简化”的来源。相同的prefix长度，Prompt-Tunning(<0.01%)微调的参数量级要比Prefix-Tunning(0.1%~1%)小10倍以上，如下图所示

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748087-2145864414.png)

为什么上面prefix-tuning只微调embedding层效果就不好，放在prompt-tuning这里效果就好了呢？因为评估的任务不同无法直接对比，个人感觉有两个因素，一个是模型规模，另一个是继续预训练，前者的可能更大些，在下面的消融实验中会提到

### 效果&消融实验

在SuperGLUE任务上，随着模型参数的上升，PromptTunning快速拉近和模型微调的效果，110亿的T5模型(上面prefix-tuning使用的是15亿的GPT2)，已经可以打平在下游多任务联合微调的LM模型，并且远远的甩开了Prompt Design（GPT3 few-shot）

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748389-1063638257.png)

作者也做了全面的消融实验，包括以下4个方面，最核心的感受就是**只要模型足够够大一切都好说**

1.  prompt长度(a)：固定其他参数，作者尝试了{1，5，20，100，150}, 当模型规模到百亿后，只要prompt长度大于1，更长的prompt并不能带来效果提升
2.  Prompt初始化(b): 作者尝试了随机uniform初始化，用标签文本空间初始化，和用Top5K高频词采样初始化，在10^8规模，标签词初始化效果最好。作者发现预测label也会在对应prompt空间内。不过到百亿规模后，初始化带来的影响就会消失
3.  T5继续预训练(c):作者认为T5本身的Span Corruption预训练目标和掩码词，并不适合冻结LM的场景，因为在微调中模型可以调整预训练目标和下游目标的差异，而只使用prompt可能无法弥合差异。其实这里已经能看出En-Dn框架在生成场景下没有GPT这样的Decoder来的自然。因此作者基于LM目标对T5进行继续预训练
4.  继续预训练step(d)：以上的继续预训练steps，继续预训练步数越高，模型效果在不同模型规模上越单调。

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748390-1466756032.png)

### 可解释

考虑Prompt-Tunning使用Embedding来表征指令，可解释性较差。作者使用cosine距离来搜索prompt embedding对应的Top5近邻。发现

*   embedding的近邻出现语义相似的cluster，例如{ Technology / technology / Technologies/ technological / technologies }, 说明**连续prompt实际可能是相关离散prompt词的聚合语义**
*   当连续prompt较长（len=100）, 存在多个prompt token的KNN相同：个人认为这和prefix-tuning使用MLP那里我的猜测相似，**prompt应该是一个整体**
*   使用标签词初始化，微调后标签词也大概率会出现在prompt的KNN中，说明初始化可以提供更好的prior信息加速收敛

P-Tuning
--------

> *   Paper: 2021.3, GPT Understands, Too
> *   prompt：Continus Prefix Prompt
> *   Task: NLU任务, 知识探测任务
> *   github: [https://github.com/THUDM/P-tuning](https://github.com/THUDM/P-tuning)
> *   Model: GPT2 & BERT

P-Tuning和Prompt-Tuning几乎是同时出现，思路也是无比相似。不过这个在prompt综述中被归类为LM+Prompt同时微调的范式，不过作者其实两种都用了。因此还是选择把p-tuning也放到这一章，毕竟个人认为LM+Prompt的微调范式属实有一点不是太必要。。。

论文同样是连续prompt的设计。不过针对上面提到的Prompt的整体性问题进行了优化。作者认为直接通过虚拟token引入prompt存在两个问题

*   离散性：如果用预训练词表的embedding初始化，经过预训练的词在空间分布上较稀疏，微调的幅度有限，容易陷入局部最优。这里到底是**局部最优**还是**有效信息prior**其实很难分清
*   整体性：多个token的连续prompt应该相互依赖作为一个整体，不谋而合了！

针对这两个问题，作者使用双向LSTM+2层MLP来对prompt进行表征, 这样LSTM的结构提高prompt的整体性，Relu激活函数的MLP提高离散型。这样更新prompt就是对应更新整个lstm+MLP部分的Prompt Encoder。下面是p-tuning和离散prompt的对比

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748385-345394847.png)

作者分别对LAMA知识探测和SuperGLUE文本理解进行了评测。针对知识抽取，作者构建的prompt模板如下，以下3是虚拟prompt词的数量，对应prompt encoder输出的embedding数

*   BERT：(3, sub,3,obj,3)
*   GPT（3，sub,3,obj）

在知识探测任务中，默认是固定LM只微调prompt。效果上P-tuning对GPT这类单项语言模型的效果提升显著，显著优于人工构建模板和直接微调，使得GPT在不擅长的知识抽取任务中可以基本打平BERT的效果。

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748377-1538023560.png)

针对SuperGLUE作者是做了LM+Prompt同时微调的设定。个人对LM+prompt微调的逻辑不是认同，毕竟1+1<2，同时微调它既破坏了预训练的语言知识，也没节省微调的参数量级，感觉逻辑上不是非常讲的通（哈哈坐等之后被打脸）。结论基本和以上知识探测相似

![](https://img2023.cnblogs.com/blog/1326688/202303/1326688-20230310083748388-633317321.png)

**开头说优点，结尾说下局限性**

*   可解释性差：这是所有连续型prompt的统一问题
*   收敛更慢: 更少的参数想要撬动更大的模型，需要更复杂的空间搜索
*   可能存在过拟合：只微调prompt，理论上是作为探针，但实际模型是否真的使用prompt部分作为探针，而不是直接去拟合任务导致过拟合是个待确认的问题
*   微调可能存在不稳定性：prompt-tuning和p-tuning的github里都有提到结果在SuperGLUE上无法复现的问题

**更多Prompt相关论文，AIGC相关玩法戳这里[DecryptPrompt](https://github.com/DSXiangLi/DecryptPrompt)**

* * *

### Reference

1.  CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION。可以当做prefix-tuning的前导文来看
2.  WRAP: Word-level Adversarial ReProgramming。介于Prefix-Tunning和Prompt-Tunning之间，这里就不细说了
3.  苏神https://kexue.fm/archives/8295