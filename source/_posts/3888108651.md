---
layout: post
title: "【爬虫+数据分析+数据可视化】python数据分析全流程《2021胡润百富榜》榜单数据！"
date: "2022-12-29T05:14:37.410Z"
---
【爬虫+数据分析+数据可视化】python数据分析全流程《2021胡润百富榜》榜单数据！
============================================

用python爬取并分析《2021胡润百富榜》的榜单数据！ 1、python爬虫讲解（requests向接口请求）。 2、python数据分析讲解（pandas数据分析及可视化画图）含：直方图、柱形图、饼图、词云图等。

目录

*   [一、爬虫](#一爬虫)
    *   [1.1 爬取目标](#11-爬取目标)
    *   [1.2 分析页面](#12-分析页面)
    *   [1.3 爬虫代码](#13-爬虫代码)
    *   [1.4 结果数据](#14-结果数据)
*   [二、数据分析](#二数据分析)
    *   [2.1 导入库](#21-导入库)
    *   [2.2 数据概况](#22-数据概况)
    *   [2.3 可视化分析](#23-可视化分析)
        *   [2.3.1 财富分布](#231-财富分布)
        *   [2.3.2 年龄分布](#232-年龄分布)
        *   [2.3.3 公司总部分布](#233-公司总部分布)
        *   [2.3.4 性别分布](#234-性别分布)
        *   [2.3.5 行业分布](#235-行业分布)
        *   [2.3.6 组织结构分布](#236-组织结构分布)
        *   [2.3.7 公司名称词云图](#237-公司名称词云图)
*   [三、整体结论](#三整体结论)
*   [四、同步视频讲解](#四同步视频讲解)
    *   [4.1 上集（爬虫讲解）](#41-上集爬虫讲解)
    *   [4.2 下集（数据分析讲解）](#42-下集数据分析讲解)
*   [五、附完整源码](#五附完整源码)

一、爬虫
====

1.1 爬取目标
--------

本次爬取的目标是，2021年胡润百富榜的榜单数据：[胡润百富 - 榜单](https://www.hurun.net/zh-CN/Rank/HsRankDetails?pagetype=rich)  
​![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229122244404-499207461.png)

页面上能看到的信息有：  
排名、财富值、排名变化、个人信息（姓名、性别、年龄）、企业信息（企业名称、所属行业）  
页面结构很整齐，数据也很完整，非常适合爬虫和数据分析使用。

1.2 分析页面
--------

老规矩，打开Chrome浏览器，按F12进入开发者模式，依次点击Network->Fetch/XHR，准备好捕获ajax请求。  
重新刷新一下页面，发现一条请求：  
​![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229122304085-1596015161.png)

在预览界面，看到一共20条（0~19）返回数据，正好对应页面上的20个富豪信息。  
所以，后面编写爬虫代码，针对这个地址发送请求就可以了。  
另外，关于翻页，我的个人习惯是，选择每页显示最多的数据量，这样能保证少翻页几次，少发送几次请求，防止被对端服务器反爬。  
所以，每页选择200条数据：  
​![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229122330636-1807231582.png)

再刷新一下页面，进行几次翻页，观察请求地址的变化规律：  
​![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229122340945-1820250955.png)

以翻到第3页为例，url中的offset（偏移量）为400，limit（每页的条数）为200，所以，可得出规律：

> offset = (page - 1) \* 200  
> limit = 200

下面开始编写爬虫代码。

1.3 爬虫代码
--------

首先，导入需要用到的库：

    import requests  # 发送请求
    import pandas as pd  # 存入excel数据
    from time import sleep  # 等待间隔,防止反爬
    import random  # 随机等待
    

根据1.2章节分析得出的结论，编写逻辑代码，向页面发送请求：

    # 循环请求1-15页
    for page in range(1, 16):
    	# 胡润百富榜地址
    	sleep_seconds = random.uniform(1, 2)
    	print('开始等待{}秒'.format(sleep_seconds))
    	sleep(sleep_seconds)
    	print('开始爬取第{}页'.format(page))
    	offset = (page - 1) * 200
    	url = 'https://www.hurun.net/zh-CN/Rank/HsRankDetailsList?num=YUBAO34E&search=&offset={}&limit=200'.format(offset)
    	# 构造请求头
    	headers = {
    		'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Mobile Safari/537.36',
    		'accept': 'application/json, text/javascript, */*; q=0.01',
    		'accept-language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
    		'accept-encoding': 'gzip, deflate, br',
    		'content-type': 'application/json',
    		'referer': 'https://www.hurun.net/zh-CN/Rank/HsRankDetails?pagetype=rich'
    	}
    	# 发送请求
    	r = requests.get(url, headers=headers)
    

用json格式解析返回的请求数据：（一行代码即可完成接收）

    json_data = r.json()
    

由于解析的字段较多，这里不再赘述详细过程，字段信息包含：

    Fullname_Cn_list = []  # 全名_中文
    Fullname_En_list = []  # 全名_英文
    Age_list = []  # 年龄
    BirthPlace_Cn_list = []  # 出生地_中文
    BirthPlace_En_list = []  # 出生地_英文
    Gender_list = []  # 性别
    Photo_list = []  # 照片
    ComName_Cn_list = []  # 公司名称_中文
    ComName_En_list = []  # 公司名称_英文
    ComHeadquarters_Cn_list = []  # 公司总部地_中文
    ComHeadquarters_En_list = []  # 公司总部地_英文
    Industry_Cn_list = []  # 所在行业_中文
    Industry_En_list = []  # 所在行业_英文
    Ranking_list = []  # 排名
    Ranking_Change_list = []  # 排名变化
    Relations_list = []  # 组织结构
    Wealth_list = []  # 财富值_人民币_亿
    Wealth_Change_list = []  # 财富值变化
    Wealth_USD_list = []  # 财富值_美元
    Year_list = []  # 年份
    

最后，依然采用我最习惯的保存数据的方法，先拼装DataFrame数据：

    df = pd.DataFrame(  # 拼装爬取到的数据为DataFrame
    		{
    			'排名': Ranking_list,
    			'排名变化': Ranking_Change_list,
    			'全名_中文': Fullname_Cn_list,
    			'全名_英文': Fullname_En_list,
    			'年龄': Age_list,
    			'出生地_中文': BirthPlace_Cn_list,
    			'出生地_英文': BirthPlace_En_list,
    			'性别': Gender_list,
    			'照片': Photo_list,
    			'公司名称_中文': ComName_Cn_list,
    			'公司名称_英文': ComName_En_list,
    			'公司总部地_中文': ComHeadquarters_Cn_list,
    			'公司总部地_英文': ComHeadquarters_En_list,
    			'所在行业_中文': Industry_Cn_list,
    			'所在行业_英文': Industry_En_list,
    			'组织结构': Relations_list,
    			'财富值_人民币_亿': Wealth_list,
    			'财富值变化': Wealth_Change_list,
    			'财富值_美元': Wealth_USD_list,
    			'年份': Year_list
    		}
    				)
    

再用pandas的to\_csv方法保存：

    # 保存结果数据
    df.to_csv('2021胡润百富榜.csv', mode='a+', index=False, header=header, encoding='utf_8_sig')
    

注意，加上这个编码格式选项（utf\_8\_sig），否则产生乱码哦。  
爬虫开发完成，下面展示结果数据。

1.4 结果数据
--------

看一下榜单上TOP20的数据吧：  
​![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229122534585-971770833.png)

数据一共2916条，19个字段信息，含：

> 排名、排名变化、全名\_中文、全名\_英文、年龄、出生地\_中文、出生地\_英文、性别、公司名称\_中文、公司名称\_英文、公司总部地\_中文、公司总部地\_英文、所在行业\_中文、所在行业\_英文、组织结构、财富值\_人民币\_亿、财富值变化、 财富值\_美元、年份。

数据信息还是很丰富的，希望能够挖掘出一些有价值的结论！

二、数据分析
======

2.1 导入库
-------

首先，导入用于数据分析的库：

    import pandas as pd  # 读取csv文件
    import matplotlib.pyplot as plt  # 画图
    from wordcloud import WordCloud  # 词云图
    

增加一个配置项，用于解决matplotlib中文乱码的问题：

    # 解决中文显示问题
    plt.rcParams['font.sans-serif'] = ['SimHei']  # 显示中文标签  # 指定默认字体
    plt.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题
    

读取csv数据：

    # 读取csv数据
    df = pd.read_csv('2021胡润百富榜.csv')
    

2.2 数据概况
--------

查看数据形状：  
​![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229122638343-1615466141.png)

查看前3名富豪：  
​![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229122647888-1330100579.png)

查看最后3名富豪：  
​![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229122659856-331938737.png)

描述性统计：  
​![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229122707108-610959870.png)

从描述性统计，可以得出结论：  
**从最大值3900亿、最小值20亿、方差242来看，分布很零散，各位富豪掌握的财富差距很大，马太效应明显。**

2.3 可视化分析
---------

### 2.3.1 财富分布

代码：

    df_Wealth = df['财富值_人民币_亿']
    # 绘图
    df_Wealth.plot.hist(figsize=(18, 6), grid=True, title='财富分布-直方图')
    # 保存图片
    plt.savefig('财富分布-直方图.png')
    

可视化图：  
![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229123307622-1010553776.png)

结论：**大部分的富豪的财富集中在20亿~400亿之间，个别顶级富豪的财富在3000亿以上。**

### 2.3.2 年龄分布

代码：

    # 剔除未知
    df_Age = df[df.年龄 != '未知']
    # 数据切割，8个分段
    df_Age_cut = pd.cut(df_Age.年龄.astype(float), bins=[20, 30, 40, 50, 60, 70, 80, 90, 100])
    # 画柱形图
    df_Age_cut.value_counts().plot.bar(figsize=(16, 6), title='年龄分布-柱形图')
    # 保存图片
    plt.savefig('年龄分布-柱形图.png')
    

可视化图：  
![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229123337393-356327537.png)

结论：**大部分富豪的年龄在50-60岁，其次是60-70和40-50岁。极少数富豪在20-30岁（年轻有为👍）**

### 2.3.3 公司总部分布

代码：

    df_ComHeadquarters = df['公司总部地_中文'].value_counts()
    # 绘图
    df_ComHeadquarters.nlargest(n=30).plot.bar(
        figsize=(16, 6),  # 图片大小
        grid=False,  # 显示网格
        title='公司总部分布TOP30-柱形图'  # 图片标题
    )
    # 保存图片
    plt.savefig('公司总部分布TOP30-柱形图.png')
    

可视化图：  
![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229123355121-809020621.png)

结论：**公司分布城市，大多集中在北上广深等一线城市，另外杭州、香港、苏州也位列前茅。**

### 2.3.4 性别分布

代码：

    df_Gender = df['性别'].value_counts()
    # 绘图
    df_Gender.plot.pie(
        figsize=(8, 8),  # 图片大小
        legend=True,  # 显示图例
        autopct='%1.2f%%',  # 百分比格式
        title='性别占比分布-饼图',  # 图片标题
    )
    # 保存图片
    plt.savefig('性别占比分布-饼图.png')
    

可视化图：  
![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229123411882-1567223335.png)

结论：**男性富豪占据绝大多数，个别女性在列（巾帼不让须眉👍）**

### 2.3.5 行业分布

代码：

    df_Industry = df['所在行业_中文'].value_counts()
    df_Industry.nlargest(n=20).plot.bar(
        figsize=(18, 6),  # 图片大小
        grid=False,  # 显示网格
        title='行业分布TOP20-柱形图'  # 图片标题
    )
    # 保存图片
    plt.savefig('行业分布TOP20-柱形图.png')
    

可视化图：  
![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229123430220-949634917.png)

结论：**百富榜中占比最多的行业分别是：房地产、医药、投资、化工等。**

### 2.3.6 组织结构分布

代码：

    df_Relations = df['组织结构'].value_counts()
    # 绘图
    df_Relations.plot.pie(
        figsize=(8, 8),  # 图片大小
        legend=True,  # 显示图例
        autopct='%1.2f%%',  # 百分比格式
        title='组织结构分布-饼图',  # 图片标题
    )
    # 保存图片
    plt.savefig('组织结构分布-饼图.png')
    

可视化图：  
![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229123446884-2058730858.png)

结论：**半数以上是未知，企业未对外开放，或榜单没有统计到；家族和夫妇占据前两类。**

### 2.3.7 公司名称词云图

代码：

    ComName_list = df['公司名称_中文'].values.tolist()
    ComName_str = ' '.join(ComName_list)
    stopwords = []  # 停用词
    # backgroud_Image = np.array(Image.open('幻灯片2.png'))  # 读取背景图片
    wc = WordCloud(
        scale=3,  # 清晰度
        background_color="white",  # 背景颜色
        max_words=1000,#最大字符数
        width=800,  # 图宽
        height=500,  # 图高
        font_path='/System/Library/Fonts/SimHei.ttf',  # 字体文件路径，根据实际情况替换
        stopwords=stopwords,  # 停用词
        # 	mask=backgroud_Image,  # 背景图片
    )
    wc.generate_from_text(ComName_str)  # 生成词云图
    wc.to_file('2021胡润百富榜_公司名称_词云图.png')  # 保存图片
    wc.to_image()  # 显示图片
    

可视化图：  
![](https://img2023.cnblogs.com/blog/2864563/202212/2864563-20221229123508257-1214945646.png)

结论：**阿里系公司占据榜首，其次是海天味业等。**

三、整体结论
======

综上所述，针对2021年胡润百富榜的榜单数据，得出如下结论：

> 财富分布：大部分的富豪的财富集中在20亿~400亿之间，个别顶级富豪的财富在3000亿以上。  
> 年龄分布：大部分富豪的年龄在50-60岁，其次是60-70和40-50岁。极少数富豪在20-30岁（年轻有为👍）  
> 城市分布：公司分布城市，大多集中在北上广深等一线城市，另外杭州、香港、苏州也位列前茅  
> 性别分布：男性富豪占据绝大多数，个别女性在列（巾帼不让须眉👍）  
> 行业分布：百富榜中占比最多的行业分别是：房地产、医药、投资、化工等  
> 组织结构分布：半数以上是未知，企业未对外开放，或榜单没有统计到；家族和夫妇占据前两类。  
> 公司名称分布：阿里系公司占据榜首，其次是海天味业等。

四、同步视频讲解
========

4.1 上集（爬虫讲解）
------------

爬虫讲解视频：  
[https://www.zhihu.com/zvideo/1492523459087896577](https://www.zhihu.com/zvideo/1492523459087896577)

4.2 下集（数据分析讲解）
--------------

可视化讲解视频：  
[https://www.zhihu.com/zvideo/1492525821340729344](https://www.zhihu.com/zvideo/1492525821340729344)

五、附完整源码
=======

完整源码：[【爬虫+数据分析+数据可视化】python数据分析全流程《2021胡润百富榜》榜单数据!](https://mp.weixin.qq.com/s?__biz=MzU5MjQ2MzI0Nw==&mid=2247484509&idx=1&sn=bbc89e431ec9950ab8b260b28d9024d3&chksm=fe1e10bdc96999ab76a30249f6024e740f3810e1c988dac960898c4a8d0c9e3c3ce04863b3b2&payreadticket=HOKjI7NWb7zWTBGgB-PVh7nKauxFMlLfNh-ndiCGdHLE5fH7BcX-FYWK9jDvv68PL1qqDME#rd)