---
layout: post
title: "强化学习-学习笔记12 | Dueling Network"
date: "2022-07-09T10:20:22.510Z"
---
强化学习-学习笔记12 | Dueling Network
=============================

![强化学习-学习笔记12 | Dueling Network](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220709135748627-1491449932.png) 这是价值学习高级技巧第三篇，前两篇主要是针对 TD 算法的改进，而Dueling Network 对 DQN 的结构进行改进，能够大幅度改进DQN的效果。

这是价值学习高级技巧第三篇，前两篇主要是针对 TD 算法的改进，而Dueling Network 对 DQN 的结构进行改进，能够大幅度改进DQN的效果。

> Dueling Network 的应用范围不限于 DQN，本文只介绍其在 DQN上的应用。

12\. Dueling Network
--------------------

### 12.1 优势函数

Advantage Function.

> 回顾一些基础概念：
> 
> 1.  折扣回报：
>     
>     \\(U\_t = R\_t + \\gamma \\cdot R\_{t+1} + \\gamma^2R+...\\)
>     
> 2.  动作价值函数：
>     
>     \\(Q\_\\pi(s\_t,a\_t)=\\mathbb{E}\[U\_t|S\_t=s\_t,A\_t=a\_t\]\\)
>     
>     消去了未来的状态 和 动作，只依赖于当前动作和状态，以及策略函数 \\(\\pi\\)。
>     
> 3.  状态价值函数：
>     
>     \\(V\_\\pi(s\_t)=\\mathbb{E}\[Q\_\\pi(s\_t,A)\]\\)
>     
>     只跟策略函数 \\(\\pi\\) 和当前状态 \\(s\_t\\) 有关。
>     
> 4.  最优动作价值函数
>     
>     \\(Q^\*(s,a)=\\mathop{max}\\limits\_{\\pi}Q\_\\pi(s,a)\\)
>     
>     只依赖于 s,a，不依赖策略函数。
>     
> 5.  最优状态价值函数
>     
>     \\(V^\*(s)=\\mathop{max}\\limits\_{a}V\_\\pi(S)\\)
>     
>     只依赖 S。
>     

下面就是这次的主角之一：

*   Optimal Advantage function 优势函数：
    
    \\(A^\*(s,a)=Q^\*(s,a)-V^\*(s)\\)
    
    V\* 作为 baseline ，优势函数的意思是动作 a 相对 V\* 的优势，A\*越好，那么优势就越大。
    

下面介绍一个优势函数有关的定理：

**定理一：**\\(V^\*(s)=\\mathop{max}\\limits\_a Q^\*(s,a)\\)

> 这一点从上面的回顾不难看出，求得最优的路径不同，但是相等。

上面提到了优势函数的定义：\\(A^\*(s,a)=Q^\*(s,a)-V^\*(s)\\)

同时对左右求最大值：\\(\\mathop{max}\\limits\_{a}A^\*(s,a)=\\mathop{max} \\limits\_{a}Q^\*(s,a)-V^\*(s)\\)，而等式右侧正是上面定理，所以右侧==0；因此优势函数关于a的最大值=0，即：

\\(\\mathop{max}\\limits\_{a}A^\*(s,a)=0\\)

我们把这个 0 值式子加到定义上，进行简单变形：

**定理二：**\\(Q^\*(s,a)=V^\*(s)+A^\*(s,a)-\\mathop{max}\\limits\_{a}A^\*(s,a)\\)

Dueling Network 就是由定理二得到的。

### 12.2 Dueling Network 原理

此前 DQN 用\\(Q(s,a;w)\\) 来近似 \\(D^\*(s,a)\\) ，结构如下：

![img](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220708180904823-1849395949.png)

而 Dueling Network 对 DQN 的结构改进原理是：

*   我们对于DQN的改进思路就是基于上面的定理2：\\(Q^\*(s,a)=V^\*(s)+A^\*(s,a)-\\mathop{max}\\limits\_{a}A^\*(s,a)\\)
    
    *   分别用神经网络 V 和 A 近似 V-star 和 A-star
    *   即：\\(Q(s,a;w^A,w^V)=V(s;w^V)+A(s,a;w^A)-\\mathop{max}\\limits\_{a}A(s,a;w^A)\\)
    *   这样也完成了对于 Q-star 的近似，与 DQN 的功能相同。
*   首先需要用一个神经网络 \\(V(s;w^V)\\) 来近似 \\(V^\*(s)\\)：
    

![image](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220709135628472-1831216809.png)

注意这里的输出是一个实数，是对状态的打分，而非向量；

*   用另一个神经网络\\(A(s,a;w^A)\\) 对\\(A^\*(s,a)\\) 进行近似:

![image](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220709135638050-1230417346.png)

这个网络和上面的网络 \\(V\\) 结构有一定的相像，可以共享卷积层的参数;

后续为了方便，令 \\(w=(w^A,w^V)\\)，即：

\\\[Q(s,a;w)=V(s;w^V)+A(s,a;w^A)-\\mathop{max}\\limits\_{a}A(s,a;w^A) \\\]

现在 左侧 与 DQN 的表示就一致了。下面搭建Dueling Network，就是上面 V 和 A 的拼接与计算：

![image](https://img2022.cnblogs.com/blog/2192866/202207/2192866-20220709135651535-1515222011.png)

*   输入 状态 s，V 和 A 共享一些 卷积层，得到特征向量；
*   分别通过不同的全连接层，A输出向量，V输出实数；
*   通过上面的式子运算输出最终结果，是对所有动作的打分；

可见Dueling Network 的输入 和 输出 和 DQN 完全一样，功能也完全一样；但是内部的结构不同，Dueling Network 的结构更好，所以表现要比 DQN好；

> 注意，Dueling Network 和 DQN 都是对 最优动作价值函数 的近似。

### 12.3 训练 Dueling Network

接下来训练参数 \\(w=(w^A,w^v)\\)，采用与 DQN 相同的思路，也就是采用 TD算法训练 Dueling Network。

之前介绍的 TD算法 的三种优化方法：

1.  经验回放 / 优先经验回放
2.  Double DQN
3.  M-step TD target

都可以用在 训练 Dueling Network 上。

### 12.4 数学原理与不唯一性

之前推导 Dueling Network 原理的时候，有如下两个式子：

*   \\(Q^\*(s,a)=V^\*(s)+A^\*(s,a)\\)
*   \\(Q^\*(s,a)=V^\*(s)+A^\*(s,a)-\\mathop{max}\\limits\_{a}A^\*(s,a)\\)

我们为什么一定要用等式 2 而不是等式 1 呢？也就是为什么要加上一个 值为 0 的 \\(\\mathop{max}\\limits\_{a}A^\*(s,a)\\)？

*   这是因为 等式1 有一个问题。
    
*   即我们无法通过学习 Q-star 来 唯一确定 V-star 和 A-star，即对于求得的 Q-star 值，可以分解成无数组 V-star 和 A-star。
    
*   \\(Q(s,a;w^A,w^V)=V(s;w^V)+A(s,a;w^A)-\\mathop{max}\\limits\_{a}A(s,a;w^A)\\)
    
*   **我们是对 左侧Q 来训练整个 Dueling Network 的**。如果 V 网络 向上波动 和 A 网络向下波动幅度相同，那么 Dueling Network 的输出完全相同，但是V-A两个网络都发生了波动，训练不好。
    
*   而加上最大化这一项就能避免不唯一性；即如果 V-star 向上波动10，A-star 向下波动10，那么整个式子的值会发生改变
    
    > 因为max项随着A-star 的变化 也减少了10，总体上升了10
    

在上面的数学推导中，我们使用的是 \\(\\max \\limits\_{a}A(s,a;w^A)\\)来近似最大项\\(\\max\\limits\_{a}A(s,a)\\)，而在实际应用中，用 \\(\\mathop{mean}\\limits\_{a}A(S,a;w^A)\\)来近似效果更好；这种替换没有理论依据，但是实际效果好。

x. 参考教程
-------

*   视频课程：[深度强化学习（全）\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1rv41167yx)
*   视频原地址：[https://www.youtube.com/user/wsszju](https://www.youtube.com/user/wsszju)
*   课件地址：[https://github.com/wangshusen/DeepLearning](https://github.com/wangshusen/DeepLearning)