---
layout: post
title: "从零搭建Pytorch模型教程（三）搭建Transformer网络"
date: "2022-04-15T17:21:16.931Z"
---
从零搭建Pytorch模型教程（三）搭建Transformer网络
=================================

​

  

> **前言** 本文介绍了Transformer的基本流程，分块的两种实现方式，Position Emebdding的几种实现方式，Encoder的实现方式，最后分类的两种方式，以及最重要的数据格式的介绍。

本文来自公众号CV技术指南的[技术总结系列](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzkyMDE2OTA3Mw==&action=getalbum&album_id=1622596881459494913#wechat_redirect "技术总结系列")

**欢迎关注公众号**[CV技术指南](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493728%26idx%3D1%26sn%3D3a30e67a71f2c18df697680c6004336b%26chksm%3Dc194577ef6e3de68e47294d8121c4f43c8170b114b5490cc129f212e8baf6aa379365a7a6fd9%26token%3D1194358432%26lang%3Dzh_CN%23rd "CV技术指南")**，专注于计算机视觉的技术总结、最新技术跟踪、经典论文解读、CV招聘信息。**

在讲如何搭建之前，先回顾一下Transformer在计算机视觉中的结构是怎样的。这里以最典型的ViT为例。

![](https://img-blog.csdnimg.cn/img_convert/d617ed47b160534045a3fb289eeedc27.png)​

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

如图所示，对于一张图像，先将其分割成NxN个patches,把patches进行Flatten，再通过一个全连接层映射成tokens,对每一个tokens加入位置编码(position embedding)，会随机初始化一个tokens，concate到通过图像生成的tokens后，再经过transformer的Encoder模块，经过多层Encoder后，取出最后的tokens(即随机初始化的tokens),再通过全连接层作为分类网络进行分类。

下面我们就根据这个流程来一步一步介绍如何搭建一个Transformer模型。、

**分块**
------

* * *

目前有两种方式实现分块，一种是直接分割，一种是通过卷积核和步长都为patch大小的卷积来分割。

### **直接分割**

直接分割即把图像直接分成多块。在代码实现上需要使用einops这个库，完成的操作是将（B，C，H，W）的shape调整为（B，(H/P \*W/P)，P\*P\*C）。

    from einops import rearrange, repeat
    from einops.layers.torch import Rearrange
    
    self.to_patch_embedding = nn.Sequential(
               Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
               nn.Linear(patch_dim, dim),
          )
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

这里简单介绍一下Rearrange。

Rearrange用于对张量的维度进行重新变换排序，可用于替换pytorch中的reshape，view，transpose和permute等操作。举几个例子

    #假设images的shape为[32,200,400,3]
    #实现view和reshape的功能
    Rearrange(images,'b h w c -> (b h) w c')#shape变为（32*200, 400, 3）
    #实现permute的功能
    Rearrange(images, 'b h w c -> b c h w')#shape变为（32, 3, 200, 400）
    #实现这几个都很难实现的功能
    Rearrange(images, 'b h w c -> (b c w) h')#shape变为（32*3*400, 200）
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

从这几个例子看可以看出，Rearrange非常简单好用，这里的b, c, h, w都可以理解为表示符号，用来表示操作变化。通过这几个例子似乎也能理解下面这行代码是如何将图像分割的。

    Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

这里需要解释的是，一个括号内的两个变量相乘表示的是该维度的长度，因此不要把"h"和"w"理解成图像的宽和高。这里实际上h = H/p1, w = W/p2，代表的是高度上有几块，宽度上有几块。h和w都不需要赋值，代码会自动根据这个表达式计算，b和c也会自动对应到输入数据的B和C。

后面的"b (h w) (p1 p2 c)"表示了图像分块后的shape: （B，(H/P \*W/P)，P\*P\*C）

这种方式在分块后还需要通过一层全连接层将分块的向量映射为tokens。

在ViT中使用的就是这种直接分块方式。

### **卷积分割**

卷积分割比较容易理解，使用卷积核和步长都为patch大小的卷积对图像卷积一次就可以了。

    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
    
    x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

在swin transformer中即使用的是这种卷积分块方式。在swin transformer中卷积后没有再加全连接层。

**Position Embedding**
----------------------

* * *

Position Embedding可以分为absolute position embedding和relative position embedding。

在学习最初的transformer时，可能会注意到用的是正余弦编码的方式，但这只适用于语音、文字等1维数据，**图像是高度结构化的数据，用正余弦不合适**。

在ViT和swin transformer中都是直接随机初始化一组与tokens同shape的可学习参数，与tokens相加，即完成了absolute position embedding。

在ViT中实现方式：

    self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
    x += self.pos_embedding[:, :(n + 1)]
    #之所以是n+1，是因为ViT中选择随机初始化一个class token，与分块得到的tokens拼接。所以patches的数量为num_patches+1。
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

在swin transformer中的实现方式：

    from timm.models.layers import trunc_normal_
    self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
    trunc_normal_(self.absolute_pos_embed, std=.02)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

在TimeSformer中的实现方式：

    self.pos_emb = torch.nn.Embedding(num_positions + 1, dim)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

以上就是简单的使用方法，这种方法属于absolute position embedding。

还有更复杂一点的方法，以后有机会单独搞一篇文章来介绍。

感兴趣的读者可以先去看看这篇论文《[ICCV2021 | Vision Transformer中相对位置编码的反思与改进](https://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&mid=2247492178&idx=1&sn=b10e7de41a7214cf881d530b51a4cce4&chksm=c1945d4cf6e3d45a0e5e060b24eb047dc0400537bc474b6de69aa8a1c4fa81c5df757f222adb&token=924461043&lang=zh_CN&scene=21#wechat_redirect "ICCV2021 | Vision Transformer中相对位置编码的反思与改进")》。

**Encoder**
-----------

* * *

Encoder由Multi-head Self-attention和FeedForward组成。

### **Multi-head Self-attention**

Multi-head Self-attention主要是先把tokens分成q、k、v，再计算q和k的点积，经过softmax后获得加权值，给v加权，再经过全连接层。

用公式表示如下：

所谓Multi-head是指把q、k、v再dim维度上分成head份，公式里的dk为每个head的维度。

具体代码如下：

    class Attention(nn.Module):
       def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
           super().__init__()
           inner_dim = dim_head *  heads
           project_out = not (heads == 1 and dim_head == dim)
    
           self.heads = heads
           self.scale = dim_head ** -0.5
           self.attend = nn.Softmax(dim = -1)
           self.dropout = nn.Dropout(dropout)
    
           self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)
           self.to_out = nn.Sequential(
               nn.Linear(inner_dim, dim),
               nn.Dropout(dropout)
          ) if project_out else nn.Identity()
    
       def forward(self, x):
           qkv = self.to_qkv(x).chunk(3, dim = -1)
           q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)
           dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
           attn = self.attend(dots)
           attn = self.dropout(attn)
    
           out = torch.matmul(attn, v)
           out = rearrange(out, 'b h n d -> b n (h d)')
           return self.to_out(out)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

这里没有太多可以解释的地方，介绍一下q、k、v的来源，由于这是self-attention，因此q=k=v(即tokens)，若是普通attention，则k= v，而q是其它的东西，例如可以是另一个尺度的tokens，或视频领域中的其它帧的tokens。

### **FeedForward**

这里不用多介绍。

    class FeedForward(nn.Module):
       def __init__(self, dim, hidden_dim, dropout = 0.):
           super().__init__()
           self.net = nn.Sequential(
               nn.Linear(dim, hidden_dim),
               nn.GELU(),
               nn.Dropout(dropout),
               nn.Linear(hidden_dim, dim),
               nn.Dropout(dropout)
          )
       def forward(self, x):
           return self.net(x)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

把上面两者组合起来就是Encoder了。

    class Transformer(nn.Module):
       def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
           super().__init__()
           self.layers = nn.ModuleList([])
           for _ in range(depth):
               self.layers.append(nn.ModuleList([
                   PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),
                   PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))
              ]))
       def forward(self, x):
           for attn, ff in self.layers:
               x = attn(x) + x
               x = ff(x) + x
           return x
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

depth指的是Encoder的数量。PreNorm指的是层归一化。

    class PreNorm(nn.Module):
        def __init__(self, dim, fn):
            super().__init__()
            self.norm = nn.LayerNorm(dim)
            self.fn = fn
        def forward(self, x, **kwargs):
            return self.fn(self.norm(x), **kwargs)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

**分类方法**
--------

* * *

数据通过Encoder后获得最后的预测向量的方法有两种典型。在ViT中是随机初始化一个cls\_token，concate到分块后的token后，经过Encoder后取出cls\_token，最后将cls\_token通过全连接层映射到最后的预测维度。

    #生成cls_token部分
    from einops import repeat
    self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
    
    cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b)
    x = torch.cat((cls_tokens, x), dim=1)
    ################################
    #分类部分
    self.mlp_head = nn.Sequential(
               nn.LayerNorm(dim),
               nn.Linear(dim, num_classes)
          )
    x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]
    
    x = self.to_latent(x)
    return self.mlp_head(x)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

在swin transformer中，没有选择cls\_token。而是直接在经过Encoder后将所有数据取了个平均池化，再通过全连接层。

    self.avgpool = nn.AdaptiveAvgPool1d(1)
    self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
    
    x = self.avgpool(x.transpose(1, 2))  # B C 1
    x = torch.flatten(x, 1)
    x = self.head(x)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

组合以上这些就成了一个完整的模型

    class ViT(nn.Module):
       def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):
           super().__init__()
           image_height, image_width = pair(image_size)
           patch_height, patch_width = pair(patch_size)
    
           num_patches = (image_height // patch_height) * (image_width // patch_width)
           patch_dim = channels * patch_height * patch_width
           assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
    
           self.to_patch_embedding = nn.Sequential(
               Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
               nn.Linear(patch_dim, dim),
          )
    
           self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
           self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
           self.dropout = nn.Dropout(emb_dropout)
           self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)
    
           self.pool = pool
           self.to_latent = nn.Identity()
           self.mlp_head = nn.Sequential(
               nn.LayerNorm(dim),
               nn.Linear(dim, num_classes)
          )
    
       def forward(self, img):
           x = self.to_patch_embedding(img)
           b, n, _ = x.shape
    
           cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b)
           x = torch.cat((cls_tokens, x), dim=1)
           x += self.pos_embedding[:, :(n + 1)]
           x = self.dropout(x)
           x = self.transformer(x)
           x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]
    
           x = self.to_latent(x)
           return self.mlp_head(x)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

**数据的变换**
---------

* * *

以上的代码都是比较简单的，整体上最麻烦的地方在于理解数据的变换。

首先输入的数据为（B, C, H, W），在经过分块后，变成了（B, n, d）。

在CNN模型中，很好理解(H，W)就是feature map，C是指feature map的数量，那这里的n，d哪个是通道，哪个是图像特征？

回顾一下分块的部分

    Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

根据这个可以知道n为分块的数量，d为每一块的内容。因此，这里的n相当于CNN模型中的C，而d相当于features。

一般情况下，在Encoder中，我们都是以（B, n, d）的形式。

在swin transformer中这种以卷积的形式分块，获得的形式为（B, C, L），然后做了一个transpose得到（B, L, C）,这与ViT通过直接分块方式获得的形式实际上完全一样，在Swin transformer中的L即为ViT中的n，而C为ViT中的d。

因此，要注意的是**在Multi-head self-attention中，数据的形式是（Batchsize, Channel, Features）**，分成多个head的是Features。

前面提到，在ViT中会concate一个随机生成的cls\_token，该cls\_token的维度即为（B, 1, d）。可以理解为通道数多了个1。

以上就是Transformer的模型搭建细节了，整体上比较简单，大家看完这篇文章后可以找几篇Transformer的代码来理解理解。如ViT, swin transformer, TimeSformer等。

    ViT：https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py
    swin: https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py
    TimeSformer:https://github.com/lucidrains/TimeSformer-pytorch/blob/main/timesformer_pytorch/timesformer_pytorch.py
    

![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")

下一篇我们将介绍如何写train函数，以及包括设置优化方式，设置学习率，不同层设置不同学习率，解析参数等。

**欢迎关注公众号**[CV技术指南](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493108%26idx%3D1%26sn%3Dab5990919859fb5a539315c9a8c5c328%26chksm%3Dc1945aeaf6e3d3fc581bf1c03845d1af687dbc7f881f814f824f87d320b2d7209d7e14dadbd0%26token%3D883967639%26lang%3Dzh_CN%23rd "CV技术指南")**，专注于计算机视觉的技术总结、最新技术跟踪、经典论文解读、CV招聘信息。**

CV技术指南创建了一个交流氛围很不错的群，除了太偏僻的问题，几乎有问必答。关注公众号添加编辑的微信号可邀请加交流群。

![](https://img-blog.csdnimg.cn/578c1ebf3ad24deb92f6675351e6f928.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAQ1bmioDmnK_mjIfljZco5YWs5LyX5Y-3KQ==,size_19,color_FFFFFF,t_70,g_se,x_16)![](https://img2022.cnblogs.com/blog/1432790/202204/1432790-20220415204408620-2044020585.gif "点击并拖拽以移动")​​

**其它文章**
--------

[从零搭建Pytorch模型教程（二）搭建网络](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494150%26idx%3D1%26sn%3Dda191e151efb8db5fef1aab64e9bec7d%26chksm%3Dc1945518f6e3dc0e19e5c83f205ae3d24b15c867b9f1038018b18bf7dae597d375f15c13a348%26scene%3D21%23wechat_redirect "从零搭建Pytorch模型教程（二）搭建网络")

[从零搭建Pytorch模型教程（一）数据读取](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493728%26idx%3D1%26sn%3D3a30e67a71f2c18df697680c6004336b%26chksm%3Dc194577ef6e3de68e47294d8121c4f43c8170b114b5490cc129f212e8baf6aa379365a7a6fd9%26scene%3D21%23wechat_redirect "从零搭建Pytorch模型教程（一）数据读取")

[YOLO系列梳理与复习（二）YOLOv4](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494393%26idx%3D1%26sn%3Dcdbad1b535816a06213cac31e7d8e4db%26chksm%3Dc19455e7f6e3dcf19d9eb19ed8aa22ddc23d2c5553ebfe5ff46f82b2534894316363975a603a%23rd "YOLO系列梳理与复习（二）YOLOv4")

[YOLO系列梳理（一）YOLOv1-YOLOv3](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494324%26idx%3D1%26sn%3D8e8eeb92ede34988d7b7c46837c3d308%26chksm%3Dc19455aaf6e3dcbc91020d6c559a7539e3ece7b0fa13104b486c48cf9662ef2ce2188cfebb91%26scene%3D21%23wechat_redirect "YOLO系列梳理（一）YOLOv1-YOLOv3")

[StyleGAN大汇总 | 全面了解SOTA方法、架构新进展](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494287%26idx%3D1%26sn%3D0ec7b88eba875777b1d94ec39cf9c121%26chksm%3Dc1945591f6e3dc87014dc2cab167216462821d3222c4ec93eac720689765222bd7d30a0fede8%26scene%3D21%23wechat_redirect "StyleGAN大汇总 | 全面了解SOTA方法、架构新进展")

[一份热力图可视化代码使用教程](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494271%26idx%3D1%26sn%3D6fb284402bc9aad6e9d578e385b59aad%26chksm%3Dc1945561f6e3dc77184450d7ea6c1e71e9d3083946c459298950c1a3d4c900d7184a5a38c440%26scene%3D21%23wechat_redirect "一份热力图可视化代码使用教程")

[一份可视化特征图的代码](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493864%26idx%3D1%26sn%3D7ad0ec5d43c8cef05c0f05794a547eb8%26chksm%3Dc19457f6f6e3dee07595386289437e74db02231b09261b19c9e419a4e7859565144e88ae2d9e%26scene%3D21%23wechat_redirect "一份可视化特征图的代码")

[工业图像异常检测研究总结（2019-2020）](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494255%26idx%3D1%26sn%3D4ddd6d58b5d8f82214f940ccab4461da%26chksm%3Dc1945571f6e3dc67941aca304627fe2a5695bf96f79776342e8547cd04ab533e609b39527824%26scene%3D21%23wechat_redirect "工业图像异常检测研究总结（2019-2020）")

[小样本学习研究综述（中科院计算所）](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494096%26idx%3D1%26sn%3D5f10d58dd01521db6d4d181922c690b3%26chksm%3Dc19456cef6e3dfd835be4909ef6212438560f12dc9c874085872d60a2ad3ac30be0cdd47b51b%26scene%3D21%23wechat_redirect "小样本学习研究综述（中科院计算所）")

[目标检测中正负样本区分策略和平衡策略总结](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494080%26idx%3D1%26sn%3D890f24c7e366e24dc5eaf35081615e23%26chksm%3Dc19456def6e3dfc8d0955a23c813ffdc95cb91c5b140e2f50a942aa2958e1d2f73934f141438%26scene%3D21%23wechat_redirect "目标检测中正负样本区分策略和平衡策略总结")

[目标检测中的框位置优化总结](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493792%26idx%3D1%26sn%3D8a8f6e4d70b029f1750f5816cff89c72%26chksm%3Dc19457bef6e3dea8ed06818c7278d1e6f2178ac35e5dd28102a2b5b78daa3837815d5520cea8%26scene%3D21%23wechat_redirect "目标检测中的框位置优化总结")

[目标检测、实例分割、多目标跟踪的Anchor-free应用方法总结](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493650%26idx%3D1%26sn%3D5a7d6ffcd1d1cbac41c8edbb67b3bcf5%26chksm%3Dc194570cf6e3de1a6542c98156ed71d99b9630df996e1c26445627c55fb8deea80bd1c12b1f1%26scene%3D21%23wechat_redirect "目标检测、实例分割、多目标跟踪的Anchor-free应用方法总结")

[Soft Sampling：探索更有效的采样策略](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493598%26idx%3D1%26sn%3D0d24fd9918011c978651a7a9d1b630bd%26chksm%3Dc19458c0f6e3d1d6f174f01b4d3b1ca8f33a13bde295c9b745c4ea87c1e8cd60521eaf3d3836%26scene%3D21%23wechat_redirect "Soft Sampling：探索更有效的采样策略")

[如何解决工业缺陷检测小样本问题](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493382%26idx%3D1%26sn%3D46cc9c3009fa3022a2c21461dd599d3c%26chksm%3Dc1945818f6e3d10edd7a2663e97f3c9e0be749000c3d59b45169da472999285990e0b0d8ab22%26scene%3D21%23wechat_redirect "如何解决工业缺陷检测小样本问题")

[关于快速学习一项新技术或新领域的一些个人思维习惯与思想总结](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493117%26idx%3D1%26sn%3Dfc82e1477d082db07ce74040cfadcb43%26chksm%3Dc1945ae3f6e3d3f578b7590e9dcca4615a4b560a55735f98cb3eb3d0995210fb905d494028f2%26scene%3D21%23wechat_redirect "关于快速学习一项新技术或新领域的一些个人思维习惯与思想总结")

​