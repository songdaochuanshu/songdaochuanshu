---
layout: post
title: "视频超分之BasicVSR++阅读笔记"
date: "2022-12-05T15:17:44.502Z"
---
视频超分之BasicVSR++阅读笔记
===================

1.介绍
====

    在这项工作中，我们通过设计二阶网格传播和流引导的可变形对齐来重新设计BasicVSR，使信息能够更有效地传播和聚合。

    如图所示，提出的二阶网格传播解决了BasicVSR中的两个限制：i）我们允许以类似网格的方式进行更积极的双向传播，ii）我们放松了BasicVSR中一阶马尔可夫特性的假设，并将二阶连接并入网络，以便可以从不同的时空位置聚合信息。这两种修改都改善了网络中的信息流，提高了网络对遮挡和精细区域的鲁棒性。 

    BasicVSR显示了使用光流进行时间对齐的优势。然而，光流对遮挡并不鲁棒。不准确的流量估计可能会危及恢复性能。为了在克服训练不稳定性的同时利用可变形对齐，我们提出了流引导可变形对齐，如图所示。在提出的模块中，我们没有直接学习DCN偏移量，而是通过使用光流场作为基础偏移量，通过流场残差量进行细化，来减少偏移量学习的负担。后者可以比原始DCN偏移更稳定地学习。

        ![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650956912802-dadad0f4-8b66-4518-b5ea-4eb6acd02c2b.png)

2.相关工作
======

    **Grid Connections**：这些设计将给定的图像/特征分解为多个分辨率，并跨分辨率采用网格来捕获精细和粗糙信息。BasicVSR++不采用多尺度设计。相反，网格结构的设计是为了以双向方式跨时间传播。我们将不同的框架与网格连接起来，以反复优化特征，提高表现力。

    **高阶传播**：研究了高阶传播以改善梯度流。这些方法展示了不同任务的改进，包括分类和语言建模。然而，这些方法没有考虑时间对齐，这在VSR的任务中至关重。为了在二阶传播中实现时间对齐，我们通过将**流引导可变形对齐扩展到二阶**，将对齐合并到我们的传播方案中。

    **可形变对齐**：TDAN使用**可变形卷积在特征级执行对齐**。EDVR进一步提出了一种金字塔级联可变形（PCD）对准，采用多尺度设计。受启发，我们采用了可变形对齐，但采用了一种新的形式来克服训练的不稳定性。我们的流动引导可变形对准不同于偏移保真度损失。后者使用光流作为训练期间的损失函数。相比之下，我们直接将**光流作为基本偏移量**纳入我们的模块，从而在训练和推理过程中提供更明确的指导。

3.方法
====

   BasicVSR++包含两个有效的修改，如图所示，给定一个输入视频，残差块是第一个用于从每帧中提取特征，然后在我们的二阶网格传播方案下传播特征，其中对齐由我们的流引导可变形对齐执行。传播后，聚集的特征通过卷积和像素洗牌生成输出图像。

        ![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650958533910-cf59bf42-c58a-46e2-8e81-09194dcedc47.png)

3.1二阶网格传播
---------

    基于双向传播的有效性，我们设计了一种网格传播方案，通过传播实现重复优化。更具体地说，中间特征以交替的方式在时间上前后传播。通过传播，来自不同帧的信息可以被“重新访问”并用于特征细化。与只传播一次特征的现有工作相比，网格传播重复地从整个序列中提取信息，提高了特征的表达能力。 为了进一步增强传播的鲁棒性，我们采用二阶连接，实现了二阶马尔可夫链。通过这种放松，可以从不同的时空位置聚集信息，提高在闭塞和精细区域的鲁棒性和有效性。

    设xi是输入图像，gi是通过多个残差块从xi中提取的特征，fji是在第j传播分支的第i个时间步计算的特征。我们描述了正向传播的过程，反向传播的过程也有类似的定义。 为了计算特征jji，我们我们首先对齐fji-1和fji-2（遵循二阶马尔可夫链）使用我们提出的流引导可变形对准:

        ![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650958993205-c39397d9-e18a-437b-b6ad-d215e371e3ea.png)

    s i→i-1 s i→i-2是表示从第i帧到(i-1)帧和(i-2)帧的光流，A表示流引导可变形对齐。然后将这些特征连接起来并传递到残差块的堆栈中，f0i\=gi，R表示残差块，c表示沿通道维度的串联。

        ![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650959303964-337d1dc0-d4b6-4f4b-bba3-243befea8d7e.png)

3.2光流指导的可形变对齐
-------------

    为了在克服不稳定性的同时利用偏移多样，我们提议利用光流来引导可变形对准，这是由可变形对准和基于流的对准之间的强大关系所驱动的。如图所示。在本节的其余部分中，我们将详细介绍正向传播的对齐过程。反向传播过程的定义与此类似。

        ![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650960651441-4b790622-498a-4586-9e73-95759bf7de8b.png)

    在第i个时间步，给定从第i个LR图像计算出的特征gi，计算上一个时间步特征fi-1，光流 s i→i-1为先前的帧，我们第一次用 s i→i-1 warp fi-1：

           ![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650960267583-b524d366-346d-4da6-bc79-f8d5388f932f.png)

    其中W表示空间warp操作。然后使用预先对齐的特征计算DCN偏移量oi→i-1和调制mask mi→i−1.我们不直接计算DCN偏移，而是计算光流的残差：

          ![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650960604832-28d89ab2-5cf3-400d-9213-3cdf34c3cfc7.png)

    这里C0,m表示卷积的堆栈，σ表示sigmoid函数。然后将DCN应用于unwarp特征fi-1，D为可形变卷积：

          ![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650961002073-7be32463-593c-46d3-a93b-0a964a2731eb.png)

    上述公式仅用于对齐单个特征，因此不直接适用于我们的二阶传播。适应二阶设置最直观的方法是将上述步骤应用于两个功能fji-1和fji-2独立。这需要加倍计算，导致效率降低。此外，单独对齐可能会忽略来自特征的补充信息。因此，我们允许**同时对齐两个功能。我们连接warp特征并且计算偏移量**oi-p（p=1,2）： 

          **![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650961193769-ea863b81-2d6d-44da-b3c7-029ad3d789a8.png)**

    然后将DCN应用于未warp的特征：

          **![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650961262996-47062efa-6cb4-4f40-a359-c1b511c2d513.png)**

    ****Discussion**：**与直接计算DCN偏移量的现有方法不同，我们提出的以流导向变形对准采用光流作为导向。好处是双重的。首先，由于已知CNN具有局部感受野，因此可以通过使用光流预对准特征来辅助偏移的学习。其次，通过只学习残差，网络只需要学习与光流的微小偏差，从而减轻典型可变形对准模块的负担。此外，DCN中的调制mask不是直接连接warp特征，而是充当注意图来权衡不同像素的贡献，提供额外的灵活性。

4.实验
====

    训练采用了两种广泛使用的数据集：REDS和Vimeo-90K。对于REDS，按照BasicVSR，我们使用REDS4作为测试集，使用REDSval4作为验证集。剩下的片段用于训练。我们使用Vid4、UDM10和Vimeo90K-T以及Vimeo-90K作为测试集。所有模型均使用两种降级（双三次（BI）和模糊下采样（BD））进行4倍下采样测试。 我们采用Adam优化器和余弦退火方案。主网络和流量网络的初始学习率设置为1×10\-4和2.5×10\-5。总迭代次数为600K，在前5000次迭代中，流量网络的权重是固定的。批量大小为8，输入LR帧的补丁大小为64×64。我们使用Charbonnier损失，因为它能更好地处理异常值，并比传统的l2\-loss性能好。我们使用预先训练好的SPyNet作为我们的流量网络。

**4.1和最新的方法比较**
---------------

          **![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650962425320-6772c942-dc3b-4742-be5f-c2db2b4c6729.png)**

**5.消融研究**
==========

    为了了解各个组件的贡献，我们从基线开始，逐步插入组件。从表3可以明显看出，每个组件都带来了相当大的改善，峰值信噪比从0.14 dB到0.46 dB不等。 理论上，我们提出的传播方案可以扩展到更高的阶数和更多的传播迭代。然而，当从一阶增加到二阶时和一到两次迭代性能增益相当可观，我们在初步实验中观察到，进一步**增加迭代次数和次数**并不会导致显著改善（PSNR为0.05dB）。

          **![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650962539475-a98e7fab-600b-4478-8cb8-93a48c0e204e.png)**

    **二阶网格传播：**如图的两个示例所示，在包含精细细节和复杂纹理的区域中，二阶传播和网格传播的贡献更为显著。在这些区域，当前帧中可用于重建的信息有限。为了提高这些区域的输出质量，需要从其他视频帧进行有效的信息聚合。通过我们的二阶传播方案，信息可以通过鲁棒有效的传播进行传输。这些补充信息基本上有助于恢复细节。如示例所示，网络使用我们的组件成功地恢复了细节，而没有我们的组件的网络则会产生模糊的输出。 

         **![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650962891509-103c5927-921c-412c-85cf-db81015a5b10.png)**

    ****光流指导的可形变对齐**：**我们将偏移量与BasicVSR++中的流量估计模块计算的光流进行了比较。通过只学习光流的残差，网络产生的偏移量与光流高度相似，但有明显的差异。与仅从运动（光流）指示的一个空间位置聚合信息的基线相比，我们提出的模块允许从周围的多个位置检索信息，提供了额外的灵活性。 

        **![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650963034402-c3b86991-8dd9-4d00-8b8b-c8c08bb802dd.png)**

    为了证明我们设计的优越性，我们将我们的对准模块与两种变体进行了比较：（1）没有使用光流（2）光流在offset-fidelity loss中使用，即光流仅用作损耗函数中的监控（而不是在我们的方法中用作基本偏移）。如果不使用光流作为指导，不稳定性会导致训练崩溃，导致PSNR值非常低。当使用偏移保真度损失时，训练是稳定的。然而，从我们的完整模型中观察到了2.17 dB的下降。我们的光流指导可变形对准直接将光流整合到网络中，以提供更明确的引导，从而获得更好的结果。

        **![](https://cdn.nlark.com/yuque/0/2022/png/22916227/1650963172011-d99f0e08-e2ed-4887-8306-14488b1779b4.png)**

    ****时间一致性**：**与滑动窗口框架相比，递归框架本质上保持了更好的时间一致性。在滑动窗口框架中，每个框架都是独立重建的。在这种设计中，无法保证输出之间的一致性。相比之下，在循环框架中，输出通过中间特征的传播而相关。时间传播本质上有助于保持更好的时间一致性。我们比较了BasicVSR++和两种最先进的方法--EDVR和BasicVSR的时间剖面。对于滑动窗口法，EDVR的时间剖面包含大量噪声，表明输出视频中存在闪烁伪影。相比之下，对于循环网络，在没有明确的时间一致性建模的情况下，来自BasicVSR和BasicVSR++的配置文件显示出更好的一致性。然而，BasicVSR的剖面仍然包含不连续性。得益于我们增强的传播和对齐，BasicVSR++能够从视频帧中聚合更丰富的信息，显示更平滑的时间过渡。

**6.总结**
========

    在这项工作中，我们使用两个新组件重新设计了BasicVSR，以提高其传播和对齐性能，从而实现视频超分辨率任务。我们的模型BasicVSR++在保持效率的同时，大大优于现有的先进水平。这些设计很好地推广到其他视频恢复任务，包括压缩视频增强。这些组件是通用的，我们推测它们将用于其他基于视频的增强或恢复任务，如去模糊和去噪。

    **论文总结**：个人觉得该论文最大的贡献是提出了光流指导的可变形对齐以及二阶网格传播，告诉我们充分利用时间域的重要性。

**论文链接：[https://arxiv.org/pdf/2104.13371.pdf](https://arxiv.org/pdf/2104.13371.pdf)**

**代码链接：[https://github.com/open-mmlab/mmediting](https://github.com/open-mmlab/mmediting)**