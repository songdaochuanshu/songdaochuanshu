---
layout: post
title: "数据统计与可视化课程总结"
date: "2022-10-15T11:18:24.771Z"
---
数据统计与可视化课程总结
============

大数定理与蒙特卡洛
---------

### 大数定律的客观背景

大量随机试验中

*   事件发生的频率稳定于某一常数
*   测量值的算术平均值具有稳定性

比如：

*   大量抛掷硬币字母使用频率
*   正面出现频率

![image-20221014135459160](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153023506-1045133608.png)

### 本福特定律

也叫纽科姆-本福德定律，反常数定律，或第一位数定律，是关于许多现实生活中的数字数据中前几位数字的频率分布的观察。

十进制中，首位数字出现的概率为：  
d，1，2，3，4，5，6，7，8，9  
p，30.1%，17.6%，12.5%，9.7%，7.9%，6.7%，5.8%，5.1%，4.6%

### 中心极限定理（CLT）

![image-20221014140850497](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153229782-214903992.png)

中心极限定理是概率论中最著名的结果之一，它不仅提供了计算独立随机变量之和的近似概率的简单方法，而且有助于解释为什么很多自然群体的经验频率呈现出钟形曲线这一值得注意的事实。

中心极限定理使得很多参数检验成为可能，只要 样本数量足够多（通常要求>30）即可，底层分 布如何没有关系。因此，t检验对正态偏离的鲁棒性高。

贝叶斯定理
-----

![](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153237328-345913160.png)

贝叶斯公式：

\\\[\\begin{eqnarray} P(B \\mid A) & = & \\frac{P(A \\mid B) P(B)}{P(A)} \\\\ {\\text {Posterior}} & = & \\frac{\\text { likelihood } \\times \\text { prior }}{\\text { evidence }} \\end{eqnarray} \\\]

似然(likelihood)：

\\\[L(\\theta \\mid x)=P(X=x \\mid \\theta) \\\]

极大似然估计(MLE):

Maximum Likelihood Estimate(MLE):找出参数θ,使得从中抽样所得的观测数据的概率最大

\\\[L(\\theta)=L\\left(x\_{1}, x\_{2}, \\ldots, x\_{n} ; \\theta\\right)=\\prod\_{i=1}^{n} p\\left(x\_{i} ; \\theta\\right) \\\]

### 判别模型 vs 生成模型

生成模型对数据的生成方式进行建模。它提出一个问题 ：根据我的生成方式假设，哪个类别\\(y\\)最有可能产生当前的特征\\(X\\)？需要对\\(P(X|Y)\\)进行建模。 判别模型不关心数据是如何生成的，它只是对给定的特征进行判别/分类。直接对\\(P(Y|X)\\)进行建模。

![image-20221014143528646](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153342196-299430906.png)

贝叶斯定理应用：

\\\[p \\text { (类别|特征) }=\\frac{p(\\text { 特征|类别 }) p \\text { (类别})}{p(\\text {特征) }}\\\\ p \\text { (病因|症状) }=\\frac{p(\\text { 症状|病因 }) p \\text { (病因})}{p(\\text {症状) }} \\\]

概率统计基础知识
--------

### 随机变量

*   在实际问题中，随机试验的结果可以用数量来 表示，由此就产生了随机变量的概念。
*   随机变量通常用大写字母 X,Y,Z,W,N 等表示。
*   而表示随机变量所取的值时, 一般采用小写字母 x, y, z, w, n等。
*   有了随机变量, 随机试验中的各种事件，就可 以通过随机变量的关系式表达出来。

![image-20221014145354938](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153345764-1096632873.png)

随机变量概念的产生是概率论发展史上的 重大事件. 引入随机变量后，对随机现象统计 规律的研究，就由对`事件及事件概率的研究`扩 大为对`随机变量及其取值规律的研究`。

我们将研究两类随机变量：离散型随机变量、连续型随机变量。

离散型随机变量表示方法 （1）公式法 （2）列表法。

### 离散型随机变量三种常见分布：

#### 1、（0-1）分布

（也称两点分布或 伯努利分布） 随机变量X只可能取0与1两个值，其分布律为：

\\\[P\\{X=k\\}=p^{k}(1-p)^{1-k}, \\quad k=0,1 \\quad(0<p<1) \\\]

或：

\\\[X \\sim\\left|\\begin{array}{cc} 0 & 1 \\\\ 1-p & p \\end{array}\\right| \\\]

#### 2、二项分布

将伯努利试验E独立地重复地进行n次 , 则称这一串重复的独立试验为n重伯努利试验。

*   “重复”是指这n次试验中PA)=p保持不变
*   “独立”是指各次试验的结果互不影响

![image-20221014150916254](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153351468-595952441.png)

#### 3、泊松分布

设随机变量X所有可能取的值为0 , 1 , 2 , … , 且概率分布为：\\(P(X=k)=\\frac{\\lambda^{k}}{k !} e^{-\\lambda}, \\quad k=0,1,2, \\cdots \\cdots,\\)其中 >0 是常数,则称 X 服从参数为的\\(\\lambda\\)泊松分布,记作X ~ π(\\(\\lambda\\) )

泊松分布是二项分布n很大而p很小时的一种极限形式

二项分布是说，已知某件事情发生的概率是p，那么做n次试验，事情发 生的次数就服从于二项分布。

泊松分布是指某段连续的时间内某件事情发生的次数，而且“某件事情 ”发生所用的时间是可以忽略的。例如，在五分钟内，电子元件遭受脉 冲的次数，就服从于泊松分布。（其它例子：一天内，110接到报警的数量；一天内，医院来挂号的病人数）

假如你把“连续的时间”分割成无数小份，那么每个小份之间都是相互 独立的。在每个很小的时间区间内，电子元件都有可能“遭受到脉冲” 或者“没有遭受到脉冲”，这就可以被认为是一个p很小的二项分布。而 因为“连续的时间”被分割成无穷多份，因此n(试验次数)很大。所以， 泊松分布可以认为是二项分布的一种极限形式。

因为二项分布其实就是一个最最简单的“发生”与“不发生”的分布，它可以描述非常多的随机的自然界现象，因此其极限形式泊松分布自然 也是非常有用的。

![image-20221014152005649](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153356550-243026178.png)

### 连续型随机变量三种常见分布:

#### 1.均匀分布

若r .v X的概率密度为：

\\\[f(x)=\\left\\{\\begin{aligned} \\frac{1}{b-a}, & a<x<b \\\\ 0, & \\text { 其它 } \\end{aligned}\\right. \\\]

则称X在区间( a, b)上服从均匀分布(uniform)，记作\\(X\\)~\\(U(a, b)\\)

若\\(X\\)~\\(U(a, b)\\):

(1) 对于长度\\(l\\)为的区间\\((c, c+l), a \\leq c<c+l \\leq b \\text {, }\\), 有

\\\[P\\{c<X \\leq c+l\\}=\\int\_{c}^{c+l} \\frac{1}{b-a} d x=\\frac{l}{b-a} \\\]

(2) \\(X\\)的分布函数为：

\\\[F(x)=P\\{X \\leq x\\}=\\left\\{\\begin{array}{ll} 0, & x<a \\\\ \\frac{x-a}{b-a}, & a \\leq x<b \\\\ 1 & x \\geq b \\end{array}\\right. \\\]

#### 2.指数分布

若r .v X具有概率密度：

\\\[f(x)=\\left\\{\\begin{array}{ll} \\frac{1}{\\theta} e^{-\\frac{x}{\\theta}}, x>0 \\\\ 0, \\qquad \\text {其它} \\end{array}\\right. \\\]

其中\\(\\theta\\)\> 0为常数, 则称 X服从参数为\\(\\theta\\)的指数分布\\(X\\)~\\(Expo(\\theta)\\).

指数分布常用于可靠性统计研究中，如元件的寿命.

若X服从参数为\\(\\theta\\)的指数分布, 则其分布函数为

\\\[F(x)=P\\{X \\leq x\\}=\\left\\{\\begin{array}{ll} 1-e^{-x / \\theta}, & x>0 \\\\ 0, & \\text { 其它 } \\end{array}\\right. \\\]

指数分布常用于可靠性统计研究中，如元件的寿命.

指数分布具有无记忆性。

\\\[\\begin{array}{c} P\\{X>s+t \\mid X>s\\}=P\\{X>t\\} . \\end{array} \\\]

#### 3.正态分布

若连续型r .v X 的概率密度为

\\\[f(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}}, \\quad-\\infty<x<\\infty \\\]

其中μ和σ( σ>0 )都是常数, 则称\\(X\\)服从参数为μ和σ的**正态分布**或**高斯分布**。记作\\(X\\)~\\(N(\\mu, \\sigma^2)\\)

**标准正态分布**

μ = 0, σ = 1 的正态分布称为`标准正态分布`.  
其密度函数和分布函数常用φ(\\(x\\)) 和Φ(\\(x\\)) 表示：

\\\[\\begin{array}{l} \\varphi(x)=\\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{x^{2}}{2}},-\\infty<x<\\infty \\\\ \\Phi(x)=\\frac{1}{\\sqrt{2 \\pi}} \\int\_{-\\infty}^{x} e^{-\\frac{t^{2}}{2}} d t,-\\infty<x<\\infty \\end{array} \\\]

定理1 若 \\(X\\)~\\(N(\\mu, \\sigma^2)\\) 则 \\(Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\)

标准正态分布的重要性在于，任何一个一般的正态分布都可以通过线性变换转化为标准正态分布.

**3\\(\\sigma\\) 准则**

由标准正态分布的查表计算可以求得，当\\(X\\)~\\(N(0, 1)\\) 时

\\\[\\begin{array}{l} P(|X| \\leq 1)=2\\Phi(1)-1=0.6826 \\\\ P(|X| \\leq 2)=2\\Phi(2)-1=0.9544 \\\\ P(|X| \\leq 3)=2\\Phi(3)-1=0.9974 \\end{array} \\\]

这说明，X的取值几乎全部集中在\[-3,3\]区间内，超出这个范围的可能性仅占不到0.3%.

六西格玛(six sigma)是一种改善企业质量流程管理的技术，以“零缺陷”的完美商业追求，带动质量成本的大幅度降低，最终实现财务成效的提升与企业竞争力的突破。

**矩(moment)**

一阶中心距是平均值，二阶中心距是方差，三阶中心矩是偏度，四阶中心距（经过归一化和转移）是峰度。

**协方差矩阵**

类似定义n 维随机变量$(X\_1, X\_2, …, X\_n) $的协方差矩阵.

若:

\\\[\\begin{aligned} c\_{i j}=& \\operatorname{Cov}\\left(X\_{i}, X\_{j}\\right) \\\\ =& E\\left\\{\\left\[X\_{i}-E\\left(X\_{i}\\right)\\right\]\\left\[X\_{j}-E\\left(X\_{j}\\right)\\right\]\\right\\} \\\\ &(\\boldsymbol{i}, j=\\mathbf{1}, 2, \\ldots, n) \\end{aligned} \\\]

都存在，称

矩阵 C为\\((X\_1, X\_2, …, X\_n)\\) 的协方差矩阵

\\\[C=\\left ( \\begin{array}{cccc} c\_{11} & c\_{12} & \\cdots & c\_{1 n} \\\\ c\_{21} & c\_{22} & \\cdots & c\_{2 n} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ c\_{n 1} & c\_{n 2} & \\cdots & c\_{n n} \\end{array}\\right ) \\\]

数理统计
----

统计：给出你手中的信息（ 样本） ， 桶（总体）里有什么？

概率：给出桶中的信息，你手里有什么？

数理统计可以分为描述性统计和统计推断，前者侧重于总结和说明被观测数据集合（样本）的特征。样本  
是从总体中抽取的，表示与我们实验相关的相似个体或事件的总集合。与描述性统计相反，统计推断从给定的样本中进一步推导出总体的特征。

*   描述性统计：呈现、组织和汇总数据
*   统计推断：根据样本中观测到的数据得出关于总体的结论

![image-20221014205923734](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014205927160-326419800.png)

描述性统计是用于数据分析的术语，它有助于以有意义的方式描述、显示或总结数据，比如数据可能出现的模式。但是，描述性统计并不允许我们在所分析的数据之外得出结论，或者就我们可能做出的任何假设得出结论。它只是描述数据的一种方式。

### 总体和样本

**总体**

一个统计问题总有它明确的研究对象.研究对象的全体称为`总体`，总体中每个成员称为`个体`，总体中所包含的个体的个数称为总体的`容量`.

总体分为有限总体和无限总体.研究某批灯泡的质量

定义：设\\(X\\)是具有分布函数\\(F\\)的随机变量，若\\(X\_1,X\_2,...,X\_n\\)是具有同一分布函数\\(F\\)的、相互独立的随机变量，则称\\(X\_1,X\_2,...,X\_n\\)为从分布函数\\(F\\)（或总体、或总体）得到的容量\\(n\\)为的简单随机样本，简称样本，它们的观察值\\(X\_1,X\_2,...,X\_n\\)称为样本值，又称为\\(X\\)的\\(n\\)个独立的观察值.

### **统计量**

由样本值去推断总体情况，需要对样本值进行“加工”，这就要构造一些样本的函数，它把样本中所含的（某一方面）的信息集中起来.

几个常见统计量：

样本平均值、样本方差、样本标准差、

样本k阶原点矩：\\(A\_{k}=\\frac{1}{n} \\sum\_{i=1}^{n} X\_{i}^{k},k=1,2,...\\)

样本k阶中心矩：\\(B\_{k}=\\frac{1}{n} \\sum\_{i=1}^{n}\\left(X\_{i}-\\bar{X}\\right)^{k}\\)

请注意:

若总体\\(X\\) 的\\(k\\)阶矩$E\\left(X{k}\\right)=\\mu{k} $ 存在, 则当\\(n\\rightarrow\\infty\\) 时, $A\_{k}=\\frac{1}{n} \\sum\_{i=1}^{n} X\_{i}^{k} \\stackrel{p}{\\longrightarrow} \\mu^{k} \\quad k=1,2, \\cdots $

事实上由\\(X\_1,X\_2,...,X\_n\\)独立且与\\(X\\)同分布，有\\(X\_1^k,X\_2^k,...,X\_n^k\\)独立且与\\(X^k\\)同分布,\\(E(X\_i^k)=\\mu^k\\),\\(k=1,2,...,n\\)再由辛钦大数定律可得上述结论.

再由依概率收敛性质知，可将上述性质推广为

\\\[g\\left(A\_{1}, A\_{2}, \\cdots, A\_{k}\\right) \\stackrel{p}{\\longrightarrow} g\\left(\\mu\_{1}, \\mu\_{2}, \\cdots, \\mu\_{k}\\right) \\\]

其中g为连续函数.

这就是矩估计法的理论根据.

**统计量（样本） vs 参数（总体）**

样本的属性如平均值或者标准差，不称为参数而是被称作统计量。推断统计是使我们能够利用样本对样  
本所来自的总体进行概括/推断的技术。

使用统计量评估参数.

### **统计三大抽样分布**

**抽样分布**

如果总体(Population)满足特定的分布，那么其样本(Sample)（统计量）满足怎样的分布规律？

关于样本均值的抽样分布，如\\(t\\)

关于样本方差的抽样分布，如\\(\\chi^2\\) 、\\(F\\)

#### **1、\\(\\chi^2\\) 分布**

\\(\\chi^2\\)分布是由正态分布派生出来的一种分布.

定义: 设\\(X\_1,X\_2,...,X\_n\\)相互独立, 都服从标准正态分布\\(N(0,1)\\), 则称随机变量：

\\\[\\chi^{2}=X\_{1}{ }^{2}+X\_{2}{ }^{2}+\\cdots+X\_{n}{ }^{2} \\\]

所服从的分布为自由度为n 的\\(\\chi^2\\)分布.

自由度（degree of freedom）记为\\(\\chi^{2} \\sim \\chi^{2}(n)\\)

卡方分布-不同自由度下的PDF曲线(概率密度函数)：

![image-20221014214814596](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014214817507-1566314855.png) ![image-20221014214931400](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014214934266-294196402.png) ![image-20221014214957094](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014214959879-1317305705.png) ![image-20221014215013103](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014215016093-1640589358.png)

#### **2、\\(t\\) 分布(学生分布）**

定义: 设\\(X～N(0,1)\\) , \\(Y～\\chi^2\\), 且\\(X\\)与\\(Y\\)相互独立，则称变量\\(t=\\frac{X}{\\sqrt{Y / n}}\\)所服从的分布为自由度为n 的t 分布.

记为\\(t\\sim t(n)\\).

分布的性质：

*   具有自由度为\\(n\\)的\\(t\\)分布\\(t\\sim t(n)\\), 其数学期望与方差为：E(t)=0, D(t)=n /(n-2)(n>2)
    
*   即当\\(n\\)足够大时，\\(t \\stackrel{\\text { 近似 }}{\\sim} N(0,1)\\).
    
    ![image-20221014220449580](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014220452564-2108111561.png)

#### **3、F分布**

定义: 设\\(U \\sim \\chi^{2}\\left(n\_{1}\\right), V \\sim \\chi^{2}\\left(n\_{2}\\right)\\)

\\(U\\)与\\(V\\)相互独立，则称随机变量\\(F=\\frac{U / n\_{1}}{V / n\_{2}}\\)

服从自由度为\\(n\_1\\)及\\(n\_2\\) 的F分布，\\(n\_1\\)称为第一自由度，\\(n\_2\\)称为第二自由度，记作\\(F\\sim F(n1,n2)\\)

由定义可见:

\\\[\\frac{1}{F}=\\frac{V / n\_{2}}{U / n\_{1}} \\sim \\boldsymbol{F}\\left(\\boldsymbol{n}\_{\\mathbf{2}}, \\boldsymbol{n}\_{\\mathbf{1}}\\right) \\\]

F分布的数学期望为:

\\\[E(F)=\\frac{n\_{2}}{n\_{2}-2} \\quad \\text { 若 } \\boldsymbol{n}\_{\\mathbf{2}}>\\mathbf{2} \\\]

即它的数学期望并不依赖于第一自由度\\(n\_1\\).

![image-20221014221232310](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014221235280-687143531.png) ![image-20221014221314760](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014221317537-555045296.png) ![image-20221014221410069](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014221412920-1213681418.png) ![image-20221014221433790](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014221436698-992788783.png)

### 参数估计概念

研究统计量的性质和评价一个统计推断的优良性，取决于其抽样分布的性质.

![image-20221015141006063](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015141009331-1433520074.png)

设有一个统计总体, 总体的分布函数为\\(F( x, \\theta )\\) ，其中\\(\\theta\\)为未知参数(\\(\\theta\\)可以是向量) .

现从该总体抽样，得样本\\(X\_1,X\_2,...,X\_n\\),要依据该样本对产生该样本的参数(\\(\\theta\\)作出估计, 或估计\\(\\theta\\)的某个已知函数 \\(g(\\theta)\\) .

这类问题称为参数估计.参数估计分为点估计和区间估计.

![image-20221015141427911](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015141430917-278451017.png)

举例：为估计总体均值μ

我们需要构造出适当的样本的函数/统计量$T(X\_1,X\_2,…X\_n) $，每当有了样本，就代入该函数中算出一个值，用来作为μ的估计值.

\\(T(X\_1,X\_2,…X\_n)\\) 称为参数μ的点估计量，把样本值代入\\(T(X\_1,X\_2,…X\_n)\\)中，得到μ的一个点估计值.

![image-20221015142448645](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015142451682-1731089682.png)

### 参数估计计算方法

#### 矩估计法

![image-20221015143356813](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015143359907-1111795458.png) ![image-20221015143421155](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015143424162-2139610693.png) ![image-20221015143458565](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015143501550-311774388.png)

#### 最大似然法（MLE）

![image-20221015143601885](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015143604828-78880791.png)

似然(likelihood)：

\\\[L(\\theta \\mid x)=P(X=x \\mid \\theta) \\\]

![image-20221015143653964](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015143657240-1098328482.png) ![image-20221015143722947](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015143725918-51137641.png) ![image-20221015144142881](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015144145935-275224578.png)

#### 估计量的评选标准

常用的几条标准是：

*   无偏性
*   有效性
*   相合性

![image-20221015144936302](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015144939284-230865528.png) ![image-20221015144956594](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015144959537-1550526826.png) ![image-20221015145010947](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015145013784-166716906.png)

### 区间估计

我们希望确定一个区间，使我们能以比较高的**可靠程度**相信它包含真参数值.通常采用95%的CI（置信区间）.

这里所说的“可靠程度”是用概率来度量的，称为置信度或置信水平(confidence level ).

习惯上把置信水平记作 \\(1 − \\alpha\\)，这里\\(\\alpha\\)是一个很小的正数.

![image-20221015150036693](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015150040247-1169643725.png)

①**置信区间（CI）的第一种求解方法：利用抽样分布(需要事先知道抽样分布)**

​ [JB Statistics – 适合所有人的统计教程！](https://www.jbstatistics.com/)

![image-20221015150503743](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015150506968-774877559.png) ![image-20221015150530644](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015150533790-1870819258.png) ![image-20221015150626435](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015150629605-826032229.png)

#### 单侧置信区间

上述置信区间中置信限都是双侧的，但对于有些实际问题，人们关心的只是参数在一个方向的界限.例如对于设备、元件的使用寿命来说，平均寿命过长没什么问题，过短就有问题了.

这时, 可将置信上限取为+∞ ，而只着眼于置信下限，这样求得的置信区间叫单侧置信区间.

![image-20221015151441543](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015151444717-440069672.png)

#### 正态总体均值与方差的区间估计

##### 单个总体\\(N(\\mu ,\\sigma^2)\\)的情况

![image-20221015151814962](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015151817955-1874977908.png) ![image-20221015151834094](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015151837003-1667801822.png) ![image-20221015151946000](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015151949149-574494676.png) ![image-20221015152311394](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152314490-1910807734.png)

##### 两个总体\\(N(\\mu\_1 ,\\sigma\_1^2)\\),\\(N(\\mu\_2 ,\\sigma\_2^2)\\)的情况

![image-20221015152328118](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152331298-1505714451.png) ![image-20221015152343486](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152346406-741837205.png) ![image-20221015152402085](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152405150-181947722.png) ![image-20221015152414245](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152417235-1090834865.png) ![image-20221015152435467](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152438500-609970315.png) ![image-20221015152458281](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152501308-862544714.png) ![image-20221015152511905](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152514894-756104673.png)

②置信区间（CI）的第二种求解方法：利用Bootstrapping（不需要事先知道抽样分布）

Bootstrapping是一种有放回的抽样

![image-20221015152842357](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152845518-1255849367.png)

![image-20221015152858641](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015152901665-589941757.png)

### 假设检验

计算一个合适的检验统计量（基于样本数据），并确定有多少证据反对原假设。如果证据足够强（如果它满足一定的显著性水平），我们可以拒绝原假设，从而支持备择假设。

拒绝/否定\\(H\_0\\)并不是说\\(H\_0\\)一定错，而只是说差异到了一定的显著程度，拒绝\\(H\_0\\)比较合理.

反之，接受\\(H\_0\\)并不是肯定\\(H\_0\\)一定对，而只是说差异还不够显著，还没有达到足以否定\\(H\_0\\)的程度.

所以假设检验又叫“显著性检验”

#### 假设检验的一般步骤(t检验)

在上面的例子的叙述中，我们已经初步介绍了假设检验的基本思想和方法.  
下面，我们再结合另一个例子，进一步说明假设检验的一般步骤.

![image-20221015153429990](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015153433072-286659930.png) ![image-20221015153553066](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015153556342-714727411.png) ![image-20221015153614916](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015153617965-1204961273.png) ![image-20221015153634243](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015153637149-718335880.png)

上述利用t 统计量得出得检验法称为`t检验法`。在实际中，正态总体的方差常为未知，所以我们常用t 检验法来检验关于正态总体均值的检验问题。

#### 假设检验的p值法（The p-value method）

p-value：p值，也称概率值，是一个数字，描述数据随机发生的可能性（即原假设为真），统计显著性水平通常用0到1之间的p值表示。p值越小，拒绝原假设的证据就越充分，当原假设成立时，检验统计量至少比观察到的检验统计量更极端（离谱）的概  
率:

\\(p < 0.05\\) 显著  
\\(p < 0.01\\) 极其显著

或原假设可被拒绝的最小显著性水平。

**p-hacking:**

在追逐超低p-value 的背景下，学者在面临这些选择做决定时会“非常微妙”，一切阻碍超低p-value 诞生的数据都会被巧妙的避开。Dr. Harvey 将为了追求超低p-value 而在因子研究中刻意选取的数据处理方法称为p-hacking。  
为了竞逐在顶级期刊上发表文章，学者们过度追求因子在原假设下的低pvalue值（即统计意义上“显著”）。学者们在追逐p-value 的道路上狂奔，却在科学的道路上渐行渐远。

![image-20221015154338960](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015154342214-250866669.png) ![image-20221015154409073](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015154412322-163950092.png)

A/B 测试是一种产品优化的方法，为同一个优化目标制定两个方案（比如两个页面），让一部分用户使用A 方案（称为控制组或对照组），同时另一部分用户使用B 方案（称为变化组或试验组），统计并对比不同方案的转化率、点击量、留存率等指标，以判断不同方案的优劣并进行决策。  
A/B测试的本质：  
A/B测试中是用对照版本和试验版本这两个样本的数据来对两个总体是否存在差异进行检验，所以其本质是使用假设检验中的独立样本t检验。

除了以上对**正态总体均值**的假设检验，还可以对**方差**进行假设检验

![image-20221015154908020](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015154911060-545277060.png)

**方差齐次检验**

在方差分析的F检验中，是以各个实验组内总体方差齐性为前提的，因此，按理应该在方差分析之前，要对各个实验组内的总体方差先进行齐性检验。如果各个实验组内总体方差为齐性，而且经过F检验所得多个样本所属总体平均数差异显著，这时才可以将多个样本所属总体平均数的差异归因于各种实验处理的不同所致；如果各个总体方差不齐，那么经过F检验所得多个样本所属总体平均数差异显著的结果，可能有一部分归因于各个实验组内总体方差不同所致。  
简单地说就是在进行两组或多组数据进行比较时，先要使各组数据符合正态分布，另外就是要使各组数据的方差相等（齐性）。

#### 假设检验的两类错误

假设检验会不会犯错误呢？由于作出结论的依据是小概率原理.

小概率事件在一次试验中基本上不会发生.不是一定不发生

![image-20221015155432933](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015155435899-107024476.png) ![image-20221015155459338](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015155502424-1386361340.png) ![image-20221015155525828](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015155528900-141950814.png)

**两类错误的概率的关系**

两类错误是互相关联的， 当样本容量固定时，一类错误概率的减少导致另一类错误  
概率的增加.

要同时降低两类错误的概率\\(\\alpha,\\beta\\)或者要在\\(\\alpha\\)不变的条件下降低，需要增加样本容量.

通常情况：控制1类错误，不关心2类错误

许多教科书和教师会说，类型1（弃真）比类型2（取伪）错误更糟糕。其基本原理可以归结为这样一种（保守的）观点：如果你坚持现状或默认假设，至少你不会让事情变得更糟。

实际情况下，应评估两类错误的现实代价，以权衡1类和2类。

对于2类错误（β），该如何度量和控制？ Power Analysis ！

### 效应量（effect size）（选学）

效能/ 势

*   效能：当一个原假设为假时拒绝它的概率——因此应该被拒绝
*   Jacob Cohen是效能分析之父
*   把效能描述为1-β
*   可接受的效能为大于等于.80-有人认为大于.70是可接受的，并且大于.90是很优秀。
*   效能分析通常是事先进行的，但也可以事后进行

效能与以下因素有关：

*   α水平
    
*   样本容量（主要决定因素）
    
*   效应量
    
*   正在进行统计的统计检验的类型
    
*   试验的设计
    
    *   放宽α的约束（取α为.1或.15）
        
    *   使用参数统计量
        
    *   增加测量的可靠性
        
    *   使用单边检验（one-sided/one-tailed）
        
    *   增加样本容量（N）
        
    *   提高设计或分析的灵敏度
        

![image-20221015160438652](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015160441870-791386237.png)

![image-20221015160454850](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015160457994-273785850.png)

### 方差分析（ANOVA）

![image-20221015161029072](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161032195-943858583.png) ![image-20221015161042843](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161045746-906509711.png) ![image-20221015161103576](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161106558-1173779136.png) ![image-20221015161134118](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161137170-2127854386.png) ![image-20221015161147387](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161150228-2094103965.png) ![image-20221015161158798](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161201833-933233436.png) ![image-20221015161215709](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161218645-1460933620.png) ![image-20221015161520022](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161523039-451106998.png) ![image-20221015161532359](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161535297-403894617.png)

![image-20221015161612592](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161615711-1713473682.png)

​ 在方差分析的F检验中，是以各个实验组内总体方差齐性为前提的，因此，按理应该在方差分析之前，要对各个实验组内的总体方差先进行齐性检验。如果各个实验组内总体方差为齐性，而且经过F检验所得多个样本所属总体平均数差异显著，这时才可以将多个样本所属总体平均数的差异归因于各种实验处理的不同所致；如果各个总体方差不齐，那么经过F检验所得多个样本所属总体平均数差异显著的结果，可能有一部分归因于各个实验组内总体方差不同所致。简单地说就是在进行两组或多组数据进行比较时，先要使各组数据符合正态分布，另外就是要使各组数据的方差相等（齐性）。

![image-20221015161843720](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161846726-804146859.png)

Levene检验是统计分析的一个组成部分。在进行其他统计分析（例如t检验和方差分析）之前，它可用于检验方差的齐次性/同质性.

![image-20221015161953129](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015161956147-2049220601.png)

Eta squared 是一种效应量指标(effect size)，常用于方差分析中。代表的是通过方差分析能够得到解释的因变量变异程度(sum of squares effect)在所有变异程度（sum of  
squares total)中所占的比例。一般希望eta square值越大越好，>0.01 为较小效应量，>0.02为中等效应量，>0.083可视为大效应量

### 相关分析

因果vs 相关

因果：当一件事（原因）导致另一件事发生（结果）。

相关：当两个或两个以上的事物看起来是相关的。

#### 皮尔逊相关系数

皮尔逊相关系数用来测度两个变量之间的线性相关性

它是自变量x能够解释因变量y信息的“百分比”。取值范围\[0,1\]，一些研究者建议用\\(R^2\\)(\\(r^2\\)) 代替r。

我们把残差平方和称为SS（拟合），即“最佳拟合线附近残差的平方和”。

我们在线性回归中使用\\(R^2\\)作为拟合优度的度量

#### 斯皮尔曼相关系数

斯皮尔曼相关系数是对两个数据集之间相关性的非参数度量。与皮尔逊相关不同，斯皮尔曼相关并不假设两个数据集都是正态分布的。这个系数在-1和+1之间变化，0表示没有相关性。-1或+1的相关性暗示了一种精确的单调关系。正相关意味着当x增加时，y也增加。负相关意味着当x增加时，y减少。

#### 肯德尔相关系数

![image-20221015163150134](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015163153175-2137336662.png) ![image-20221015163205992](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015163208922-1009388293.png)

### 回归分析&GLzM

ANOVA & 线性回归

#### 广义线性模型Generalized linear model

在统计学中，广义线性模型（GLM or GzLM）是普通线性回归的灵活推广。GLM通过允许线性模型通过链接函数与因变量相关。

广义线性模型的3个模块：

*   线性预测模型
*   链接函数
*   概率分布

GLM （一般线性模型）是GLzM（广义线性模型）的一个特例。当广义线性模型GLzM使用恒等链接函数和一个正态分布时，就是一般线性模型GLM。

![image-20221015163356044](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015163359053-851942468.png) ![image-20221015163423923](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015163426798-1161407620.png)

#### Logit函数

![image-20221015163821192](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015163824251-352689678.png) ![image-20221015163846437](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221015163849375-715991983.png)

如果使用logit函数作为链接函数，并使用二项式分布/伯努利分布作为概率分布，那么该模型为逻辑回归。

内容整理不尽详细之处请查询课件：[My\_Courses/数据统计与可视化 at main · ranxi2001/My\_Courses (github.com)](https://github.com/ranxi2001/My_Courses/tree/main/%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96)