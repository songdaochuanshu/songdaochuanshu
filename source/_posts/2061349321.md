---
layout: post
title: "数据统计与可视化课程总结"
date: "2022-10-14T19:21:16.014Z"
---
数据统计与可视化课程总结
============

大数定理与蒙特卡洛
---------

### 大数定律的客观背景

大量随机试验中

*   事件发生的频率稳定于某一常数
*   测量值的算术平均值具有稳定性

比如：

*   大量抛掷硬币字母使用频率
*   正面出现频率

![image-20221014135459160](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153023506-1045133608.png)

### 本福特定律

也叫纽科姆-本福德定律，反常数定律，或第一位数定律，是关于许多现实生活中的数字数据中前几位数字的频率分布的观察。

十进制中，首位数字出现的概率为：  
d，1，2，3，4，5，6，7，8，9  
p，30.1%，17.6%，12.5%，9.7%，7.9%，6.7%，5.8%，5.1%，4.6%

### 中心极限定理（CLT）

![image-20221014140850497](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153229782-214903992.png)

中心极限定理是概率论中最著名的结果之一，它不仅提供了计算独立随机变量之和的近似概率的简单方法，而且有助于解释为什么很多自然群体的经验频率呈现出钟形曲线这一值得注意的事实。

中心极限定理使得很多参数检验成为可能，只要 样本数量足够多（通常要求>30）即可，底层分 布如何没有关系。因此，t检验对正态偏离的鲁棒性高。

贝叶斯定理
-----

![](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153237328-345913160.png)

贝叶斯公式：

\\\[\\begin{eqnarray} P(B \\mid A) & = & \\frac{P(A \\mid B) P(B)}{P(A)} \\\\ {\\text {Posterior}} & = & \\frac{\\text { likelihood } \\times \\text { prior }}{\\text { evidence }} \\end{eqnarray} \\\]

似然(likelihood)：

\\\[L(\\theta \\mid x)=P(X=x \\mid \\theta) \\\]

极大似然估计(MLE):

Maximum Likelihood Estimate(MLE):找出参数θ,使得从中抽样所得的观测数据的概率最大

\\\[L(\\theta)=L\\left(x\_{1}, x\_{2}, \\ldots, x\_{n} ; \\theta\\right)=\\prod\_{i=1}^{n} p\\left(x\_{i} ; \\theta\\right) \\\]

### 判别模型 vs 生成模型

生成模型对数据的生成方式进行建模。它提出一个问题 ：根据我的生成方式假设，哪个类别\\(y\\)最有可能产生当前的特征\\(X\\)？需要对\\(P(X|Y)\\)进行建模。 判别模型不关心数据是如何生成的，它只是对给定的特征进行判别/分类。直接对\\(P(Y|X)\\)进行建模。

![image-20221014143528646](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153342196-299430906.png)

贝叶斯定理应用：

\\\[p \\text { (类别|特征) }=\\frac{p(\\text { 特征|类别 }) p \\text { (类别})}{p(\\text {特征) }}\\\\ p \\text { (病因|症状) }=\\frac{p(\\text { 症状|病因 }) p \\text { (病因})}{p(\\text {症状) }} \\\]

概率统计基础知识
--------

### 随机变量

*   在实际问题中，随机试验的结果可以用数量来 表示，由此就产生了随机变量的概念。
*   随机变量通常用大写字母 X,Y,Z,W,N 等表示。
*   而表示随机变量所取的值时, 一般采用小写字母 x, y, z, w, n等。
*   有了随机变量, 随机试验中的各种事件，就可 以通过随机变量的关系式表达出来。

![image-20221014145354938](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153345764-1096632873.png)

随机变量概念的产生是概率论发展史上的 重大事件. 引入随机变量后，对随机现象统计 规律的研究，就由对`事件及事件概率的研究`扩 大为对`随机变量及其取值规律的研究`。

我们将研究两类随机变量：离散型随机变量、连续型随机变量。

离散型随机变量表示方法 （1）公式法 （2）列表法。

### 离散型随机变量三种常见分布：

1、（0-1）分布：（也称两点分布或 伯努利分布） 随机变量X只可能取0与1两个值，其分布律为：

\\\[P\\{X=k\\}=p^{k}(1-p)^{1-k}, \\quad k=0,1 \\quad(0<p<1) \\\]

或：

\\\[X \\sim\\left|\\begin{array}{cc} 0 & 1 \\\\ 1-p & p \\end{array}\\right| \\\]

2、二项分布：将伯努利试验E独立地重复地进行n次 , 则称这一串重复的独立试验为n重伯努利试验。

*   “重复”是指这n次试验中PA)=p保持不变
*   “独立”是指各次试验的结果互不影响

![image-20221014150916254](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153351468-595952441.png)

3、泊松分布

设随机变量X所有可能取的值为0 , 1 , 2 , … , 且概率分布为：\\(P(X=k)=\\frac{\\lambda^{k}}{k !} e^{-\\lambda}, \\quad k=0,1,2, \\cdots \\cdots,\\)其中 >0 是常数,则称 X 服从参数为的\\(\\lambda\\)泊松分布,记作X ~ π(\\(\\lambda\\) )

泊松分布是二项分布n很大而p很小时的一种极限形式

二项分布是说，已知某件事情发生的概率是p，那么做n次试验，事情发 生的次数就服从于二项分布。

泊松分布是指某段连续的时间内某件事情发生的次数，而且“某件事情 ”发生所用的时间是可以忽略的。例如，在五分钟内，电子元件遭受脉 冲的次数，就服从于泊松分布。（其它例子：一天内，110接到报警的数量；一天内，医院来挂号的病人数）

假如你把“连续的时间”分割成无数小份，那么每个小份之间都是相互 独立的。在每个很小的时间区间内，电子元件都有可能“遭受到脉冲” 或者“没有遭受到脉冲”，这就可以被认为是一个p很小的二项分布。而 因为“连续的时间”被分割成无穷多份，因此n(试验次数)很大。所以， 泊松分布可以认为是二项分布的一种极限形式。

因为二项分布其实就是一个最最简单的“发生”与“不发生”的分布，它可以描述非常多的随机的自然界现象，因此其极限形式泊松分布自然 也是非常有用的。

![image-20221014152005649](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014153356550-243026178.png)

### 连续型随机变量三种常见分布

#### 1.均匀分布

若r .v X的概率密度为：

\\\[f(x)=\\left\\{\\begin{aligned} \\frac{1}{b-a}, & a<x<b \\\\ 0, & \\text { 其它 } \\end{aligned}\\right. \\\]

则称X在区间( a, b)上服从均匀分布(uniform)，记作\\(X\\)~\\(U(a, b)\\)

若\\(X\\)~\\(U(a, b)\\):

(1) 对于长度\\(l\\)为的区间\\((c, c+l), a \\leq c<c+l \\leq b \\text {, }\\), 有

\\\[P\\{c<X \\leq c+l\\}=\\int\_{c}^{c+l} \\frac{1}{b-a} d x=\\frac{l}{b-a} \\\]

(2) \\(X\\)的分布函数为：

\\\[F(x)=P\\{X \\leq x\\}=\\left\\{\\begin{array}{ll} 0, & x<a \\\\ \\frac{x-a}{b-a}, & a \\leq x<b \\\\ 1 & x \\geq b \\end{array}\\right. \\\]

#### 2.指数分布

若r .v X具有概率密度：

\\\[f(x)=\\left\\{\\begin{array}{ll} \\frac{1}{\\theta} e^{-\\frac{x}{\\theta}}, x>0 \\\\ 0, \\qquad \\text {其它} \\end{array}\\right. \\\]

其中\\(\\theta\\)\> 0为常数, 则称 X服从参数为\\(\\theta\\)的指数分布\\(X\\)~\\(Expo(\\theta)\\).

指数分布常用于可靠性统计研究中，如元件的寿命.

若X服从参数为\\(\\theta\\)的指数分布, 则其分布函数为

\\\[F(x)=P\\{X \\leq x\\}=\\left\\{\\begin{array}{ll} 1-e^{-x / \\theta}, & x>0 \\\\ 0, & \\text { 其它 } \\end{array}\\right. \\\]

指数分布常用于可靠性统计研究中，如元件的寿命.

指数分布具有无记忆性。

\\\[\\begin{array}{c} P\\{X>s+t \\mid X>s\\}=P\\{X>t\\} . \\end{array} \\\]

#### 3.正态分布

若连续型r .v X 的概率密度为

\\\[f(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}}, \\quad-\\infty<x<\\infty \\\]

其中μ和σ( σ>0 )都是常数, 则称\\(X\\)服从参数为μ和σ的**正态分布**或**高斯分布**。记作\\(X\\)~\\(N(\\mu, \\sigma^2)\\)

**标准正态分布**

μ = 0, σ = 1 的正态分布称为`标准正态分布`.  
其密度函数和分布函数常用φ(\\(x\\)) 和Φ(\\(x\\)) 表示：

\\\[\\begin{array}{l} \\varphi(x)=\\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{x^{2}}{2}},-\\infty<x<\\infty \\\\ \\Phi(x)=\\frac{1}{\\sqrt{2 \\pi}} \\int\_{-\\infty}^{x} e^{-\\frac{t^{2}}{2}} d t,-\\infty<x<\\infty \\end{array} \\\]

定理1 若 \\(X\\)~\\(N(\\mu, \\sigma^2)\\) 则 \\(Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\)

标准正态分布的重要性在于，任何一个一般的正态分布都可以通过线性变换转化为标准正态分布.

**3\\(\\sigma\\) 准则**

由标准正态分布的查表计算可以求得，当\\(X\\)~\\(N(0, 1)\\) 时

\\\[\\begin{array}{l} P(|X| \\leq 1)=2\\Phi(1)-1=0.6826 \\\\ P(|X| \\leq 2)=2\\Phi(2)-1=0.9544 \\\\ P(|X| \\leq 3)=2\\Phi(3)-1=0.9974 \\end{array} \\\]

这说明，X的取值几乎全部集中在\[-3,3\]区间内，超出这个范围的可能性仅占不到0.3%.

六西格玛(six sigma)是一种改善企业质量流程管理的技术，以“零缺陷”的完美商业追求，带动质量成本的大幅度降低，最终实现财务成效的提升与企业竞争力的突破。

**矩(moment)**

一阶中心距是平均值，二阶中心距是方差，三阶中心矩是偏度，四阶中心距（经过归一化和转移）是峰度。

**协方差矩阵**

类似定义n 维随机变量$(X\_1, X\_2, …, X\_n) $的协方差矩阵.

若:

\\\[\\begin{aligned} c\_{i j}=& \\operatorname{Cov}\\left(X\_{i}, X\_{j}\\right) \\\\ =& E\\left\\{\\left\[X\_{i}-E\\left(X\_{i}\\right)\\right\]\\left\[X\_{j}-E\\left(X\_{j}\\right)\\right\]\\right\\} \\\\ &(\\boldsymbol{i}, j=\\mathbf{1}, 2, \\ldots, n) \\end{aligned} \\\]

都存在，称

矩阵 C为\\((X\_1, X\_2, …, X\_n)\\) 的协方差矩阵

\\\[C=\\left ( \\begin{array}{cccc} c\_{11} & c\_{12} & \\cdots & c\_{1 n} \\\\ c\_{21} & c\_{22} & \\cdots & c\_{2 n} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ c\_{n 1} & c\_{n 2} & \\cdots & c\_{n n} \\end{array}\\right ) \\\]

数理统计
----

统计：给出你手中的信息（ 样本） ， 桶（总体）里有什么？

概率：给出桶中的信息，你手里有什么？

数理统计可以分为描述性统计和统计推断，前者侧重于总结和说明被观测数据集合（样本）的特征。样本  
是从总体中抽取的，表示与我们实验相关的相似个体或事件的总集合。与描述性统计相反，统计推断从给定的样本中进一步推导出总体的特征。

*   描述性统计：呈现、组织和汇总数据
*   统计推断：根据样本中观测到的数据得出关于总体的结论

![image-20221014205923734](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014205927160-326419800.png)

描述性统计是用于数据分析的术语，它有助于以有意义的方式描述、显示或总结数据，比如数据可能出现的模式。但是，描述性统计并不允许我们在所分析的数据之外得出结论，或者就我们可能做出的任何假设得出结论。它只是描述数据的一种方式。

### 总体和样本

**总体**

一个统计问题总有它明确的研究对象.研究对象的全体称为`总体`，总体中每个成员称为`个体`，总体中所包含的个体的个数称为总体的`容量`.

总体分为有限总体和无限总体.研究某批灯泡的质量

定义：设\\(X\\)是具有分布函数\\(F\\)的随机变量，若\\(X\_1,X\_2,...,X\_n\\)是具有同一分布函数\\(F\\)的、相互独立的随机变量，则称\\(X\_1,X\_2,...,X\_n\\)为从分布函数\\(F\\)（或总体、或总体）得到的容量\\(n\\)为的简单随机样本，简称样本，它们的观察值\\(X\_1,X\_2,...,X\_n\\)称为样本值，又称为\\(X\\)的\\(n\\)个独立的观察值.

### 样本及抽样分布

#### **统计量**

由样本值去推断总体情况，需要对样本值进行“加工”，这就要构造一些样本的函数，它把样本中所含的（某一方面）的信息集中起来.

几个常见统计量：

样本平均值、样本方差、样本标准差、

样本k阶原点矩：\\(A\_{k}=\\frac{1}{n} \\sum\_{i=1}^{n} X\_{i}^{k},k=1,2,...\\)

样本k阶中心矩：\\(B\_{k}=\\frac{1}{n} \\sum\_{i=1}^{n}\\left(X\_{i}-\\bar{X}\\right)^{k}\\)

请注意:

若总体\\(X\\) 的\\(k\\)阶矩$E\\left(X{k}\\right)=\\mu{k} $ 存在, 则当\\(n\\rightarrow\\infty\\) 时, $A\_{k}=\\frac{1}{n} \\sum\_{i=1}^{n} X\_{i}^{k} \\stackrel{p}{\\longrightarrow} \\mu^{k} \\quad k=1,2, \\cdots $

事实上由\\(X\_1,X\_2,...,X\_n\\)独立且与\\(X\\)同分布，有\\(X\_1^k,X\_2^k,...,X\_n^k\\)独立且与\\(X^k\\)同分布,\\(E(X\_i^k)=\\mu^k\\),\\(k=1,2,...,n\\)再由辛钦大数定律可得上述结论.

再由依概率收敛性质知，可将上述性质推广为

\\\[g\\left(A\_{1}, A\_{2}, \\cdots, A\_{k}\\right) \\stackrel{p}{\\longrightarrow} g\\left(\\mu\_{1}, \\mu\_{2}, \\cdots, \\mu\_{k}\\right) \\\]

其中g为连续函数.

这就是矩估计法的理论根据.

**统计量（样本） vs 参数（总体）**

样本的属性如平均值或者标准差，不称为参数而是被称作统计量。推断统计是使我们能够利用样本对样  
本所来自的总体进行概括/推断的技术。

使用统计量评估参数.

### **统计三大抽样分布**

**抽样分布**

如果总体(Population)满足特定的分布，那么其样本(Sample)（统计量）满足怎样的分布规律？

关于样本均值的抽样分布，如\\(t\\)

关于样本方差的抽样分布，如\\(\\chi^2\\) 、\\(F\\)

#### **1、\\(\\chi^2\\) 分布**

\\(\\chi^2\\)分布是由正态分布派生出来的一种分布.

定义: 设\\(X\_1,X\_2,...,X\_n\\)相互独立, 都服从标准正态分布\\(N(0,1)\\), 则称随机变量：

\\\[\\chi^{2}=X\_{1}{ }^{2}+X\_{2}{ }^{2}+\\cdots+X\_{n}{ }^{2} \\\]

所服从的分布为自由度为n 的\\(\\chi^2\\)分布.

自由度（degree of freedom）记为\\(\\chi^{2} \\sim \\chi^{2}(n)\\)

卡方分布-不同自由度下的PDF曲线(概率密度函数)：

![image-20221014214814596](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014214817507-1566314855.png)

![image-20221014214931400](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014214934266-294196402.png)

![image-20221014214957094](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014214959879-1317305705.png)

![image-20221014215013103](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014215016093-1640589358.png)

#### **2、\\(t\\) 分布(学生分布）**

定义: 设\\(X～N(0,1)\\) , \\(Y～\\chi^2\\), 且\\(X\\)与\\(Y\\)相互独立，则称变量\\(t=\\frac{X}{\\sqrt{Y / n}}\\)所服从的分布为自由度为n 的t 分布.

记为\\(t\\sim t(n)\\).

分布的性质：

*   具有自由度为\\(n\\)的\\(t\\)分布\\(t\\sim t(n)\\), 其数学期望与方差为：E(t)=0, D(t)=n /(n-2)(n>2)
    
*   即当\\(n\\)足够大时，\\(t \\stackrel{\\text { 近似 }}{\\sim} N(0,1)\\).
    
    ![image-20221014220449580](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014220452564-2108111561.png)
    

#### **3、F分布**

定义: 设\\(U \\sim \\chi^{2}\\left(n\_{1}\\right), V \\sim \\chi^{2}\\left(n\_{2}\\right)\\)

\\(U\\)与\\(V\\)相互独立，则称随机变量\\(F=\\frac{U / n\_{1}}{V / n\_{2}}\\)

服从自由度为\\(n\_1\\)及\\(n\_2\\) 的F分布，\\(n\_1\\)称为第一自由度，\\(n\_2\\)称为第二自由度，记作\\(F\\sim F(n1,n2)\\)

由定义可见:

\\\[\\frac{1}{F}=\\frac{V / n\_{2}}{U / n\_{1}} \\sim \\boldsymbol{F}\\left(\\boldsymbol{n}\_{\\mathbf{2}}, \\boldsymbol{n}\_{\\mathbf{1}}\\right) \\\]

F分布的数学期望为:

\\\[E(F)=\\frac{n\_{2}}{n\_{2}-2} \\quad \\text { 若 } \\boldsymbol{n}\_{\\mathbf{2}}>\\mathbf{2} \\\]

即它的数学期望并不依赖于第一自由度\\(n\_1\\).

![image-20221014221232310](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014221235280-687143531.png)

![image-20221014221314760](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014221317537-555045296.png)

![image-20221014221410069](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014221412920-1213681418.png)

![image-20221014221433790](https://img2022.cnblogs.com/blog/2910984/202210/2910984-20221014221436698-992788783.png)