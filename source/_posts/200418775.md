---
layout: post
title: "MySQL MHA信息的收集【Filebeat+logstash+MySQL】"
date: "2023-04-16T01:09:53.722Z"
---
MySQL MHA信息的收集【Filebeat+logstash+MySQL】
=======================================

一.项目背景
------

随着集团MHA集群的日渐增长，MHA管理平台话越来越迫切。而MHA平台的建设第一步就是将这些成百上千套的MHA集群信息收集起来，便于查询和管理。

MHA主要信息如下:

（1）基础配置信息；

（2）运行状态信息；

（3）启动及FailOver的log信息。

集团目前数据库的管理平台是在Archery的基础上打造，所以，需要将此功能嵌入到既有平台上。通过Archery系统进行查询展示。

二.架构
----

![](https://img2023.cnblogs.com/blog/780228/202304/780228-20230415095747826-1661925485.png)

 简单来说，通过 Filebeat + Logstash + MySQL 架构 来收集保存各个集群的配置信息、启动及FailOver的log信息 和运行状态信息。

运行状态信息是通过一个小程序获取的，这个小程序每分钟执行一次，会把执行结果输出到文件中。当然这个文件是被failbeat监控的。

三.实现
----

### 3.1 获取MHA状态的脚本

文件为mha\_checkstatus.py

#!/usr/bin/python
# -\*- coding: UTF-8 -\*-

import os
import io
import re
import ConfigParser

Path\='/etc/mha'
#fout=open('输出文件名','w')
for Name in os.listdir(Path) :
  Pathname\= os.path.join(Path,Name)
 #\# print(Pathname)
 #\# print(Name)
  config =ConfigParser.ConfigParser()
  try:
    config.read(Pathname)
    server\_item \= config.sections()
    server1\_host \= ''  ##MHA cnf 配置文件中的节点1
    server2\_host = ''  ##MHA cnf 配置文件中的节点2
    server3\_host = ''  ##MHA cnf 配置文件中的节点3
    mha\_cnf\_remark = ''
    if 'server1' in server\_item:
      server1\_host \= config.get('server1','hostname')
    else:
       mha\_cnf\_remark \= mha\_cnf\_remark + 'Server1未配置;'
    if 'server2' in server\_item:
      server2\_host \= config.get('server2','hostname')
    else:
      mha\_cnf\_remark \= mha\_cnf\_remark + 'Server2未配置;'
    if 'server3' in server\_item:
      server3\_host \= config.get('server3','hostname')

      ##print(mha\_cnf\_remark)
  except Exception as e:
    print(e)

  mha\_status\_result \=''
  ###20190330
  Name = Name.replace(".cnf", "")

  ###集群一主一从
  if server1\_host <> '' and server2\_host <> '' and server3\_host == '':
    cmd\_mha\_status \='/???/???/bin/masterha\_check\_status --conf='+Pathname
    with os.popen(cmd\_mha\_status) as mha\_status:
      mha\_status\_result \= mha\_status.read()
      if 'running(0:PING\_OK)' in mha\_status\_result:
        print(Name+':::'+Pathname+':::0:::'+server1\_host+':::'+mha\_status\_result)
        print(Name+':::'+Pathname+':::0:::'+server2\_host+':::'+mha\_status\_result)
      if 'stopped(2:NOT\_RUNNING)' in mha\_status\_result:
        print(Name+':::'+Pathname+':::None:::'+server1\_host+':::'+mha\_status\_result)
        print(Name+':::'+Pathname+':::None:::'+server2\_host+':::'+mha\_status\_result)

  ####集群一主两从
  if server1\_host <> '' and server2\_host <> '' and server3\_host <> '':
    cmd\_mha\_status \='/???/???/bin/masterha\_check\_status --conf='+Pathname
    with os.popen(cmd\_mha\_status) as mha\_status:
      mha\_status\_result \= mha\_status.read()
      if 'running(0:PING\_OK)' in mha\_status\_result:
        print(Name+':::'+Pathname+':::0:::'+server1\_host+':::'+mha\_status\_result)
        print(Name+':::'+Pathname+':::0:::'+server2\_host+':::'+mha\_status\_result)
        print(Name+':::'+Pathname+':::0:::'+server3\_host+':::'+mha\_status\_result)
      if 'stopped(2:NOT\_RUNNING)' in mha\_status\_result:
        print(Name+':::'+Pathname+':::None:::'+server1\_host+':::'+mha\_status\_result)
        print(Name+':::'+Pathname+':::None:::'+server2\_host+':::'+mha\_status\_result)
        print(Name+':::'+Pathname+':::None:::'+server3\_host+':::'+mha\_status\_result)

 概况说明，就是到存放MHA配置的文件夹，根据每个集群的配置文档，去逐一执行下masterha\_check\_status，把结果格式化，输出到指定的文件中。这个就是每个集群的状态数据。通过filebeat实时汇报上去。

触发的方式可以是crontab，每分钟执行一次。再本案中是输出到 /???/checkmhastatus/masterha\_check\_status.log 中。

形式类似如下：

\*/1 \* \* \* \* python /???/????/mha\_checkstatus.py >>   /???/????/masterha\_check\_status.log

### 3.2 表的设计及脚本

#### 3.2.1 运行状态表 dbmha\_status

CREATE TABLE \`dbmha\_status\` (
  \`id\` int NOT NULL AUTO\_INCREMENT,
  \`host\` varchar(100) NOT NULL,
  \`clustername\` varchar(200) NOT NULL,
  \`logpath\` varchar(500) NOT NULL,
  \`confpath\` varchar(500) NOT NULL,
  \`mhstatus\` varchar(100) NOT NULL,
  \`serverip\` varchar(100) NOT NULL,
  \`info\` varchar(2000) NOT NULL,
  \`create\_time\` datetime DEFAULT CURRENT\_TIMESTAMP COMMENT '收集时间',
  PRIMARY KEY (\`id\`),
  KEY \`idx\_createtime\` (\`create\_time\`)
) ENGINE\=InnoDB AUTO\_INCREMENT\=1 DEFAULT CHARSET\=utf8mb4 COLLATE\=utf8mb4\_0900\_ai\_ci;

#### 3.2.2 mha log 信息表 dbmha\_log

CREATE TABLE \`dbmha\_log\` (
  \`id\` int NOT NULL AUTO\_INCREMENT,
  \`host\` varchar(100) NOT NULL,
  \`clustername\` varchar(200) NOT NULL,
  \`filename\` varchar(200) NOT NULL,
  \`logpath\` varchar(500) NOT NULL,
  \`message\` longtext NOT NULL,
  \`create\_time\` datetime DEFAULT CURRENT\_TIMESTAMP COMMENT '收集时间',
  PRIMARY KEY (\`id\`)
) ENGINE\=InnoDB AUTO\_INCREMENT\=1 DEFAULT CHARSET\=utf8mb4 COLLATE\=utf8mb4\_0900\_ai\_ci;

#### 3.2.3 MHA 基础配置表 dbmha\_conf\_info

CREATE TABLE \`dbmha\_conf\_info\` (
  \`id\` int NOT NULL AUTO\_INCREMENT,
  \`host\` varchar(100) NOT NULL,
  \`clustername\` varchar(200) NOT NULL DEFAULT '',
  \`confpath\` varchar(500) NOT NULL DEFAULT '',
  \`manager\_log\` varchar(500) NOT NULL DEFAULT '',
  \`manager\_workdir\` varchar(500) NOT NULL DEFAULT '',
  \`master\_binlog\_dir\` varchar(500) NOT NULL DEFAULT '',
  \`failover\_script\` varchar(500) NOT NULL DEFAULT '',
  \`online\_change\_script\` varchar(500) NOT NULL DEFAULT '',
  \`password\` varchar(128) NOT NULL DEFAULT '',
  \`ping\_interval\` varchar(100) NOT NULL DEFAULT '',
  \`remote\_workdir\` varchar(100) NOT NULL DEFAULT '',
  \`repl\_password\` varchar(128) NOT NULL DEFAULT '',
  \`repl\_user\` varchar(20) NOT NULL DEFAULT '',
  \`ssh\_user\` varchar(20) NOT NULL DEFAULT '',
  \`user\` varchar(20) NOT NULL DEFAULT '',
  \`serverip1\` varchar(100) NOT NULL DEFAULT '',
  \`port1\` varchar(10) NOT NULL DEFAULT '',
  \`candidate\_master1\` varchar(5) NOT NULL DEFAULT '',
  \`check\_repl\_delay1\` varchar(20) NOT NULL DEFAULT '',
  \`serverip2\` varchar(100) NOT NULL DEFAULT '',
  \`port2\` varchar(10) NOT NULL DEFAULT '',
  \`candidate\_master2\` varchar(5) NOT NULL DEFAULT '',
  \`check\_repl\_delay2\` varchar(20) NOT NULL DEFAULT '',
  \`serverip3\` varchar(100) NOT NULL DEFAULT '',
  \`port3\` varchar(10) NOT NULL DEFAULT '',
  \`candidate\_master3\` varchar(5) NOT NULL DEFAULT '',
  \`check\_repl\_delay3\` varchar(20) NOT NULL DEFAULT '',
  \`info\` longtext NOT NULL,
  \`create\_time\` datetime DEFAULT CURRENT\_TIMESTAMP COMMENT '收集时间',
  PRIMARY KEY (\`id\`),
  KEY \`idx\_createtime\` (\`create\_time\`)
) ENGINE\=InnoDB AUTO\_INCREMENT\=1 DEFAULT CHARSET\=utf8mb4 COLLATE\=utf8mb4\_0900\_ai\_ci;

### 3.3 filbeat 中关于读取文件的配置

..............
\- type: log
  paths:
    \- /???/????/masterha\_check\_status.log
  fields:
    log\_type: mha\-status
    db\_host: 111.111.XXX.1XX    ###这个IP为mha Mnaager所在serverip

\- type: log
  paths:
    \- /???/mhaconf/\*.cnf
  fields:
    log\_type: mha-cnf
    db\_host: 111.111.XXX.XXX
  multiline.type: pattern
  multiline.pattern: '^\\\[server \[\[:space:\]\] default'
  multiline.negate: true
  multiline.match: after


- type: log
  paths:
    - /???/????/mha/\*/\*.log
  fields:
    log\_type: mysql\-mha
    db\_host: 111.111.XXX.XXX
................

### 3.4 Logstash 的配置文件

\# Sample Logstash configuration for creating a simple
# Beats \-> Logstash -> Elasticsearch pipeline.

input {
  beats {
    port \=> 5044
  }
}

filter {

    if \[fields\]\[log\_type\] == "mysql-mha" {
        grok {
            match \=> \["message", "(?m)^%{TIMESTAMP\_ISO8601:timestamp} %{BASE10NUM} \\\[%{WORD:error\_level}\\\] %{GREEDYDATA:error\_msg}$"\]
        }
        grok {
            match \=> { "\[log\]\[file\]\[path\]" => ".\*(\\\\|\\/).\*(\\\\|\\/)(?<product>.\*)(\\\\|\\/).\*"}
        }
        grok {
            match \=> { "\[log\]\[file\]\[path\]" => ".\*(\\\\|\\/).\*(\\\\|\\/).\*(\\\\|\\/)(?<filename>.\*)"}
        }
        date {
            match\=> \["timestamp", "ISO8601"\]
            remove\_field \=> \["timestamp"\]
        }
        mutate {
        copy \=> { "\[log\]\[file\]\[path\]" => "logpath"
                 "\[fields\]\[db\_host\]" => "manager\_ip" }
        }
        mutate {
            remove\_field \=> \["@version", "beat", "input", "offset", "prospector", "source", "tags"\]
        }
    }


    if \[fields\]\[log\_type\] == "mha-cnf" {
        mutate {
        split => \["message","server"\]
        add\_field \=> {"message1" => "%{\[message\]\[1\]}"}
        add\_field \=> {"messages1" => "%{\[message\]\[2\]}"}
        add\_field \=> {"messages2" => "%{\[message\]\[3\]}"}
        add\_field \=> {"messages3" => "%{\[message\]\[4\]}"}
        add\_field \=> {"dft\_password" => "\*\*\*\*\*\*\*\*\*"}
        add\_field \=> {"dft\_repl\_password" => "\*\*\*\*\*\*\*\*\*"}
        }
        kv {
             source \=> "message1" 
             field\_split \=> "\\n"
             include\_keys \=> \["manager\_log", "manager\_workdir", "master\_binlog\_dir", "master\_ip\_failover\_script", "master\_ip\_online\_change\_script", "ping\_interval", "remote\_workdir", "repl\_user", "ssh\_user", "user" \]
             prefix \=> "dft\_"
             remove\_char\_value \=> "<>\\\[\\\]," 
        }
        kv {
             source \=> "messages1"
             field\_split \=> "\\n"
             include\_keys \=> \["candidate\_master", "check\_repl\_delay", "hostname", "port" \]
             prefix \=> "s1\_"
        }
        kv {
             source \=> "messages2"
             field\_split \=> "\\n"
             default\_keys \=> \[ "s2\_candidate\_master", "",
                         "s2\_check\_repl\_delay", "",
                         "s2\_hostname","",
                          "s2\_port",""
                          \]
             include\_keys \=> \["candidate\_master", "check\_repl\_delay", "hostname", "port" \]
             prefix \=> "s2\_"
        }
        kv {
             source \=> "messages3"
             field\_split \=> "\\n"
             default\_keys \=> \[ "s3\_candidate\_master", "",
                         "s3\_check\_repl\_delay", "",
                         "s3\_hostname","",
                          "s3\_port","" 
                          \]
             include\_keys \=> \["candidate\_master", "check\_repl\_delay", "hostname", "port" \]
             prefix \=> "s3\_"
        }
        grok {
            match \=> { "\[log\]\[file\]\[path\]" => ".\*(\\\\|\\/).\*(\\\\|\\/)(?<product>.\*)(\\\\|\\/).\*"}
            match \=> { "\[log\]\[file\]\[path\]" => ".\*(\\\\|\\/).\*(\\\\|\\/).\*(\\\\|\\/)(?<filename>.\*)"}
        }
        mutate {
             copy \=> { "\[fields\]\[db\_host\]" => "manager\_ip" }
             copy \=> { "\[log\]\[file\]\[path\]" => "conf\_path" }
             gsub \=> \[
                      "message", "需要加密的\*\*\*密\*\*\*码", "\*\*\*\*\*\*\*\*\*",
                      "message", "需要加密的其他字符", "\*\*\*\*\*\*\*\*\*"
                      \]
        }
        date {
            match\=> \["timestamp", "ISO8601"\]
            remove\_field \=> \["timestamp"\]
        }
        mutate {
            remove\_field \=> \["@version", "beat", "input", "offset", "prospector", "source", "tags"\]
        }
    }

    if \[fields\]\[log\_type\] == "mha-status" {
       mutate {
        split => \["message",":::"\]
        add\_field \=> {"cluster\_name" => "%{\[message\]\[0\]}"}
        add\_field \=> {"conf\_path" => "%{\[message\]\[1\]}"}
        add\_field \=> {"masterha\_check\_status" => "%{\[message\]\[2\]}"}
        add\_field \=> {"server" => "%{\[message\]\[3\]}"}
        add\_field \=> {"info" => "%{\[message\]\[4\]}"}
         }

        grok {
            match \=> { "\[log\]\[file\]\[path\]" => ".\*(\\\\|\\/).\*(\\\\|\\/).\*(\\\\|\\/)(?<filename>.\*)"}
        }
        mutate {
             copy \=> { "\[fields\]\[db\_host\]" => "manager\_ip" }
        }
        date {
            match\=> \["timestamp", "ISO8601"\]
            remove\_field \=> \["timestamp"\]
        }
        mutate {
            remove\_field \=> \["@version", "beat", "input", "offset", "prospector", "source", "tags"\]
        }
    }

}


output {
    if \[fields\]\[log\_type\] == "mysql-mha" {
      jdbc {
           driver\_jar\_path \=> "/???/???/logstash-7.6.0/vendor/jar/jdbc/mysql-connector-java-5.1.47.jar"
           driver\_class \=> "com.mysql.jdbc.Driver"
           connection\_string \=> "jdbc:mysql://120.120.XXX.XXX:3306/archery?user=ts23\_dbhacluster&password=???????"
           statement \=> \["INSERT INTO dbmha\_log (host,clustername,filename,logpath, message) VALUES(?, ?, ?, ?, ?)","%{manager\_ip}","%{product}","%{filename}","%{logpath}","%{message}"\]
       }
    }

    if \[fields\]\[log\_type\] == "mha-status" {
      jdbc {
           driver\_jar\_path \=> "/???/???/logstash-7.6.0/vendor/jar/jdbc/mysql-connector-java-5.1.47.jar"
           driver\_class \=> "com.mysql.jdbc.Driver"
           connection\_string \=> "jdbc:mysql://120.120.XXX.XXX:3306/archery?user=ts23\_dbhacluster&password=???????"
           statement \=> \["INSERT INTO dbmha\_status (host,clustername,logpath,confpath,mhstatus,serverip,info) VALUES(?, ?, ?, ?, ?, ?, ?)","%{manager\_ip}","%{cluster\_name}","%{filename}","%{conf\_path}","%{masterha\_check\_status}","%{server}","%{info}"\]
       }
   }
    if \[fields\]\[log\_type\] == "mha-cnf" {
      jdbc {
           driver\_jar\_path \=> "/???/???/logstash-7.6.0/vendor/jar/jdbc/mysql-connector-java-5.1.47.jar"
           driver\_class \=> "com.mysql.jdbc.Driver"
           connection\_string \=> "jdbc:mysql://120.120.XXX.XXX:3306/archery?user=ts23\_dbhacluster&password=???????"
           statement \=> \["INSERT INTO dbmha\_conf\_info (host,clustername,confpath,manager\_log,manager\_workdir,master\_binlog\_dir,failover\_script,online\_change\_script,password,ping\_interval,remote\_workdir,repl\_password,repl\_user,ssh\_user,user,serverip1,port1,candidate\_master1,check\_repl\_delay1,serverip2,port2,candidate\_master2,check\_repl\_delay2,serverip3,port3,candidate\_master3,check\_repl\_delay3,info) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)","%{manager\_ip}","%{product}","%{conf\_path}","%{dft\_manager\_log}","%{dft\_manager\_workdir}","%{dft\_master\_binlog\_dir}","%{dft\_master\_ip\_failover\_script}","%{dft\_master\_ip\_online\_change\_script}","%{dft\_password}","%{dft\_ping\_interval}","%{dft\_remote\_workdir}","%{dft\_repl\_password}","%{dft\_repl\_user}","%{dft\_ssh\_user}","%{dft\_user}","%{s1\_hostname}","%{s1\_port}","%{s1\_candidate\_master}","%{s1\_check\_repl\_delay}","%{s2\_hostname}","%{s2\_port}","%{s2\_candidate\_master}","%{s2\_check\_repl\_delay}","%{s3\_hostname}","%{s3\_port}","%{s3\_candidate\_master}","%{s3\_check\_repl\_delay}","%{message}"\]
       }
   }

}

 这个配置还是相对复杂难懂的。这个文件配置了对三种文件的读取，我们就看读取mha配置文件的部分【\[fields\]\[log\_type\] == "mha-cnf"】，我们挑其中的几个点说下，更多的内容可参照logstash官网--[https://www.elastic.co/guide/en/logstash/current/filter-plugins.html](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html)

首先，我们是 “server” 关键字，把文件中的配置信息，分割成不同的部分。

接着，因为配置文件的格式是 key=value的样式，所以需要借助 kv{}，其中的参数说下：field\_split---定义字段间的分隔符；include\_keys--定义只读去规定的特定key；prefix---格式化字段名字，加个前缀名字，主要是用来区分server 1 部分和 server2、、、之间的分别。

 通过【match => { "\[log\]\[file\]\[path\]" => ".\*(\\\\|\\/).\*(\\\\|\\/)(?<product>.\*)(\\\\|\\/).\*"}】，获取product字段，我们是通过mha的配置文件的名字来定义集群的名字，即规范了mha配置文件的名字的命名来自于集群的名字，反推得知了配置文件的名字，就知道了集群的名字。【match => { "\[log\]\[file\]\[path\]" => ".\*(\\\\|\\/).\*(\\\\|\\/).\*(\\\\|\\/)(?<filename>.\*)"}】这个地方的filename包含了文件的后缀。

四.平台前端
------

我们是把此项目嵌入到既有的Archery平台中，增加了3个查询界面，界面的实现，在此就不具体展开了。

需要注意的是，界面需要支持模糊查询，例如支持MHA Manager Server IP查询（方便查询各个Manager节点上有多少集群）；支持集群名字的模糊查询；支持节点serverIP的模糊查询。

五.补充说明
------

**Q.1 为什么用MySQL存储信息，ELK是更成熟的架构啊？**

是的，用elasticsearch来存储这种文本信息更常见。我们用MySQL替代elasticsearch基于以下考虑：（１）我们既有的管理平台使用的是MySQL，把他们保存到MySQL　便于集成；（２）这些数据，不仅仅是Log，还有些是基础数据，放到MySQL便于相互管理、聚合展示（３）这是数据量并不大，例如mha.log,只有在启动或者failover时才有变化，conf信息也是很少的，所以，从数据量也一点考虑，也不需要保存到MySQL。

**Q.2 Logstash 可以把数据写入到MySql中吗？**

是可以的。主要是logstash-output-jdbc、logstash-codec-plain插件的安装。

如果是离线的环境下安装，可以参考 《logstash 离线安装logstash-output-jdbc》

[https://blog.csdn.net/sxw1065430201/article/details/123663108](https://blog.csdn.net/sxw1065430201/article/details/123663108)

**Q.3 MHA log 文件夹中 原有一个 .health ，里面是MHA每分钟的健康性报告，那为什么还要自己写Python程序获取呢？**

因为.healthy 的内容不是换行符结尾，而filebeat是以换行符来判断的（https://www.elastic.co/guide/en/beats/filebeat/7.4/newline-character-required-eof.html 有详细说明）。

简单来说，filebeat读取不了。

**Q.4 MHA 健康性检查的原理**

具体的原理可以参考此文章的分析说明--《mha检测mysql状况方式》

[https://blog.csdn.net/weixin\_35411438/article/details/113455263](https://blog.csdn.net/weixin_35411438/article/details/113455263)

**Q.5 历史数据的删除**

mha log 信息表 dbmha\_log

MHA 基础配置表 dbmha\_conf\_info

以上两张表基本上很少变化，量不大，其数据无需定期删除。

运行状态表 dbmha\_status，此表每个集群每分钟（具体crontab定义）都会有新数据插入，数据量增长较大，应设置定时任务，定期删除历史数据，例如删除7天前的数据。