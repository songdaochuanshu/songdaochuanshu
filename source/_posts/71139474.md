---
layout: post
title: "YARN线上动态资源调优"
date: "2022-04-21T17:26:37.783Z"
---
YARN线上动态资源调优
============

![YARN线上动态资源调优](https://img2022.cnblogs.com/blog/1047345/202204/1047345-20220421221729982-304356901.png) 线上Hadoop集群资源严重不足，可能存在添加磁盘，添加CPU，添加节点的操作，那么在添加这些硬件资源之后，我们的集群是不能立马就利用上这些资源的，需要修改集群Yarn资源配置，然后使其生效。

背景
==

线上Hadoop集群资源严重不足，可能存在添加磁盘，添加CPU，添加节点的操作，那么在添加这些硬件资源之后，我们的集群是不能立马就利用上这些资源的，需要修改集群Yarn资源配置，然后使其生效。

现有环境
====

服务器：12台，内存64Gx12=768G，物理cpu16x12=192，磁盘12Tx12=144T

组件：Hadoop-2.7.7，Hive-2.3.4，Presto-220，Dolphinscheduler-1.3.6，Sqoop-1.4.7

**分配策略**

由于我们的版本是Hadoop-2.7.7，有些默认配置是固定一个值，好比可用内存8G，可用CPU核数8核，如果调优就需要我们配置的东西比较多。

官方Yarn参数配置：[https://hadoop.apache.org/docs/r2.7.7/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](https://hadoop.apache.org/docs/r2.7.7/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)

未来的版本(Hadoop-3.0+)其实是有自动检测硬件资源的机制，需要开启配置：`yarn.nodemanager.resource.detect-hardware-capabilities`，然后会自动计算资源配置，不过这个是默认关闭的，每个节点的NodeManager可用内存配置 `yarn.nodemanager.resource.memory-mb` 和CPU核数 `yarn.nodemanager.resource.cpu-vcores` 也是受此配置影响， 默认配置都是-1，则可用内存为8G，CPU核数为8核。如果开启了自动监测硬件资源，其他配置则可以忽略不用配置，简化了配置。

官方Yarn参数配置：[https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)

还有一个重要的配置 `yarn.nodemanager.vmem-pmem-ratio` ，表示NodeManager上的Container物理内存不足时，可用使用虚拟内存，默认为物理内存的2.1倍。

修改配置
====

yarn-site.xml
-------------

修改yarn-site.xml，有则修改，没有则添加，这里我们设置单个节点可用内存30G，可用CPU核数为16

    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>30720</value>
        <discription>每个节点可用内存,默认8192M(8G),这里设置30G</discription>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
        <discription>单个任务可申请最少内存，默认1024MB</discription>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>20480</value>
        <discription>单个任务可申请最大内存，默认8192M(8G),这里设置20G</discription>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>2048</value>
        <discription>默认为1536。MR运行于YARN上时，为AM分配多少内存。默认值通常来说过小，建议设置为2048或4096等较大的值。</discription>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>16</value>
        <discription>默认为8。每个节点可分配多少虚拟核给YARN使用，通常设为该节点定义的总虚拟核数即可。</discription>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>32</value>
        <discription>分别为1/32，指定RM可以为每个container分配的最小/最大虚拟核数，低 于或高于该限制的核申请，会按最小或最大核数来进行分配。默认值适合 一般集群使用。</discription>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value>
        <discription>分别为1/32，指定RM可以为每个container分配的最小/最大虚拟核数，低 于或高于该限制的核申请，会按最小或最大核数来进行分配。默认值适合 一般集>群使用。</discription>
    </property>
    <property>
      <name>yarn.nodemanager.vcores-pcores-ratio</name>
      <value>2</value>
      <discription>每使用一个物理cpu，可以使用的虚拟cpu的比例，默认为2</discription>
    </property>
    ​
    <property>
      <name>yarn.nodemanager.vmem-pmem-ratio</name>
      <value>5.2</value>
      <discription>物理内存不足时,使用的虚拟内存，默认是2.1，表示每使用1MB的物理内存，最多可以使用2.1MB的虚拟内存总量。</discription>
    </property>

mapred-site.xml
---------------

修改mapred-site.xml，有则修改，没有则添加

给单个任务设置内存资源，注意这里的值不能大于上面对应的内存 `yarn.scheduler.maximum-allocation-mb`

    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>2048</value>
        <discription>默认均为1024，调度器为每个map/reduce task申请的内存数。各Job也可以单独指定。</discription>
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>2048</value>
        <discription>默认均为1024，调度器为每个map/reduce task申请的内存数。各Job也可以单独指定。</discription>
    </property>

集群生效
====

分发
--

修改完配置后，一定要将配置分发到其他集群节点，使用 scp 或者 xsync 工具分发到其他节点，这里举例分发到一个节点

`cd /data/soft/hadoop/hadoop-2.7.7/etc/hadoop`

`` scp -r yarn-site.xml mapred-site.xml data002:`pwd` ``

动态重启
----

为了不影响集群使用，我们单独启停Yarn，Yarn的服务主要有两个NodeManager和ResourceManager，单独启停命令

`yarn-daemon.sh stop nodemanager` `yarn-daemon.sh start nademanager`

`yarn-daemon.sh stop resourcemanager` `yarn-daemon.sh start resourcemanager`

根据集群节点逐个执行以上命令，ResourceManager对应的节点执行对应的命令。这样我们就保证了集群资源的动态调优。

当然，如果单个重启麻烦，也可以执行Yarn的重启命令

`stop-yarn.sh` `start-yarn.sh`

ResourceManager对应的从节点单独启停即可。

> 更多请在公号平台搜索：选手一号位，本文编号：2004，回复即可获取。