---
layout: post
title: "贝叶斯与卡尔曼滤波(1)--三大概率"
date: "2023-02-22T01:12:28.327Z"
---
贝叶斯与卡尔曼滤波(1)--三大概率
==================

贝叶斯与卡尔曼滤波(1)--三大概率
==================

贝叶斯滤波主要是通过概率统计的方法，主要是贝叶斯公式，对随机信号进行处理，减小不确定度

贝叶斯滤波处理的随机变量主要是一个随机过程。\\(x\_1, x\_2, x\_3 ...\\),互不独立

与之对应的就是一个确定过程，比如：自由落体\\(v = g\*t\\)，就是一个确定的过程

我们之前所学的大部分都是一些要求相对独立的数学，比如大数定律，中心极限定理，数理统计三大分布都需要独立同分布。

随机过程的难度相比于确定过程要高很多，最大的不同在于随机过程无法做随机试验了。

那么问题来了，随机试验是干什么的？随机试验最大的作用是为了给概率赋值的，比如抛硬币。为啥那么抛硬币正反的概率都是0.5呢？这就涉及到两种学说，主观概率学说以及大数定律学说（随机试验为基础）。

随机试验的条件：

*   在相同条件下，实验可以重复进行 （这其实就是随机实验之间的独立性）
*   一次实验，结果不确定，所有可能的结果已知
*   实验之前，实验结果预先未知

在抛硬币这个实验中，实验可以多次重复进行，由大数定律，设\\(n\\)为试验次数，\\(\\mu\\)为正面朝上的次数

那么根据大数定律，在\\(n\\)次独立的实验中，对于任意正数\\(\\varepsilon\\)，有

\\\[\\lim\_{n \\to \\infty} P(|\\frac{\\mu}{n} - P\_1| < \\varepsilon) = 1 \\\]

当\\(n \\to \\infty\\)时， \\(\\frac{\\mu}{n}\\)依概率收敛于\\(P\_1\\).

经过大量的实验测试，这个概率在0.5上下波动，因此就定义为0.5

那么问题来了，对于一个随机过程来说，\\(x\_1, x\_2, x\_3 ...\\)互不独立，那么如何给这个概率赋值呢？

举个例子，股票。相对股票做随机试验，那么必须会时光倒流，这显示是不可能的。除了股票，像分子的扩散，气温的变化都是无法做随机试验的。一般来说与时间有关的东西，都是无法做随机试验的。

随机过程，\\(x\_1, x\_2, x\_3 ...\\)不独立，那么可以有以下推断

\\\[x\_k = f(x\_{k-1}) \\\]

\\\[P(x\_k) = f(P(x\_{k-1})) \\\]

这就体现了不独立性。那么有了这个信息，我们是否可以研究随机过程呢？答案也是不可以的，因为你只找到了他们的关系，但是必须要给随机过程的起点\\(P(x\_1)\\)赋予初值，初值的选取是很重要的。

但是上面说过由于不独立性，我们无法通过大数定律赋予\\(P(x\_1)\\)初值。

实际上，有的初值是可以做随机试验的额，比如随机游走\\(x\_k = x\_{k-1} +D\\),$D $为位移

\\\[P(D=1) = 0.5 \\\]

\\\[P(D=-1) = 0.5 \\\]

初值\\(P(x\_0 = 0) = 1\\)。

但是更多情况下，初值是不可以做随机试验的，只能使用主观概率，也就是猜一个概率出来。

以上面的例子来看，抛硬币正面朝上的概率0.5这个事情来看，两种说法，主观概率与大数定律学说都存在不严谨的地方。主观概率就不说了，肯定是不严谨了，但是大数定律看似严谨，实际上独立性这个属性是无法保证，同时也是无法证明这个独立性的。一般来说判断独立性都是通过经验的，因此大数定律也是存在一定的主观性的。有人会说，证明独立性只需要说明\\(P(A)=P(B)\\)就可以了，但是要证明这个等式，必须要对两个概率赋值，而要对概率赋值，必须使用大数定律，这就成了一个鸡生蛋还是蛋生鸡的问题。因此在无法做随机试验的情况下，使用主观概率也是比较科学的做法

这就是概率论的两大学派，支持主观概率的也叫贝叶斯学派，支持大数定律的也叫频率学派，目前以频率学派占主导地位。

回到主观概率上，随机过程\\(x\_1, x\_2, x\_3 ...\\)互不独立，那么\\(P(x\_1)\\)该如何给呢？对于一些比较简单的随机过程，比如抛硬币，我们可以给一个0.5，但是对于一些比较复杂的过程，比如股票，每个人看法不一， 导致主观概率的选取不通用，那么不同的主观概率会导致不同的结果，这显示不是我们想要的。气温的变化，分子的扩散，本质上还是一个客观的过程，我们希望尽可能削弱主观的差异，那么应该怎么做呢，我们主要说贝叶斯滤波的方法。

我们需要引入外部观测，比如对于股票来说，每个人对涨跌的看法都是不一样，但是如果加上一个外部观测，比如得到消息，某公司老板卷钱跑路了，那么几乎所有人都会下调对该股票的收益预期。

引入外部观测，可以尽可能地减弱主观概率的影响

flowchart LR A\[主观概率\]-->B\[外部观测\] B-->C\[相对客观的概率: 后验概率\]

> 主观概率也叫做先验概率，主观概率和先验概率是存在一定区别的，但是我们可以把两者当作是一个东西，目前涉及的知识面，可以忽略两者的区别。

先验概率通过贝叶斯公式转化为后验概率。

先说一下符号

\\(X, Y\\),大写为随机变量，\\(x, y\\)，小写为随机变量的取值，代表随机试验的一个可能的结果

离散变量：\\(P(X=x) = P\_x\\), 例如：

\\\[P(X=k) = e^{-\\lambda}\\frac{\\lambda^{k}}{k!} \\\]

连续变量：

\\\[P(X < x) = \\int\_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^2}{2}}dt \\\]

条件概率：

*   离散

\\\[P(X=x|Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)} \\\]

*   连续

\\\[P(X=x|Y=y) =\\int\_{-\\infty}^{x} \\frac{f(x, y)}{f(y)}dy \\\]

下面以一个温度例子来学习贝叶斯滤波

首先，给出先验概率分布：此处以一个离散变量表示，如果是连续变量，那么需要给出概率密度函数。

\\\[\\begin{cases} P(T=10)=0.8\\\\ P(T=11)=0.2 \\end{cases} \\\]

其次，给出温度计的测量温度\\(T\_m\\)(m：measure,测量的意思)。问题来了，既然有了温度计的值了，还要贝叶斯干什么，还整这么复杂干什么？问题在于，任何传感器都是有误差的。温度计测量到的温度，不一定是准确的。假设$T\_m = 10.3 $

最后，使用贝叶斯公式，求得后验概率分布

\\\[P(T=10|T\_m=10.3)=\\frac{P(T\_m=10.3|T=10)P(T=10)}{P(T\_m=10.3)} \\\]

\\\[P(T=11|T\_m=10.3)=\\frac{P(T\_m=10.3|T=11)P(T=11)}{P(T\_m=10.3)} \\\]

其中：

*   \\(P(T=10|T\_m=10.3)\\)就是后验概率
*   \\(P(T\_m=10.3)\\)就是先验概率
*   \\(P(T\_m=10.3|T=10)\\)就是似然概率

**似然概率**：代表观测的准确度

\\(P(T\_m=10.3|T=10)\\)当真实温度为10的时候，温度计测的温度为10.3的概率，代表传感器的精度。

问题来了，先验概率分布需要给出所有可能的分布，概率和必须为1。那么似然概率需不需要写成一个概率分布，概率和为1呢？答案是不需要的。\\(P(T\_m=10.3|T=10)\\)与\\(P(T\_m=10.3|T=11)\\)是对两个不同的真实值下的测量概率，可以说是两个随机试验，他们两个的概率没有任何关系。似然概率是用来衡量传感器的不确定性的，不确定性不受测量的真实值的影响的。比如传感器的精度是±1，那么测量一个冰水与沸水，传感器的误差都是±1，它是传感器本身的性质。

后验概率的概率和为1。

那么还有一个概率，\\(P(T\_m=10.3)\\)是什么呢？

很多教材里面，直接说\\(P(T\_m=10.3)\\)与T无关，所以\\(P(T=10|T\_m=10.3) = \\eta P(T\_m=10.3|T=10)P(T=10)\\)

那么，为什么\\(P(T\_m=10.3)\\)与T无关呢？很多人都会有一个困惑，\\(T\_m = 10.3\\)是一个已经发生的事件，所以\\(P(T\_m=10.3)=1\\)。这就是搞混了随机变量的取值与随机变量的概率，这两者是完全不同的概念。比如抛硬币，一次随机试验中发生了正面朝上，那么正面朝上的概率依然是0.5，本次结果为正面朝上并不影响正面朝上的概率。\\(T\_m=10.3\\)只是一次随机试验的结果而已，不能只看到一次结果，就把这个事件发生的概率定为1。随机试验的结果不影响分布律。

根据全概率公式：

\\\[P(T\_m=10.3)=P(T\_m=10.3|T=10)P(T=10)+P(T\_m=10.3|T=11)P(T=11) \\\]

可以看到，\\(P(T\_m=10.3)\\)与T有关的，那为什么很多教材上说\\(P(T\_m=10.3)\\)与T无关呢？**因为**\*\*\\(P(T\_m=10.3)\\)\*\***与T的取值无关，与T的分布律是有关的。**

在上面的公式中可以看到，\\(P(T\_m=10.3|T=10)\\)是似然概率，\\(P(T=10)\\)是先验概率。而似然概率是传感器本身的性质，因此在某种长度上，也可以说\*\*\\(P(T\_m=10.3)\\)\*\*与T的取值无关。

继续进行计算：

\\\[P(T=10|T\_m=10.3)=\\frac{P(T\_m=10.3|T=10)P(T=10)}{P(T\_m=10.3)}=\\eta P(T\_m=10.3|T=10)P(T=10) \\\]

\\\[P(T=11|T\_m=10.3)=\\frac{P(T\_m=10.3|T=11)P(T=11)}{P(T\_m=10.3)}=\\eta P(T\_m=10.3|T=11)P(T=11) \\\]

可以近似于：

\\\[后验概率=\\eta×似然概率×先验概率 \\\]

那么\\(\\eta\\)怎么计算呢？其实很简单，因为所有的后验概率相加为1，所以

\\\[\\sum 后验概率 = \\eta \\sum 似然概率 × 先验概率 \\\]

\\\[\\eta = \\frac{1}{ \\sum 似然概率 × 先验概率} \\\]

> 为什么叫似然概率呢？

似然：likelihood，可能性。源于最大似然估计。他表示那个原因最有可能导致了结果。

比如A班有99男1女，B班有1男99女。那么随机数抽取一个班，再随机抽一个人进行观测，结果是女，那么最有可能是从B班抽出来的。

\\\[P(状态|观测)=\\eta P(观测|状态)P(状态) \\\]

状态为因，观测为果。**后验概率为由果推因，似然概率是由因推果**。

> 如果两个随机变量存在一定的函数关系，他们是不是一定不独立?  
> 答：不一定。

> 等价命题：如果两个随机变量相互独立，他们是不是一定没有函数关系？  
> 答：不一定。

独立未必没有函数关系，虽然听起来匪夷所思，但这是事实。

举个例子，一个必然事件，\\(Y = X+1\\)，\\(P(X=1)=1\\),\\(P(Y=2)=1\\),\\(P(X=1, Y=2)=1\\),两者有函数关系，但是他们是独立的。

这个例子看起来没有太多说服力，那么说一个非必然事件的例子

设有一个正态概率分布\\(N(\\mu, \\sigma^2)\\)，\\((\\mu, \\sigma)\\)未知，从此分布中，抽取\\(n\\)个独立的样本，\\(X\_1, X\_2,X\_3,...,X\_n\\)独立同分布，则下面两个随机变量相互独立。

\\\[\\overline{X}=\\frac{X\_1+X\_2+...+X\_n}{n} \\\]

\\\[S^2 = \\frac{1}{n}\\sum\_{i=1}^{n}(X\_i - \\overline X)^2 \\\]

均值和方差相互独立只有再正态分布中才有。显然，他们两个是存在函数关系的。

关于样本均值与样本方差的独立性证明，可以参考这个[视频](https://www.bilibili.com/video/av63950352 "视频")